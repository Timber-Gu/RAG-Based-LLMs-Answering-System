# Multi-Model LangChain ML Q&A Assistant

A sophisticated multi-agent system for Machine Learning and Deep Learning Q&A, powered by different specialized LLMs and **Pinecone's hosted embeddings**:

- **Research Agent**: Ollama (Llama 3.1) - For literature reviews and research findings
- **Theory Agent**: GPT-4 - For mathematical explanations and theory
- **Implementation Agent**: Claude 3.5 Sonnet - For code generation and practical examples
- **Knowledge Base**: Pinecone with hosted `llama-text-embed-v2` - For semantic search

## üöÄ Quick Setup

### Prerequisites
- OpenAI API Key (for GPT-4)
- Anthropic API Key (for Claude) 
- **Pinecone API Key** (for hosted embeddings)
- Ollama (only for research agent)

### Installation

1. **Install Dependencies**
```bash
pip install -r requirements.txt
```

2. **Install and Setup Ollama** (for Research Agent only)
```bash
# Install Ollama from https://ollama.ai/
# Then pull the required model:
ollama pull llama3.1
```

3. **Configure API Keys**
Create a `.env` file:
```bash
# Required API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here

# Pinecone Configuration
VECTOR_STORE_TYPE=pinecone
PINECONE_INDEX_NAME=myproject
PINECONE_ENVIRONMENT=us-east-1
EMBEDDING_MODEL=llama-text-embed-v2  # Hosted by Pinecone

# Model Configuration
RESEARCH_MODEL=llama3.1
THEORY_MODEL=gpt-4
IMPLEMENTATION_MODEL=claude-3-5-sonnet-20241022
```

4. **Run the Application**
```bash
python main.py
```

## ü§ñ Agent Specializations

### Research Agent (Ollama - Llama 3.1)
- Literature reviews and paper summaries
- Recent research findings and trends
- Academic citations and references
- **Runs locally** - No API costs!

### Theory Agent (OpenAI GPT-4)
- Mathematical foundations and derivations
- Algorithm explanations
- Conceptual understanding
- Step-by-step breakdowns

### Implementation Agent (Claude 3.5 Sonnet)
- Code examples and implementations
- Best practices and optimizations
- Debugging and troubleshooting
- Framework-specific guidance

### Knowledge Base (Pinecone + Hosted Embeddings)
- **Hosted `llama-text-embed-v2` embeddings** - No local setup required!
- Semantic search across ML/DL papers and documentation
- Automatic embedding generation and indexing
- Fast, scalable vector search

## üõ†Ô∏è Configuration

The system automatically routes queries based on keywords:
- **"paper", "research", "study"** ‚Üí Research Agent
- **"explain", "theory", "mathematical"** ‚Üí Theory Agent  
- **"code", "implement", "example"** ‚Üí Implementation Agent

### Key Benefits of Pinecone Hosted Embeddings

‚úÖ **No Local Model Setup**: Embeddings are generated by Pinecone's hosted `llama-text-embed-v2` model  
‚úÖ **High Performance**: 1024-dimensional vectors with superior retrieval quality  
‚úÖ **Multilingual Support**: Supports 26 languages including English, Spanish, Chinese, Hindi  
‚úÖ **Fast Query Speed**: p99 latencies 12x faster than OpenAI Large  
‚úÖ **No Local Resources**: No need to download or run embedding models locally

## üîß API Usage

```python
from src.agents.langchain_agents import LangChainMLAgents

# Initialize agents with Pinecone hosted embeddings
agents = LangChainMLAgents()

# Process queries with semantic search
result = agents.process_query("Explain backpropagation algorithm")
print(f"Agent used: {result['agent_used']}")
print(f"Response: {result['response']}")

# Check system health
health = agents.health_check()
print(f"All models configured: {health['all_llms_configured']}")
```

## üåê Web Interface

Start the FastAPI server:
```bash
uvicorn src.api.langchain_server:app --reload
```

Then visit: http://localhost:8000/docs

## üìä Health Check

Test your setup:
```bash
curl http://localhost:8000/health
```

Expected response when properly configured:
```json
{
  "status": "healthy",
  "gpt4_connection": true,
  "ollama_connection": true, 
  "claude_connection": true,
  "pinecone_connection": true,
  "all_llms_configured": true
}
```

## üéØ Query Examples

- **Research**: "What are recent advances in transformer architectures?"
- **Theory**: "Explain the mathematical foundation of attention mechanisms"
- **Implementation**: "Show me PyTorch code for a transformer block"

## üõü Troubleshooting

### Pinecone Setup
- Get Pinecone API key: https://app.pinecone.io/
- Ensure `PINECONE_API_KEY` is set in `.env` file
- The system automatically creates indexes with hosted embeddings

### Ollama Issues (Research Agent Only)
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve

# Pull required model
ollama pull llama3.1
```

### API Key Issues
- Get OpenAI API key: https://platform.openai.com/api-keys
- Get Anthropic API key: https://console.anthropic.com/
- Get Pinecone API key: https://app.pinecone.io/
- Ensure all keys are properly set in `.env` file

## üìà Cost Optimization

- **Research queries**: Free (Ollama runs locally)
- **Theory queries**: Moderate cost (GPT-4)
- **Implementation queries**: Low-moderate cost (Claude)
- **Embeddings**: Low cost (Pinecone hosted embeddings)

The system is designed to balance performance with cost-effectiveness by using:
- Local Ollama for research (free)
- Pinecone hosted embeddings for semantic search (very cost-effective)
- Cloud LLMs only when needed for specialized tasks

## üÜï What's New

- ‚úÖ **Simplified Setup**: No need to install or configure local embedding models
- ‚úÖ **Better Performance**: Pinecone's `llama-text-embed-v2` provides superior retrieval quality
- ‚úÖ **Automatic Scaling**: Pinecone handles all embedding generation and indexing
- ‚úÖ **Multilingual Support**: Built-in support for 26 languages
- ‚úÖ **Cost Effective**: Pay only for what you use with Pinecone's hosted service 