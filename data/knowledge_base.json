[
  {
    "id": "llm_concept_35c1b18b9d",
    "title": "Transformer Architecture",
    "content": "Title: Transformer Architecture\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_e9304943d5",
    "title": "Self-Attention Mechanism",
    "content": "Title: Self-Attention Mechanism\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_17aaaf99d2",
    "title": "Multi-head Attention",
    "content": "Title: Multi-head Attention\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_6bd69dc0d7",
    "title": "Positional Encoding",
    "content": "Title: Positional Encoding\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_569bc1b5a5",
    "title": "Masked Language Modeling",
    "content": "Title: Masked Language Modeling\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_755a95618e",
    "title": "Next-Token Prediction",
    "content": "Title: Next-Token Prediction\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_dfa3ce771f",
    "title": "Transfer Learning",
    "content": "Title: Transfer Learning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_bf001e95fe",
    "title": "Fine-tuning",
    "content": "Title: Fine-tuning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_89561ac0e8",
    "title": "Zero-shot Learning",
    "content": "Title: Zero-shot Learning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_165306efad",
    "title": "Few-shot Learning",
    "content": "Title: Few-shot Learning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_bf44b86892",
    "title": "In-context Learning",
    "content": "Title: In-context Learning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_37abd4850a",
    "title": "Chain-of-Thought Prompting",
    "content": "Title: Chain-of-Thought Prompting\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_2ef7ad9248",
    "title": "Retrieval Augmented Generation",
    "content": "Title: Retrieval Augmented Generation\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_12e9e94d70",
    "title": "Reinforcement Learning from Human Feedback",
    "content": "Title: Reinforcement Learning from Human Feedback\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_1ca1eec2c3",
    "title": "Constitutional AI",
    "content": "Title: Constitutional AI\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_ca88d5201c",
    "title": "Direct Preference Optimization",
    "content": "Title: Direct Preference Optimization\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_0312d03745",
    "title": "Mixture of Experts",
    "content": "Title: Mixture of Experts\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_dd4a9a3cf1",
    "title": "Flash Attention",
    "content": "Title: Flash Attention\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_63a10e9b34",
    "title": "Key-Value Cache",
    "content": "Title: Key-Value Cache\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_bcf29674b7",
    "title": "Rotary Position Embedding",
    "content": "Title: Rotary Position Embedding\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_278062b566",
    "title": "Grouped Query Attention",
    "content": "Title: Grouped Query Attention\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_54e27ec31f",
    "title": "Sliding Window Attention",
    "content": "Title: Sliding Window Attention\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_451cbde027",
    "title": "Parameter-Efficient Fine-Tuning",
    "content": "Title: Parameter-Efficient Fine-Tuning\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_5a862b2755",
    "title": "Low-Rank Adaptation",
    "content": "Title: Low-Rank Adaptation\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_12be5bb221",
    "title": "Quantization",
    "content": "Title: Quantization\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_d60730b287",
    "title": "Knowledge Distillation",
    "content": "Title: Knowledge Distillation\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_c1c54b52e4",
    "title": "Sparse Attention",
    "content": "Title: Sparse Attention\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_50ee17dd48",
    "title": "Continuous Batching",
    "content": "Title: Continuous Batching\n\nThis entry represents an important concept in Large Language Models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_a04f066cba",
    "title": "GPT (Generative Pre-trained Transformer)",
    "content": "Title: GPT (Generative Pre-trained Transformer)\n\nA family of large language models developed by OpenAI. GPT models use the transformer architecture and are trained using a combination of unsupervised pre-training on large text corpora and supervised fine-tuning. GPT models generate text by predicting the next token given previous context.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_0c4765c2bc",
    "title": "LLaMA (Large Language Model Meta AI)",
    "content": "Title: LLaMA (Large Language Model Meta AI)\n\nA series of foundation language models developed by Meta AI, known for their efficiency and strong performance despite smaller parameter counts. LLaMA models have been widely used as open-weight models that serve as the foundation for many fine-tuned variants.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_892ea6abc9",
    "title": "Claude",
    "content": "Title: Claude\n\nA family of large language models developed by Anthropic, focused on helpful, harmless, and honest AI. Claude models are trained using Constitutional AI and RLHF techniques to align with human values and reduce harmful outputs.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_7fa3892806",
    "title": "RLHF (Reinforcement Learning from Human Feedback)",
    "content": "Title: RLHF (Reinforcement Learning from Human Feedback)\n\nA technique used to align language models with human preferences. RLHF involves collecting human feedback on model outputs, training a reward model based on this feedback, and then using reinforcement learning to optimize the language model against the reward model.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_5a5bbd88ec",
    "title": "RAG (Retrieval-Augmented Generation)",
    "content": "Title: RAG (Retrieval-Augmented Generation)\n\nA technique that enhances language model outputs by retrieving relevant documents from an external knowledge base. RAG combines the knowledge access of information retrieval systems with the fluent text generation capabilities of large language models.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_fafb8e0ca2",
    "title": "LoRA (Low-Rank Adaptation)",
    "content": "Title: LoRA (Low-Rank Adaptation)\n\nA parameter-efficient fine-tuning method that freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_2b68b87dc9",
    "title": "QLoRA (Quantized Low-Rank Adaptation)",
    "content": "Title: QLoRA (Quantized Low-Rank Adaptation)\n\nAn extension of LoRA that quantizes the pre-trained language model to 4-bit precision while maintaining high performance. QLoRA enables fine-tuning of models on consumer GPUs that would otherwise be too large to fit in memory.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "llm_concept_b759f3230e",
    "title": "PEFT (Parameter-Efficient Fine-Tuning)",
    "content": "Title: PEFT (Parameter-Efficient Fine-Tuning)\n\nA family of techniques that allow adaptation of pre-trained language models to specific tasks using only a small number of trainable parameters. PEFT methods include adapter layers, prefix tuning, prompt tuning, and LoRA.",
    "source": "LLM concept database",
    "authors": [],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "type": "concept"
  },
  {
    "id": "2506.24128v1_abstract",
    "title": "Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint",
    "content": "Title: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\n\nAbstract: We use [CII] observations of a large QSO sample to segregate sources by host\ngalaxy mass, aiming to identify those in the most massive hosts. [CII]\nluminosity, a known tracer of molecular gas, is taken as a proxy for host mass\nand used to rank 190 QSOs at z>5.9, spanning a 6-mag UV luminosity range\n(-22<Muv<-28). Particularly valuable are ALMA data from a cycle-10 CISTERN\nprogram, providing [CII] coverage for 46 UV-faint (M_{UV,AB}>-24.5) and 25\nespecially UV-faint (Muv>-23.5) QSOs, improving statistics by 5x and 6x,\nrespectively. Taking massive host galaxies to be those where L[CII]>1.8x10^9\nLsol (median L[CII] of UV-bright QSOs), we identify 61 QSOs, including 13 which\nare UV-faint and 7 especially UV-faint. Using these selections and recent QSO\nluminosity functions (LFs), we present the first characterization of UV\nluminosity distribution for QSOs in massive host galaxies and quantify [CII]\nLFs for both UV-bright and UV-faint QSOs. While ~3% of massive-host QSOs are\nUV-bright (Muv<-26), >~85% are UV-faint (Muv>-24.5). This wide dispersion in UV\nluminosities reflects variations in dust obscuration, accretion efficiency, and\nblack hole mass. Though spectroscopy is needed for definitive conclusions,\nblack hole mass appears to be the dominant factor driving variations in the UV\nluminosity, based on 34 [CII]-luminous (L[CII]>1.8x10^9 Lsol) QSOs distributed\nacross a ~3-mag baseline in UV luminosity and with measured MBH. At Muv~-23,\nthe median extrapolated log10 (MBH/Msol) is 8.1+/-0.4, consistent with the\nlocal relation. SMBHs in UV-bright QSOs thus appear to be ~15(-9)(+25)x more\nmassive than typical for massive host galaxies at z~6.",
    "source": "http://arxiv.org/abs/2506.24128v1",
    "authors": [
      "R. J. Bouwens",
      "E. Banados",
      "R. Decarli",
      "J. Hennawi",
      "D. Yang",
      "H. Algera",
      "M. Aravena",
      "E. Farina",
      "A. Gloudemans",
      "J. Hodge",
      "H. Inami",
      "J. Matthee",
      "R. Meyer",
      "R. P. Naidu",
      "P. Oesch",
      "H. J. A. Rottgering",
      "S. Schouws",
      "R. Smit",
      "M. Stefanon",
      "P. van der Werf",
      "B. Venemans",
      "F. Walter",
      "Y. Fudamoto"
    ],
    "categories": [
      "astro-ph.GA"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24128v1_content",
    "title": "Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint",
    "content": "arXiv:2506.24128v1  [astro-ph.GA]  30 Jun 2025Astronomy &Astrophysics manuscript no. faintqso ©ESO 2025\nJuly 1, 2025\nBeneath the Surface: >85% of z >5.9 QSOs in Massive Host\nGalaxies are UV-Faint\nRychard J. Bouwens1, Eduardo Bañados2, Roberto Decarli3, Joseph Hennawi1,4, Daming Yang1, Hiddo Algera5,6,7,\nManuel Aravena8, Emanuele Farina9, Anniek Gloudemans9, Jacqueline Hodge1, Hanae Inami6, Jorryt Matthee10,\nRomain Meyer11, Rohan P. Naidu12, Pascal Oesch11,13,14, Huub J.A. Rottgering1, Sander Schouws1, Renske Smit15,\nMauro Stefanon16,17, Paul van der Werf1, Bram Venemans1, Fabian Walter2, and Yoshinobu Fudamoto18,19\n1Leiden Observatory, Einsteinweg 55, NL-2333 CC Leiden, The Netherlands, e-mail: bouwens@strw.leidenuniv.nl\n2Max-Planck-Institut für Astronomie, Königstuhl 17, 69117 Heidelberg, Germany,\n3INAF–Osservatorio di Astrofisica e Scienza dello Spazio, via Gobetti 93 /3, I-40129, Bologna, Italy\n4Department of Physics, Broida Hall, University of California, Santa Barbara, Santa Barbara, CA 93106-9530, USA\n5Institute of Astronomy and Astrophysics, Academia Sinica, 11F of Astronomy-Mathematics Building, No.1, Sec. 4, Roosevelt Rd,\nTaipei 106216, Taiwan, R.O.C.\n6Hiroshima Astrophysical Science Center, Hiroshima University, 1-3-1 Kagamiyama, Higashi-Hiroshima, Hiroshima 739-8526,\nJapan\n7National Astronomical Observatory of Japan, 2-21-1, Osawa, Mitaka, Tokyo, Japan\n8Instituto de Estudios Astrofísicos, Facultad de Ingeniería y Ciencias, Universidad Diego Portales, Av. Ejército 441, Santiago, Chile\n9International Gemini Observatory /NSF NOIRLab, 670 N A’ohoku Place Hilo, HI 96720, USA\n10Institute of Science and Technology Austria (ISTA), Am Campus 1, 3400 Klosterneuburg, Austria\n11Department of Astronomy, University of Geneva, Chemin Pegasi 51, 1290 Versoix, Switzerland\n12MIT Kavli Institute for Astrophysics and Space Research, 70 Vassar Street, Cambridge, MA 02139, USA\n13Cosmic Dawn Center (DAWN), Copenhagen, Denmark\n14Niels Bohr Institute, University of Copenhagen, Jagtvej 128, København N, DK-2200, Denmark\n15Astrophysics Research Institute, Liverpool John Moores University, 146 Brownlow Hill, Liverpool L3 5RF, UK\n16Departament d’Astronomia i Astrofísica, Universitat de València, C. Dr. Moliner 50, E-46100 Burjassot, València, Spain\n17Unidad Asociada CSIC ”Grupo de Astrofísica Extragaláctica y Cosmología” (Instituto de Física de Cantabria - Universitat de\nValència), Spain\n18Steward Observatory, University of Arizona, 933 N Cherry Avenue, Tucson, AZ 85721, USA\n19Center for Frontier Science, Chiba University, 1-33 Yayoi-cho, Inage-ku, Chiba 263-8522, Japan\nReceived June 2025; Accepted soon after\nABSTRACT\nWe use [CII]158 µm observations of a large QSO sample to segregate sources by host galaxy mass, aiming to identify those in the\nmost massive hosts. [CII] luminosity, a known tracer of molecular gas, is taken as a proxy for host mass and used to rank 190 QSOs\natz>5.9, spanning a 6-mag UV luminosity range ( −22<MUV,AB<−28). Particularly valuable are ALMA data from a cycle-10\nCISTERN program, providing [CII] coverage for 46 UV-faint ( MUV,AB>−24.5) and 25 especially UV-faint ( MUV,AB>−23.5) QSOs,\nimproving statistics by 5 ×and 6×, respectively. Taking massive host galaxies to be those where L[CII]>1.8×109L⊙(median L[CII]of\nUV-bright QSOs), we identify 61 QSOs, including 13 which are UV-faint and 7 especially UV-faint. Using these selections and recent\nQSO luminosity functions (LFs), we present the first characterization of UVluminosity distribution for QSOs in massive host galaxies\nand quantify [CII] LFs for both UV-bright and UV-faint QSOs. While ∼3% of massive-host QSOs are UV-bright ( MUV,AB<−26),\n≳85% are UV-faint ( MUV,AB>−24.5). This wide dispersion in UVluminosities reflects variations in dust obscuration, accretion\nefficiency, and black hole mass. Though spectroscopy is needed for definitive conclusions, black hole mass appears to be the dominant\nfactor driving variations in the UVluminosity, based on 34 [CII]-luminous ( L[CII]>1.8×109L⊙) QSOs distributed across a ∼3-mag\nbaseline in UVluminosity and with measured MBH. AtMUV,AB∼−23, the median extrapolated log10(MBH/M⊙) is 8.1±0.4, consistent\nwith the local relation. SMBHs in UV-bright QSOs thus appear to be ∼15+25\n−9×more massive than typical for massive host galaxies at\nz∼6.\nKey words. Galaxies: active, (Galaxies:) quasars: emission lines, Galaxies: high-redshift, Galaxies: star formation\n1. Introduction\nAn exciting frontier in extragalactic cosmology has been the dis-\ncovery of supermassive 109M⊙black holes (SMBHs) in bright\nQSOs just 700-900 Myr after the Big Bang and the questions\nthese inspire regarding how such SMBHs form, seemingly re-\nquiring very massive ( >104M⊙) black hole seeds or super Ed-dington accretion (e.g. Fan et al. 2001; Haiman & Loeb 2001;\nV olonteri et al. 2003; Mortlock et al. 2011; Bañados et al. 2018;\nV olonteri et al. 2021; Fan et al. 2023; Wang et al. 2024). Results\nfrom JWST data suggest the formation of massive black holes\nin the early universe is ubiquitous (e.g. Kocevski et al. 2023;\nHarikane et al. 2023; Larson et al. 2023; Maiolino et al. 2024;\nMatthee et al. 2024; Labbe et al. 2023; Kokorev et al. 2024;\nArticle number, page 1 of 13\n\nA&A proofs: manuscript no. faintqso\nAkins et al. 2024) and occur at redshifts as high as z =10.2\n(Goulding et al. 2023).\nThe discovery of a widespread population of SMBHs in the\nearly universe has inspired the development of models to ex-\nplain their build-up. One challenge in these e fforts is accounting\nfor the large variability in the detectability of individual SMBHs,\ndriven by factors such as episodic gas accretion and obscuration\nby dust (e.g. Urry & Padovani 1995; Padovani 2017). To mitigate\nthis, it is useful to link SMBH growth to that of their host galax-\nies, leveraging our general understanding of how stellar and halo\nmasses evolve. Unlike SMBHs, host galaxies are consistently\nobservable in surveys and tend to build up their halo, stellar, and\ngas content in a generally predictable fashion. By measuring host\ngalaxy masses across cosmic time and rank ordering the galax-\nies according to mass (e.g. Vale & Ostriker 2004; Conroy et al.\n2006), we can connect QSOs /SMBHs with galaxies of a given\nmass, providing us with a framework for quantifying the growth\nof SMBHs across cosmic time.\nThe entire enterprise of characterizing host galaxy proper-\nties for bright QSOs to facilitate a QSO-host galaxy connection\nis challenging due to the di fficulty in disentangling host galaxy\nlight from the QSO itself (e.g. Schramm et al. 2008; Matsuoka\net al. 2014; Ciesla et al. 2015). Fortunately, enormous progress\nis now being made with JWST thanks to its exceptional sensi-\ntivity and resolving power, providing for robust separation of\nQSO and host galaxy light and thus more reliable stellar mass\ndeterminations (e.g. Ding et al. 2023). Despite these advances,\ncharacterizing the host galaxy properties for the brightest QSOs\nstill remains di fficult (Yue et al. 2024), even with JWST .\nAn appealing alternative to using rest-frame optical light to\nprobe host galaxy properties is far-infrared (far-IR) line emis-\nsion from tracers such as [CII] 158µm. The [CII] line is a well-\nestablished tracer of molecular gas mass (Zanella et al. 2018;\nMadden et al. 2020; Vizgan et al. 2022a), neutral gas mass\n(Heintz et al. 2021; Vizgan et al. 2022b), and star formation rates\n(SFR; De Looze et al. 2014). It also o ffers several observational\nadvantages: it is readily detectable with facilities like ALMA\n(e.g. Decarli et al. 2018), largely una ffected by dust obscuration,\nfree from contamination by bright QSO emission at the same\nfrequency, and minimally influenced by the QSO in powering\nemission within photodissociation regions (PDRs) (e.g. Stacey\net al. 2010; Sargsyan et al. 2014). Given the expected correlation\nbetween [CII] luminosity and host galaxy molecular gas mass\n(Zanella et al. 2018; Vizgan et al. 2022a), this line provides a\npowerful tool for distinguishing QSOs according to the mass of\ntheir host galaxies (but see however Kaasinen et al. 2024) – al-\nlowing for a detailed look at QSO demographics in massive sys-\ntems.\nTo map out the demographics of QSOs in massive host galax-\nies using far-IR diagnostics like [CII], it is useful to consider\nQSOs from as wide a range as possible in UVluminosity, which\nis why we draw from both traditional UVbright type-1 QSO\nsamples and also fainter selections of QSOs in the rest- UVsuch\nas pursued by SHELLQs (Subaru High-z Exploration of Low-\nLuminosity Quasars Matsuoka et al. 2016). This ensures inclu-\nsion of type-1 QSOs with a range of BH masses, Eddington ra-\ntios, and dust obscuration,1providing a much more representa-\ntive selection of SMBHs in massive galaxies than the UV-bright\npopulation provides, a topic that has been extensively discussed\nin the literature (e.g. Willott et al. 2005; Lauer et al. 2007; Reines\n& V olonteri 2015; Shankar et al. 2020; Wu et al. 2022; Li et al.\n1Of course, this only samples moderately obscured QSOs to the extent\nthey are included in type-1 QSO selections.\nCISTERNLiterature\nCISTERNLiterature“UV-Faint”\n(~5x increase)# of QSOs Probed in [CII] Massive \nHost Galaxieslog10 (Mgas/M⦿)1011\n9.510.511.5\n50403020100−22−23−24−25−26−27−28MUV (mag)6070“Especially UV Faint”“UV-Bright”Fig. 1. (upper panel ) Measured [CII] luminosities vs. MUVluminosities\nforz>5.9 QSOs from CISTERN ( red circles ) and from the literature\n(black circles ). The solid downward pointing triangles indicate 5 σup-\nper limits on the [CII] luminosities of QSO where no line is detected\nwith ALMA. The black stars correspond to the Fujimoto et al. (2022)\nand Endsley et al. (2023) QSO that were identified in deep multiwave-\nlength data over legacy fields and not from wide-area QSO searches.\nShown along the right vertical axis is the approximate gas mass one\nwould expect for the host galaxy of a given [CII] luminosity using the\ncanonical Zanella et al. (2018) relation. ∼21% of UV-faint QSOs show\nluminous [CII] emission and thus appear to reside in massive host galax-\nies. ( lower panel ) Number of [CII] observations for z>5.9 QSOs vs.\nUV luminosity from the literature ( filled black histogram ) and from\nCISTERN ( filled red histogram ). Thanks to observations from CIS-\nTERN, there has been a dramatic ∼5×increase in coverage of [CII]\ninUV-faint ( MUV,AB>−24.5) QSOs with ALMA.\n2022). Ideally the selections used for these purposes should even\ninclude the so-called Little Red Dot (LRD: Matthee et al. 2024;\nGreene et al. 2024) population identified with JWST (e.g. Ko-\ncevski et al. 2023; Labbe et al. 2023), but their consistent faint-\nness in both the far-IR and [CII] 158µm(e.g. Akins et al. 2024;\nSetton et al. 2025; Xiao et al. 2025; Casey et al. 2025) suggests\nthey are associated with much lower mass host galaxies than the\ngalaxies hosting UV-bright QSOs.\nThe large selection of lower luminosity QSOs recently ob-\ntained from the CISTERN ([CII]-SelecTed Early universe Ref-\nereNce, PI: Bouwens) cycle-10 ALMA program (Bouwens et\nal. 2025, in prep) has proven particularly useful for obtaining\n[CII] coverage of z ≳6 QSOs over a wide range in UVluminos-\nity. CISTERN obtained ALMA coverage probing the [CII] line\nfor 45 UV-faint (−24.5<MUV,AB) and 25 especially UV-faint\n(−23.5<MUV,AB) QSOs at z >5.9, increasing the [CII]-coverage\nof z≳6 QSOs in these luminosity ranges by 5 ×and 6×, respec-\ntively. When considering QSOs of all UV luminosities, [CII]\nArticle number, page 2 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nTable 1. Observational Data Sets Utilized Here to Probe [CII] Emission\nfrom z>5.9 QSOs.\n# QSOs\nData Redshift UVLuminositya\nSet Range Faint Intrmd Bright\nThis Work\nCISTERN 5.9-7.0 46 32 8\n2019.1.00074.S 6.1-6.5 4 6 0\nEarlier Work\nWillott et al. (2017) 6.0-6.2 3 0 0\nIzumi et al. (2018) 5.9-6.4 3 0 0\nDecarli et al. (2018) 6.0-6.4 0 2 10\nD18 Litb6.0-6.7 0 4 4\nIzumi et al. (2019) 6.1-6.1 1 2 0\nEilers et al. (2020) 5.9-6.5 0 2 4\nVenemans et al. (2020) 6.0-7.5 0 5 20\nIzumi et al. (2021a) 7.0-7.1 1 0 0\nIzumi et al. (2021b) 6.7-6.8 1 0 0\nFujimoto et al. (2022) 7.1-7.2 1 0 0\nEndsley et al. (2023) 6.8-6.9 1 0 0\nWang et al. (2024)c6.5-7.6 0 14 16\nTotal 5.9-7.6 61 67 62\naFaint, Intrmd, Bright subsamples refer to QSOs with UVlu-\nminosities satisfying the criteria −24.5≤MUV,AB,−26.0≤\nMUV,AB<−24.5, and MUV,AB<−26.0, respectively.\nbLiterature compilation presented in Table 2 of Decarli et al.\n(2018) including results from Walter et al. (2009), Wang et al.\n(2013), Willott et al. (2013), Willott et al. (2015), and Mazzuc-\nchelli et al. (2017).\ncA magnification factor of 50 has been adopted in correcting the\nUVluminosity of J043947.08 +163415.7 for gravitational lens-\ning (Fan et al. 2019).\ncoverage is available for 190 z >5.9 QSOs, 44% of which are\nprovided by the CISTERN program. More information on both\nthe CISTERN program and the basic results will be provided in\nBouwens et al. (2025, in prep).\nWith [CII] luminosity information, we can then segregate the\n190 QSOs in our CISTERN +archive sample according to the\napparent mass of their host galaxies. Most of our attention will\nbe devoted to QSOs associated with the most massive galaxies,\ngiven their ubiquity amongst UV-bright QSOs and noteworthy\nfrequency amongst even fainter QSOs, as we will see here (and\nsee Izumi et al. 2021a). Of course, besides the luminosity of the\n[CII] line, the mass of host galaxies are also commonly probed\nusing the dynamical mass (as inferred from the width of [CII])\nand using the far-IR luminosity (e.g. Willott et al. 2013; Izumi\net al. 2019, 2021a; Neeleman et al. 2021), and we will consider\nthese alternate measures in the massive host galaxy samples we\nconstruct.\nWe then leverage published UVLFs for QSOs (e.g. Mat-\nsuoka et al. 2018c; Schindler et al. 2023; Matsuoka et al. 2023)\nto provide volume density information of the QSOs with [CII]\ncoverage, allowing us to compute [CII] LF results for QSOs and\nthus compare the prevalence of UV-bright and UV-faint QSOs in\nhost galaxies of a given mass. Additionally, UVLF information\nallows us to derive various QSO distribution functions for host\ngalaxies of a given mass, e.g., including for the UVand bolo-\nmetric luminosities. Determinations of the black hole mass and\nEddington ratio distributions for QSOs in massive host galaxies\nis also possible, but also requires spectroscopy for the sourcesused to construct the distribution functions. Given the lack of\nsuch information for the fainter QSOs in our massive host galaxy\nsample, we will explore an extrapolation of results from brighter\nQSOs to make a first estimate.\nHere we provide a brief plan for the paper. In §2, we present\nthe observational data we utilize, and in §3, we present the dis-\ntribution of [CII] 158µmluminosities we measure vs. UVluminos-\nity for our composite CISTERN +archival sample and use that\nto map out the UV luminosity distribution for QSOs in mas-\nsive host galaxies, while deriving [CII] 158µmLF results for UV-\nbright and UV-faint QSOs. Finally, making use of trends in MBH\nandλEddinUVluminosity, we present our derived MBHand\nλEdddistributions for QSOs in massive host galaxies. For sim-\nplicity and for ease of comparison with other studies, we adopt\nthe standard so-called concordance cosmology, with Ωm=0.3,\nΩΛ=0.7, and H0=70 km /s/Mpc. All magnitude measurements\nare given using the AB magnitude system (Oke & Gunn 1983)\nunless otherwise specified.\n2. Observational Data\nHere we make use of ALMA observations targeting the [CII]\nline from 190 QSOs at z >5.9 QSOs. Observations from the\ncycle-10 CISTERN program (2023.1.00443.S: R.J. Bouwens et\nal. 2025, in prep) provide us with key new information for this\nanalysis, but supplement these observations with [CII] results on\n104 QSOs from archival programs. UVluminosities are taken\nfor QSOs from Fan et al. (2023) where available and other-\nwise computed based on the available photometry in the near-IR\nband closest to rest-frame 1450Å (but which is not impacted by\nabsorption blueward of 1216Å). Table 1 provides a convenient\nsummary of the data sets and QSOs included in this analysis.\nNew observations from the cycle-10 CISTERN program will\nbe key to this analysis due to the huge increase in coverage of\n[CII] for QSOs which are relative UV-faint ( MUV,AB>−24.5).\nOf the 86 z≥5.9 QSOs with delivered data from the CIS-\nTERN program, 46 corresponded to lower luminosity (or UV-\nfaint) MUV,AB≥ −24.5 QSOs, 32 corresponded to intermedi-\nate luminosity−26.0≤MUV,AB<−24.5 QSOs, and 8 corre-\nsponded to high luminosity MUV,AB<−26.0 QSOs. By contrast,\npublished measurements on [CII] 158µmonly exist for 11 fainter\nQSOs ( MUV,AB≥−24.5) from earlier programs, only 9 of which\nwere identified based on wide-area search e fforts. Earlier pro-\ngrams do nevertheless provide essential statistical information\nonUV-bright population. Figure 1 ( lower panel ) illustrates how\nnew observations contribute to coverage of [CII] for both UV-\nbright and UV-faint QSOs.\nFor sources from the CISTERN program, the integration\ntimes (per source) generally ranged from 5 to 20 minutes in dura-\ntion, with line and continuum sensitivities of 0.4 mJy /beam and\n73µJy/beam, respectively. The line sensitivity was set to 0.65\nmJy/(66 km /s channel) to allow for the detection (at 5 σ) of [CII]\nISM cooling lines to a limit of ∼4-5×108L⊙(equivalent to a\nSFR of 40-50 M⊙/yr assuming the De Looze et al. 2014 SFR-\nL[CII]relation). A synthesized beam of >0.9” was requested to\navoid overresolving the typically extended [CII] emission from\nbright star-forming galaxies and QSOs at z≥6. More details will\nbe provided on both the observations and results for individual\nsources in the CISTERN survey paper (R.J. Bouwens et al. 2025,\nin prep). For sources from other programs, the integration times\nwere generally∼3-4×longer, with 2×higher sensitivity.\nOur analysis of the ALMA observations of each target pro-\nceeded as follows. For each target, measurement sets were cre-\nArticle number, page 3 of 13\n\nA&A proofs: manuscript no. faintqso\nTable 2. Compilation of UV-Faint QSOs in Massive Host Galaxiesaat z>5.9\nSource MUV,AB L[CII] FWHM [CII] LIR\nID R.A. Decl. (mag) z[CII]/109L⊙ (km/s) (1012L⊙) Refa\nBased on [CII] 158µmData From This Work (CISTERN)\nJ132308.18 +012619.2 13:23:08.18 +01:26:19.2−23.0 6.026 5.1±0.1 373±8 3.8±0.1 This Work, [1]\nJ144045.91 +001912.9c14:40:45.91 +00:19:12.9−23.2 6.549 8.6 ±0.1 515±8 8.4±0.3 This Work, [1]\nHSCJ142903−010443 14:29:03.08 −01:04:43.4−23.3 6.796 3.7 ±0.1 458±21 2.5±0.1 This Work, [2]\nJ132700.35 +014147.7 13:27:00.35 +01:41:47.7−23.4 6.217 5.2±0.2 224±5 5.2±0.2 This Work, [1]\nHSCJ114632−015438d11:46:32.66−01:54:38.3−23.5 6.158 3.4 ±0.1 186±6 1.7±0.1 This Work, [3,4]\nJ145520.26 +031833.0 14:55:20.26 +03:18:33.0−23.8 6.053 2.1±0.1 483±32 1.9±0.1 This Work, [1]\nJ122331.91 +025721.5 12:23:31.91 +02:57:21.5−24.3 6.258 1.8±0.3 451±109 4.0±0.2 This Work, [1]\nILTJ2336 +1842 23:36:24.69 +18:42:48.7−24.3 6.578 2.7 ±0.1 393±17 3.1±0.2 This Work, [5]\nHSCJ225520 +050343 22:55:20.78 +05:03:43.3−24.5 6.172 2.1±0.2 317±29 5.1±0.2 This Work, [6]\nFrom the Literature\nJ0958 +0139d09:58:58.27 +01:39:20.2−21.7 6.853 8.8 ±0.8 652±17 9.1±0.5e[7,8]\nVIMOS2911 22:19:17.23 +01:02:48.9−23.1 6.149 2.6 ±0.1 264±15 2.2±0.1e[9]\nJ1243 +0100 12:43:53.93 +01:00:38.5−24.1 7.075 2.5 ±0.1 280±12 5.4±0.1e[10]\nJ1205−0000d12:05:05.09−00:00:27.9−24.4 6.723 1.9 ±0.1 536±26 3.8±0.1e[4,11]\naAs defined by L[CII]>1.8×109L⊙\nbReferences: [1] =Matsuoka et al. (2022), [2] =Matsuoka et al. (2018b), [3] =Matsuoka et al. (2018a), [4] =Kato et al. (2020),\n[5]=Gloudemans et al. (2022), [6] =Matsuoka et al. (2019a), [7] =Endsley et al. (2022), [8] =Endsley et al. (2023), [9] =Willott\net al. (2017), [10] =Izumi et al. (2021a), [11] =Izumi et al. (2021b)\ncRed near-IR colors of this source in ground-based near-IR data +WISE (Matsuoka et al. 2022) suggest this QSO is dust reddened\n(Bouwens et al. 2025, in prep).\ndAmongst those UV-Faint QSOs suggested to be obscured by dust (Kato et al. 2020; Endsley et al. 2022).\neTo ensure consistency with the other QSOs in this study, the IR luminosities tabulated here are recomputed using the reported\ncontinuum fluxes for these QSOs and the Venemans et al. (2020) methodology.\nated using the ScriptForPI.py script that came for the QA2 de-\nlivery using CASA (CASA Team et al. 2022), and the data were\ntime averaged in 30-s segments, and then finally the data was\nreimaged using the tclean task adopting natural weighting. Nat-\nural weighting maximizes the S /N we are able to achieve for line\nand continuum detections from CISTERN, while slightly lower\nthan S /N over the default reductions provided by JAO (where a\nBriggs weighting robust parameter of 0.5 are utilized). In prob-\ning the dust continuum emission from targets in CISTERN, the\nfrequency range surrounding candidate ( >5σ) [CII] lines (±0.4\nGHz) is excluded in producing continuum images of the targeted\nQSOs.\nUse was also made of a limited quantity of archival ALMA\nobservations from the cycle-7 program 2019.1.00074.S (PI:\nIzumi) targeting the [CII] line in 12 z >5.9 QSOs. Results for\n2 QSOs from this sample are discussed in detail in Izumi et al.\n(2021a) and Izumi et al. (2021b).\n3. Results\n3.1. Search for [CII] Line Emission from z >5.9 QSOs\nWe performed an analysis of the full set of ALMA data deliv-\nered to us as part of the CISTERN program. In each data cube,\nwe have made use of our own line search code to look for >5σ\nline emission from each of our targets. Briefly, with this code,\nwe quantify the rms noise in each spectral channel (which for\nour delivered data have a width of ∼8 km /s), coadd in quadra-\nture the significance levels over spectral windows running from\n100 km /s to 800 km /s, and then look for candidate >5σlines in\nthe running significance levels calculated for each spectral win-\ndow. Given that sources are expected to be largely unresolvedin the observations obtained as part of CISTERN, with beam\nsizes≥0.9′′, it is possible to conduct this search using peak fluxes\n(i.e., without any additional spatial smoothing or tapering of the\nALMA observations). Plausible detections of the [CII] line are\nseen for 62 of the 87 targets from the delivered CISTERN data,\nfor a success rate of 71%. Of the 12 targeted QSOs in cycle-7\nprogram 2019.1.00074.S (PI: Izumi), a [CII] line is identified in\n8 of the QSOs.\n3.2. Using [CII] to Identify QSO in Massive Host Galaxies\nover a Wide Range in UVLuminosity\nHere we use the [CII] luminosity to segregate our QSOs by host\ngalaxy mass. Given utility of the [CII] luminosity as a measure\nof both the molecular gas mass and SFR, the present approach is\nwell motivated. It is worth noting however that in many analyses\nof QSOs and their host galaxies, use is made of the apparent dy-\nnamical mass as the fiducial mass of the QSO host galaxies (e.g.\nWillott et al. 2013; Izumi et al. 2019, 2021a; Neeleman et al.\n2021). Given the sensitivity of these mass estimates to the incli-\nnation angle inferred for the host galaxies and the low spatial res-\nolution of our ALMA observations, we decided to use the [CII]\nluminosities as a proxy for host galaxy mass.\nWe compare the [CII] luminosities we infer for our QSO\nsample (based on the peak fluxes) with their UVluminosities in\nFigure 1 (upper panel) . 5σupper limits are computed for QSOs\nwithout a detected [CII] 158µmline assuming a FWHM of 225\nkm/s (median FWHM [CII]of QSOs with L[CII]<5×108L⊙ob-\nserved by Izumi et al. 2018, 2019) and coverage of [CII] 158µm\nwith the acquired ALMA band-6 data. Along the right vertical\naxis, we indicate the approximate mass in molecular gas one\nArticle number, page 4 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nHost \nLiterature [`CII] \nBright SampleLiterature [`CII] \nBright SampleLiterature [`CII] \nBright SampleCISTERN +\nLiterature\nResultsCISTERN +\nLiterature\nResultsCISTERN +\nLiterature\nResults\nFig. 2. Median [CII] luminosities ( left), IR luminosities LIR(center ), and [CII] FWHMs ( right ) vs. UVluminosity for QSOs in massive host galax-\nies, as inferred from their luminous ( L[CII]>1.8×109L⊙) [CII] emission. The plotted error bars are 1 σ. Our Mdyndeterminations are derived using\nthe measured FWHMs of the [CII] line and a fitting formula from Neeleman et al. (2021). Measurements for the individual QSOs contributing\nto the medians for QSOs at brighter and fainter UVluminosities are shown with the grey and red circles, respectively (from Appendix A and\nTable 2). The characteristics of UV-Bright QSOs are derived mostly from the literature while the characteristics of UV-faint sample are predomi-\nnantly derived from CISTERN. UV-Faint QSOs which are [CII] bright appear to have similar far-IR properties to UV-Bright QSOs that are [CII]\nbright, with the possible exception of their IRluminosities which could be impacted by the varying brightness of the AGN itself (Schneider et al.\n2015; Kirkpatrick et al. 2015). This suggests that the two populations live in similar mass host galaxies.\nwould infer for host galaxies using the measured [CII] luminos-\nity and the Zanella et al. (2018) relation. While there is a weak\ncorrelation between the observed [CII] luminosity for QSOs and\ntheir UV luminosities, as Izumi et al. (2019) noted, enormous\nscatter is present in the relation, with a modest fraction of QSOs\nbeing both very bright in the rest- UVand faint in [CII] or being\nvery bright in [CII] and faint in the rest- UV.\nOf particular interest are those QSOs which are UV-faint but\nare extremely luminous in [CII]. In a few cases, their [CII] lu-\nminosities rival the [CII] luminosities of UV-bright ( MUV,AB∼\n−27 mag) QSOs. The most [CII] luminous of these QSOs\n(J144045.91 +001912.9) has a UVluminosity of−23.2 mag and\nis brighter than all but 2 other QSOs shown in our Table 1 com-\npilation. Given that [CII] is a known tracer of the molecular\ngas mass in host galaxies, the implication is that these UV-faint\nQSOs live in similar mass host galaxies as UV-bright QSOs.\nTo investigate whether the host galaxies of these [CII]-\nluminous UV-faint QSOs really are as massive as QSOs at\nthe bright end of the UVLF, we identify a selection of [CII]-\nluminous QSOs across a range of UVluminosity and evaluate\nwhether the physical properties appear to show any dependence\non the UVluminosities of the QSOs. For our [CII] selection, we\nrequire sources show L[CII]>1.8×109L⊙. As 1.8×109L⊙is the\nmedian luminosity of QSOs in our UV-bright ( MUV,AB<−26.0)\nsubsample, the purpose of this selection is to identify QSOs\nwhose host galaxies were on the high-mass end of the QSO pop-\nulation at high UVluminosities. 63 of the 190 QSOs considered\nas part of this analysis qualify as having high-mass host galax-\nies using this definition. This [CII] luminosity (1.8 ×109L⊙) also\ncorresponds to host galaxy gas masses of ∼1010.5M⊙.\nThese [CII] luminous QSOs are then segregated into 4\nbrighter UVluminosity bins and one faint bin, i.e., (1) −28.0<\nMUV,AB<−27.0, (2)−27.0<MUV,AB<−26.5, (3)−26.5<\nMUV,AB<−26.0, (4)−26.0<MUV,AB<−24.5, and (5)\n−24.5<MUV,AB<−21.5. The number of [CII]-bright QSOs in\neach UVluminosity bin is 9, 14, 8, 19, and 13, respectively, out\nof 11, 29, 21, 67, and 61 QSOs, respectively, resulting in a [CII]-\nluminous fraction of 0.82, 0.48, 0.38, 0.28, and 0.21, respec-\ntively, for these di fferent UV-luminosity bins, showing a clear\nmonotonic decrease from brighter UV luminosities to fainter.The 13 UV-faint QSOs that we discovered to be [CII] luminous\nleveraging the new CISTERN data and from archival observa-\ntions or the literature) are presented in Table 2.\nFor QSOs in each UVluminosity bin, we quantify three dif-\nferent characteristics of the QSOs, which should provide an ap-\nproximate probe of the overall mass of their host galaxies: (1)\ntheir [CII] luminosity L[CII]which is a known tracer of the host\ngalaxy gas mass,2(2) the IR luminosity LIRof these sources, and\n(3) the FWHM of the [CII] line which correlates with dynami-\ncal mass of the host galaxies. We compute the IR luminosity (8\n- 1000µm) of QSOs using the same methodology and assump-\ntions as Venemans et al. (2020), following the da Cunha et al.\n(2013) treatment in accounting for the impact of the CMB on\nthe measured fluxes.\nWe present the median L[CII],LIR, and FWHM [CII]derived\nfor sources in each UVluminosity bin in the left, center, and\nright panels, respectively, of Figure 2. Our Mdyndeterminations\nare derived using the measured FWHMs of the [CII] line and a\nfitting formula from Neeleman et al. (2021):\nMdyn=(4.0+4.0\n−2.0(+2.4\n−2.8)) FWHM\nkm s−1!2\n×105M⊙ (1)\nDeterminations for the brighter sample and faint sample are\nshown with solid black circles and solid red circle, respectively.\nFor each of the three characteristics, the median derived for the\nfaint sample is consistent with that found for the brighter sam-\nples, with the possible exception of the IR luminosities com-\nputed for the QSOs where one would expect a contribution from\nAGN heating for the brighter QSOs (Schneider et al. 2015; Kirk-\npatrick et al. 2015). The broad consistency between both their\n[CII] luminosities (by construction through the selection criteria)\nandMdynsuggest a broad similarity in the host galaxy masses for\neach of these samples.\n2Even though the [CII] luminosity is used in constructing the QSO\nsubsamples used for this exercise, we decided to include the [CII] lu-\nminosity here along with the other two measures of mass to allow for\nside-by-side comparisons.\nArticle number, page 5 of 13\n\nA&A proofs: manuscript no. faintqso\nQSO LF for Massive Host Galaxies (where L[CII]>1.8 x 109 L⦿)0.03UV LF of QSOs (Schindler+2023)\n0.15Cumulative Fraction\nFig. 3. (upper ) Fraction ( vertical axis ) of massive host galaxies (with\nL[CII]>1.8×109L⊙:solid line ) brighter than some UVluminosity\n(horizontal axis ). Our analysis indicates only 15% and 3% of the QSOs\nin massive host galaxies have UVluminosities brighter than −24.5 mag\nand−26.0 mag, respectively (see §3.3). The vast majority of the QSOs\nareUVfaint, i.e.,>−24.5 mag. The cumulative fraction presented here\nis based on a consideration of QSOs brightward of −22 mag where\nbroad [CII] coverage of QSO samples exists. ( lower ) V olume density\nof QSOs with massive host galaxies vs. UVluminosity ( solid line ). The\ndashed line represents the Schindler et al. (2023) LF and is shown for\ncomparison. While the percentage of UV-bright QSOs having massive\nhost galaxies (48-82%) is much ∼3-4×higher than for the UV-faint\n(MUV,AB>−24.5) QSOs (21-28%), >85% of the QSOs with massive\nhost galaxies are still UV-faint.\n3.3. UVLuminosity Distribution of QSOs in Massive Host\nGalaxies\nThanks to the substantial [CII] coverage we have of UV-bright\nQSOs and now UV-faint QSOs, we can provide a first quantifica-\ntion of the UVluminosity distribution of massive host galaxies.\nAs in the previous section, we take QSOs to live in a massive\nhost galaxy if its [CII] luminosity is greater than 1.8 ×109L⊙.\nTo perform this exercise, we start with the QSO UVLF re-\nsults of Schindler et al. (2023) at z∼6. We then break the UV\nLF into 5 di fferent bins of UVluminosity−28<MUV,AB<−27,\n−27<MUV,AB<−26,−26<MUV,AB<−25,−25<MUV,AB<\n−24, and−24<MUV,AB<−22 and then multiply the LF at that\nUVluminosity by the fraction of sources with [CII] 158µmlumi-\nnosities in excess of 1.8 ×109L⊙. We present the result in lower\npanel of Figure 3 to MUV,AB∼−22 where CISTERN provides us\ngood coverage of [CII] 158µm. The step-like structure in the QSO\nLF we infer for massive host galaxies derives from the [CII] dis-\ntribution being split across distinct UV-luminosity bins. From\nMUV < −26−24.5 < MUV < −22.0≳30×log10 Volume Density φ/(Mpc3/dex)log10 Mgas/M⦿ (Host Galaxy)111011.510.5Fig. 4. [CII] luminosity functions inferred for UV-faint ( −24.5<\nMUV,AB<−22.0:red circles ) and UV-bright QSOs ( MUV,AB<−26.0:\nblue solid circles ) at z∼6 based on the distribution of [CII] luminosities\nseen for QSOs at a given UV luminosity (largely from CISTERN, a new\nALMA program) and the Schindler et al. (2023) z∼6 UV LF. Uncer-\ntainties are computed by adding in quadrature the contribution from ev-\nery QSO whose measured [CII] luminosity corresponds to a given bin.\nThe downward pointing triangle is a 1 σupper limit. For z∼7 QSOs\n(Matsuoka et al. 2023), both LFs would be ∼5×lower. See §3.3 for\ndetails. The upper axis indicates the approximate host galaxy gas mass\nadopting the [CII] conversion factor from Zanella et al. (2018). The vol-\nume density of [CII] luminous galaxies which are UV-faint is∼29+10\n−7×\nhigher than [CII] luminous galaxies that are UV-bright. Significant un-\ncertainties remain as to what (dust, low MBH, extreme sub-Eddington\naccretion?) drives di fferences between UV-bright and UV-faint QSOs\nin massive host galaxies (§4).\nthe figure, we can easily see that the vast majority of QSOs in\nmassive host galaxies appear to be UV-faint.\nTo better quantify what fraction of QSOs in massive host\ngalaxies are UV-bright or UV-faint, we plot the cumulative frac-\ntion above a given UVluminosity in the upper panel of Figure 3.\nOnly 15% of the QSOs lie brightward of −24.5 mag, and just\n3% lie brightward of −26 mag, meaning that the vast majority\n(>85%) of QSOs in massive host galaxies are UV-faint, i.e.,\nMUV,AB>−24.5 mag. Note that we only consider the fraction\nto aUVluminosity of−22 mag due to our sampling of the [CII]\ndistribution for fainter QSOs only extends to that UVluminosity,\nbut this seems likely to improve in the future thanks to ALMA\ncoverage of fainter QSO samples identified with JWST .\nIn addition to our quantifying the UVluminosity distribution\nof QSOs in massive host galaxies, it is interesting to derive the\nvolume density of UV-bright and UV-faint QSOs as a function\nof their host galaxy gas masses. If we continue treating L[CII]as a\nproxy for the host galaxy mass, this can be done by deriving the\n[CII] luminosity function for UV-bright and UV-faint QSOs.\nAs in our earlier determination of UVluminosity distribu-\ntion for QSOs with massive host galaxies, we again break the\nUVLF into 5 di fferent bins in UVluminosity. We then reassign\ngalaxies in each UVLF bin to QSOs with a given [CII] luminos-\nity in accordance with the observed L[CII]distribution in that UV\nluminosity bin. Here we are e ffectively making the assumption\nthat targeting of specific z >6 QSOs was largely a function of the\napparent brightness of QSOs and the spectroscopic redshift be-\ning known. In computing the uncertainties on the [CII] LF by\nArticle number, page 6 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nTable 3. Binned [CII] LF Results for QSOs at z∼6a\nlog10(L[CII]/L⊙) log10(ϕ∗dex−1Mpc−3)\nBright ( MUV,AB<−26) QSOs\n8.83 −9.64±0.19\n9.17 −8.97±0.09\n9.50 −9.23±0.11\n9.83 −9.56±0.18\n10.16 −11.07±1.62\nFaint (−24.5<MUV,AB<−22) QSOs\n8.83 −7.48±0.14\n9.17 −7.83±0.21\n9.50 −7.83±0.21\n9.83 −7.96±0.25\n10.16 <−8.38b\na[CII] LF results shown in Figure 4. The results are inferred\nusing the z∼6 QSO LF from Schindler et al. (2023) and the\nobserved L[CII]vs.MUVdistribution shown in Figure 1. Results\nhere are as in Figure 4.\nb1σUpper Limit\nadding in quadrature the contribution from every QSO with a\n[CII] luminosity measurement. LF bins with larger uncertainties\non the volume density are based on a smaller number of QSOs\nwith that luminosity measurement.\nWe present the results for a UV-bright ( MUV,AB<−26.0)\nandUV-faint (−24.5<MUV,AB<−22.0) QSO selection in Fig-\nure 4 and Table 3. It is immediately clear that over a broad range\nof [CII] 158µmluminosities (or equivalently host galaxy masses)\nthat there are∼20-30×more UV-faint QSOs than there are UV-\nbright QSOs. For [CII] luminosities in excess of 1.8 ×109L⊙, the\nexcess is 29+12\n−8. Similar excesses would be obtained by making\nthe comparison using other thresholds in [CII] 158µmluminosity.\nThe sensitivity of the ALMA data available to CISTERN limits\nour ability to probe this relation below L[CII]∼108.5L⊙.\nWe find a similar excess of [CII]-bright, UV-faint QSOs rela-\ntive to UV-bright systems using other z∼6 QSO LF determina-\ntions. For the Matsuoka et al. (2018c) and Willott et al. (2010c)\nLFs, the excess of UV-faint [CII]-bright systems over bright sys-\ntems is 24+10\n−7and 33+13\n−9, respectively, which is consistent ( ±0.1\ndex) of that found using the Schindler et al. (2023) LF. If we in-\nstead rely on the UVLF of QSOs at z∼7 (Matsuoka et al. 2023),\nthe excess would be 17+7\n−5, with a∼5×lower normalization.\nResults from this section strongly suggest that the vast ma-\njority (>85%) of QSOs with massive host galaxies are UVfaint,\ni.e., with MUV,AB>−24.5 mag. By contrast, only a small frac-\ntion of massive host galaxies ( <3%) contain QSOs which are as\nbright as those discovered in early QSO work using the Sloan\nDigital Sky Survey (e.g. Fan et al. 2001).\n4. Discussion\n4.1. UV-Faint QSOs in Massive Host Galaxies at z∼6: How\ndo they differ from UV-Bright QSOs?\nIn the previous section, we demonstrated that the vast major-\nity (>85%) of QSOs with massive host galaxies are UV-faint,\ni.e.,MUV,AB>−24.5 mag, which is much fainter than the initial\nQSOs discovered at z ∼6 (Fan et al. 2006). As UV-faint QSOs\nrepresent the dominant form of QSO in massive host galaxies,\nit is essential we ascertain their nature and how they di ffer fromUV-bright QSOs at z∼6-7 that have been the focus of study for\nthe past 2 decades.\nThere are three di fferent physical explanations for why the\nmajority of QSOs in massive host galaxies are UV-faint rather\nthan UV-bright: (1) dust obscuration, (2) lower BH masses, and\n(3) extreme sub-Eddington accretion. We will discuss the pos-\nsibility of dust obscuration as part of a separate analysis which\nlooks in detail at rest- UVcolors (power-law slopes αλ) of the\nfainter QSOs using various near-IR ground-based imaging and\nWISE observations, but this only appears to be the case for\n∼25% of the QSOs in our [CII]-bright, but UV-faint selection\n(R.J. Bouwens et al. 2025, in prep).3Two of the clearest cases\nof dust obscuration in our CISTERN +literature selection, i.e.,\nHSCJ114632−015438 and J1205 −0000, have already been dis-\ncussed by Kato et al. (2020) and Izumi et al. (2021b), respec-\ntively.\nTo evaluate whether one of the two other potential expla-\nnations as to why most QSOs in massive host galaxies are UV\nfaint, measurements of the BH masses for the UV-faint QSOs in\nour CISTERN +archival sample are needed. Unfortunately, only\ntwo QSOs satisfying our constraints for living in a massive host\ngalaxy (i.e., L[CII]>1.8×109L⊙) have such measurements, i.e.,\nJ1205−0000 and J1243 +0100, and J1205−0000 seem most con-\nsistent with dust obscuration hypothesis.4As for J1243 +0100\nwhere MUV,AB=−24.13 mag and L[CII]=2.5×109L⊙, Mat-\nsuoka et al. (2019b) derive MBH=(3.3±2.0)×108M⊙from\nspectroscopy of Mg ii2800 Å, while inferring λEddto be equal to\n0.34±0.20. The mass of the BH in J1243 +0100 is∼10×lower\nthan is the case for UV-bright QSOs at z∼6 The accretion ef-\nficiency is also lower by 2 ×. This suggests that J1243 +0100’s\n∼10×lower BH mass appears to be the most important factor\ndriving its faintness in the rest- UVrelative to UVbright QSOs\nat z∼6.\nIf the primary driver of the variations in UVluminosity of\nQSOs is dust obscuration or extreme sub-Eddington accretion,\nthis would mean that UV-faint QSOs could just correspond to\ndifferent phases in the life time of UV-bright QSOs, as implied\nby the relatively short QSO life times measured for most z >6\nQSOs (Davies et al. 2019; Eilers et al. 2020, 2021) and halo\nmasses inferred for QSOs (Shen et al. 2007; Eilers et al. 2024;\nPizzati et al. 2024).\n4.2. QSOs in Massive Host Galaxies: How Does MBHand\nλEddfor Depend on MUV?\nFor the remaining sources, we do not yet have spectroscopy\nprobing their BH masses to ascertain whether their faintness in\ntheUVis the result of (1) their BH masses being lower than\n3Of course, this is only a preliminary estimate based on the derived\nslopesαλfrom the available near-IR and WISE imaging data. For a\nmore definitive estimate, it could be valuable to obtain more sensitive\nobservations to redder wavelengths (e.g., using JWST /MIRI imaging\nobservations) might reveal a more substantial obscured component (e.g.\nLyu et al. 2024).\n4Using photometry in WISE, Kato et al. (2020) suggest J1205 −0000\nmay be a partially reddened QSO. Applying an extinction correction\nto the results of Onoue et al. (2019), Kato et al. (2020) find MBH=\n2.9+0.3\n−0.8×109M⊙andλEdd=Lbol/LEdd=0.22+0.04\n−0.03. These MBHresults\nare consistent with what has been found for UV-bright QSOs at z∼6\n(Kurk et al. 2007; Jiang et al. 2007; Willott et al. 2010a; De Rosa et al.\n2011, 2014; Venemans et al. 2015) although the accretion e fficiency\nλEddis 2×lower, in line with lower-redshift QSO results as reported by\nShen et al. (2011).\nArticle number, page 7 of 13\n\nA&A proofs: manuscript no. faintqso\n,\nLiterature [`CII] Bright Sample with MBH MeasurementsExtrapolated\nValueMBH Distribution for Massive Host Galaxies Including Fainter QSOs \nOnly UV Bright QSOsMedian MBH \nLocal Universe\n(KH13)λEdd  = 0.01\nλEdd  = 1.0λEdd  = 0.1\nFig. 5. (left) Median measured MBH(solid black circles ) from the literature for QSOs in Massive Host Galaxies with L[CII]>1.8×109L⊙vs.\nUVluminosity. The smaller solid grey points show measured MBHvs.MUVfor the massive host galaxy sample. The MBHmeasurements for a\nMUV,AB∼−24.1 QSO at z=7.07 (Izumi et al. 2021a) is shown with the grey star. The dotted diagonal lines indicate MBH’s corresponding to\nEddington ratios λEddof 1, 0.1, and 0.01. There is a strongly suggestive correlation between the black hole masses of QSOs in massive host\ngalaxies and their UVluminosities, suggesting that UVbrightness of QSOs in massive host galaxies is largely driven by the BH mass. The open\nred circle shows the extrapolated MBHatMUV,AB∼−23.0 mag, together with the 1 σuncertainty based on the extrapolation. (right) Distribution\nofMBHin massive host galaxies at z>6 in UV-bright ( MUV,AB<−26) QSOs ( black line ) and including QSOs down to a UVluminosity of\n−22.0 mag ( red line ). The precise mass where the MBHdistribution peaks has a 1 σuncertainty of±0.4 dex ( red arrows ) and is sensitive to the\nMUV,AB=−22 cut-o ffthe QSO LF we assume and also the best-fit slope of the MBHvs.MUVrelation. The purple bracketed region indicates the\napproximate range of MBHfor QSOs in similar mass host galaxies in the local universe (e.g., Kormendy & Ho 2013: KH13). The median MBH\nfor QSOs in massive host galaxies at z∼6 appears to be consistent with the local value (see §4.3), but the uncertainties are too large to be sure.\nFurther measurements of MBHfor the UV-faint QSOs in massive host galaxies at z∼6 are required to resolve this open question.\ntheUV-bright QSO population or (2) accretion e fficienciesλEdd\nbeing lower.\nTo make sense of the dominant UV-faint QSO population in\nmassive host galaxies, it makes sense to consider those QSOs in\nmassive host galaxies (which we take to mean L[CII]>1.8×109\nL⊙) and which do have measured MBHandλEdd. 34 [CII]-\nluminous QSOs appear to have those measurements available\nand extend from−29.09 mag to−24.13 mag, with the major-\nity in the range−28<MUV,AB<−25. Results for these QSOs\nare included in Table A.1 from Appendix A. Note that these are\napproximately the same set of QSOs that we considered in §3.2\nto look for consistency in the apparent host galaxy masses vs. a\nfunction of UVluminosity.\nFor each subsample, we compute the median MBH,λEdd, and\nMUVfor sources in the selection. For our estimates of λEddfor in-\ndividual QSOs, we used the observed UVluminosities (1450Å)\nto estimate Lbolusing the equations presented by Runnoe et al.\n(2012) and then took λEdd=Lbol/LEdd. The results are shown in\nleft panels of Figures 5 and 6. In the case of MBH, we find a clear\ntrend from higher MBHto lower MBHmasses as one moves faint-\nward in UVluminosity. If we adopt a simple power-law relation-\nship in modeling in the MBHvs.MUVresults (also including the\nIzumi et al. 2021a z =7.075 source), we recover the following\nbest-fit relationship:\nlog10(MBH/M⊙)=(9.27+0.11\n−0.10)+(−0.26+0.10\n−0.11)(MUV+27) (2)\nBased on this fitting formula, the expected median MBHfor our\nsample of UV-faint QSOs in massive host galaxies is shown in\nFigure 5 with the open red circle ( left panel ).\nWe should remark that evidence for such a correlation be-\ntween the UVluminosity and MBHhas already been the subject\nof extensive discussion already in a number of earlier studies\n(e.g. Willott et al. 2010a; Izumi et al. 2018; Onoue et al. 2019;Li et al. 2022). What is distinct in the current analysis is the\nconsistent application of similar host galaxy selection criteria to\nquantify trends in UVluminosity.\nNo clear trend is evident in the accretion e fficiency log10λEdd\nvs.UVluminosity (Figure 6). Again assuming a simple power-\nlaw relationship in modeling the λEddvs.MUVresults, we find\nthe following best-fit relationship:\nlog10λEdd=(−0.21+0.09\n−0.09)+(−0.09+0.11\n−0.10)(MUV+27) (3)\nBased on this fitting formula, the expected median λEddfor our\nsample of UV-faint QSOs in massive host galaxies is shown in\nFigure 6 with the open red circle ( left panel ).\nSimilar to our discovery of a correlation between MBHand\nUVluminosity in massive host galaxies, earlier studies have also\nreported on there being a limited correlation between the accre-\ntion e fficiencyλEddof QSOs and their MUV(e.g. Willott et al.\n2010b; Farina et al. 2022) for QSOs at UVluminosities of−23\nmag and brightward. This is consistent with the idea that QSOs\nare either accreting at near their Eddington rates – as would\nbe needed to be identified as a MUV,AB<−22 QSO – or at\nextreme sub-Eddington rates (Narayan et al. 1998; Kondapally\net al. 2025) (and thus qualify as not be identified as a z >5.9\nQSO).5\n4.3. Distribution of MBHandλEddfor QSOs in Massive Host\nGalaxies at z∼6-7\nIt is interesting to use the results derived in previous section to\ntry to estimate the approximate distribution in MBHandλEddfor\nQSOs in massive host galaxies and then to compare the derived\n5Of course, for some z ∼6 QSOs, quenching might already be occur-\nring (Onoue et al. 2024).\nArticle number, page 8 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nLiterature [`CII] Bright Sample with MBH + λEdd\nMeasurementsExtrapolated\nValueDistribution for Massive Host Galaxies Including Fainter QSOs \nOnly UV Bright\nQSOs\nFig. 6. (left) Median inferred λEdd(solid black circles ) from the literature for QSOs in Massive Host Galaxies with L[CII]>1.8×109L⊙vs.UV\nluminosity. The grey points and grey star are as in Figure 5. No strong correlation is seen between the UVluminosity of QSOs in massive host\ngalaxies and their accretion e fficiencyλEdd. As such, it would appear that MBHis the dominant factor impacting the UVbrightness (or Lbol) of\nz∼6-7 QSOs and the duty cycle cycle (i.e., variations in λEddhas a more limited impact. The open red circle shows the extrapolated MBHat\n−23.0 mag, together with the 1 σuncertainty based on the extrapolation. (right) Distribution of λEddin massive host galaxies at z>6 in UV-\nbright ( MUV,AB<−26) QSOs ( black line ) and including QSOs down to a UVluminosity of−22.0 mag ( red line ). Given the large uncertainties\nin the precise dependence λEddshows on UVluminosity, the peak of the distribution is very uncertain ( ±0.4 dex: red arrows indicate the 1 σ\nuncertainties ).\ndistribution with that found in the local universe. Given the sen-\nsitive dependence the measured MBHappear to show on UVlu-\nminosity, it is possible that inclusion of fainter QSOs may have\na significant impact on the median MBHinferred for QSOs in\nmassive host galaxies.\nFor this exercise, we consider the UVluminosity distribution\nof QSOs in massive host galaxies that we present in Figure 3.\nFor each 0.1-mag bin in UVluminosity, we distribute QSOs into\ndifferent bins of MBHandλEddaccording to the empirical rela-\ntionship presented in the left panels of Figure 5 and 6. In con-\nstructing the MBHandλEddfor QSOs brighter than −25 mag, we\nmake use of the empirical distribution seen in the observations\n(Appendix A), but for QSOs with UVluminosities fainter than\n−25 mag, we shift the observed distribution of MBHandλEddto\nlower luminosities adopting the best-fit trends derived in §4.2.\nWe present the results in the right panels of Figure 5 and 6.\nThe black curves only include those UV-bright QSOs in massive\nhost galaxies, while the red curves include QSOs above a UV\nluminosity of−22 mag (where there is a reasonable sampling\nofL[CII]distribution). For context, in Figure 5, we show the ap-\nproximate range of MBH(purple bracketed region ) we would ex-\npect for QSOs in similar mass host galaxies in the local universe.\nIn deriving this range (108.1to 108.5M⊙), we make use of both\nthe Kormendy & Ho (2013) MBH-Mdynrelation and the median\ndynamical mass Neeleman et al. (2021) find for [CII]-luminous\n(L[CII]>1.8×109L⊙) QSOs, i.e.,∼4×1010M⊙.\nThe distribution in MBHwe derive for QSOs in massive host\ngalaxies at z∼6-7 peaks at a mass of ∼108.1M⊙. The distribution\ninMBHwe estimate for QSOs in massive galaxies is consistent\nwith the local relation, but a factor of 15+25\n−9lower in mass than\nfound for UV-bright QSOs (Shen et al. 2019; Yang et al. 2021;\nFarina et al. 2022), suggesting that the first UV-bright QSOs dis-\ncoveries at z∼6 (e.g. Fan et al. 2001) were simply extreme high\nmass outliers in MBHvs.Mhost,galspace.\nThere has already been significant discussion (e.g. Willott\net al. 2005; Fine et al. 2006; Lauer et al. 2007; Li et al. 2022)\nsuggesting that UV-bright QSOs might be simply high-massoutliers on the MBHvs.Mhost,galrelation and one can more re-\nliably assess evolution in this relationship by focusing on host\ngalaxies with lower-mass BHs. In particular, Willott et al. (2013,\n2015, 2017) make use of both near-IR spectroscopy and dynam-\nical information from ALMA observations for a small sample of\nUV-faint QSOs and find suggestive evidence for UV-faint QSOs\nshowing a MBHvs.Mdynrelation that is completely consistent\nwith the local universe (Kormendy & Ho 2013). Further support\nfor this idea come from observations of a larger sample of UV-\nfaint QSOs at z∼6-7 by Izumi et al. (2018), Onoue et al. (2019),\nand Izumi et al. (2021a) and now also leveraging observations\nfrom a JWST /NIRSpec program (Onoue et al. 2021; Ding et al.\n2023).\nWe find that the distribution in the accretion e fficiencyλEdd\npeaks at 10−0.6, slightly lower than the value for UV-luminous\ngalaxies but still broadly consistent, suggesting that duty cycle\nonly has a limited impact on the composition of QSO samples\nover a range of UV luminosities (but see however Wu et al.\n2022). Instead, it would appear based on our results that MBH\nis the dominant factor impacting the UVbrightness (or Lbol) of\nz∼6-7 QSOs, as has been found previously (e.g. Willott et al.\n2010a).\nWe emphasize that there are large uncertainties in the MBH\nandλEdddistributions we derive. If we take the dependence of\nMBHonMUVto be 1σhigher or lower than the best-fit trend,\nthen the peak in MBHfor the distribution shifts higher or lower\nby 0.4 dex and similarly for the peak in λEddif we propagate the\nuncertainties.\nGiven current uncertainties, it is essential that spectroscopic\ncampaigns on UV-faint QSOs at z≥6 continue, so that MBHand\nλEddcan be derived for a much larger number of QSOs. While\nprogress on this front is already being made (e.g. Ding et al.\n2023; Onoue et al. 2024) with JWST and in the near future with\nthe substantial Aether data set (Farina et al. 2024), obtaining\nthese measurements for UV-faint QSOs in massive host galaxies\nis especially valuable.\nArticle number, page 9 of 13\n\nA&A proofs: manuscript no. faintqso\nGiven that these same host galaxies may feature both UV-\nfaint and UV-bright QSOs at di fferent points in their life cycles,\nit is clear that focusing more on the UV-faint phase with JWST\nshould be a priority so as to better match the resources commit-\nted to the study of UV-bright QSOs. While some UV-faint QSOs\nwill be so due to their lower mass BHs, others will be so due to\nvariations in their accretion e fficiency or obscuration as implied\nby recent results on QSO lifetime and clustering (e.g. Eilers et al.\n2020, 2021, 2024; Pizzati et al. 2024).\n5. Summary\nWe use observations of the [CII] 158µmISM cooling line to segre-\ngate QSOs as a function of mass of the host galaxy, with the aim\nof identifying particularly massive host galaxies. The [CII] lumi-\nnosities of the QSOs, a known tracer of the molecular gas mass,\nis treated as a proxy for host galaxy mass and is used to sort\n190 QSOs at z >5.9, spanning a 6-mag range in UVluminosity\n(−28<MUV,AB<−22). Particularly valuable for this enterprise\nare the [CII] coverage from a cycle-10 program CISTERN on 46\nUV-faint ( MUV,AB>−24.5) QSOs and 25 especially UV-faint\n(MUV,AB>−23.5) QSOs, improving statistics by 5 ×and 6×, re-\nspectively.\nTaking QSOs with [CII] luminosities in excess of 1.8 ×109\nL⊙as having a massive host galaxy, 61 such QSOs are found,\nincluding 13 and 7 of which are UV-faint ( MUV,AB>−24.5)\nand especially UV-faint ( MUV,AB>−23.5). The bulk (9 /13) of\ntheUV-faint QSOs discovered to be bright in [CII] are from the\nnew CISTERN program. Remarkably, these UV-faint QSOs also\nshow similar dynamical masses MdyntoUV-bright QSOs and\nhave estimated IR luminosities only slightly fainter than UV-\nbright, [CII]-bright QSOs, strongly suggesting their host galaxy\nmasses are similar to UV-bright QSOs.\nUsing these selections and recent QSO luminosity functions\n(Schindler et al. 2023), we present the first characterization of\nUVluminosity distribution for QSOs in massive host galaxies\nand quantify [CII] LFs for both UV-bright ( MUV,AB<−26) and\nUV-faint ( MUV,AB>−24.5) QSOs. While some (3%) massive-\nhost QSOs are UV-bright, ≳85% are fainter than −24.5 mag. Of\nnote, the volume density of UV-faint ( MUV,AB>−24.5) QSOs of\na given host galaxy mass appears to be ∼29+10\n−7×larger than UV-\nbright ( MUV,AB<−26) QSOs with similar mass host galaxies.\nPossible explanations for the broad UV luminosity range\nfor QSOs with a given host mass include varying dust obscu-\nration, a wide range of accretion e fficiencies, and range of black\nhole masses. While a definitive answer will require spectroscopy,\nwe can already make a preliminary assessment of the probable\ndrivers by extrapolating published MBHandλEddresults on 34\n[CII]-luminous ( L[CII]>1.8×109L⊙), moderately UV-bright\n(−28<MUV<−25) QSOs at z>5.9 to fainter UVluminosi-\nties.\nBased on apparent trends in MBHandλEdd, we estimate a\nmedian log10MBH/M⊙∼8.1±0.4 in UV-faint QSOs in mas-\nsive host galaxies (Figure 5) while the median log10λEddis\n∼−0.6±0.4 (Figure 6). Given that >85% of QSOs in massive\nhost galaxy systems are UV-faint, this suggests that SMBHs in\nUV-bright QSOs are∼15+25\n−9×higher in mass than the more typ-\nical host galaxy system with log10MBH/M⊙∼8.1 at z∼6. These\nresults suggest that the median MBH/M∗,galaxy relation at z∼6-7\nis consistent with the local universe (Kormendy & Ho 2013).\nRecognizing the large uncertainties that exist in the extrap-\nolation of MBHandλEddresults to lower UVluminosities, it is\nessential to obtain sensitive spectroscopy for a much larger num-\nber of UV-faint QSOs at z≥6 to derive MBHandλEdd. Progressis already underway with the JWST in this area (e.g. Onoue et al.\n2021; Ding et al. 2023; Onoue et al. 2024) and with the Aether\nprogram (Farina et al. 2024), but there could be more focus on\nthose UV-faint QSOs which are hosted by particularly massive\ngalaxies. By careful comparisons between the two populations\n(UV-bright vs. UV-faint), the community will not only gain a\nbetter understanding of SMBH growth in UV-bright QSOs, but\nshould also have a comprehensive view of the full phenomenol-\nogy of this growth during the earliest phases of the universe.\nIn the near future, we expect to make even more progress\nin understanding QSO demographics by comparing the current\n[CII] LFs for UV-bright and UV-faint QSOs with [CII] LFs de-\nrived on the basis of z ∼7 galaxy samples such as for the Reion-\nization Era Bright Emission Line Survey (REBELS: Bouwens\net al. 2022). This should provide us with key information on the\nduty cycle of QSOs in massive host galaxies, which will be es-\nsential to construct an accurate model of SMBH growth in the\nearly universe. Incorporation of JWST-discovered LRD popula-\ntion in successful model would also be interesting, but as of yet,\nmost far-IR observations fail to detect [CII] or the dust contin-\nuum (Labbe et al. 2023; Akins et al. 2024; Setton et al. 2025;\nXiao et al. 2025; Casey et al. 2025).\nAcknowledgements. MA acknowledges support from ANID Basal Project\nFB210003 and and ANID MILENIO NCN2024_112. E.P.F. is supported by the\ninternational Gemini Observatory, a program of NSF NOIRLab, which is man-\naged by the Association of Universities for Research in Astronomy (AURA) un-\nder a cooperative agreement with the U.S. National Science Foundation, on be-\nhalf of the Gemini partnership of Argentina, Brazil, Canada, Chile, the Republic\nof Korea, and the United States of America. This paper is based on data ob-\ntained with the ALMA Observatory, under the program 2023.1.00443.S. ALMA\nis a partnership of ESO (representing its member states), NSF(USA) and NINS\n(Japan), together with NRC (Canada), MOST and ASIAA (Taiwan), and KASI\n(Republic of Korea), in cooperation with the Republic of Chile. The Joint ALMA\nObservatory is operated by ESO, AUI /NRAO and NAOJ.\nReferences\nAkins, H. B., Casey, C. M., Lambrides, E., et al. 2024, arXiv e-prints,\narXiv:2406.10341\nBañados, E., Venemans, B. P., Mazzucchelli, C., et al. 2018, Nature, 553, 473\nBouwens, R. J., Smit, R., Schouws, S., et al. 2022, ApJ, 931, 160\nCASA Team, Bean, B., Bhatnagar, S., et al. 2022, PASP, 134, 114501\nCasey, C. M., Akins, H. B., Finkelstein, S. L., et al. 2025, arXiv e-prints,\narXiv:2505.18873\nChehade, B., Carnall, A. C., Shanks, T., et al. 2018, MNRAS, 478, 1649\nCiesla, L., Charmandaris, V ., Georgakakis, A., et al. 2015, A&A, 576, A10\nConroy, C., Wechsler, R. H., & Kravtsov, A. V . 2006, ApJ, 647, 201\nda Cunha, E., Groves, B., Walter, F., et al. 2013, ApJ, 766, 13\nDavies, F. B., Hennawi, J. F., & Eilers, A.-C. 2019, ApJ, 884, L19\nDe Looze, I., Cormier, D., Lebouteiller, V ., et al. 2014, A&A, 568, A62\nDe Rosa, G., Decarli, R., Walter, F., et al. 2011, ApJ, 739, 56\nDe Rosa, G., Venemans, B. P., Decarli, R., et al. 2014, ApJ, 790, 145\nDecarli, R., Walter, F., Venemans, B. P., et al. 2018, ApJ, 854, 97\nDing, X., Onoue, M., Silverman, J. D., et al. 2023, Nature, 621, 51\nEilers, A.-C., Hennawi, J. F., Davies, F. B., & Simcoe, R. A. 2021, ApJ, 917, 38\nEilers, A.-C., Hennawi, J. F., Decarli, R., et al. 2020, ApJ, 900, 37\nEilers, A.-C., Mackenzie, R., Pizzati, E., et al. 2024, ApJ, 974, 275\nEndsley, R., Stark, D. P., Fan, X., et al. 2022, MNRAS, 512, 4248\nEndsley, R., Stark, D. P., Lyu, J., et al. 2023, MNRAS, 520, 4609\nFan, X., Bañados, E., & Simcoe, R. A. 2023, ARA&A, 61, 373\nFan, X., Narayanan, V . K., Lupton, R. H., et al. 2001, AJ, 122, 2833\nFan, X., Strauss, M. A., Becker, R. H., et al. 2006, AJ, 132, 117\nFan, X., Wang, F., Yang, J., et al. 2019, ApJ, 870, L11\nFarina, E. P., Arrigoni Battaia, F., Banados, E., et al. 2024, A 3D view of the\nfirst QSOs: A JWST /NIRSpec survey program, JWST Proposal. Cycle 3, ID.\n#5645\nFarina, E. P., Schindler, J.-T., Walter, F., et al. 2022, ApJ, 941, 106\nFine, S., Croom, S. M., Miller, L., et al. 2006, MNRAS, 373, 613\nFujimoto, S., Brammer, G. B., Watson, D., et al. 2022, Nature, 604, 261\nGloudemans, A. J., Duncan, K. J., Saxena, A., et al. 2022, A&A, 668, A27\nGoulding, A. D., Greene, J. E., Setton, D. J., et al. 2023, ApJ, 955, L24\nGreene, J. E., Labbe, I., Goulding, A. D., et al. 2024, ApJ, 964, 39\nArticle number, page 10 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nHaiman, Z. & Loeb, A. 2001, ApJ, 552, 459\nHarikane, Y ., Zhang, Y ., Nakajima, K., et al. 2023, ApJ, 959, 39\nHeintz, K. E., Watson, D., Oesch, P. A., Narayanan, D., & Madden, S. C. 2021,\nApJ, 922, 147\nIzumi, T., Matsuoka, Y ., Fujimoto, S., et al. 2021a, ApJ, 914, 36\nIzumi, T., Onoue, M., Matsuoka, Y ., et al. 2019, PASJ, 71, 111\nIzumi, T., Onoue, M., Matsuoka, Y ., et al. 2021b, ApJ, 908, 235\nIzumi, T., Onoue, M., Shirakata, H., et al. 2018, PASJ, 70, 36\nJiang, L., Fan, X., Vestergaard, M., et al. 2007, AJ, 134, 1150\nKaasinen, M., Venemans, B., Harrington, K. C., et al. 2024, A&A, 684, A33\nKato, N., Matsuoka, Y ., Onoue, M., et al. 2020, PASJ, 72, 84\nKirkpatrick, A., Pope, A., Sajina, A., et al. 2015, ApJ, 814, 9\nKocevski, D. D., Onoue, M., Inayoshi, K., et al. 2023, ApJ, 954, L4\nKokorev, V ., Caputi, K. I., Greene, J. E., et al. 2024, ApJ, 968, 38\nKondapally, R., Best, P. N., Duncan, K. J., et al. 2025, MNRAS, 536, 554\nKormendy, J. & Ho, L. C. 2013, ARA&A, 51, 511\nKurk, J. D., Walter, F., Fan, X., et al. 2007, ApJ, 669, 32\nLabbe, I., Greene, J. E., Bezanson, R., et al. 2023, arXiv e-prints,\narXiv:2306.07320\nLarson, R. L., Finkelstein, S. L., Kocevski, D. D., et al. 2023, ApJ, 953, L29\nLauer, T. R., Tremaine, S., Richstone, D., & Faber, S. M. 2007, ApJ, 670, 249\nLi, J., Silverman, J. D., Izumi, T., et al. 2022, ApJ, 931, L11\nLyu, J., Alberts, S., Rieke, G. H., et al. 2024, ApJ, 966, 229\nMadden, S. C., Cormier, D., Hony, S., et al. 2020, A&A, 643, A141\nMaiolino, R., Scholtz, J., Witstok, J., et al. 2024, Nature, 627, 59\nMatsuoka, Y ., Iwasawa, K., Onoue, M., et al. 2022, ApJS, 259, 18\nMatsuoka, Y ., Iwasawa, K., Onoue, M., et al. 2019a, ApJ, 883, 183\nMatsuoka, Y ., Iwasawa, K., Onoue, M., et al. 2018a, ApJS, 237, 5\nMatsuoka, Y ., Onoue, M., Iwasawa, K., et al. 2023, ApJ, 949, L42\nMatsuoka, Y ., Onoue, M., Kashikawa, N., et al. 2018b, PASJ, 70, S35\nMatsuoka, Y ., Onoue, M., Kashikawa, N., et al. 2016, ApJ, 828, 26\nMatsuoka, Y ., Onoue, M., Kashikawa, N., et al. 2019b, ApJ, 872, L2\nMatsuoka, Y ., Strauss, M. A., Kashikawa, N., et al. 2018c, ApJ, 869, 150\nMatsuoka, Y ., Strauss, M. A., Price, III, T. N., & DiDonato, M. S. 2014, ApJ,\n780, 162\nMatthee, J., Naidu, R. P., Brammer, G., et al. 2024, ApJ, 963, 129\nMazzucchelli, C., Bañados, E., Venemans, B. P., et al. 2017, ApJ, 849, 91\nMortlock, D. J., Warren, S. J., Venemans, B. P., et al. 2011, Nature, 474, 616\nNarayan, R., Mahadevan, R., & Quataert, E. 1998, in Theory of Black Hole Ac-\ncretion Disks, ed. M. A. Abramowicz, G. Björnsson, & J. E. Pringle, 148–182\nNeeleman, M., Novak, M., Venemans, B. P., et al. 2021, ApJ, 911, 141\nOke, J. B. & Gunn, J. E. 1983, ApJ, 266, 713\nOnoue, M., Ding, X., Izumi, T., et al. 2021, A Complete Census of Supermassive\nBlack Holes and Host Galaxies at z =6, JWST Proposal. Cycle 1, ID. #1967\nOnoue, M., Ding, X., Silverman, J. D., et al. 2024, arXiv e-prints,\narXiv:2409.07113\nOnoue, M., Kashikawa, N., Matsuoka, Y ., et al. 2019, ApJ, 880, 77\nPadovani, P. 2017, Frontiers in Astronomy and Space Sciences, 4, 35\nPizzati, E., Hennawi, J. F., Schaye, J., et al. 2024, MNRAS, 534, 3155\nReed, S. L., Banerji, M., Becker, G. D., et al. 2019, MNRAS, 487, 1874\nReines, A. E. & V olonteri, M. 2015, ApJ, 813, 82\nRunnoe, J. C., Brotherton, M. S., & Shang, Z. 2012, MNRAS, 422, 478\nSargsyan, L., Samsonyan, A., Lebouteiller, V ., et al. 2014, ApJ, 790, 15\nSchindler, J.-T., Bañados, E., Connor, T., et al. 2023, ApJ, 943, 67\nSchneider, R., Bianchi, S., Valiante, R., Risaliti, G., & Salvadori, S. 2015, A&A,\n579, A60\nSchramm, M., Wisotzki, L., & Jahnke, K. 2008, A&A, 478, 311\nSetton, D. J., Greene, J. E., Spilker, J. S., et al. 2025, arXiv e-prints,\narXiv:2503.02059\nShankar, F., Weinberg, D. H., Marsden, C., et al. 2020, MNRAS, 493, 1500\nShen, Y ., Richards, G. T., Strauss, M. A., et al. 2011, ApJS, 194, 45\nShen, Y ., Strauss, M. A., Oguri, M., et al. 2007, AJ, 133, 2222\nShen, Y ., Wu, J., Jiang, L., et al. 2019, ApJ, 873, 35\nStacey, G. J., Hailey-Dunsheath, S., Ferkinho ff, C., et al. 2010, ApJ, 724, 957\nUrry, C. M. & Padovani, P. 1995, PASP, 107, 803\nVale, A. & Ostriker, J. P. 2004, MNRAS, 353, 189\nVenemans, B. P., Verdoes Kleijn, G. A., Mwebaze, J., et al. 2015, MNRAS, 453,\n2259\nVenemans, B. P., Walter, F., Neeleman, M., et al. 2020, ApJ, 904, 130\nVizgan, D., Greve, T. R., Olsen, K. P., et al. 2022a, ApJ, 929, 92\nVizgan, D., Heintz, K. E., Greve, T. R., et al. 2022b, ApJ, 939, L1\nV olonteri, M., Haardt, F., & Madau, P. 2003, ApJ, 582, 559\nV olonteri, M., Habouzit, M., & Colpi, M. 2021, Nature Reviews Physics, 3, 732\nWalter, F., Riechers, D., Cox, P., et al. 2009, Nature, 457, 699\nWang, F., Yang, J., Fan, X., et al. 2024, ApJ, 968, 9\nWang, R., Wagg, J., Carilli, C. L., et al. 2013, ApJ, 773, 44\nWillott, C. J., Albert, L., Arzoumanian, D., et al. 2010a, AJ, 140, 546\nWillott, C. J., Albert, L., Arzoumanian, D., et al. 2010b, AJ, 140, 546\nWillott, C. J., Bergeron, J., & Omont, A. 2015, ApJ, 801, 123\nWillott, C. J., Bergeron, J., & Omont, A. 2017, ApJ, 850, 108\nWillott, C. J., Delorme, P., Reylé, C., et al. 2010c, AJ, 139, 906\nWillott, C. J., Omont, A., & Bergeron, J. 2013, ApJ, 770, 13\nWillott, C. J., Percival, W. J., McLure, R. J., et al. 2005, ApJ, 626, 657\nWu, J., Shen, Y ., Jiang, L., et al. 2022, MNRAS, 517, 2659\nXiao, M., Oesch, P. A., Bing, L., et al. 2025, arXiv e-prints, arXiv:2503.01945\nYang, J., Wang, F., Fan, X., et al. 2021, ApJ, 923, 262\nYue, M., Eilers, A.-C., Simcoe, R. A., et al. 2024, ApJ, 966, 176\nZanella, A., Daddi, E., Magdis, G., et al. 2018, MNRAS, 481, 1976\nArticle number, page 11 of 13\n\nA&A proofs: manuscript no. faintqso\nAppendix A: Sample of [CII]-Luminous QSOs with\nMBHMeasurements Used for Extrapolation to\nLower UVLuminosities\nAs part of this study, we determined that only a small fraction\nof the QSOs in massive host galaxies were UV-bright and the\nvast majority were UV-faint. To make a first estimate of how the\ncharacteristics of these fainter QSOs compare with the brighter\nQSOs, we made use of a selection of QSOs which showed [CII]\nluminosities in excess of 1.8 ×109L⊙(and thus appear to live in\na massive host galaxy) and which also have measurements of\nMBHfrom near-IR spectroscopy. We present the QSOs we use\nfor these purposes in Table A.1. For convenience, the QSOs pre-\nsented in this table are ordered by the inferred UVluminosity.6\n6We do not include the lower luminosity QSOs from Onoue et al.\n(2019) here due to these QSOs having [CII] luminosities below the\n1.8×109L⊙threshold for identifying massive host galaxies.\nArticle number, page 12 of 13\n\nBouwens et al.: Beneath the Surface: >85% of z>5.9 QSOs in Massive Host Galaxies are UV-Faint\nTable A.1. MBHMeasurements Available For [CII]-Luminous QSOs to Derive Trends vs. UVLuminosity\nRight MUV L[CII] FWHM [CII] fcont MBH\nQSO ID Ascension Decl (mag) z [CII]/109L⊙/(km/s)/mJy/108M⊙Ref\nJ0100 +2802 01:00:13.03 +28:02:25.8−29.09 6.327 3.8 ±0.2 405±20 1.4±0.1 97.34 [5]\nJ2310 +1855 23:10:38.88 +18:55:19.7−27.80 6.003 8.7 ±1.4 393±21 8.3±0.6 21.91 [5]\nJ025−33 01:42:43.72 −33:27:45.6−27.76 6.337 5.7 ±0.2 370±16 2.5±0.1 25.89 [1]\nJ1148 +5251 11:48:16.65 +52:51:50.4−27.56 6.419 4.2 ±0.4 287±28 1.8 52.99 [2]\nJ0706 +2921 07:06:26.38 +29:21:05.5−27.44 6.604 2.2 ±0.3 413±36 0.7±0.1 21.13 [4]\nPSOJ158−14 10:34:46.50 −14:25:15:9−27.41 6.069 11.5 780 ±27 3.5±0.1 17.04 [5]\nP036+03 02:26:01.87 +03:02:59.2−27.28 6.541 3.4 ±0.1 237±7 2.6±0.1 37.37 [5]\nJ1509−1749 15:09:41.78 −17:49:26.8−27.14 6.123 2.3 631 ±70 1.7±0.1 26.44 [5]\nP231−20 15:26:37.84 −20:50:00.9−27.14 6.587 3.5 ±0.3 393±35 4.4±0.2 40.75 [5]\nJ0038−1527 00:38:36.10 −15:27:23.6−27.13 7.034 3.2 ±0.3 339±25 1.0±0.1 13.56 [4]\nJ1319 +0950 13:19:11.29 +09:50:51.5−26.99 6.135 4.0 ±0.4 532±57 5.1±0.2 18.84 [5]\nP183+05 12:12:26.97 +05:05:33.5−26.99 6.439 7.2 ±0.3 397±19 4.8±0.2 30.08 [5]\nJ2002−3013 20:02:41.59 −30:13:21.7−26.90 6.688 2.2 ±0.2 308±36 3.2±0.2 16.22 [4]\nPSOJ011 +09 00:45:33.57 +09:01:56.9−26.85 6.470 2.2 449 ±66 1.2±0.1 6.3 [4]\nP359−06 23:56:32.45 −06:22:59.3−26.74 6.172 2.6 ±0.1 341±18 0.8±0.1 11.23 [5]\nJ1007 +2115 10:07:58.26 +21:15:29.2−26.73 7.515 2.8 ±0.2 349±56 3.0±0.1 14.32 [4]\nJ0923 +0402 09:23:47.12 +04:02:54.6−26.68 6.633 2.0 ±0.2 350±36 0.3±0.1 17.74 [4]\nJ0224−4711 02:24:26.54 −47:11:29.4−26.67 6.522 5.4 ±0.2 334±15 2.4±0.1 21.88 [3]\nJ0252−0503 02:52:16.64 −05:03:31.8−26.63 7.001 2.7 ±0.5 393±96 1.2±0.1 12.84 [4]\nJ0910−0414 09:10:54.54 −04:14:06.9−26.61 6.636 4.2 ±0.2 783±40 3.7±0.1 36 [4]\nJ0411−0907 04:11:28.63 −09:07:49.7−26.58 6.826 2.1 ±0.5 371±67 0.3±0.1 9.48 [4]\nJ2318−3029 23:18:33.10 −30:29:33.6−26.39 6.146 2.2 ±0.1 293±17 3.1±0.1 14.56 [5]\nP308−21 20:32:10.00 −21:14:02.4−26.29 6.236 3.4 ±0.2 541±32 1.2±0.1 16.79 [5]\nJ1135 +5011 11:35:08.92 +50:11:32.6−26.16 6.585 1.9 ±0.1 425±75 0.2±0.1 14.85 [4]\nJ2054−0005 20:54:06.50 −00:05:14.6−26.15 6.039 3.1 ±0.1 236±12 3.2±0.1 14.81 [5]\nP338+29 22:32:55.15 +29:30:32.2−26.14 6.658 2.0 ±0.1 740±310 1.0±0.2 30.58 [4]\nJ0305−3150 03:05:16.92 −31:50:55.8−26.13 6.614 5.9 ±0.4 225±15 5.3±0.2 5.4 [5]\nJ1048−0109 10:48:19.08 −01:09:40.4−25.96 6.676 2.1 ±0.2 299±24 2.8±0.1 22.94 [5]\nJ1058 +2930 10:58:07.72 +29:30:41.7−25.68 6.585 2.7 ±0.5 336±46 0.4±0.1 5.42 [4]\nJ0109−3047 01:09:53.14 −30:47:26.3−25.59 6.790 1.9 ±0.2 354±34 0.5±0.1 11.02 [5]\nJ0525−2406 05:25:59.68 −24:06:23.0−25.47 6.540 5.5 ±0.2 259±16 4.3±0.1 2.93 [4]\nJ0246−5219 02:46:55.90 −52:19:49.9−25.36 6.888 3.1 ±0.2 400±29 2.6±0.1 10.51 [4]\nJ0319−1008 03:19:41.66 −10:08:46.0−25.36 6.828 2.5 ±0.7 727±205 0.1±0.1 3.98 [4]\nJ0910 +1656 09:10:13.65 +16:56:30.2−25.34 6.729 1.9 ±0.3 379±50 0.2±0.1 4.09 [4]\nJ1243 +0100 12:43:53.93 +01:00:38.5−24.13 7.070 2.5 ±0.1 280±12 1.5±0.1 3.3 [6]\naReferences: [1] =Chehade et al. (2018), [2] =Shen et al. (2019), [3] =Reed et al. (2019), [4] =Yang et al. (2021), [5] =Farina\net al. (2022), [6] =Matsuoka et al. (2019b)\nArticle number, page 13 of 13\n\n",
    "source": "http://arxiv.org/abs/2506.24128v1",
    "authors": [
      "R. J. Bouwens",
      "E. Banados",
      "R. Decarli",
      "J. Hennawi",
      "D. Yang",
      "H. Algera",
      "M. Aravena",
      "E. Farina",
      "A. Gloudemans",
      "J. Hodge",
      "H. Inami",
      "J. Matthee",
      "R. Meyer",
      "R. P. Naidu",
      "P. Oesch",
      "H. J. A. Rottgering",
      "S. Schouws",
      "R. Smit",
      "M. Stefanon",
      "P. van der Werf",
      "B. Venemans",
      "F. Walter",
      "Y. Fudamoto"
    ],
    "categories": [
      "astro-ph.GA"
    ],
    "type": "content"
  },
  {
    "id": "2506.24126v1_abstract",
    "title": "Controlling the false discovery rate under a non-parametric graphical dependence model",
    "content": "Title: Controlling the false discovery rate under a non-parametric graphical dependence model\n\nAbstract: We propose sufficient conditions and computationally efficient procedures for\nfalse discovery rate control in multiple testing when the $p$-values are\nrelated by a known \\emph{dependency graph} -- meaning that we assume\nindependence of $p$-values that are not within each other's neighborhoods, but\notherwise leave the dependence unspecified. Our methods' rejection sets\ncoincide with that of the Benjamini--Hochberg (BH) procedure whenever there are\nno edges between BH rejections, and we find in simulations and a genomics data\nexample that their power approaches that of the BH procedure when there are few\nsuch edges, as is commonly the case. Because our methods ignore all hypotheses\nnot in the BH rejection set, they are computationally efficient whenever that\nset is small. Our fastest method, the IndBH procedure, typically finishes\nwithin seconds even in simulations with up to one million hypotheses.",
    "source": "http://arxiv.org/abs/2506.24126v1",
    "authors": [
      "Drew T. Nguyen",
      "William Fithian"
    ],
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24126v1_content",
    "title": "Controlling the false discovery rate under a non-parametric graphical dependence model",
    "content": "arXiv:2506.24126v1  [stat.ME]  30 Jun 2025Controlling the false discovery rate under a\nnon-parametric graphical dependence model\nDrew T. Nguyen and William Fithian\nJuly 1, 2025\nAbstract\nWe propose sufficient conditions and computationally efficient procedures for false\ndiscovery rate control in multiple testing when the p-values are related by a known\ndependency graph —meaning that we assume independence of p-values that are not\nwithin each other’s neighborhoods, but otherwise leave the dependence unspecified. Our\nmethods’ rejection sets coincide with that of the Benjamini–Hochberg (BH) procedure\nwhenever there are no edges between BH rejections, and we find in simulations and a\ngenomics data example that their power approaches that of the BH procedure when\nthere are few such edges, as is commonly the case. Because our methods ignore all\nhypotheses not in the BH rejection set, they are computationally efficient whenever\nthat set is small. Our fastest method, the IndBH procedure, typically finishes within\nseconds even in simulations with up to one million hypotheses.\n1 Introduction\nThe false discovery rate (FDR) is a popular error metric in large-scale multiple testing. Given\nindependent, or arbitrarily dependent, p-values corresponding to null hypotheses, it can be\ncontrolled at a level α∈(0,1)by the Benjamini–Hochberg (BH) procedure, though arbitrary\ndependence requires an adjustment to the αlevel known as the Benjamini–Yekutieli (BY)\ncorrection. Though there are now many ways to control FDR, the BH procedure remains the\nmost frequently used, due to its simplicity, speed, and availability in software. The impact\nof Benjamini and Hochberg (1995) is felt widely, but especially in “-omics” sciences such as\ngenomics, transcriptomics, and proteomics.\nIn all these applications, it is rare to apply the BY correction for arbitrary dependence.\nThis owes at least partly to a belief that the correction, originally derived by Benjamini and\nYekutieli (2001), makes BH over-conservative in practical settings, and so is not worth the\nsignificant loss in power. Indeed, the full BY correction is only knownto be necessary under\nspecially crafted settings involving strong, unusual correlations. Simulation studies typically\nshow that, under more realistic conditions, uncorrected BH does control FDR—for example,\nthe work of Kim and van de Wiel (2008), which is inspired by the analysis of microarrays.\nUnfortunately, finite-sample theoretical results are unavailable for many real-world depen-\ndence settings. Previous analyses have focused on a restrictive set of dependence conditions,\nsuch as positive regression dependence or parametric dependence. So the claim that BH\nis safe to use in practice is not, yet, rigorously founded—a state of affairs also remarked\nupon by Su (2018) and Chi et al. (2022), both of whom call it “unsettling”. After all, the use\nof uncorrected BH throughout the scientific literature is, implicitly, a bet on this claim of\nsafety.\nOur contribution to the literature is a way to control FDR when the analyst cannot\nassume full independence among all the p-values, but is willing to assume some of the\np-values are independent of some others. We formalize this with the notion of a dependency\ngraphfor the p-values. In practice, analysts can often safely assume a considerable degree of\n1\n\n0.20.40.60.81.0\n100 1000 10000\nmTP Ratiomethod\nBY\nIndBH3Figure 1: The expected true positive ratio, relative to BH, of our IndBH(3)method against the\nBY correction ( α= 0.1). We test the hypotheses Hi:µi= 0against two-sided alternatives\nwith X∼ N m(µ,Σ), with non-null proportion 10%and non-null means µi= 3distributed\nuniformly at random, where Σhas equicorrelated blocks ( ρ= 0.5). For every m, the blocks\nare of fixed size 100, so the dependency graph becomes sparse for large m.\nindependence, constituting a sparse dependency graph, while any leftover dependence does\nnot require the analyst to specify a parametric form. Given this graph, we propose a new\nprocedure called Independent Set BH, or IndBH. It controls FDR, reduces to BH in the\nspecial case of full independence, and with further computational effort, its power can be\nimproved.\nFigure 1 illustrates the performance of one such improvement, the IndBH(3)procedure,\nunder a dependency graph model with equicorrelated blocks of test statistics. When the\ngraph is sparse, our method has nearly the same power as the BH procedure, whereas the\nBY correction of Lm:=Pm\nj=1(1/j)is overly conservative. (This may seem unfair because\nBY is not graph-aware, but even a graph aware correction is likely an overcorrection.)\nComputationally, all of our methods at level αcan be implemented by running them at the\nadjusted level αR/mon just the p-values for hypotheses in the BH rejection set. On real\ndata, this set is often small, so this means they can scale to typical data problems with a\nvery large number of hypotheses.\nIn Section 2, we present our nonparametric dependency graph model and give sufficient\nconditions for procedures to guarantee FDR control that extend the conditions of Blanchard\nand Roquain (2008). Section 3 details our main examples of FDR controlling procedures,\nincluding several that are implemented in the R package depgraphFDR . Section 4 outlines\nour computational strategy. Section 5 illustrates our method in simulations and real data,\nand Section 6 concludes.\n1.1 Prior work.\nWe study multiple testing procedures with finite-sample FDR control using a set of m\np-values, assuming a nonparametric restriction on their dependency structure. Benjamini\nand Yekutieli (2001) initiated this line of work by studying the BH procedure, with their\nresults subsequently generalized by Sarkar (2002) and Blanchard and Roquain (2008), among\nothers. With few exceptions, however, these results typically focus only on three dependence\nconditions: independence, positive regression dependence (PRD), and arbitrary dependence,\nas originally introduced by Benjamini and Yekutieli (2001). Both independence and PRD\nare restrictive, and arbitrary dependence is overly pessimistic for most applications.\nWe study a setting which differs from these three conditions. Several other works have\nalso recently done so; Guo and Sarkar (2020) study block dependence, Chi et al. (2022) study\n2\n\nnegative dependence, and the approach of Fithian and Lei (2020) is applicable to specific\nparametric and nonparametric settings. Sarkar (2023) showed that a variant of BH controls\nFDR on two-sided p-values for multivariate Gaussian means, when the covariance matrix is\nknown up to a constant.\nOur approach is connected to concepts described by Blanchard and Roquain (2008) and\nFithian and Lei (2020), as we discuss further in Section 6.\nAsymptotic approaches provide a complementary way to study the FDR of the BH\nprocedure under weak dependence. One example is Farcomeni (2007), who show that BH\ncontrols FDR as the number of hypotheses m→ ∞, if the dependence decays quickly enough\ninm. However, it can be unclear whether the dependence is weak enough, relative to m,\nfor the asymptotic regime to apply to the data problem at hand. On the other hand, our\nprocedures can take any known dependency graph as a hyper-parameter, control FDR for\nall finite m, and reduce to BH when the dependency graph has no edges.\n2 FDR control in dependency graphs\n2.1 Multiple testing and the FDR\nThe multiple testing problem is to control a form of type I error while rejecting a subset\nS⊂ {1, . . . , m }of null hypotheses H1, . . . , H m, based on data X∼P. Generically, we take\namultiple testing procedure MT(α)to be represented by some rejection function, denoted\nRMT\nα:X7→S, where α∈[0,1]will come to denote the level of the procedure.\nLetPbe a family of distributions such that P∈ P, and each null hypothesis Hi⊊Pto\nbe a subfamily. Denote the true null hypotheses H0(P) ={i:P∈Hi}to be the subfamilies\nthat actually contain P, and let m0=|H0|be the number of true nulls.\nIn the present work, we assume that X, the data used for testing, comes in the form of\np-values p= (p1, . . . , p m)∈[0,1]m, such that piis stochastically larger than Unif[0,1], or\nsuper-uniform , whenever i∈ H 0. The procedure Ris then a set-valued function on [0,1]m.\nA notion of type I error, the false discovery rate (FDR) of a procedure MT(α), is defined\nby Benjamini and Hochberg (1995) as\nFDR P(RMT\nα) =Ep∼P\u0002\nFDP(RMT\nα(p))\u0003\nwhere FDP (RMT\nα(p)) =|RMT\nα(p)∩ H 0|\n|RMTα(p)|\nwith the convention that 0/0 = 1inside the expectation. (We omit the dependence on Pif\nit is clear from context.)\nIn multiple testing, a standard goal is to design a procedure with high power which\nsatisfies supP∈PFDR P(RMT\nα)≤αfor every fixed level α∈[0,1]or, more simply put, that\nFDR (RMT\nα)≤αwhenever the distribution of psatisfies certain assumptions encoded by P,\nsuch as mutual independence or positive regression dependence. We then say that MT(α)\ncontrols FDR at level αunder these assumptions.\n2.2 Testing in the dependency graph model\nIn this work, we formalize “local dependence” via the dependency graph model, as in previous\nwork on central limit theorems (Chen and Shao, 2004) and Hoeffding-type inequalities\n(Janson, 2004) under local dependence.\nRecall that the p-value vector phas length m, and let Dbe an undirected graph with\nnodes {1, . . . , m }. For each i, letND,i⊂ {1, . . . , m }denote the neighborhood of node iinD,\ni.e. the set of nodes with edges to i. We suppress the dependence on Dif it is clear from\ncontext. If A⊂ {1, . . . , m }, letAc={1, . . . , m } \\A.\nDefinition 2.1. Dis adependency graph for the model Pif, whenever P∈ Pand we draw\np∼P,piis independent of (pj)j∈Nc\nD,ifor every i.\n3\n\nWhen Dis a dependency graph, we call Nithedependency neighborhood of node i.\nNote that any dependency graph must trivially include all self-edges since picannot be\nindependent of itself. We generally ignore these in the exposition, and in our figures, but the\nreader may assume they are always present; in particular, the dependency neighborhood Ni\nalways includes iitself.\nThe dependency graph, used as an input to our methods, encodes known independence;\nasparsegraph says that the existing dependence is weak. Note that if D′is a dependency\ngraph for P, then any denser graph Dis also a dependency graph for the same P, where\ndenser means that the set of edges of Dcontains that of D′. This means that our methods\nachieve FDR control even when the graphs are not the sparsest possible, though power\nimproves from having a sparser graph.\nLater, we will also need the graph-theoretic notion of an independent set inD, meaning\na set I⊂ {1, . . . , m }such every distinct i, j∈Iare not connected by an edge in D, and we\nsayIismaximal if it is not strictly contained by any other independent set.\nThis setting of dependency graphs must be distinguished from the more common assump-\ntion of undirected graphical models. In a graphical model, the graph Dinstead encodes\nconditional independence, and in particular, the neighborhood Nisatisfies pi⊥ ⊥pNc\nicon-\nditionally onpNi\\{i}. Dependency graphs involve marginal independence and can be more\nrelevant in generic multiple testing, when the goal is not explicitly prediction or model\nselection.\nThese definitions imply that, when Dis a dependency graph, the p-values (pi)i∈Iare\nmutually independent whenever Iis an independent set, so the probabilistic and graph-\ntheoretic uses of “independent” coincide.\nBefore proceeding, we settle some notation to state our later results.\n2.2.1 Notation\nFirst, we define an important modification of the neighborhoods. If Dis a graph with\nneighborhoods N1, . . . , N m, let N◦\ni:=Ni\\ {i}denote the “punctured neighborhood” of\ni. This contains all nodes in i’s neighborhood except iitself. We will often mask the\np-values corresponding to N◦\niin the following sense: Let 1A: [0,1]m→[0,1]mbe the p-value\ntransformation setting the indices in Ato1, and leaving the rest unchanged, that is:\n(1Ap)i:=(\n1i∈A\npii∈Ac\nThis represents masking the entries Aof the p-value vector p. Such masking can eliminate\ndependencies between piand the rest of p, since piis independent of 1Nip. Often, we will see\nA=N◦\ni, which masks all the p-values in the dependent neighborhood of a given pi, except i\nitself.\nWe make use of the shorthands\np−i:= (p1, . . . , p i−1, pi+1, . . . , p m),\nSi:= (pj)j∈Nc\ni.\nIn particular, when Dis a dependency graph, note that piis independent of Si.\nForp-value vectors pandq, we use the symbol ⪯to mean element-wise comparison,\nmeaning p⪯qifpi≤qifor each i. For a subset B⊂[m], letD[B]be the subgraph of D\ninduced by restricting to nodes that are also in B, and if xis a vector in Rm, letxBdenote\nthe sub-vector with indices in B.\nLet2[m]be the power set of [m] :={1, . . . , m }. Throughout, Rα: [0,1]m→2[m]\nrepresents a general multiple testing procedure that takes p-values as input and returns a\nrejection set. If it is a specific multiple testing procedure, say RMT\nα, we will also refer to it as\nMT(α)For example, BH(α)is the BH procedure at level α, also written RBH\nα, and Bonf (α)\n4\n\n1\n234\n5p1’s node\np2’s nodep3’s nodep4’s node\np5’s node\nFigure 2: Some notation depicted on a dependency graph with m= 5p-values, which will\nserve as a running example in this paper. We circled the node i= 1and shaded the nodes\nforN◦\ni={2,3}. The statistic Si= (p4, p5)corresponds to nodes outside of these.\nis the Bonferroni correction, also written RBonf\nα. Occasionally, we will also use as input a\np-value vector of length different than m, which will be clear from context.\nFigure 2 depicts some of the important notation so far.\n2.3 Simple sufficient conditions for FDR control\nWith only three simple conditions, we can obtain FDR control under the dependency graph.\nWe say that Rαis aD-adapted procedure if it satisfies these conditions for all p.\n(P1) (Self-consistency.) If i∈ R α(p), then pi≤α|Rα(p)|/m.\n(P2) (Monotonicity.) If p′⪯p, then Rα(p)⊆ R α(p′).\n(P3) (Neighbor-blindness.) i∈ R α(p)⇐⇒ i∈ R α(1N◦\nip).\nProperty (P1) was first introduced in a more general form by Blanchard and Roquain\n(2008), and property (P2) has been studied for FDR control since at least Tamhane and Liu\n(2008). The condition (P3) is new, giving a clean solution to FDR control in our setting.\n2.3.1 Main results\nTheorem 1. Suppose Rα: [0,1]m→2[m]is aD-adapted multiple testing procedure, and\nthatpiis superuniform whenever i∈ H 0. Then whenever Dis a dependency graph for p, the\nprocedure Rαcontrols the FDR at level α|H0|/monp.\nEven though the result is implied by Theorem 2, we provide a self-contained proof. It\ndemonstrates how any D-adapted procedure compares each pito a threshold which depends\nonly on p-values independent of pi.\nProof.LetR=Rα. Because\nFDR =X\ni∈H0E\u00141{i∈ R(p)}\n|R(p)|\u0015\n,\nit is sufficient to show that\nE\u00141{i∈ R(p)}\n|R(p)|\u0015\n≤α/m (1)\n5\n\nfor an arbitrary i∈ H 0. Let q=1N◦\nip, which depends on ponly through piandSi, so that\nqi=pi, and q−iis a function of Sialone.\nWe first characterize the rejection event in terms of local thresholds, using all three of\nthe conditions. Observe by (P2) that there exists a function βi: [0,1]m−1→Rsatisfying\ni∈ R α(p)⇔pi≤αβi(p−i)\nm, (2)\nwhere by (P1),\nβi(p−i)≤ |R α(p)|whenever i∈ R α(p), (3)\nand by (P3),\nβi(p−i) =βi(q−i). (4)\nIn other words, βi(p−i)represents a lower bound on the number of rejections we eventually\nmake, on the event where Hiis rejected, and it depends only on the other p-values p−i. In\nfact,(4)implies that βi(p−i)depends only on Si, the p-valuesoutside of i’s dependency\nneighborhood , and is therefore independent of pi.\nAs a result, we have that for i∈ H 0,\nE\u00141{i∈ R(p)}\n|R(p)|\u0015\n≤E\u00141{i∈ R(p)}\n|R(q)|\u0015\n(P2)\n≤E\"\n1{pi≤αβi(q−i)\u000e\nm}\nβi(q−i)#\nEqs. (2) −(4)\n=E\"\nE\"\n1{pi≤αβi(q−i)\u000e\nm}\nβi(q−i)|Si##\n(tower rule)\n≤E\"\nαβi(q−i)\u000e\nm\nβi(q−i)#\n(super-uniformity)\n=α/m.\nNext, recall from the multiple testing literature that pispositive regression dependent\n(PRD) on a set A⊂[m]if for any increasing setK⊂Rm, meaning one such that p∈K, p⪯\np′implies p′∈K, the probability P(p∈K|pi=t)is increasing in twhenever i∈A.\nWe say that pispartially positive regression dependent (PPRD) on Awith respect to\nthe graph Dif, for any increasing set K⊂R|Nc\ni|, the probability P(pNc\ni∈K|pi=t)is\nincreasing in twhenever i∈A, a strictly weaker assumption than PRD for any D. Now we\ncan strengthen the result of Theorem 1.\nTheorem 2. Suppose Rα: [0,1]m→2[m]is aD-adapted multiple testing procedure, and\nthatpiis superuniform whenever i∈ H 0. Then whenever the input vector pis PPRD on H0\nwith respect to D, the procedure Rαcontrols the FDR at level α|H0|/m.\nThese two results generalize a standard pair of results from the literature, as discovered by\nBenjamini and Yekutieli (2001) and generalized by Blanchard and Roquain (2008): namely,\nthat the BH procedure controls FDR under independence and positive regression dependence.\nThe proof is deferred to Appendix A, using a related leave-one-out technique based on\nthe superuniformity lemma of Ramdas et al. (2019).\n2.3.2 Remarks\nDependency graphs in practice. Given a vector of p-values, a practitioner can often\nidentify a dependency graph by simply identifying, for each pi, which pj’s are independent\nofpi. For example, in spatiotemporal settings, independent p-values are distant in space\n6\n\nor time. A particularly useful special case is block dependence , when there exist blocks\nofp-values with mutual independence between but not necessarily within blocks, see e.g.\nassumption 2(a) of Ignatiadis and Huber (2021), which is equivalent to a dependency graph\nDwhose connected components are all cliques. Such blocks arise in genome-wide testing,\ncorresponding to p-values from different chromosomes, or microplate assays, corresponding\ntop-values from different plates.\nA BY correction for the graph? A seemingly natural approach would be to find the\nworst-case FDR of the BH(α)procedure under a dependency graph, and use this to correct\ntheα-level of the BH procedure. But we find that the bounds we are able to derive are,\nlike BY, overly conservative, while our preferred methods typically deliver finite-sample\nguarantees with good power. A tight estimate would nevertheless be of independent interest.\nDefine PDbe the family of distributions Pfor which Dis a dependency graph, and let the\nworst-case FDR be\nγD(α) := sup\b\nFDR P(RBH\nα) :P∈ PD\t\n. (5)\nWe do not compute γD(α), but in Appendix D can be found upper and lower bounds on it,\naccompanied by some discussion.\nLocal thresholds. Equation (2)says that any monotone procedure can be viewed as a\nthresholding procedure on every p-value separately. Together with (3)and(4), we see that\nD-adapted procedures simply “reject Hiwhenever pi≤ci(Si)”, for a certain function ci. We\ncallci(Si) =αβi(p−i)/mthe procedure’s ith local threshold, where βi(p−i)only depends\nonpithrough Si=pNc\niby(4). We also call βi(p−i)the procedure’s ithrejection lower\nbound(r.l.b.), as it satisfies equation (3). This local threshold representation leads to an\neffective computational strategy for our procedures (see Section 4).\nSelf-consistency gap. The bound (3)can be quite loose, especially if the r.l.b. βi(p−i)\ntends to be far from |Rα(p)|. In that case, the underlying procedure is conservative. The\ngap in(3)can be interpreted as a “self-consistency gap”—the looseness with which the\nself-consistency criterion is satisfied. This gap is some sense optimizable, allowing us to\nincrease the local threshold and mitigate the conservatism (see Section 3.3).\n2.4 Parametric examples\nThough assumptions on dependency graphs can often be made without a specific parametric\nmodel for the p-values, a few parametric models nevertheless fall naturally into our framework,\nas we now describe.\nExample 2.1 (Multivariate z-statistics) .IfZ∼ N m(µ,Σ)is a multivariate normal random\nvector of length m, then if Σis known, then the graph Dgiven by Ni={j: Σij̸= 0}is\na dependency graph for Z, and therefore also for one- or two-sided p-values based on the\ncoordinates of Z. Additionally, Zis PPRD with respect to the graph with neighborhoods\nNi={j: Σij<0}, and so are one-sided p-values based on its coordinates.\nExample 2.2 (Multivariate t-statistics) .Now suppose that Z∼ N m(µ, σ2Ψ), where Ψ\nis known and σ2is not, but we can use the independent random variable νˆσ2∼σ2χ2\nνto\nestimate it. Then for Ti=Zi/p\nΨi,iˆσ2, we justify the following result in Appendix A.\nProposition 1. LetDbe the graph with neighborhoods Ni={j: Ψij̸= 0}fori= 1, . . . , m.\nThen the test statistics T2= (T2\n1, . . . , T2\nm), are PPRD on [m]with respect to D.\nConsequently, two-sided p-values based on the coordinates of T2are PPRD with respect\ntoD.\n7\n\nExample 2.3 (Linear models) .The Gaussian linear model for a fixed design matrix\nX∈Rn×misaspecialcaseofthepreviousexample. Weobservearesponse y∼ N n(Xβ, σ2Im)\nwhere β∈Rm, σ2>0are both unknown, and supposing that Xsatisfies n > mwith full\ncolumn rank, we compute the ordinary least squares estimate and residual sum of squares\nˆβ= (X⊤X)−1X⊤y∼ N m(β, σ2(X⊤X)−1),and (n−m)ˆσ2=RSS=∥y−Xˆβ∥2∼σ2χ2\nn−m.\nThen using the previous example, we know that for Ti=ˆβi/q\n(X⊤X)−1\ni,iˆσ2, the test statistic\nT2is PPRD on [m]with respect to the graph defined by Ni={j: (X⊤X)−1\ni,j= 0}. This\nmay be reasonable in an approximate sense; when the rows of Xare iid with finite second\nmoment, the entries of (X⊤X)−1go to zero as ngrows.\n3 Graph-adapted procedures\nTheorem1guaranteeFDRcontrolforproceduresadaptedtotheunderlyingdependencygraph.\nThis section presents a family of graph-adapted procedures that trade off statistical power\nwith computational efficiency. Note that, while we focus our exposition on independence\nrelations, any graph-adapted procedure also controls FDR under partial positive dependence\nby Theorem 2.\nBefore presenting our first procedure, we recall some preliminaries. Recall that the BH\nprocedure at level α, which we refer to as BH(α), returns a rejection set according to the\nfollowing rule: Let p(1)≤ ··· ≤ p(m)be the p-value order statistics. Then\nRBH\nα(p) :={i:pi≤αr∗/m},where r∗:= max {r:p(r)≤αr/m}. (6)\nThe Bonferroni correction at level α, abbreviated Bonf (α), is simply\nRBonf\nα(p) :={i:pi≤α/m}.\nThese are in fact D-adapted procedures, but for special choices of the graph D:BH(α)is\nadapted (only) for the empty graph where Ni={i}for all i, andBonf(α)is adapted to any\ngraph, but in particular the complete graph where Ni={1, . . . , m }for all i. Indeed, each of\nthe procedures below will reduce to BH when Dis the empty graph, and to Bonferroni when\nDis the complete graph.\nBetween these two extremes, we will introduce several graph-adapted procedures for a\ngeneral graph D. A first idea would be to recall, from the theory of multiple testing, how we\ncan express BH via the local thresholds\nRBH\nα(p) ={i:pi≤α|RBH\nα(pi←0)|/m}.\nwhere pi←0is equal to pexcept with pireplaced by 0. Then, given a dependency graph D,\nwe could adjust the BH procedure’s local threshold for piby masking the p-values in its\nneighborhood, leading to the method:\nRnaiv Dα(p) ={i:pi≤α|RBH\nα(1N◦\nipi←0)|/m}.\nIn calculating pi’s threshold, recall that the operator 1N◦\nimasks all the p-values in pi’s\nneighborhood Ni, except for piitself, which makes the local threshold independent of pi—a\nproperty that BH has under full independence.\nThis procedure interpolates between BHand Bonferroni, as we wished, but unfortunately,\nit is not self-consistent according to (P1). Even worse, it does not control FDR, as the\nfollowing proposition shows (see Appendix E.1 for the proof and further intuition).\nProposition 2. There exists a distribution Ponm= 3p-values with dependency graph D\nsuch that FDR P(Rnaiv D)> α.\n8\n\nThis does not mean that our sufficient conditions are necessary, and in fact they are not\n(see Appendix E.2). However, it does demonstrate that simply modifying BH may not lead\nto FDR control—new ideas will be necessary. For us, this will be the graph-theoretic notion\nof independent sets.\nWe now move to our first procedure satisfying the sufficient conditions of Section 2.\n3.1 IndBH\nRecall that an independent set in Dis a subset I⊂[m]of nodes, where no two distinct\nnodes are connected. Let Ind(D)be the collection of independent sets in D.\nThe Independent Set BH procedure (at level α, for graph D), orIndBH D(α), simply\nre-runs BH on a masked p-value vector for every independent set in Dand unions the result.\nFormally, it is defined by\nRIndBH Dα (p) :=[\nI∈Ind(D)RBH\nα(1Icp), (7)\nand we suppress Din the notation if it is clear from context. This controls FDR, as we show\nnext.\nProposition 3. TheIndBH D(α)procedure is D-adapted, and hence controls FDR at level α\nwhenDis a dependency graph for p.\nProof.First observe that, for any independent set I⊆D, the procedure RBH\nα(1Icp)is\nD-adapted. Thus, it suffices to show that a union of D-adapted procedures is D-adapted. To\nthis end, let R(1)\nαandR(2)\nαbeD-adapted, and let Run\nα(p) =R(1)\nα(p)∪ R(2)\nα(p). We will show\nRun\nαisD-adapted.\nMonotonicity and neighborblindness of Run\nα(p)can be seen directly, while self-consistency\nholds because\ni∈ Run\nα(p)⇒pi≤α\nmmax{|R(1)\nα(p)|,|R(2)\nα(p)|} ⇒ pi≤α\nm|Run\nα(p)|.\nwhere the first implication is by self-consistency of both R(1)\nαandR(2)\nα. Hence Run\nαis\nD-adapted.\nNote that the collection of all independent sets IndBH (D)can be replaced with the\ncollection of maximal independent sets only without changing the procedure. Computing\nmaximal independent sets is NP-hard, but as mentioned in the Introduction, we can ignore\nallp-values not in the BH rejection set, so on real data, this problem is typically tractable,\nwith further computational tricks described in Section 4.\nLet us now understand this method intuitively. When the graph Dis sparse, the (maximal)\nindependent sets can be large, making this procedure powerful. When Dis completely sparse,\ni.e. the empty graph, then [m]is the only maximal independent set, recovering BH(α), and\nwhen Dis completely dense, i.e. the complete graph, the only independent sets are the\nsingletons ({i})m\ni=1, recovering Bonf (α).\nWhen working by hand, a slightly different way to think about the rejection set can\nbe useful. Let us say a non-empty independent set Cis acertificate set for IndBH if\npj≤α|C|/mfor all j∈C. This holds if and only if C⊂ RBH\nα(1Icp)for some I∈Ind(D), so\nthat equivalently, IndBH (α)rejects Hiwhenever there exists a certificate set Ccontaining i.\nFor small graphs, we can go node by node and look for such a Cto compute rejections.\nAs an example, consider the p-values and dependency graph in Figure 3 with m= 5\np-values. When running IndBH (α)with α= 0.05, there are three certificate sets, namely\n{1,4},{2,4}, and{3}. These “certify” the rejection of H1, . . . , H 4, but H5remains unrejected\n(even though it would be rejected by BH(α)). Ifp3were increased beyond 0.01,H3would\nnot be rejected either, as {3}would no longer be a certificate set.\n9\n\n1\n234\n5p1= 0.02\np2= 0.02p3= 0.01p4= 0.02\np5= 0.04\nFigure 3: An example realization of m= 5 p-values on the graph from Figure 2. If\nIndBH (0.05)were run with this graph and p-values, then it would reject H1, . . . H 4, while\nBH(0.05)would reject everything.\nCould there be some way to reject H5? Computing the certificate sets {1,4}and{2,4}\ndid not depend on any p-values from p5’s neighborhood, so perhaps we can leverage these\nsets together, rather than separately, to reject H5while still retaining neighbor-blindness.\nWe discuss this next.\n3.2 Improving IndBH\nWe now describe ways to improve IndBHwith additional computation. Most important is\nSection 3.2.1, which describes an approach we can run in practice. Section 3.2.2 characterizes\nthe optimal D-adapted procedure, which we do not run in practice. All proofs are deferred\nto Section 3.3.\n3.2.1 IndBH(k)\nFork≥1, we define the IndBH(k)(α)procedure by the rejection set\nRIndBH(1)\nα (p) :=RIndBH\nα (p),RIndBH(k+1)\nα (p) :=(\ni:pi≤α|{i} ∪ RIndBH(k)\nα (1N◦\nip)|\nm)\n.(8)\nIndBH(k+1)amounts to re-running IndBH(k)on a masked p-value vector (to enforce neigh-\nborblindness), and plugging the results into to a new threshold for pi. It controls FDR, and\nalways improves over IndBH(k)by the following proposition.\nProposition 4. TheIndBH(k)\nD(α)procedure is D-adapted for k≥2, and hence controls FDR\nat level αwhenDis a dependency graph for p. Also, we have RIndBH(k−1)\nα (p)⊂ RIndBH(k)\nα (p).\nProof.A direct consequence of Theorem 3 from Section 3.3.\nAt the bottom of the recursion, IndBH(k)could make exponentially many calls to IndBH\naskincreases, in addition to the complexity already inherent in IndBHasmincreases. This\nlimits our ability to run IndBH(k)for large k. In our simulations, we found that running\nIndBH(3)targets a nice tradeoff between power and computational effort.\nIn Figure 4, we update Figure 3 to show how H5can be rejected by IndBH(2), despite\nbeing not rejected by IndBH.\n10\n\n1\n234\n5p1= 0.02\np2= 0.02p3= 0.01\n(masked)p4= 0.02\np5= 0.04\nFigure 4: The same graph and p-values from Figure 3, but circling i= 5, with its punctured\nneighborhood N◦\n5={3}shaded. Even after masking p3(i.e. replacing it with 1), we still\nhave the certificate sets {1,4}and{2,4}, soRIndBH\nα (1N◦\n5p) ={1,2,4}atα= 0.05. As a\nresult, the iterated procedure IndBH(2)(α)rejects H5, because p5≤α|{1,2,4,5}|/m= 0.04.\n3.2.2 The optimal procedure\nIt turns out that IndBH(k)isnotthe optimal D-adapted procedure for any k, even when\ntaking k→ ∞, as we show in Appendix E.3. (Here, “optimal” means “most liberal”). Instead,\nconsider the iterates\nR(1)\nα=RBH\nα,R(k+1)\nα (p) :=(\ni:pi≤α|{i} ∪ R(k)\nα(1N◦\nip)|\nm)\n. (9)\nThis is the same iteration used forIndBH(k), just with a different starting point. Now, define\ntheD-adapted step-up procedure SUD(α)to be the fixed point of this iteration, that is,\nRSUDα(p) :=R(k∗)\nα(p),whenever k∗satisfies R(k∗)\nα(p) =R(k∗+1)\nα (p).\nWe do not use the name “step-up” to imply “stepping up” from the least significant p-value\nto the most significant as, for example, Romano and Shaikh (2006) use the term. Our intent\nis to align with how Blanchard and Roquain (2008) use it: they mean the most liberal of all\nself-consistent procedures, and we mean the most liberal of all D-adapted procedures.\nThe next proposition, whose proof we again defer to Section 3.3, says that such a k∗\nalways exists and that SU Dis optimal.\nProposition 5. There exists some k∗≥1such that R(k∗)\nα(p) =R(k∗+1)\nα (p). TheSUD(α)\nprocedure is then D-adapted and optimal in the sense that RSUDα(p)⊃ RD\nα(p)where RD\nα(p)is\nany other D-adapted procedure.\nProof.A direct consequence of Theorem 3 from Section 3.3.\nLetting R(k)be defined as in the iterations (9), and letting RIndBH(∞)\nDα (p)be the fixed\npoint of the iterations (8), we hence have the following relations between the procedures\ndefined thus far:\nRIndBH Dα (p)⊆ RIndBH(2)\nDα (p)⊆ ··· ⊆ RIndBH(∞)\nDα (p)⊆ RSUDα(p),and\nRSUDα(p)⊆ ··· ⊆ R(2)\nα(p)⊆ R(1)\nα(p) =RBH\nα(p).\nThe first line of procedures are all FDR controlling under D. Note that the inclusion\nRIndBH(∞)\nDα (p)⊆ RSUDα(p)can be strict.\n11\n\nWe do not implement SUD, as there can always be cases where k∗is too large to be\nreasonably computed, and stopping the iteration early does not yield a D-adapted procedure.\nBut for any k≥2one can always cap the number of iterations to be at most kand still\nobtain FDR control from by randomly pruning the set R(k)(p), an idea which comes from\nFithian and Lei (2020). In light of our empirical observation that the non-randomized\nprocedure IndBH(3)(α)seems to perform nearly as well as BH(α)in practical settings, we\ndo not recommend using a randomized procedure; we refer the reader to Appendix C for\ndetails on the randomized approach.\n3.3 The gap chasing update\nIn this section, we do not propose new methods. Instead, we prove and discuss the results\non the shared iteration in (8) and (9), which we call the “gap chasing” update.\nTheorem 3 (Gap chasing) .For some initial monotone multiple testing procedure R(1)\nα,\nconsider the iterates\nR(k+1)\nα (p) :=(\ni:pi≤α|{i} ∪ R(k)\nα(1N◦\nip)|\nm)\n. (10)\nfor some dependency graph Dand its neighborhoods Ni, and let RD\nαbe any D-adapted\nprocedure.\n(a) When R(1)\nα=RD\nαthen for k≥1,R(k)\nα(p)isD-adapted, and R(k)\nα(p)⊂ R(k+1)\nα (p).\n(b)When R(1)\nα=RBH\nα, then for k≥1,R(k)\nα(p)⊃ RD\nα, and R(k)\nα(p)⊃ R(k+1)\nα(p). Also,\nthere exists k∗≥2such that R(k∗)\nα(p) =R(k∗+1)\nα (p), andR(k∗)\nα(p)isD-adapted.\nProof.(a).It suffices to prove the result for k= 1. LetRα:=R(1)\nαandR+\nα:=R(2)\nα. First,\nobserve that because Rαis monotone, R+\nαis also monotone, and also R+\nαis neighborblind\nby construction. Next, note that, by neighbor-blindness and self-consistency of Rα, we have\ni∈ R α(p)⇔i∈ R α(1N◦\nip)\n⇒pi≤α|Rα(1N◦\nip)|/m\n⇒pi≤α|{i} ∪ R α(1N◦\nip)|/m,\nimplying that\nRα(p)⊂ R+\nα(p). (11)\nWe can use this to show the self-consistency:\ni∈ R+\nα(p)⇔pi≤α|{i} ∪ R α(1N◦\nip)|/m,andi∈ R+\nα(p)\n⇒pi≤α|{i} ∪ R+\nα(1N◦\nip)|/m,andi∈ R+\nα(p)\n⇔pi≤α|R+\nα(1N◦\nip)|/m,andi∈ R+\nα(p).\nThis shows the first claim, and the second claim was established in (11).\n(b). First, suppose for induction that RD\nα(p)⊂ R(k)\nα(p). Then we have\nRD\nα(p)⊂(\ni:pi≤α|{i} ∪ RD\nα(1N◦\nip)|\nm)\n⊂(\ni:pi≤α|{i} ∪ R(k)\nα(1N◦\nip)|\nm)\n=R(k+1)\nα (p).\nBy self-consistency of RDand BH, the base case RD\nα(p)⊂ R(1)\nα(p)holds, showing the first\nclaim.\n12\n\nNext, suppose for induction that R(k−1)\nα (p)⊃ R(k)\nα(p). Then\nR(k)\nα(p) =(\ni:pi≤α|{i} ∪ R(k−1)\nα (1N◦\nip)|\nm)\n⊃(\ni:pi≤α|{i} ∪ R(k)\nα(1N◦\nip)|\nm)\n=R(k+1)\nα (p).\nThe base case holds by monotonicity of BH, showing the second claim.\nFinally, because R(k)\nα(p)⊃ R(k+1)\nα (p)for all kand all sets are finite, there must be a k∗\nsuch that R(k∗)\nα(p) =R(k∗+1)\nα (p). For every k≥2, the iterates R(k)\nαare neighbor-blind by\ndefinition and monotone by induction, and whenever i∈ R(k∗)\nα(p),\nR(k∗)\nα(p) =(\ni:pi≤α|{i} ∪ R(k∗)\nα(1N◦\nip)|\nm)\n⊂(\ni:pi≤α|R(k∗)\nα(p)|\nm)\nsoR(k∗)\nαisD-adapted, showing the last claim.\nIntuitively, the gap chasing update leading to R(k+1)\nαin(10)enforces neighborblindness\nby masking the p-value vector, while reducing the “self-consistency gap” of R(k)\nαdescribed in\nSection 2.3.2. In the case of full independence (empty dependency graph), the fixed point\nR(k∗)\nαof the iteration satisfies\nR(k∗)\nα(p) =(\ni:pi≤α|{i} ∪ R(k∗)\nα(p)|\nm)\n=(\ni:pi≤α|R(k∗)\nα(p)|\nm)\n.\nHence property (P1) is satisfied with logical equivalence, and the self-consistency gap has\nbeen removed. If R(1)\nα=RBonf\nα, the fixed point under full independence is\nRBH−\nα(p) :={i:pi≤αr∗/m},where r∗:= max {r:p(k)≤αk/m ∀k≤r},(12)\nthe “step-down” variant of the BH procedure, defined by e.g. Finner and Roters (2001).\nFinally, we briefly note that Theorem 3(a) is a consequence of general closure properties\nforD-adapted procedures. An investigation of these is relegated to Appendix F.\n4 Computation\nIn this section, we summarize our computational strategy: full details appear in Appendix B.\nWe focus mostly on the computation of IndBH, sinceIndBH(k)may call IndBHrecursively\nmany times.\nComputing IndBHinvolves listing the maximal independent sets of graphs, which is\nNP-hard (because it solves the clique decision problem, which is NP complete). Unfortunately,\nwe do need the exactresults—approximations do not suffice for FDR control. Though this\nmay seem intractable, we have already mentioned that IndBHcan ignore all hypotheses\noutside the BH rejection set. With the additional tweaks in this section, IndBH,IndBH(2),\nand even IndBH(3)seem to be tractable on real data. They ultimately rely on off-the-shelf\nimplementations of the independent set routines, but we call them only when needed.\nWe now settle some notation for this section. Take LargestInd (K)to denote anylargest\nindependent in a graph K—for us, it does not matter which—and let us define the sublevel\nsetQ(r), and a certain independent set Ii(r)that contains i:\nQ(r) ={j:pj≤αr/m},and (13)\nIi(r) ={i} ∪LargestInd (D[Q−i(r)])where Q−i(r) ={j /∈Ni:pj≤αr/m},(14)\nsuppressing dependence on the p-values and α. This just says that Q(r)is that index set\nwith p-values smaller than αr/m, and Ii(r)is the largest independent set containing iamong\nthe graph nodes Q(r).\n13\n\nAs an important preliminary, recall that any D-adapted procedure has a local threshold\nrepresentation as described in Section 2.3.2. In the case of IndBH, we can write the following\nproposition, proved in Appendix A. Recall that D[B]is the induced subgraph of Dfrom\nnodes B⊂[m].\nProposition 6. IndBH (α)rejects Hiif and only if pi≤αβIndBH\nα,i (p−i)/m, where\nβIndBH\nα,i (p−i) := max {r:|Ii(r)| ≥r}.\nThis representation directly associates IndBH with a computational strategy: for every i,\ncompute Ii(r)for all rusing an algorithm that implements LargestInd on a subgraph. By\ntesting on each Hiseparately, this strategy is also parallelizable (though we did not use the\nparallelized versions to measure running time). We will refine this strategy in the ensuing\nsections.\n4.1 Reducing the graph size\nOur first major computational saving is to discard all the p-values piwhere Hiwas not\nrejected by BH, reducing to the graph D[RBH\nα(p)]and its p-values. Running IndBH with\nthis graph and p-values, at the adjusted level α|RBH\nα(p)|/m, gives the same rejection set.\nEssentially, this is possible because IndBH is a union of BH rejection sets, and BH ignores\nallp-values not in its rejection set.\nIn practice, it is common that most hypotheses are null—a sparse signal pattern. In this\ncase,RBH\nα(p)makes very few rejections relative to m, allowing us to drastically shrink the\nsize of the problem. We now state a formal and slightly more general version of this claim.\nProposition 7. Suppose we have ¯rsuch that whenever pi> α¯r/m, theIndBH(k)\nD(α)\nprocedure fails to reject Hi. ThenIndBH(k)\nD(α)run on the p-values (pi)m\ni=1is equivalent\ntoIndBH(k)\nD′(α′)run on (pi)i∈Q(¯r), using the subgraph D′=D[Q(¯r)]and adjusted level\nα′=α|Q(¯r)|/m. In particular, this holds for ¯r=|RBH\nα(p)|, which has Q(¯r) =RBH\nα(p).\nBecause of this reduction, the size of the BH rejection set RBH\nα(p)is more important than\nthe number of hypotheses mfor the computational efficiency of our methods. For example,\nlet us perturb slightly the setting of Figure 1, which tests the hypotheses Hi:µi= 0under\nGaussian block dependence, where the blocks have equal size 100 and µi= 3for every\nnon-null Hi. We will check the effect of the size of RBH\nα(p)on the runtime.\nSuppose there are 10% randomly placed non-nulls among m= 2·105hypotheses. Then on\nan M1 Macbook Pro, our implementation of IndBH (0.1)runs in about a second on average,\nandIndBH(3)(0.1)in about 7 minutes. On the other hand, if the non-null percentage is only\n1%, then even when m= 106,IndBH (0.1)can run in about a second, but IndBH(3)(0.1)\ntakes only about five seconds. In the latter case, the BHrejection set is small, and our\nmethods see the accompanying speedup.\nBy Proposition 7, any time the input graph Dis mentioned from this point, the reader\ncan safely substitute instead the subgraph D′=D[RBH\nα(p)]and its corresponding p-values,\nas long as the subsitutions m′=|RBH\nα(p)|andα′=αm′/mare also made for mandα.\n4.2 Speedups based on connected components\nOur second major computational saving comes from the fact that, especially after the\nreduction of Section 4.1, the resulting graph has many small connected components.\nLetD1, . . .D¯kbe all ¯kconnected components of D. In this section, we show how to\ncombine computations done separately on each of these connected components.\n14\n\n4.2.1 Caching independence numbers from each component\nLetκ[i]be the index for the component such that Dκ[i]contains ias a node. Crucially,\nbecause Ii(r)is a largest independent set, we can write it as a union of largest independent\nsets from each of its component graphs:\nIi(r) ={i} ∪LargestInd (Dκ[i][Q−i(r)])∪[\nk̸=κ[i]LargestInd (Dk[Q(r)]),\nfrom which it follows that\n|Ii(r)|= 1 + IndNum (Dκ[i][Q−i(r)]) +X\nk̸=κ[i]IndNum (Dk[Q(r)]),\nwhere IndNum (K)denotes the size of the largest independent set of a graph K, called its\nindependence number . Computing independence numbers is NP-hard with respect to graph\nsize, though it is more efficient with small connected components.\nImportantly, observe that the terms IndNum (Dk[Q(r)])in the summation do notdepend\non the hypothesis i∈Q(¯r), so those values can be computed once, cached, and reused for\nevery i. Details on the caching can be found in Appendix B.\nWe still may need to compute IndNum (Dκ[i][Q−i(·)])for each i, but not always, as\ndescribed next.\n4.2.2 Cheap checks from the components\nWe can avoid the computation of IndNum (Dκ[i][Q−i(·)])most of the time. Assuming we have\nthe cached values IndNum (Dk[Q(r)]) := Vk,r, define\nβ+= max\n\nr≤¯r:¯kX\nk=1Vk,r≥r\n\n,and\nβ−= max\n\nr≤¯r:¯kX\nk=1Vk,r−max\nk′Vk,r+ 1≥r\n\n,\nwhich satisfy β+≥βIndBH\nα,i (p−i)andβ−≤βIndBH\nα,i (p−i). It follows that IndBHcan reject Hi\nifpi≤αβ−/mand fails to reject Hiifpi> αβ+/m. These two checks can be done on all\nhypotheses at once, and takes care of most of them.\nFurther details on the checks we use can be found in Appendix B.\n4.2.3 When the components are fully connected\nIfDkis fully connected, then IndNum (Dk[Q(r)])is easy to compute; it is equal to 1if\nthe minimum p-value in Dkis below αr/m. Additionally, IndNum (Dκ[i][Q−i(r)])becomes\nidentically zero, since the graph Dκ[i][Q−i(r)]has no nodes.\nWhen the graph Dencodes block dependence, every Dkis fully connected, and we have\nthe following proposition, proven in Appendix A.\nProposition 8. Suppose blocks B1, . . . , B ¯k⊂[m]form a disjoint partition of [m], and that\nthe neighborhoods (Ni)m\ni=1of a graph Dsatisfy Ni=Bkwhenever i∈Bk. Defining\nj∗\nk= argmin\nj∈Bkpjand M=¯k[\nk=1{j∗\nk},\nwe have\nRIndBH Dα (p) ={i:pi≤α\f\fRBH\nα(1Mcp)\f\f/m}.\n15\n\nUnder block dependence, this says that we can compute IndBH almost as efficiently as\nBH itself. This form of IndBH was first discovered by Guo and Sarkar (2020), who proved\nits FDR control in this special case. For us, we note that block dependence is not strictly\nrequired to apply this computational shortcut; as long as the components of the graph are\ncliques after the reduction of Section 4.1, we can apply Proposition 8.\nWhen the components of the graph are not cliques, we could treat them as cliques to\ncheaply compute a large subset of the IndBH rejection set. Though the checks in Section\n4.2.2 are better for this purpose, they are more expensive, so we use this shortcut to help\ncompute IndBH(k)as explained in Appendix B.\n4.3 Considerations for IndBH(2)\nNow we briefly consider computation for IndBH(2). Similar commentary applies to IndBH(k)\nwhere k >2.\nRecall that IndBH(2)(α)rejects Hiif and only if pi≤α|{i} ∪ RIndBH\nα (1N◦\nip)|/m. This\nnaively requires us to compute RIndBH\nα (1N◦\nip)for every i, but we can skip this calculation\nfor most iby making the following observations.\nFirst, we can compute |RIndBH\nα (p)|once and check whether i∈ RIndBH\nα (p). If it is, then we\nimmediately know that i∈ RIndBH(2)\nα (p)by Proposition 4. Also, if pi> α|{i}∪RIndBH\nα (p)|/m,\nwe immediately know that i /∈ RIndBH(2)\nα (p), by monotonicity of RIndBH\nα.\nFor the remaining hypotheses where we must compute RIndBH\nα (1N◦\nip), we do not need\nto redo all the independent set calculations from RIndBH\nα (p)—most of the intermediate\ncomputations cached in Section 4.2.1 can be reused. Again, full details are deferred to\nAppendix B.\n5 Experiments\nIn this section, we have selected four experimental settings to illustrate one of our main\npoints: even though IndBH and its improvements are always less powerful than BH, they\ntend to be about as powerful when the dependency graph is sparse, while still provably\ncontrolling FDR. Each of the first three simulation settings uses a family of dependency\nstructures which gets sparser as the number of hypotheses mgrows.\nLetH1= [m]\\ H0. Aside from the FDR, for a method MT we compute the true positive\nratio and rejection ratio as:\nTP Ratio =E\u0014|H1∩ RMT\nα(p)|\n|H1∩ RBHα(p)|| H1∩ RBH\nα(p)̸=∅\u0015\nRej. Ratio =E\u0014|RMT\nα(p)|\n|RBHα(p)|| RBH\nα(p)̸=∅\u0015\nUsing these metrics, we compare the IndBH and IndBH(3)procedures to the BH and BY\nprocedures in each of these four settings. Further experiment runs, varying the simulation\nparameters and comparing additional methods, can be found in Appendix G.\n5.1 Block dependence with scattered signals.\nIn Figure 5, we consider a distribution of p-values which is block dependent. To specify this\ndistribution, first we set the following simulation parameters: null proportion π0∈[0,1],\ntarget power tarpow ∈[0,1], equicorrelation ρ∈[0,1], and block size bwhich evenly divides\nevery m, the number of hypotheses.\nThen for m∈ {100,500,2500,10000 ,50000}, the two-sided p-values are generated based\nonX∼ N m(µ,Σ), with distribution parameters determined as follows. We first choose a\nsubset of hypotheses H1⊂[m]of size ⌊(1−π0)m⌋uniformly at random to be non-null.\n16\n\nThen we set the mean µ∈Rmasµi= 0ifi /∈ H i. For i∈ H i, if signal strength is\nfixed, we set µi=µ∗, and if signal strength is random, then µiiid∼Exp(µ∗)with the mean\nparameterization. In each case µ∗∈Ris tuned so that the BH procedure has power tarpow\non the ensuing p-values.\nWe set the covariance matrix Σto have equicorrelated blocks with correlation ρ:\nΣ = diag(Σ ρ, . . . , Σρ|{z}\nm/btimes)∈Rm×mwhere Σρ=\n1ρ . . . ρ\nρ1......\n.........ρ\nρ . . . ρ 1\n∈Rb×b.\nBecause bis fixed as mincreases, this represents a dependency graph that gets sparser as\nmincreases. Finally, we generated two-sided p-values for the null hypotheses Hi= 0as\npi= 2(1 −Φ(|Xi|)).\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0250.0500.0750.100\n0.40.60.81.0\nmmethod\nBH\nBY\nIndBH\nIndBH3\nFigure 5: Results for block dependence with scattered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.6, ρ= 0.5, b= 100.\nFigure 5 shows that in the case of fixed signal strengths, the performance of IndBH\nimproves as the dependency graph becomes sparser, but does not nearly match the perfor-\nmance of BH, as IndBH(3)does. But fixed signal strengths tend to exaggerate the difference\nbetween multiple testing procedures—when the threshold is near (α/m)·2(1−Φ(µ∗)), slightly\nincreasing it can lead to many more rejections. On the other hand, random signal strengths\nlead to all procedures performing more favorably compared to BH, and IndBHperforms\nabout as well as BH.\n5.2 Banded dependence with clustered signals.\nIn Figure 6, we consider a distribution of p-values where the non-null signal pattern is\nsomewhat adversarial to our method. Specifically, by first generating cluster centers on\n{1, . . . , m }and drawing non-nulls indices randomly near the cluster centers, we make it so\nthat many non-nulls lie within each other’s neighborhoods. Neighborblindness then prevents\nthem from “assisting” each other in being rejected using D-adapted procedures.\nThe simulation parameters we set are null proportion π0∈[0,1], target power tarpow ∈\n[0,1], equicorrelation ρ∈[0,1], band size b′, average number of points per cluster λ0∈[0,1],\n17\n\nand cluster tightness τ >0.\nAgain, for m∈ {100,500,2500,10000 ,50000}, the p-values are generated based on\nX∼ N m(µ,Σ), with their distribution determined as follows. First, given H1, we specify\nµin the same way as in Section 5.1, tuning an additional parameter µ∗so that BH has\npower tarpow. Next, we take Σto be a banded Toeplitz covariance matrix: specifically,\nΣij=ρ|i−j|if|i−j| ≤ ⌊(b′−1)/2⌋, and zero otherwise.\nFinally, the choice of H1⊂[m]is now more complicated. We draw this set from the\nfollowing point process on Z. Let PP(λ)denote a Poisson point process on [m], that takes\nvalues as multi-sets of points. Then draw the realized cluster centers jand daughters hfrom\nj∼PP(η), η(i) :=η01{i∈[m]} (15)\nh∼PP(λ), λ(i) :=λ0X\nj∈jfσ(i, j),where (16)\nfσ(i, j) := exp\u0012\n−1\n2σ2(i−j)2\u0013.X\ni∈Zexp\u0012\n−1\n2τ2(i−j)2\u0013\n. (17)\nFinally, we get H1fromhby retaining the unique points, while discarding any points not\nin{1, . . . , m }. The daughters hare made up of nj∼Pois(λ0)points in Zassigned to each\npoint j, with locations distributed according to fσ(·, j). Therefore, the parameter η0controls\nthe average number of clusters, τthe tightness of the cluster, and λ0the average number of\ndaughter points per cluster. Given λ0, we set η0= (1−π0)m/λ 0so that there are roughly\n(1−π0)mnon-nulls on average.\nFigure 7 illustrates a few realizations of H1. We remark that the similar Thomas point\nprocess is used in the ecological sciences to model the locations of trees (Wiegand and\nMoloney, 2013).\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0250.0500.0750.100\n0.40.60.81.0\nmmethod\nBH\nBY\nIndBH\nIndBH3\nFigure 6: Results for banded dependence with clustered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.6, ρ= 0.5, b′= 100 , λ0= 20, τ=\n6.\nDue the somewhat adversarial simulation setting, when the signal strength is fixed, both\nIndBH and IndBH(3)perform more poorly compared to BH up to m= 500, but this graph is\nquite dense when b′= 100is the size of an average neighborhood. The performance recovers\nstarting at m= 2500.\n18\n\nm = 500\n0 100 200 300 400 500\nindex\nm = 2500\n0 500 1000 1500 2000 2500\nindex\nm = 10000\n0 2500 5000 7500 10000\nindexFigure 7: A rug plot of one generation of the non-nulls H1in the simulation setting of Figure\n6. A ruler of length b′= 100is also shown, representing the size of the typical neighborhood.\n5.3 Ensuring FDR control.\nIn Figure 8, we return to block dependence, but now with distributions that significantly\ninflate the FDR of the BH procedure. The distributions we chose are rather contrived:\nan “adversarial” distribution of p-values, defined in Appendix D.2, as well as one-sided\np-values from a negatively correlated Gaussian. In this section, we take the null proportion\nπ0= 1, so the remaining relevant parameters are the block size band, for the Gaussian, the\nequicorrelation ρ, which must be negative to inflate the FDR.\nWe checked the FDR and rejection ratio for m∈ {3,6,9,18,27}. In all these cases, IndBH\nand IndBH(3)seem to behave the same.\nAdversarial GaussianFDR Rej. Ratio\n3 10 303 10 300.250.500.75\n0.250.500.751.00\nNumber of hypotheses (m)method\nBH\nBY\nIndBH\nIndBH3\nFigure 8: Results with distributions designed to inflate FDR. FDR control level set to\nα= 0.5. Simulation parameters set to b= 3andρ=−0.354.\n19\n\n5.4 A real data example.\nIn this section, we work through an analysis using our methods on genome-wide association\nstudy (GWAS) data. Specifically, we use p-values from the schizophrenia GWAS of Ripke et al.\n(2014). Each individual p-value encodes associations between an individual single-nucleotide\npolymorphism (SNP) with schizophrenia. In our analysis, we mimicked the approach of\nYurko et al. (2020) to subset to only those SNPs which appeared as eQTLs in the BrainVar\nstudy of Werling et al. (2020), i.e. those SNPs which were associated with gene expression in\nthe developing brain. The raw dataset contained over 8 million p-values, each representing a\nSNP, but after filtering for INFO score >0.6and applying the BrainVar filter, we were left\nwith about 140,000.\nGWAS p-values are often not independent due to linkage disequilibrium (LD), which is\nthe tendency for some SNPs to be inherited together. For a given population, this is often\nmeasured by r2: the correlation between number of alleles at two SNP locations (Slatkin,\n2008). Such dependence formally invalidates the guarantees of the BH procedure.\nPurcell et al. (2007) propose using the PLINK software to find SNPs which are in low\nLD with each other and “prune to a reduced subset of approximately independent SNPs”,\na popular technique. But instead of performing this LD pruning, we instead used LD to\ngenerate a dependency graph between the SNP p-values—we drew an edge between two\nSNPs if and only if they are on the same chromosome andtheir LD as measured by r2is\ngreater than 0.2, which is the PLINK default. As a reference to compute r2, we used the\nEuropean Ancestry data from 1000 Genomes, Phase 3 (Fairley et al., 2020). Results are\nshown in Figure 9.\n01020\n1 2 345678910111213141516171819202122\nChromosomal position−log10(p)\nDep. Matrix (Chromosome 1)ColumnRow2000\n4000\n6000\n8000\n10000\n2000600010000\nDep. Matrix (Chromosome 2)ColumnRow2000\n4000\n6000\n8000\n10000\n2000600010000\n0200040006000\nBHBYIndBH1IndBH3\nMethod ( α=0.01)Number of Rejections\nFigure 9: Top: A Manhattan plot of the SNP p-values from the schizophrenia GWAS of Ripke\net al. (2014), after filtering to about 140,000 p-values. Bottom left, middle : Dependency\nmatrices for SNP p-values within a single chromosome. (SNPs on different chromosomes are\nassumed independent). Bottom right : Results for each method.\nAs will often be the case, the dependency matrix in this application is sparse, so that\n20\n\nIndBH(3)retains most of the power of BH. In fact, there seems to be a block dependence\nstructure, not only between chromosomes but even within a singlechromosome. This is\nlikely a manifestation of haplotype blocks in the human genome (Wall and Pritchard, 2003).\n6 Discussion\nIn this work, we have developed procedures for FDR control in multiple testing under a\nnatural dependence constraint: a dependency graph on p-values. We now discuss connections\nto the literature and compare to the the BY and the BH procedure.\n6.1 Connections to existing frameworks\nLet us now understand existing theoretical frameworks for achieving finite-sample FDR\ncontrol. We mostly focus on those of Blanchard and Roquain (2008) and Fithian and Lei\n(2020), though related results were also derived by Tamhane et al. (1998), Sarkar (2002),\nand Finner et al. (2007).\n6.1.1 Two simple sufficient conditions for FDR control\nConsider the following notions originally proposed by Blanchard and Roquain (2008). Given\na “shape function” s:R→R, a procedure Rαiss-self-consistent, or s-SC, if i∈ R α(p)⇒\npi≤αs\u0000\n|Rα(p)|\u0001\n/m. Also, the pair of random variables (pi, r∗)satisfies s-dependency\ncontrol, or s-DC, if\nE\u00141{pi≤cs(r∗)}\nr∗\u0015\n≤c.\nHenceforth, we assume s(r) :=r, and no longer mention the shape function s. Property SC\nthen becomes exactly the condition (P1).\nBlanchard and Roquain (2008) show that whenever Rαis SC and (pi,|R(p)|)is DC for all\ni∈ H 0, then Rαcontrols the FDR. Here, our work departs from theirs, as our D-adaptivity\ncondition does not necessarily imply that (pi,|Rα(p)|)satisfies DC. It does, however, imply\nthat (pi,|Rα(1N◦\nip)|)satisfies DC, as seen in the proof of Theorem 2.\nThe self-consistency gap (3), which makes procedures conservative, is not relevant under\nindependence or PRD, because BH has no self-consistency gap and is easy to compute, while\nbeing the most liberal self-consistent procedure. In our setting, the most liberal D-adapted\nprocedure is SUD, but it did not seem easy to compute. We instead started from IndBH D, an\nefficiently computable procedure with reasonable performance, and applied the gap chasing\niterates to improve it.\n6.1.2 Conditional calibration\nThe gap chasing iteration (10)can be viewed as an instantiation of the conditional calibration\nframework of Fithian and Lei (2020), which we now describe.\nWe begin with general data X∈ Xfrom which p-values p1, . . . , p mare computed. For\neach i, write down three ingredients: a calibratable, data-dependent threshold τi(c;X)which\nis non-decreasing in cfor all X, satisfying τi(0;X) = 0; a conditioning statistic Sion which\npiis superuniform conditional on Si; and any function βi:X →Rat all. Then, suppose\nˆc(Si)is chosen such that\nE\u00141{pi≤τi(ˆc(Si);X)}\nβi(X)|Si\u0015\n≤α/mfor all i. (18)\nConsider the procedure RCC\nα(X) :=\b\ni:pi≤τi(ˆc(Si);X)\t\n. Fithian and Lei (2020) show\nthatRCC\nα(X)“almost” controls FDR in finite samples—an auxiliary randomization step is\n21\n\nalso required. But RCC\nα(X)does control FDR in finite samples if βiwas chosen such that\ni∈ RCC\nα(X)⇒βi(X)≤ |RCC\nα(X)|almost surely. (19)\nThis says that each βicorresponds to a rejection lower bound, but not necessarily that\nRCC\nα(X)is self-consistent in general.\nIn our setting, let X=pbe our dataset, and Rinit\nα(X)be any initial procedure. Consider\nthe threshold τi(c;X) =cand conditioning statistic Si=pNc\ni. It can be shown that\ncalibrating based on\nβi(X) = inf\n{pi:pi≤α|{i}∪Rinitα(1N◦\nip)|/m}\f\f{i} ∪ Rinit\nα(1N◦\nip)\f\f\nis equivalent to applying the gap-chasing iterations (10)onRinit\nα. The resulting RCC\nα(X)\nsatisfies(19)whenRinit\nαisD-adapted, again showing the FDR control of IndBH(k)fork >1.\nA different choice, namely βi(X) =|{i} ∪ Rinit\nα(1N◦\nip)|, leads to a larger ˆc(Si), and hence\na more powerful procedure than ours. But this comes at much greater computational cost.\nThe condition in (18) becomes\nZ1\n01{pi≤ˆc(Si)}\n|{i} ∪ Rinitα(1N◦\nipi←t)|dt≤α/mfor all i, (20)\nwhere pi←t)is equal to pexcept that piis set to t. We apparently must compute\nRinit\nα(1N◦\nipi←t)for all t∈[0,1], whereas gap chasing only requires a single call to Rinit\nα(1N◦\nip)\nfor each i.\nWe also remark that this choice does notlead to a self-consistent procedure as defined in\n(P1), demonstrating the suboptimality of self-consistency as an algorithm design principle.\n(Solari and Goeman (2017) also discovered a procedure which is not self-consistent, controls\nFDR under PRDS, and dominates the BH procedure.)\n6.2 Comparisons and Conclusion\nTo conclude, we will make a few practical observations on the two most well-known FDR\ncontrolling methods in the literature, the BY and the BH procedure, by comparing them to\nour methods.\n6.2.1 Versus BY\nAs discussed in the Introduction, the correction of the BY(α)procedure might seem unnec-\nessarily severe, but in practice it is often better than running Bonf(α). In our simulations,\nhowever, BY(α)is often dominated by IndBH (α), including in the case of the complete\ngraph, when IndBH (α)is equivalent to Bonf (α). Let us now explain this briefly.\nBy monotonicity, both BY(α)andIndBH (α)reject hypothesis Hiifpi≤αβi,α(p−i)/m,\nfor some function βi,α: [0,1]m−1→[0, m]. But where BYlowers FDR to its nominal level\nby decreasing α,IndBHdirectly limits the sensitivity of βi,α(p−i)to dependent p-values in\nits argument, as shown explicitly in the proof of Theorem 1.\nIf the dependent neighborhood Niis small and the non-null signal pattern is reasonably\nsparse—both true in typical problems—then it is unlikely many small p-values are in Ni, so\nthey can still assist in the rejection of Hi, and IndBH makes more rejections. But when the\nNibecome very large, or there are many non-nulls, or both, then true signal p-values could\nget masked and BY can make more rejections.\nIn the case of the complete graph, IndBHreduces to Bonf(α), which wins if BY(α)makes\nless than ≈log(m)rejections. This only occurs when the signals are very sparse, but when\nmis small, even a non-null proportion of 1−π0= 0.1can be sparse enough, which is what\noccurs in our simulations.\nUltimately, in typical multiple testing problems, both Dand the signal pattern are at\nleast moderately sparse, which seems to favor the IndBHprocedure over BY. But in these\nproblems IndBH performs about as well as BH, so why not just run BH?\n22\n\n6.2.2 Versus BH\nNote that IndBH, and all other D-adapted procedures, are always less powerful than BH.\nBut a belief among many researchers—both theoretical and applied—is that the uncorrected\nBH procedure can generally be used in practice without inflating the FDR much, see e.g.\nGoeman and Solari (2014). If this is true, then these D-adapted procedures may be less\nappealing methodologically.\nHowever, our methods perform similarly to BH in practice with a provable guarantee,\nwhile the theoretical literature has not fully confirmed the robustness of BH in finite samples.\nSome progress is being made—see prior work, in particular Chi et al. (2022)—but only\nsparsely. A particularly important unproved conjecture in multiple testing is the exactFDR\ncontrol for two-sided testing of multivariate Gaussian means (Reiner-Benaim, 2007; Roux,\n2018; Sarkar, 2023), which is a basic setting often assumed in practice.\nFor now, one practical role for our methods could be as a robustness check. Given a\ndependency graph D, if the output of IndBH(3)and BH are very different, then the user could\npause and ask whether there might be an unfavorable dependence structure that IndBH(3)\nis protecting against (that is, besides the multivariate Gaussian).\nFinally, it bears reminding that even when BH controls FDR, the FDP may have high\nvariance under dependence, making the FDR control misleading: see for example Kluger\nand Owen (2024). We look forward to future work that will further illuminate the safety of\nthe BH procedure under dependence.\nSoftware and Reproducibility\nOur Rpackage depgraphFDR is available at\nhttps://github.com/drewtnguyen/depgraphFDR\nCode and instructions to reproduce the plots in this paper is available at\nhttps://github.com/drewtnguyen/depgraphFDRpaper\nAcknowledgments\nD.T.N. thanks Etienne Roquain and Rina Barber for helpful conversations.\nReferences\nYoav Benjamini and Yosef Hochberg. Controlling the False Discovery Rate: A Practical and\nPowerful Approach to Multiple Testing. Journal of the Royal Statistical Society. Series B\n(Methodological) , 57(1):289–300, 1995. ISSN 0035-9246. URL https://www.jstor.org/\nstable/2346101 . Publisher: [Royal Statistical Society, Wiley].\nYoav Benjamini and Daniel Yekutieli. The control of the false discovery rate\nin multiple testing under dependency. The Annals of Statistics , 29(4):1165–\n1188, August 2001. ISSN 0090-5364, 2168-8966. doi: 10.1214/aos/1013699998.\nURL https://projecteuclid.org/journals/annals-of-statistics/volume-29/\nissue-4/The-control-of-the-false-discovery-rate-in-multiple-testing/10.\n1214/aos/1013699998.full . Publisher: Institute of Mathematical Statistics.\nGilles Blanchard and Etienne Roquain. Two simple sufficient conditions for FDR control.\nElectronic Journal of Statistics , 2, October 2008. doi: 10.1214/08-EJS180.\nLouis H. Y. Chen and Qi-Man Shao. Normal approximation under local dependence. The\nAnnals of Probability , 32(3), July 2004. ISSN 0091-1798. doi: 10.1214/009117904000000450.\n23\n\nZiyu Chi, Aaditya Ramdas, and Ruodu Wang. Multiple testing under negative dependence.\narXiv preprint arXiv:2212.09706 , 2022.\nGábor Csárdi, Tamás Nepusz, Kirill Müller, Szabolcs Horvát, Vincent Traag, Fabio Zanini,\nand Daniel Noom. Igraph for R: R interface of the igraph library for graph theory and\nnetwork analysis. Zenodo, January 2025.\nSusan Fairley, Ernesto Lowy-Gallego, Emily Perry, and Paul Flicek. The International\nGenome Sample Resource (IGSR) collection of open human genomic variation resources.\nNucleic Acids Research , 48(D1):D941–D947, January 2020. ISSN 0305-1048. doi: 10.1093/\nnar/gkz836.\nAlessio Farcomeni. Some Results on the Control of the False Discovery Rate\nunder Dependence. Scandinavian Journal of Statistics , 34(2):275–297, 2007.\nISSN 1467-9469. doi: 10.1111/j.1467-9469.2006.00530.x. URL https://\nonlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9469.2006.00530.x . _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9469.2006.00530.x.\nHelmut Finner and M. Roters. On the False Discovery Rate and Expected Type I Errors. Bio-\nmetrical Journal , 43(8):985–1005, 2001. ISSN 1521-4036. doi: 10.1002/1521-4036(200112)\n43:8<985::AID-BIMJ985>3.0.CO;2-4.\nHelmut Finner, Thorsten Dickhaus, and Markus Roters. Dependency and false\ndiscovery rate: Asymptotics. The Annals of Statistics , 35(4):1432–1455, Au-\ngust 2007. ISSN 0090-5364, 2168-8966. doi: 10.1214/009053607000000046.\nURL https://projecteuclid.org/journals/annals-of-statistics/volume-35/\nissue-4/Dependency-and-false-discovery-rate-Asymptotics/10.1214/\n009053607000000046.full . Publisher: Institute of Mathematical Statistics.\nWilliam Fithian and Lihua Lei. Conditional calibration for false discovery rate control under\ndependence, July 2020. URL http://arxiv.org/abs/2007.10438 . arXiv:2007.10438\n[math, stat].\nJelle J. Goeman and Aldo Solari. Multiple hypothesis testing in genomics. Statis-\ntics in Medicine , 33(11):1946–1978, 2014. ISSN 1097-0258. doi: 10.1002/sim.\n6082. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6082 . _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6082.\nWenge Guo and Sanat Sarkar. Adaptive controls of FWER and FDR under block dependence.\nJournal of Statistical Planning and Inference , 208:13–24, September 2020. ISSN 03783758.\ndoi: 10.1016/j.jspi.2018.03.008. URL https://linkinghub.elsevier.com/retrieve/\npii/S0378375819301181 .\nNikolaos Ignatiadis and Wolfgang Huber. Covariate powered cross-weighted mul-\ntiple testing. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology) , 83(4):720–751, 2021. ISSN 1467-9868. doi: 10.1111/rssb.12411.\nURL https://onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12411 . _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12411.\nSvante Janson. Large deviations for sums of partly dependent random variables. Random\nStructures & Algorithms , 24(3):234–248, May 2004. ISSN 1042-9832, 1098-2418. doi:\n10.1002/rsa.20008.\nKyung In Kim and Mark A. van de Wiel. Effects of dependence in high-dimensional multiple\ntesting problems. BMC bioinformatics , 9:114, February 2008. ISSN 1471-2105. doi:\n10.1186/1471-2105-9-114.\n24\n\nDan M. Kluger and Art B. Owen. A central limit theorem for the Benjamini-Hochberg\nfalse discovery proportion under a factor model. Bernoulli , 30(1), February 2024. ISSN\n1350-7265. doi: 10.3150/23-BEJ1615.\nRaffaele Marino, Lorenzo Buffoni, and Bogdan Zavalnij. A Short Review on Novel Approaches\nfor Maximum Clique Problem: From Classical algorithms to Graph Neural Networks and\nQuantum algorithms, March 2024.\nShaun Purcell, Benjamin Neale, Kathe Todd-Brown, Lori Thomas, Manuel A. R. Ferreira,\nDavid Bender, Julian Maller, Pamela Sklar, Paul I. W. de Bakker, Mark J. Daly, and\nPak C. Sham. PLINK: A Tool Set for Whole-Genome Association and Population-Based\nLinkage Analyses. The American Journal of Human Genetics , 81(3):559–575, September\n2007. ISSN 0002-9297, 1537-6605. doi: 10.1086/519795.\nAaditya K. Ramdas, Rina F. Barber, Martin J. Wainwright, and Michael I.\nJordan. A unified treatment of multiple testing with prior knowledge us-\ning the p-filter. The Annals of Statistics , 47(5):2790–2821, October 2019.\nISSN 0090-5364, 2168-8966. doi: 10.1214/18-AOS1765. URL https:\n//projecteuclid.org/journals/annals-of-statistics/volume-47/issue-5/\nA-unified-treatment-of-multiple-testing-with-prior-knowledge-using/10.\n1214/18-AOS1765.full . Publisher: Institute of Mathematical Statistics.\nAnat Reiner-Benaim. FDR Control by the BH Procedure for Two-Sided Corre-\nlated Tests with Implications to Gene Expression Data Analysis. Biometrical Jour-\nnal, 49(1):107–126, 2007. ISSN 1521-4036. doi: 10.1002/bimj.200510313. URL\nhttps://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.200510313 . _eprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.200510313.\nStephan Ripke et al. Biological Insights From 108 Schizophrenia-Associated Genetic Loci.\nNature, 511(7510):421–427, July 2014. ISSN 0028-0836. doi: 10.1038/nature13595.\nJoseph P. Romano and Azeem M. Shaikh. Stepup Procedures for Control of Generalizations\nof the Familywise Error Rate. The Annals of Statistics , 34(4):1850–1873, 2006. ISSN\n0090-5364.\nMarine Roux. Inférence de graphes par une procédure de test multiple avec application en\nNeuroimagerie . thesis, Université Grenoble Alpes (ComUE), September 2018.\nSanat K. Sarkar. Some Results on False Discovery Rate in Stepwise\nmultiple testing procedures. The Annals of Statistics , 30(1), February\n2002. ISSN 0090-5364. doi: 10.1214/aos/1015362192. URL https:\n//projecteuclid.org/journals/annals-of-statistics/volume-30/issue-1/\nSome-Results-on-False-Discovery-Rate-in-Stepwise-multiple-testing/10.\n1214/aos/1015362192.full .\nSanat K. Sarkar. On Controlling the False Discovery Rate in Multiple Testing of the Means\nof Correlated Normals Against Two-Sided Alternatives, April 2023.\nMontgomery Slatkin. Linkage disequilibrium — understanding the evolutionary past and\nmapping the medical future. Nature Reviews Genetics , 9(6):477–485, June 2008. ISSN\n1471-0064. doi: 10.1038/nrg2361.\nAldo Solari and Jelle J. Goeman. Minimally adaptive BH: A tiny but uniform improvement of\nthe procedure of Benjamini and Hochberg. Biometrical Journal. Biometrische Zeitschrift ,\n59(4):776–780, July 2017. ISSN 1521-4036. doi: 10.1002/bimj.201500253.\nWeijie J. Su. The FDR-Linking Theorem, December 2018. URL http://arxiv.org/abs/\n1812.08965 . arXiv:1812.08965 [math, stat].\n25\n\nAjit C Tamhane and Lingyun Liu. On weighted hochberg procedures. Biometrika , 95(2):\n279–294, 2008.\nAjit C. Tamhane, Wei Liu, and Charles W. Dunnett. A generalized step-up-down multiple\ntest procedure. Canadian Journal of Statistics , 26(2):353–363, June 1998. ISSN 0319-5724,\n1708-945X. doi: 10.2307/3315516.\nShuji Tsukiyama, Mikio Ide, Hiromu Ariyoshi, and Isao Shirakawa. A New Algorithm for\nGenerating All the Maximal Independent Sets. SIAM J. Comput. , 6(3):505–517, September\n1977. ISSN 0097-5397. doi: 10.1137/0206036.\nJeffrey D. Wall and Jonathan K. Pritchard. Haplotype blocks and linkage disequilibrium in\nthe human genome. Nature Reviews Genetics , 4(8):587–597, August 2003. ISSN 1471-0064.\ndoi: 10.1038/nrg1123.\nRuodu Wang and Aaditya Ramdas. False discovery rate control with e-values, December\n2021. URL http://arxiv.org/abs/2009.02824 . arXiv:2009.02824 [math, stat].\nDonna M. Werling, Sirisha Pochareddy, Jinmyung Choi, Joon-Yong An, Brooke Sheppard,\nMinshi Peng, Zhen Li, Claudia Dastmalchi, Gabriel Santpere, André M. M. Sousa, Andrew\nT. N. Tebbenkamp, Navjot Kaur, Forrest O. Gulden, Michael S. Breen, Lindsay Liang,\nMichael C. Gilson, Xuefang Zhao, Shan Dong, Lambertus Klei, A. Ercument Cicek,\nJoseph D. Buxbaum, Homa Adle-Biassette, Jean-Leon Thomas, Kimberly A. Aldinger,\nDiana R. O’Day, Ian A. Glass, Noah A. Zaitlen, Michael E. Talkowski, Kathryn Roeder,\nMatthew W. State, Bernie Devlin, Stephan J. Sanders, and Nenad Sestan. Whole-Genome\nand RNA Sequencing Reveal Variation and Transcriptomic Coordination in the Developing\nHuman Prefrontal Cortex. Cell Reports , 31(1):107489, April 2020. ISSN 2211-1247. doi:\n10.1016/j.celrep.2020.03.053.\nThorsten Wiegand and Kirk A. Moloney. Handbook of Spatial Point-Pattern Analysis in\nEcology. Chapman and Hall/CRC, New York, December 2013. ISBN 978-0-429-13811-9.\ndoi: 10.1201/b16195.\nQinghua Wu and Jin-Kao Hao. A review on algorithms for maximum clique problems.\nEuropean Journal of Operational Research , 242(3):693–709, May 2015. ISSN 03772217.\ndoi: 10.1016/j.ejor.2014.09.064.\nRonald Yurko, Max G’Sell, Kathryn Roeder, and Bernie Devlin. A selective inference\napproach for false discovery rate control using multiomics covariates yields insights into\ndisease risk. Proceedings of the National Academy of Sciences , 117(26):15028–15035, June\n2020. doi: 10.1073/pnas.1918862117. URL https://www.pnas.org/doi/abs/10.1073/\npnas.1918862117 . Publisher: Proceedings of the National Academy of Sciences.\nA Proofs\nFor the proofs here and elsewhere in the appendix, we define\npi←t:= (p1, . . . , p i−1, t, pi+1, . . . , p m).\nTheorem 2. Suppose Rα: [0,1]m→2[m]is aD-adapted multiple testing procedure, and\nthatpiis superuniform whenever i∈ H 0. Then whenever the input vector pis PPRD on H0\nwith respect to D, the procedure Rαcontrols the FDR at level α|H0|/m.\nFor the proof, we will use the following “superuniformity lemma”. Originally shown by\nBlanchard and Roquain (2008), the following statement is modified from Lemma 1(b) of\nRamdas et al. (2019).\n26\n\nLemma 1. For any coordinate-wise non-increasing function f: [0,1]m→[0,∞), ifpis\nPRD on a set A, then whenever i∈Aandpiis superuniform,\nE\u00141{pi≤f(p)}\nf(p)\u0015\n≤1\nwhere we use the convention that 0/0 = 1.\nProof of Theorem 2. Following the proof of Theorem 1, it is again sufficient to show that\nE\u00141{i∈ R(p)}\n|R(p)|\u0015\n≤α/m\nfori∈ H 0. Let q=1N◦\nip, the p-value vector with node i’s neighborhood masked (excepting\niitself). Since pis PPRD on H0w.r.t to D, we have that qis PRD on H0, and so\nE\u00141{i∈ R(p)}\n|R(p)|\u0015\n≤E\u00141{i∈ R(q)}\n|R(q)|\u0015\n(P2) and (P3)\n=E\"\n1{qi≤α|R(q)|\u000e\nm}\n|R(q)|#\n(P1)\n≤α/m by (P2) and Lemma 1.\nIn this, we only used that p7→ |R α(p)|is coordinate-wise nonincreasing, which is weaker\nthan Assumption (P2).\nProposition 1. LetDbe the graph with neighborhoods Ni={j: Ψij̸= 0}fori= 1, . . . , m.\nThen the test statistics T2= (T2\n1, . . . , T2\nm), are PPRD on [m]with respect to D.\nProof of Proposition 1. Fithian and Lei (2020) demonstrate the formula\nT2\nj=U2\ni,j\u0012ν+T2\ni\nΨj,jVi\u0013\n+Ψi,jTiUi,j\nΨi,is\nν+T2\nj\nΨj,jVi+\u0012Ψi,j\nΨi,iTi\u00132\n(21)\nfor any pair i, j, where Ui=Z−i−Ψ−i,iΨ−1\ni,iZiandVi=νˆσ2+Z2\ni/Ψi,i.\nNow for any i∈[m]and increasing set Ki⊂R|Nc\ni|, define\nf(s) :=P(T∈Ki|T2\ni=s) =Eh\nEh\n1{TNc\ni∈Ki|Ui, Vi, T2\ni=s}ii\n,\nBecause T2\njis increasing in T2\niwhenever Ψi,j= 0in(21), it follows that the indicator is\n1whenever sis greater than some threshold depending on Ui, Vi, and Ki, so that f(s)is\nincreasing in sandT2is PPRD with respect to D.\nProposition 6. IndBH (α)rejects Hiif and only if pi≤αβIndBH\nα,i (p−i)/m, where\nβIndBH\nα,i (p−i) := max {r:|Ii(r)| ≥r}.\nProof of Proposition 6. First, if i∈ RIndBH(p), there exists an independent set Jcontaining\nisuch that pj≤α|J|/mfor all j∈J. Then |J| ≤max{r:|Ii(r)| ≥r}, since J′=Ii(|J|)\nhas size at least |J|, because\nJ∈n\n{i} ∪B:B∈Ind(D[Q−i(|J|)])o\n,\nandJ′is the largest member of this set, by definition. Then |J| ≤βIndBH\nα,i (p−i), so that\npi≤αβIndBH\nα,i (p−i)/m.\nOn the other hand, let Bi=Ii(βIndBH\nα,i (p−i)), which satisfies |Bi|=βIndBH\nα,i (p−i). We\ncan hence write {pi≤αβIndBH\nα,i (p−i)/m} ⇔ { pi≤α|Bi|/m}. The set Biis an independent\nset which contains iand where pj≤α|Bi|/mfor all j∈ Bi. But this is a certificate set as\ndefined in Section 3.1, so we must have i∈ RIndBH\nα (p).\n27\n\nProposition 7. Suppose we have ¯rsuch that whenever pi> α¯r/m, theIndBH(k)\nD(α)\nprocedure fails to reject Hi. ThenIndBH(k)\nD(α)run on the p-values (pi)m\ni=1is equivalent\ntoIndBH(k)\nD′(α′)run on (pi)i∈Q(¯r), using the subgraph D′=D[Q(¯r)]and adjusted level\nα′=α|Q(¯r)|/m. In particular, this holds for ¯r=|RBH\nα(p)|, which has Q(¯r) =RBH\nα(p).\nTo justify this proposition, we use a lemma from which it quickly follows. To avoid\nexcessive notation, we will simply define RIndBH(k)\nα,−i(p−i)⊂[m]\\ {i}to be the result of\nrunningIndBHon the the subvector p−iand subgraph D[[m]\\ {i}], but returning an index\nset corresponding to that of the original vector p. We define the quantity RBH\nα,−i(p−i)similarly,\nbut without reference to the graph.\nLemma 2. For any k≥1, on the event that i /∈ RIndBH(k)\nα (p), we have the identity\nRIndBH(k)\nα (p) =RIndBH(k)\nα (1{i}p) =RIndBH(k)\nα′,−i(p−i),\nwhere α′=α(m−1)/m.\nProof of Lemma 2. We will prove this by induction. For k= 1, recall the definition of IndBH\nfrom Equation (7):\nRIndBH(1)\nα (p) =[\nI∈Ind(D)RBH\nα(1Icp)\nwhich is just a union of BH rejection sets. It is a well known fact from the theory of multiple\ntesting that whenever i /∈ RBH\nα(p), we have RBH\nα(p) =RBH\nα(1{i}p) =RBH\nα′,−i(p−i). Now\nsuppose i /∈ RIndBH(1)\nα (p). Then it is in none of BH rejection sets that make up the union of\nEquation (7). So using the well-known fact, we have\n[\nI∈Ind(D)RBH\nα(1Icp) =[\nI∈Ind(D)RBH\nα(1{i}1Icp) =[\nI∈Ind(D)RBH\nα′,−i((1Icp)−i)\nwhich shows the claim for k= 1, using the definition of IndBH. Now suppose it holds for\nsome k≥1. Suppose j /∈ RIndBH(k+1)\nα (p). Then by Proposition 4 and monotonicity, we also\nhave j /∈ RIndBH(k)\nα (1N◦\nip)for any i. We then have\nRIndBH(k+1)\nα (p) =(\ni:pi≤α|{i} ∪ RIndBH(k)\nα (1N◦\nip)|\nm)\n=(\ni̸=j:pi≤α|{i} ∪ RIndBH(k)\nα (1N◦\nip)|\nm)\n=(\ni̸=j:pi≤α|{i} ∪ RIndBH(k)\nα (1{j}1N◦\nip)|\nm)\n=(\ni̸=j:pi≤α|{i} ∪ RIndBH(k)\nα′,−j((1N◦\nip)−j)|\nm)\n=(\ni̸=j:pi≤α′|{i} ∪ RIndBH(k)\nα′,−j((1N◦\nip)−j)|\nm−1)\n.\nHere we used the induction hypothesis in the third and fourth equalities. The third expression\non the right-hand side is precisely RIndBH(k+1)\nα (1{i}p), and the last is RIndBH(k+1)\nα′,−i (p−i), so\nwe are done.\nProof of Proposition 7. Use Lemma 2repeatedly for every i /∈Q(¯r)to show the claim. The\nreason why it holds for ¯r=|RBH\nα(p)|is because if pi> α¯r/m, then Hicannot be rejected\nby BH. Because BH contains every D-adapted procedure, then Hicannot be rejected by\nIndBH(k)for any k.\n28\n\nProposition 8. Suppose blocks B1, . . . , B ¯k⊂[m]form a disjoint partition of [m], and that\nthe neighborhoods (Ni)m\ni=1of a graph Dsatisfy Ni=Bkwhenever i∈Bk. Defining\nj∗\nk= argmin\nj∈Bkpjand M=¯k[\nk=1{j∗\nk},\nwe have\nRIndBH Dα (p) ={i:pi≤α\f\fRBH\nα(1Mcp)\f\f/m}.\nProof of Proposition 8. LetR=RIndBH Dα (p)and\nR′={i:pi≤α|RBH\nα(1Mcp)|/m}={i:pi≤α|C∗|/m}\nwhere C∗=|RBH\nα(1Mcp)|. We will need to show that R=R′.\nFor the forward inclusion, recall from the discussion in Section 3.1 that i∈ Rif there\nexists a certificate set Csuch that i∈C, and hence pi≤α|C|/mby definition of certificate\nsets. In this setting, the certificate set with the largest cardinality is precisely C∗, so it\nfollows that pi≤α|C∗|/mandR ⊂ R′.\nFor the backward inclusion, suppose that pi≤α|C∗|/m. Let κ[i]be the index for the\nblock that contains i. Then Ci= (C∗\\ {j∗\nκ[i]})∪ {i}has the same cardinality as C∗, but\nremains a certificate set. Because it contains i, we know i∈ R, and because iwas arbitrary,\nR′⊂ R.\nB Computational details\nHere we present our computational strategy for IndBH(k)in full detail. The main algorithm\nis presented as Algorithm 1, but we describe it here at a high level.\nAfter subsetting to the BH rejection set, we precompute a certain table V, whose entries\nrecord the independence numbers of various different subgraphs, to be used as a reference\nwhenIndBHis eventually called. Then each time IndBH(ℓ)is called to determine the\nrejection status of each hypothesis Hi, we first run IndBH(ℓ−1)and then perform inexpensive\ninclusion/exclusion checks based on its rejection set. When these are inconclusive for a\nhypothesis i, we call IndBH(k−1), but on the masked vector 1N◦\nip.\nThis continues until finally IndBHis called at the bottom of the recursion. In this case,\nwe cheaply perturb the table Vinto the table eVto account for the masking, and again\nperform inclusion/exclusion checks for the rejection status of each Hi. These checks depend\nonly on eV. When they are inconclusive, we must exactly compute the IndBH rejection set\nusing an expensive fallback.\nDetailed explanations of these steps now ensue. We adopt the notation from Section 4.\nB.1 Initial filtering\nSection 4.1 and Proposition 7 in the main text already explain why this filtering step does\nnot hamper the correctness of the algorithm. Though it is the most important step, the\nsections that follow do not technically depend on it for their correctness.\nThis step—discarding all p-values which exceed α¯r/m— could potentially be improved\nby applying Proposition 7 with a smaller choice of ¯r. For example, any upper bound to the\nsize ofSUD(α)would work—such bounds were demonstrated in the proof of Theorem 3. We\nleave any possible further improvements to be addressed in the package documentation of\ndepgraphFDR .\n29\n\nAlgorithm 1: Pseudocode for RIndBH(k)\nDα (p).\n/* SETUP */\n/* Initial filtering: Section B.1 */\np←p[RBHα(p)]\nD←D[RBH\nα(p)]\nα←α|RBH\nα(p)|/m\n/* Precomputation: Section B.2 */\nV←precompute (p,D)\n/* DEFINE MEMOIZED PROCEDURES */\ndefRbase(q):\n// Base case: an implementation of IndBH\neV←update (V)// Updating V: Section B.3\n/* Inclusion exclusion checks: Section B.4 */\nC(1)←check(1)−(q,eV)\nD(1)←check(1)+(q,eV)\nR ←∅\nfori←1tomdo\nifi∈ C(1)then\nR=R ∪ { i}\nelse if i /∈ D(1)then\n// do nothing\nelse\nβ(1)\ni←beta(q,eV)// Expensive fallback: Section B.5\nifpi≤αβ(1)\ni/mthen\nR=R ∪ { i}\nreturn R\ndefR(ℓ)(q):\nifℓ= 1then\nreturn Rbase(q)// Run base case\n/* Inclusion exclusion checks: Section B.6 */\nC(ℓ)←check(ℓ)−(q)\nD(ℓ)←check(ℓ)+(q)\nR ←∅\nfori←1tomdo\nifi∈ C(ℓ)then\nR=R ∪ { i}\nelse if i /∈ D(ℓ)then\n// do nothing\nelse\nβ(ℓ)\ni← R(ℓ−1)(1Niq)// We mask Ni, not N◦\ni: Section B.7\nifpi≤αβ(ℓ)\ni/mthen\nR=R ∪ { i}\nreturn R\n/* RETURN REJECTION SET */\nreturn R(k)(p)\n30\n\nB.2 Precomputing the table V\nRecall from Proposition 6 that\nβIndBH\nα,i (p−i) := max {r:|Ii(r)| ≥r},\nwhere we have from Section 4.2.1 that\n|Ii(r)|= 1 + IndNum (Dκ[i][Q−i(r)]) +X\nk̸=κ[i]IndNum (Dk[Q(r)])\n= 1 + IndNum (Dκ[i][Q−i(r)]) +X\nk̸=κ[i]Vk,r,\nwhere we define the table V∈R¯k×|RBH\nα(p)|byVk,r:=IndNum\u0000\nDk[Q(r)]\u0001\n. This table is\nuseful as its elements get reused for every i. To compute V, our strategy is as follows:\nStep 1.Compute and save the connected components D1, . . . ,D¯k. Initialize Vas the zero\nmatrix in R¯k×|RBH\nα(p)|.\nStep 2. For components k= 1, . . . , ¯k,\n•Compute and save the maximal independent sets (MIS) Ik,1, . . . , Ik,¯ℓofDk.\n•For sets ℓ= 1, . . . , ¯ℓandr= 1, . . . ,|RBH\nα(p)|,\n–Compute uk,r,ℓ:= #{i∈Ik,ℓ:pi≤αr/m}. (For this we use R’stable\nfunction on ⌈mpIk,ℓ/α⌉.)\n•TakeVk,r= max ℓuk,r,ℓ.\nToperformthegraph-theoreticcalculations, weusedthe igraphpackagein R(Csárdietal.,\n2025). In particular, the function igraph::maximal_ivs is an off-the-shelf implementation\nof the algorithm of Tsukiyama et al. (1977) for finding maximal independent sets. More\nmodern algorithms for the maximal independent set problem—in its equivalent form of the\nmaximal clique problem—are reviewed in Wu and Hao (2015) and Marino et al. (2024).\nWe can also optimize further by reducing the number of columns of V. Let\n¯r(1):= max\n\nr≤ |RBH\nα(p)|:¯kX\nk=1Vk,r≥r\n\n. (22)\nBecauseP¯k\nk=1Vk,r≥ |Ii(r)|, comparing to Proposition 6 we have βIndBH\nα,i (p−i)≤¯r(1). (We\ncan also see this by recognizing ¯r(1)as the size of the largest certificate set). Computing\nIndBH only requires columns V·,rsuch that r≤¯r(1), where ¯r(1)isanyupper bound to\nβIndBH\nα,i (p−i), so the remaining columns can be safely dropped.\nB.3 Updating the table V\nWorking through the recursion of Section 3.2.1, it can be seen that the eventual calls to IndBH\nfromIndBH(k)take the form RIndBH\nα (1N◦\nik−1. . .1N◦\ni21N◦\ni1p)for some sequence i1, . . . , i k−1.\nLet us generically denote this modified vector as\n˜p:=1N◦\nik−1. . .1N◦\ni21N◦\ni1p,\nleaving the sequence of masked indicies N◦\nik−1, . . . , N◦\ni2, N◦\ni1implicit.\n(Note that Algorithm 1 actually takes ˜p:=1Nik−1. . .1Ni21Ni1p. This is somewhat\ncomputationally convenient, and still correct. The explanation is deferred to Section B.7.)\nBefore we can use the table Vto help us run IndBHon˜p, we need to adjust it to\naccount for the masking, because Vwas computed using the unmodified p. We hence\nrepeat Step 2 from Appendix B.2, but only for those connected components overlapping\nwith N◦\ni1∪ ··· ∪ N◦\nik−1. Denote the resulting table as eV. We call this subroutine updatein\nAlgorithm 1.\n31\n\nB.4 Inclusion and exclusion: IndBH\nWrite β∗\ni=βIndBH\nα,i (˜p−i). We have\ni∈ RBH\nα(˜p)⇔˜pi≤αβ∗\ni/m⇔ri≤β∗\ni,\nwhere ri=⌈m˜pi/α⌉. Let\nβ+= max\n\nr≤¯r(1):¯kX\nk=1eVk,r≥r\n\n,\nβ−= max\n\nr≤¯r(1):¯kX\nk=1eVk,r−max\nk′eVk′,r+ 1≥r\n\n,\nβ−\ni= max\n\nr≤¯r(1):¯kX\nk=1eVk,r−eVκ[i],r+ 1≥r\n\n.\nThese quantities can be computed entirely from eVand satisfy\nβ−≤β−\ni≤β∗\ni≤β+\nfor all i, justifying the following shortcuts.\n1.check(1)−. Whenever ri≤β−orri≤β−\ni, declare i∈ RIndBH\nα (˜p). We represent\nC(1)={i:ri≤β−\ni}in Algorithm 1.\n2.check(1)+. Whenever ri> β+, declare i /∈ RIndBH\nα (˜p). We represent D(1)={i:ri≤\nβ+}in Algorithm 1.\nThese checks typically take care of most of the hypotheses.\nThe checks based on β+andβ−can be performed for all hypotheses at once, and the\none based on β−\nican be performed for all hypotheses in a connected component at once. In\nour implementation, we perform the β+andβ−checks first.\nB.5 Fallback: computing IndBH exactly\nWhen all the cheap checks prove inconclusive, we must run IndBH exactly on the modified\nvector ˜p. We do this by computing\nβIndBH\nα,i (˜p−i) := max {r:|eIi(r)| ≥r},\nwhich we call betain Algorithm 1. We adapt the definitions from Section B.2 to add a tilde\nsymbol to mean “computed with ˜pinstead of p”. That is, we let\n|eIi(r)|= 1 + IndNum (Dκ[i][eQ−i(r)]) +X\nk̸=κ[i]IndNum (Dk[eQ(r)])\n= 1 + IndNum (Dκ[i][eQ−i(r)]) +X\nk̸=κ[i]eVk,r,\nwhere eQ(r) ={i: ˜pi≤αr/m}.\nNow given eVfrom Section B.3, it only remains to compute IndNum\u0010\nDκ[i][eQ−i(r)]\u0011\n. We\nagain use Step 2 from Section B.2, but only for the connected component Dκ[i]which contains\ni, and restricting its independent sets to also contain i.\n32\n\nB.6 Inclusion and exclusion: IndBH(ℓ)\nFor completeness, we translate the checks from Section 4.3 into Algorithm 1’s notation.\nAdditionally, we use an auxiliary procedure R(ℓ)∗whose definition is deferred to Section\nB.8—it is a quickly computed subset of R(ℓ)that is the whole set when the connected\ncomponents are cliques.\n1.check(ℓ)−. Whenever i∈ R(ℓ−1)(˜p)ori∈ R(ℓ)∗(˜p), declare i∈ R(ℓ)(˜p). We represent\nC(ℓ)=R(ℓ−1)(˜p)∪ R(ℓ)∗(˜p)in Algorithm 1.\n2.check(ℓ)+. Whenever pi> α|{i} ∪ R(ℓ−1)(˜p)|/m, declare i /∈ R(ℓ)(˜p). We represent\nD(ℓ)={i:ri≤ |{i} ∪ R(ℓ−1)(˜p)|}in Algorithm 1.\nBoth of these checks can be performed on all hypotheses at once.\nNote that we always perform check(ℓ)−first, and any ithat was not rejected by check(ℓ)−\nsatisfies i /∈ R(ℓ−1)(˜p). Hence in our implementation of check(ℓ)+, we take |{i}∪R(ℓ−1)(˜p)|=\n1 +|R(ℓ−1)(˜p)|.\nB.7 A masking detail\nThe main text defined IndBH(k)in Equation (8), reproduced here:\nRIndBH(1)\nα (p) :=RIndBH\nα (p),\nRIndBH(k+1)\nα (p) :=(\ni:pi≤α|{i} ∪ RIndBH(k)\nα (1N◦\nip)|\nm)\n.\nThe following proposition shows that we can use an alternative definition, where every\nelement of Niis masked, not just the punctured neighborhood N◦\ni.\nProposition 9. Fork≥1, let\nAk+1=(\ni:pi≤α|{i} ∪ RIndBH(k)\nα (1N◦\nip)|\nm)\n,\nBk+1=(\ni:i∈ RIndBH(k)\nα (p),orpi≤α|{i} ∪ RIndBH(k)\nα (1Nip)|\nm)\n.\nWe have the equality Ak+1=Bk+1for all k≥1.\nIt also follows from this proposition that masking by Niinstead of N◦\niin the call to\nR(ℓ−1)in Algorithm 1 is correct, because it occurs after check(ℓ)−, which checks whether\ni∈ RIndBH(k)\nα (p), which is convenient for the implementation.\nProof of Proposition 9. First, we know Ak+1⊃ RIndBH(k)\nα (p)from Proposition 4, and com-\nbined with the monotonicity of RIndBH(k)\nαwe have Ak+1⊃ Bk+1for all k≥1. Next, suppose\ni∈ Ak+1. Ifi∈ RIndBH(k)\nα (p), then i∈ Bk+1by definition. If i /∈ RIndBH(k)\nα (p), we can apply\nLemma 2 to equate RIndBH(k)\nα (1Nip) =RIndBH(k)\nα (1N◦\nip)for all i, so that again i∈ Bk+1,\nshowing that Ak+1⊂ Bk+1.\nB.8A fast subset of IndBH(ℓ)based on fully connected components\nGiven an input graph Dwith connected components D1, . . . ,D¯k, define a new graph D∗to\nbe the graph where the nodes of each component form a clique. Since it is more connected\nthanD, theIndBH D∗(α)procedure rejects a subsetofIndBH D(α). Similarly, IndBH(ℓ)\nD∗(α)\nreturns a subset of IndBH(ℓ). We use R(ℓ)∗to refer to the rejection set of IndBH(ℓ)\nD∗(α).\n33\n\nDue to Proposition 8, IndBH D∗is highly computationally efficient. IndBH(ℓ)\nD∗(α)is also\nmore efficient because the same rejection threshold for pi(computed in Appendix B.7), can\nbe used for every p-value in its component. Other smaller optimizations also become possible,\nwhich we leave to the depgraphFDR documentation.\nImportantly, we base D∗on the graph Dobtained afterthe Appendix B.1 filtering,\ndiscarding p-values that exceed α¯r/m. The rejection (sub)sets computed by IndBH D∗(α)\nandIndBH(ℓ)\nD∗(α)hence depend on the choice ¯r—there could be more connected components\nfor a smaller ¯r.\nWe simply use ¯r=|RBH\nα(p)|, but any ¯rsatisfying the conditions of Proposition 7\nwould work. Further details and possible improvements are again left to the depgraphFDR\ndocumentation.\nC Randomized pruning for SU D\nLetu1, . . . , u m∼Unif[0,1]be iid external randomness. Then for any monotone procedure\nRinit\nα(p), define the adjusted procedure Radj\nα, and its pruned version Rrand\nα:\nRadj\nα(p) ={i:pi≤α|{i}∪Rinit\nα(1N◦\nip)|/m}; ˜ui=(\nui|{i} ∪ Rinit\nα(1N◦\nip)|/mifi∈ Radj\nα(p)\n∞ o.w.\nandRrand\nα(p;u) :=RBH\n1(˜u).\nProposition 10. Rrand\nα(p;u)controls FDR under a dependency graph D.\nAs noted by Fithian and Lei (2020), this can also be expressed as the BH(1)procedure\napplied to the p-values\u0000\nui|{i} ∪ Rinit\nα(1N◦\nip)|/|Radj\nα(p)|\u0001\ni∈Radj\nα(p).\nIn this form, it becomes transparent that the pruning step is a patch for a failure of the\nself-consistency property, after one round of gap chasing. If Radj\nαwere actually self-consistent,\nthen no rejections are pruned, and pruning is unlikely if Radj\nαis almost self-consistent.\nProof.Similarly to Theorem 1 it suffices to show that\nE\u00141{i∈ Rrand\nα(p;u)}\n|Rrandα(p;u)|\u0015\n≤α/m.\nWe proceed as\nE\u00141{i∈ Rrand\nα(p;u)}\n|Rrandα(p;u)|\u0015\n=E\u00141{i∈ RBH\n1(˜u)}\n|RBH\n1(˜u)|\u0015\n=E\u00141{i∈ R α(p)}1{i∈ RBH\n1(˜u)}\n|RBH\n1(˜u)|\u0015\n=E\u00141{i∈ R α(p)}1{˜ui≤ |RBH\n1(˜u)|/m}\n|RBH\n1(˜u)|\u0015\n=E\u00141{i∈ R α(p)}1{˜ui≤ |RBH\n1(˜ui←0)|/m}\n|RBH\n1(˜ui←0)|\u0015\n=E\"\n1{i∈ R α(p)}1{ui|{i} ∪ Rinit\nα(1N◦\nip)|/m≤ |RBH\n1(˜ui←0)|/m}\n|RBH\n1(˜ui←0)|#\n=E\"\nE\"\n1{i∈ R α(p)}1{ui|{i} ∪ Rinit\nα(1N◦\nip)|/m≤ |RBH\n1(˜ui←0)|/m}\n|RBH\n1(˜ui←0)|#\n|p, u−i#\n=E\u00141{i∈ R α(p)}\n|{i} ∪ Rinitα(1N◦\nip)|\u0015\n34\n\n≤E\"\n1{pi≤α|{i} ∪ Rinit\nα(1N◦\nip)|/m}\n|{i} ∪ Rinitα(1N◦\nip)|#\n=E\"\nE\"\n1{pi≤α|{i} ∪ Rinit\nα(1N◦\nip)|/m}\n|{i} ∪ Rinitα(1N◦\nip)||pNc\ni##\n≤α/m.\nD BH under a dependency graph\nSection 2.3.2, Equation (5) defined the quantity\nγD(α) := sup\b\nFDR P(RBH\nα) :P∈ PD\t\n.\nIn the ensuing sections, we establish the bounds\n1−¯kY\nk=1\n1−α|Bk|\nm|Bk|X\ns=11/s\n≤γD(α)≤α\nmmX\ni=1\u0010\n|Ni| −\u0000\n|Ni| −1\u0001\nm−1\n|Ni|−1\u0011\n(23)\nwhere B1, . . . , B ¯krepresents any clique cover of D, and N1, . . . , N mrepresents the neighbor-\nhoods of node iinD, and αis such that 1−α|Bk|\nmP|Bk|\ns=11/s > 0for every k.\nTo better interpret this result, first, suppose the graph is fully connected. Then it is a\nclique, and the whole graph can be used as clique cover, while Ni=mfor every i, obtaining\nαmX\ns=11/s≤γD(α)≤α\u0010\nm−\u0000\nm−1\u0001\nm−1\nm−1\u0011\n.\nThis upper bound does not exactly equal the lower bound, as we know it should from\nBenjamini and Yekutieli (2001), but it does satisfy (m−\u0000\nm−1\u0001\nm−1\nm−1)\u000ePm\ns=11/s→1as\nm→ ∞, so it almost recovers the Benjamini and Yekutieli (2001) result.\nNext, suppose Dis a block dependent graph where all blocks have equal size b, the clique\ncover can be chosen to be precisely those blocks. Then there are ¯k=m/bblocks and the\nleft-hand side of (23) is\n1− \n1−αb\nmbX\ns=11/s!m/b\n=αbX\ns=11/s+O(α2)≈αlog(b),\nso in this case we must at least pay approximately an inflation of factor log(b). Unfortunately,\nin this case, and for general graphs, the upper and lower bounds are quite different. We\nconjecture that γD(α)is much closer to our lower bound than our upper bound. Indeed, the\nupper bound is especially loose: the right side of (23) becomes\nα\u0010\nb−\u0000\nb−1\u0001\nm−1\nb−1\u0011\n,\nwhich is O(b)and often becomes vacuous even for moderately large b, exceeding the always-\nvalid bound of αPm\ns=11/s.\nNext, we show how to obtain these bounds.\nD.1 An FDR upper bound\nProposition 11. Whenever Dis a dependency graph for p, the procedure BH(α)controls\nthe FDR at level LDαonp, where\nLD:= (1 /m)mX\ni=1LD,i,and LD,i=|Ni| −\u0000\n|Ni| −1\u0001\nm−1\n|Ni|−1.\n35\n\nProof of Proposition 11. Label the elements of N◦\ni=Ni\\ {i}asN◦\ni={j1, . . . , j ni}, so that\n|N◦\ni|=ni. Let p(i,#k)←0be defined such that\np(i,#k)←0\nj =\n\n0forj∈ {j1, . . . , j k} ∪ {i}\n1forj∈ {jk+1, . . . , j n∗}\npjforj /∈Ni.\nThen let pmax(Si) := sup{pi:∃kwith pi≤α|RBH\nα(p(i,#k)←0)|/m}, and for pi≤pmax, let\nk∗(pi, Si) := min {k:pi≤α|RBH\nα(p(i,#k)←0)|/m}. As in the proof of Theorem 1, we have\nFDR =X\ni∈H0E\u00141{i∈ RBH\nα(p)}\n|RBHα(p)|\u0015\n, (24)\nso it is sufficient to show that\nE\u00141{i∈ RBH\nα(p)}\n|RBHα(p)|\u0015\n≤αLD,i/m (25)\nfor an arbitrary i∈ H 0. We upper bound the conditional expectation almost surely:\nE\u00141{i∈ RBH\nα(p)}\n|RBHα(p)||Si\u0015\n=E\u00141{pi≤α|RBH\nα(p)|/m}\n|RBHα(p)||Si\u0015\n=E\u00141{pi≤α|RBH\nα(pi←0)|/m}\n|RBHα(pi←0)||Si\u0015\n(i)\n≤E\u0014\nmax\n0≤k≤ni1{pi≤α|RBH\nα(p(i,#k)←0)|/m}\n|RBHα(p(i,#k)←0)||Si\u0015\n=Zpmax\n01\n|RBHα(p(i,#k∗(pi,Si))←0)|dpi\n=:Z1\n0g(pi;Si)dpi,\nwhere\ng(pi;Si) :=(\n|RBH\nα(p(i,#k∗(pi,Si))←0)|−1pi∈[0, pmax(Si)]\n0 pi∈[pmax(Si),1].\nThefunction gisnecessarilyapiecewise,decreasingfunction,withrange G⊂ {1,1/2, . . . , 1/m,0}\nwhere |G\\ {0}| ≤ni+ 1, and which satisfies pi≤αg(pi;Si)−1/mwhenever g(pi;Si)>0.\nForany gsatisfyingtheseconditions,enumerateitsrangeas G={1/r1,1/r2, . . . , 1/rni+1,0}\nfor integers 1≤r1≤ ··· ≤ rni+1≤m. Then there exists a function ˜gsatisfying these\nconditions, with the same range, of the form\n˜g(pi) =\n\nr−1\n1 pi∈[0, αr1/m]\nr−1\n2 pi∈[αr1/m, αr 2/m]\n...\nr−1\nni+1pi∈[αrni/m, αr ni+1/m]\n0 pi∈[αrni/m,1]\nin whichR1\n0g(pi;Si)dpi≤R1\n0˜g(pi;Si)dpi, which evaluates to\nZ1\n0˜g(pi)dpi=α\nm\u0012\n1 +\u0012\n1−r1\nr2\u0013\n+\u0012\n1−r2\nr3\u0013\n+···+\u0012\n1−rni\nrni+1\u0013\u0013\n.\n36\n\nThe function g∗which solves max gR1\n0g(pi;Si)dpiis then specified by maximizing the above\nas an integer optimization problem. Relaxing to real valued r1, . . . , r ni+1∈[1, m]instead\ngives the optimizer. r∗\nk=mk−1\nni. Plugging this in gives the upper bound\nZ1\n0g(pi;Si)dpi(ii)\n≤Z1\n0g∗(pi;Si)dpi≤α\nm\u0010\n(ni+ 1)−nim−1/ni\u0011\n.\nRecalling that |Ni|=ni+ 1, using the law of total expectation gives the result.\nThis approach is unoptimizable in the sense that there exists a distribution where the\nconditional expectation is equal to the marginal, and the a.s. inequalities (i) and (ii)\nbecome equalities. But the overall bound is likely very loose, because to achieve it, a single\ndistribution would have to saturate all of the inequalities in (25)at once. A more refined\napproach should probably analyze more than one term of (24) at a time.\nD.2 An FDR lower bound\nLetB1, . . . , B ¯k⊂[m]be a clique cover of D: that is, a partition of the nodes of the\ngraph such that each Bkis a clique in D. Let bk:=|Bk|. LetPkrefer to the probability\ndistribution supported on {0,1, . . . , b k}such that Pr(X=s) = (αbk/m)·1/sfors≥0and\nPr(X= 0) = 1 −(αbk/m)Pbk\ns=1(1/s). It follows that we must restrict αas well so that\nPr(X= 0)≥0.\nGiven such a clique cover, consider the following hierarchical way to sample a p-value\nvector p∈[0,1]m.\n•Independently for each k∈ {1, . . . , ¯k},\n–Sample sk∼Pk.\n–Sample a subset Sk⊂Bkof size skuniformly at random.\n•Conditionally on S1, . . . , S k, independently for i∈ {1, . . . , m },\n–Sample\npi∼(\nUnif[α(sκ[i]−1)/m, αs κ[i]/m]ifi∈Sκ[i]\nUnif[αbκ[i]/m,1] ifi /∈Sκ[i]\nProposition 12. When pis sampled as above, then it has dependency graph D, satisfies\npi∼Unif[0,1]marginally for every i, and under the global null, the BH procedure has\nFDR (RBH\nα)≥1−¯kY\nk=1 \n1−αbk\nmbkX\ns=11/s!\n.\nIt follows that the worst-case FDR γD(α)over all models with dependency graph Dis\nlower bounded by this quantity.\nBlock dependence. Not every choice of clique cover gives a useful bound. For example,\ntaking B1, . . . , B mwith Bi={i}lower bounds the FDR by (1−(1−α/m)m), which is\nsmaller than α. But under block dependence, a good choice of clique cover is the blocks\nthemselves. This choice, with the above sampling distribution, was used in Section 5.3 for\nexhibiting FDR inflation.\n37\n\nProof.The random vector phas dependency graph equal to the disjoint union of the clique\ncovers. It follows that it has dependency graph Dbecause Dis strictly denser than the union,\nin the sense of having more edges.\nAlso, piis marginally uniform, as we can compute its density f(x). For x∈[α(s−\n1)/m, αs/m ]and any s∈ {0, . . . , b κ[i]}, we have\nf(x) = lim\nδx→0Pr(pi∈[x, x+δx], i∈Sκ[i], sκ[i]=s)\nδx\n= lim\nδx→0Pr(pi∈[x, x+δx]|i∈Sκ[i], sκ[i]=s)\nδxPr(i∈Sκ[i]|sκ[i]=s) Pr(sκ[i]=s)\n= lim\nδx→0(1/δx) (m·δx/α )\u0000\ns/bκ[i]\u0001\u0000\nαbκ[i]/m·1/s\u0001\n= lim\nδx→01\n= 1.\nand similarly, for x∈[αbκ[i]/m,1],\nf(x) = lim\nδx→0Pr(pi∈[x, x+δx], i /∈Sκ[i])\nδx\n= lim\nδx→0Pr(pi∈[x, x+δx]|i /∈Sκ[i])\nδxPr(i /∈Sκ[i])\n= lim\nδx→0Pr(pi∈[x, x+δx]|i /∈Sκ[i])\nδx\u0000\n1−Pr(i∈Sκ[i])\u0001\n= lim\nδx→0Pr(pi∈[x, x+δx]|i /∈Sκ[i])\nδx\n1−bκ[i]X\ns=0Pr(i∈Sκ[i]|sκ[i]=s) Pr(sκ[i]=s)\n\n= lim\nδx→0(1/δx)δx\n(1−αbκ[i]/m)\n1−bκ[i]X\ns=1\u0000\ns/bκ[i]\u0001\u0000\nαbκ[i]/m·(1/s)\u0001\n\n= lim\nδx→01\n= 1.\nFinally, we lower bound the FDR of BH under the global null, which is equal to the probability\nof rejecting any hypothesis. Since BH rejects if any sk>1, and since the s1, . . . , s ¯kare\njointly independent, we can write\nFDR (RBH\nα)≥Pr(sk>0for some k∈ {1, . . . , ¯k})\n= 1−Pr(sk= 0for all k∈ {1, . . . , ¯k})\n= 1−¯kY\nk=1 \n1−αbk\nmbkX\ns=11/s!\n.\nE Further examples\nE.1 A naive adjustment of BH.\nConsider the procedure\nRnaiv Dα(p) ={i:pi≤α|RBH\nα(1N◦\nip)|/m},\nwhich is a simple modification of the BH procedure’s thresholds which makes it neighbor-\nblind, and it is monotone as well. But it loses the self-consistency property and does not\ncontrol FDR under dependency graph D, which we can demonstrate with 3p-values.\n38\n\nProposition 2. There exists a distribution Ponm= 3p-values with dependency graph D\nsuch that FDR P(Rnaiv D)> α.\nProof.LetDbe the graph with nodes {1,2,3}and, aside from self-edges, only a single edge\nbetween {2,3}. Then apply the construction of Section D.2 with the cover {{1},{2,3}},\nwhich has dependency graph D.\nWhen p∼Pis sampled in this way, then under the global null,\nFDR P(Rnaiv Dα(p)) =α\u0012\n1 +2α2(1−α)\n3(3−2α)2\u0013\n> α\nby reading off of Table 1.\nTo understand this failure more intuitively, it is revealing to inspect the FDR contribution\ntermsFDR i(Rα) :=E\u0002\n1{i∈ R α(p)}/|Rα(p)|\u0003\n. In this case, because node 1has no other\nnodes in its neighborhood, N◦\n1=∅and\nFDR 1(Rα) =E\u00141{1∈ RBH\nα(p)}\n|Rα(p)|\u0015\n=E\u00141{p1≤α|RBH\nα(p)|/m}\n|Rα(p)|\u0015\n> α/m, (26)\nwhere in this case m= 3p-values. Our main approach, used in Theorem 1 and 2, would\nhave been to show that FDR i(Rα)≤α/mfor every i.\nRoughly, the set RBH\nα(1N◦\n1p) =RBH\nα(p)used in p1’s local threshold does not account for\nwhat the other agents can “see”: that is, p2andp3cannot see each other’s p-values in their\nthresholds, which can lead to fewer rejections such that RBH\nα(1N◦\n1p)fails to be a rejection\nlower bound. Overall, RBH\nα(1N◦\n1p)>|Rα(p)|too often, and H1is rejected too frequently, so\nFDR 1(Rα)> α/m.\nIn this specific example, we also have FDR 2(Rα) =FDR 3(Rα)< α/m, but the total\nFDR (Rα) =P3\ni=1FDR i(Rα)still exceeds α.\nE.2 The sufficient conditions are not necessary.\nExample E.1 (Neighbor-blindness is not necessary) .The BY procedure is not neighbor-\nblind, but controls FDR under any dependency graph D.\nExample E.2 (Monotonicity is not necessary) .The procedure that rejects Hiif and only if\npi∈[α/m, 2α/m]where mis the number of hypotheses is not monotone, but controls FDR\nunder any dependency graph D.\nExample E.3 (Self-consistency is not necessary) .Applying conditional calibration as\ndescribed by the discussion surrounding Equation (20)controls FDR under a dependency\ngraph D, but is not self-consistent in general.\nE.3 IndBH(∞)is not optimal.\nFigure 10 demonstrates that IndBH(∞)(α)is different from SU (α)in general.\nF Closure properties for D-adapted procedures\nGiven an initial procedure Rαand an independent set I∈Ind(D), define N◦\nI=S\ni∈IN◦\ni,\nand\nRI\nα(p) =(\nI∪ Rα(p)if∀i∈I, p i≤α|I∪ Rα(1N◦\nIp)|/m\nRα(p)otherwise\n39\n\nEvents {p1∈A1} { p1∈A2} { p1∈A3} { p1∈A×}\n{p2∈A2, p3∈A2}α2\n9,{1,2,3}α2\n9,{1,2,3}α2\n9,{1}α(1−α)\n3,{}\n{p2∈A1, p3∈A3}α2θ\n9,{1,2}α2θ\n9,{1,2}α2θ\n9,{1,2}α(1−α)θ\n3,{2}\n{p2∈A1, p3∈A×}α2(1−θ)\n9,{1,2}α2(1−θ)\n9,{1,2}α2(1−θ)\n9,{2}α(1−α)(1−θ)\n3,{2}\n{p2∈A3, p3∈A1}α2θ\n9,{1,3}α2θ\n9,{1,3}α2θ\n9,{1,3}α(1−α)θ\n3,{3}\n{p2∈A×, p3∈A1}α2(1−θ)\n9,{1,3}α2(1−θ)\n9,{1,3}α2(1−θ)\n9,{3}α(1−α)(1−θ)\n3,{3}\n{p2∈A3, p3∈A3}α(1−α)θ2\n3,{1}α(1−α)θ2\n3,{1}α(1−α)θ2\n3,{1} (1−α)2θ2,{}\n{p2∈A3, p3∈A×}α(1−α)θ(1−θ)\n3,{1}α(1−α)θ(1−θ)\n3,{}α(1−α)θ(1−θ)\n3,{} (1−α)2θ(1−θ),{}\n{p2∈A×, p3∈A3}α(1−α)θ(1−θ)\n3,{1}α(1−α)θ(1−θ)\n3,{}α(1−α)θ(1−θ)\n3,{} (1−α)2θ(1−θ),{}\n{p2∈A×, p3∈A×}α(1−α)(1−θ)2\n3,{1}α(1−α)(1−θ)2\n3,{}α(1−α)(1−θ)2\n3,{} (1−α)2(1−θ)2,{}\nTable 1: Table of joint probabilities and rejection sets for the events in the row and column\nheaders, given the joint distribution and multiple testing procedure of Example XX. We set\nAk= [α(k−1)/3, αk/ 3],A×= [α,1], and θ=α/3\n1−2α/3.\nThis can be related to the gap chasing update of Theorem 3. Let\nR+\nα(p) =(\ni:pi≤α|{i} ∪ R α(1N◦\nip)|\nm)\n,\nsoR+\nαis the result of the update applied to Rα. We remark that if RαisD-adapted, then\nthe formula\nR+\nα(p) =m[\ni=1R{i}\nα(p)\nholds—the inclusion R+\nα(p)⊂Sm\ni=1R{i}\nα(p)can be seen directly, while the reverse inclusion\nR+\nα(p)⊃Sm\ni=1R{i}\nα(p)can also be seen directly once we recall from the proof of Theorem\n3(a) that R+\nα(p)⊃ R α(p)whenever RαisD-adapted.\nThe next two propositions, combined with the above observation, give an alternate proof\nfor Theorem 3(a). However, we frame them now as closure properties of the family of\nD-adapted procedures. Each defines an operation on D-adapted procedures that returns a\nnew such procedure.\nProposition 13. LetR(1)\nαandR(2)\nαareD-adapted, and define Run\nα(p) =R(1)\nα(p)∪ R(2)\nα(p).\nThenRun\nαisD-adapted.\nProof.Proven as part of the proof of Proposition 3.\nProposition 14. IfRαisD-adapted, then for any I∈Ind(D),RI\nαisD-adapted.\nProof.Self-consistency holds because if i∈ R α(p), then\npi≤α|Rα(p)|/m≤ |RI\nα(p)|/m\nby self-consistency of Rα, and if i∈ RI\nα(p)but not in Rα(p), we must have I∪Rα(p) =RI\nα(p)\nand\npi≤α|I∪ Rα(1N◦\nIp)|/m≤α|I∪ Rα(p)|/m≤α|RI\nα(p)|/m.\n40\n\n1\n234\n5p1= 0.00\np2= 0.00p3= 0.00p4= 0.04\np5= 0.04\nFigure 10: When α= 0.05, theIndBH(∞)(α)procedure gives the same rejection set as\nIndBH(2)(α)—the fixed point is reached after only one iteration. This rejection set is\nH3, H4, H5, but SU (α)rejects everything.\nMonotonicity holds because Rαis monotone. Finally, neighbor-blindness holds because, if\ni /∈I, then i∈ RI\nα(1N◦\nip)⇒i∈ RI\nα(p)by the neighbor-blindness of Rα, and if i /∈I, the\nevent {∀i∈I, p i≤α|I∪ Rα(1N◦\nIp)|/m}does not depend on any p-values in N◦\ni, so again\ni∈ RI\nα(1N◦\nip)⇒i∈ RI\nα(p). Monotonicity gives i∈ RI\nα(1N◦\nip)⇐i∈ RI\nα(p).\nGiven this, a natural object would be the family of procedures generated by these\noperations, given a starting set of procedures. But even if the initial procedure is trivial, we\ncan derive non-trivial procedures. For example, if Rαalways returns the empty set, applying\nProposition 14 with any independent set Ito it gives the set\nRI\nα(p) =(\nIifI=RBH\nα(1Icp)\n∅o.w..\nWe can repeat this for all I′⊂Iand use Proposition 13 to union them all together, to\nget the set RBH\nα(1Icp) =∪I′⊂IRI\nα(p). Then using Proposition 13 for all I∈Ind(D)gets\nRIndBH Dα (p).\nFor a given α, letcloD(α)be the D-adapted closure —the family of D-adapted procedures\nthat can be generated using Propositions 13 and 14, starting from the set {∅}, where ∅is\ntheD-adapted “procedure” that always returns the empty set. We have seen that for any\nI∈Ind(D), both RBH\nα(1Ic(·))andRIndBH\nαare in cloD(α). However, it contains more.\nProposition 15. Forα∈[0,1),RSUDαis contained inside cloD(α).\nBecause SUDis optimal, this suggests that the operations of Propositions 13 and 14 in\nsome sense maximally explore the space of D-adapted procedures.\nProof.LetRαbeD-adaptive. Due to the optimality of SUD, it is enough to show that\nRα(p)⊂ R′\nα(p)for some R′\nα∈cloD(α)for all p. Our strategy will to be induct over\nsubsets K⊂[m]in decreasing cardinality, showing that for every Kthere exists a procedure\nR′\nα∈cloD(α)satisfying\nRα(1N◦\nKp)⊂ R′\nα(1N◦\nKp)for all p. (27)\nThe result follows by taking K=∅.\nFor what follows, let K∗⊂Kdenote the D-singletons inK— elements of Kwhich, seen\nas nodes in D, are disconnected from all other nodes in K. (Note that K∗is always an\nindependent set.) Then for any K⊂[m],\n(N◦\nK)c⊂K∗∪Kc. (28)\n41\n\nFirst, let A= [m], which will serve as an induction base case. Then\nRα(1N◦\nAp)⊂ RBH\nα(1N◦\nAp) =RBH\nα(1A∗cp),\nbut we saw, in the discussion around the definition of cloD(α), that if R′(p) :=RBH\nα(1A∗cp) =\nR′(1A∗cp), we know R′is incloD(α), because A∗is an independent set.\nNext, assume that the induction hypothesis (27)holds for all Kof cardinality k+ 1, and\nsuppose that A⊂[m]has cardinality k. Because α <1, self-consistency gives i∈ R α(p)⇒\npi<1, and we have the implications\ni∈ R α(1N◦\nAp)⇒i∈(N◦\nA)c⇒i∈A∗ori /∈A,\nby (28). Then by neighbor-blindness of Rα,\nRα(1N◦\nAp) =[\nj∈Rα(1N◦\nAp)Rα(1N◦\nj1N◦\nAp)⊂A∗∪[\nj /∈ARα(1N◦\nj1N◦\nAp)⊂A∗∪ Qα(1N◦\nAp)\nwhere Qα(p) :=S\nj /∈ARj\nα(p)for some procedures Rj\nα∈cloD(α)satisfying Rj\nα(1N◦\nj1N◦\nAp)⊃\nRα(1N◦\nj1N◦\nAp), which exist by induction. Because it takes unions, we have Qα(p)∈cloD(α).\nFor a set T⊂[m], let\nSC(T) =[\n{S⊂T:∀i∈S, p i≤α|S|/m}.\nWhenever T1⊂T2, it follows that SC(T1)⊂SC(T2). Using this fact, along with the\nself-consistency of Rα, as well as of Qαfrom Propositions 13 and 14, we have\nRα(1N◦\nAp) = SC( Rα(1N◦\nAp))⊂SC(Rα(1N◦\nAp)) =B∗∪ Qα(1N◦\nAp)\nfor some subset B∗⊂A∗, such that for all i∈B∗, we have pi≤α|Qα(1N◦\nAp)|/m.\nBut then it follows that, because 1N◦\nA1N◦\nB∗=1N◦\nA, and defining\nQB∗\nα(p) :=(\nB∗∪ Qα(p)∀i∈B∗, pi≤α|Qα(1N◦\nB∗p)|/m\nQα(p) o.w.\nwe must have QB∗\nα(1N◦\nAp) =B∗∪ Qα(1N◦\nAp), and hence Rα(1N◦\nAp)⊂ QB∗\nα(1N◦\nAp). This\nchoice of B∗works for a specific p, but we can always take\nR′\nα(p) =[\nB∗⊂A∗QB∗\nα(p)\nso that Rα(1N◦\nAp)⊂ R′\nα(1N◦\nAp)for all p. Since we only used the operations of Proposition\n13 and 14, R′\nα∈cloD(α), we showed the inductive step as needed.\nG More simulation results\nHere we reproduce several more simulation settings by varying the parameters in Figures 5\nand 6. We also compare against two additional methods at level α= 0.1, called BYgraph\nandeBHin the figures.\nBYgraph refers to the BH (α′)procedure where\nα′= sup\n\na: 1− \n1−ab\nmbX\ns=11/s!m/b\n≤α\n\n,\nwith b= 100throughout to match the underlying simulation settings. This is based on\nthe FDR lower bound derived in Appendix D, specifically the version for block dependence\n42\n\nwhere all blocks have size b. We do not show any formal FDR guarantees for this procedure.\nHowever, when the graph Ddoes encode equally b-sized blocks, the power of BYgraph upper\nbounds the power of BH (γ−1\nD(α)), where γD(·)was defined in (5).\neBHrefers to a way to use the eBH procedure of Wang and Ramdas (2021), modeled\nafter their Example 2. We obtained e-values ei=λpλ−1\niwith λ= 1/2, and ran eBH with\nboosting factor (2/α)1/2. Effectively, this is just running the BH procedure with modified\nlevel α′=√\n2αand modified p-values p′\ni= 2√pi.\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3\nFigure 11: Results for block dependence with scattered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.6, ρ= 0.5, b= 100. (This is the\nsame setting as Figure 5).\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3\nFigure 12: Results for banded dependence with clustered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.6, ρ= 0.5, b′= 100 , λ0= 20, τ=\n6. (This is the same setting as Figure 6.)\n43\n\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3Figure 13: Results for block dependence with scattered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.99,tarpow = 0.6, ρ= 0.5, b= 100.\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3\nFigure 14: Results for banded dependence with clustered signals. FDR control level set\ntoα= 0.1. Simulation parameters set to π0= 0.99,tarpow = 0.6, ρ= 0.5, b′= 100 , λ0=\n20, τ= 6.\n44\n\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3Figure 15: Results for block dependence with scattered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.3, ρ= 0.5, b= 100.\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3\nFigure 16: Results for banded dependence with clustered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.9,tarpow = 0.3, ρ= 0.5, b′= 100 , λ0= 20, τ=\n6.\n45\n\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.0000.0250.0500.0750.100\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3Figure 17: Results for block dependence with scattered signals. FDR control level set to\nα= 0.1. Simulation parameters set to π0= 0.99,tarpow = 0.3, ρ= 0.5, b= 100.\nfixed signal strength random signal strengthFDR TP Ratio\n100 1000 10000 100 1000 100000.000.030.060.09\n0.000.250.500.751.00\nmmethod\nBH\nBY\nBYgraph\neBH\nIndBH1\nIndBH3\nFigure 18: Results for banded dependence with clustered signals. FDR control level set\ntoα= 0.1. Simulation parameters set to π0= 0.99,tarpow = 0.3, ρ= 0.5, b′= 100 , λ0=\n20, τ= 6.\n46\n\n",
    "source": "http://arxiv.org/abs/2506.24126v1",
    "authors": [
      "Drew T. Nguyen",
      "William Fithian"
    ],
    "categories": [
      "stat.ME",
      "math.ST",
      "stat.TH"
    ],
    "type": "content"
  },
  {
    "id": "2506.24125v1_abstract",
    "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation",
    "content": "Title: FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation\n\nAbstract: Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.",
    "source": "http://arxiv.org/abs/2506.24125v1",
    "authors": [
      "Jiacheng Cui",
      "Xinyue Bi",
      "Yaxin Luo",
      "Xiaohan Zhao",
      "Jiacheng Liu",
      "Zhiqiang Shen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24125v1_content",
    "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation",
    "content": "arXiv:2506.24125v1  [cs.CV]  30 Jun 2025FADRM: Fast and Accurate Data Residual Matching\nfor Dataset Distillation\nJiacheng Cui∗1, Xinyue Bi∗2, Yaxin Luo1, Xiaohan Zhao1, Jiacheng Liu1, Zhiqiang Shen†1\n1VILA Lab, MBZUAI2University of Ottawa\n∗Equal Contribution†Corresponding Author\nCode: FADRM (GitHub)\nAbstract\nResidual connection has been extensively studied and widely applied at the model\narchitecture level. However, its potential in the more challenging data-centric ap-\nproaches remains unexplored. In this work, we introduce the concept of Data Resid-\nual Matching for the first time, leveraging data-level skip connections to facilitate\ndata generation and mitigate data information vanishing. This approach maintains\na balance between newly acquired knowledge through pixel space optimization and\nexisting core local information identification within raw data modalities, specifi-\ncally for the dataset distillation task. Furthermore, by incorporating optimization-\nlevel refinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU mem-\nory usage by 50%. Consequently, the proposed method Fast and Accurate Data\nResidual Matching for Dataset Distillation ( FADRM ) establishes a new state-\nof-the-art, demonstrating substantial improvements over existing methods across\nmultiple dataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and 50.0%\nin multi-model dataset distillation, surpassing RDED by +5.7% and outperforming\nstate-of-the-art multi-model approaches, EDC and CV-DD, by +1.4% and +4.0%.\n1 Introduction\nFADRM+ (Ours)FADRM (Ours)SRe2L++ EDCG-VBSM\nIPC=50 IPC=10 IPC=1\nFigure 1: Total training hours on a single RTX-\n4090 vs.test set accuracy, comparing prior state-\nof-the-art methods with our proposed framework\n(+ denotes multi-model distillation).In recent years, the computer vision and natural\nlanguage processing communities have predom-\ninantly focused on model-centric research, driv-\ning an unprecedented expansion in the scale of\nneural networks. Landmark developments such\nas LLMs and MLLMs in ChatGPT [ 25,1], Gem-\nini [35], DeepSeek [ 18] and other large-scale\nfoundation models have shown the tremendous\npotential of deep learning architectures. How-\never, as these models grow in complexity, the\ndependency on high-quality, richly informative\ndatasets has become increasingly apparent, set-\nting the stage for a paradigm shift towards data-\ncentric approaches. Historically, the emphasis\non building bigger and more complex models\nhas often overshadowed the critical importance\nof the data. While model-centric strategies have\nPreprint.\n\n200×200Distilled Images\nResampler\nMerge ɑ=0.5Downsample \nMerge ɑ=0.5Residual Block 1 Residual Block 2\nMerge ɑ=0.5Residual Block 3UpsampleDownsampleOriginal Patches\nRecover\nRecover\nOriginal \nPatches\n200×200224×224 200×200 224×224Residual224×224\nData Residual Block \nData Residual Block \nData Residual Block \n224×224\nRecover\nupdate\n +\nFigure 2: Overview of FADRM . It starts by downsampling the real data patches (both 1 ×1 and\n2×2 [32] can be used as initialization and perform well in our experiments, meanwhile imposing\ndownsampling to reduce cost). These downsampled images are subsequently processed through\na series of proposed Data Residual Blocks . Each block utilizes a pretrained model to optimize\nthe images within a predefined optimization budget, resamples them to a target resolution, and\nincorporates residual connections from the original patches via a mixing ratio α. Finally, the images\nundergo an additional recovery stage, without residual connections, to produce the final distilled data.\ndelivered impressive results, they tend to overlook the benefits of optimizing data quality, which\nis essential for achieving higher performance with lower data demands. Recent advancements in\ndata-centric research highlight the importance of improving information density, reducing the volume\nof required data, and expediting the training process of large-scale models, thus presenting a more\nholistic approach to performance enhancement.\nWithin this evolving landscape, dataset distillation [ 38], also called dataset condensation [ 14,47,43]\nhas emerged as a pivotal area of research. The goal of dataset distillation is to compress large-scale\ndatasets into smaller, highly informative subsets that retain the essential characteristics of the original\ndata. This approach not only accelerates the training process of complex models but also mitigates the\nstorage and computational challenges associated with massive datasets. Despite significant progress,\nmany existing state-of-the-art methods in dataset distillation still struggle with issues related to\nscalability, generalization across diverse data resolutions, realism and robustness.\nWhile residual connections have been well studied and widely implemented in the model architecture\ndesign field, primarily to prevent gradient vanishing and ensure effective feature propagation, their\npotential within data-centric paradigms remains largely unexplored. At the model level, residual\nconnections help maintain the flow of gradients and enable deeper network architectures. In contrast,\nat the data level, similar connections can potentially prevent the loss of critical original dataset\ninformation and improve scalability and generalization across architectures during the data distillation\nprocess. This observation and design introduce a novel perspective on leveraging residual mechanisms\nbeyond traditional model optimization, especially in the challenging domain of dataset optimization.\nIn this work, we introduce for the first time the concept of Data Residual Matching for dataset\ndistillation. Our approach leverages data-level skip connections, a novel idea for data-centric task\nto prevent real data information vanishing in multi-block data synthesis architecture. We call our\nmethod Fast and Accurate DataResidual Matching ( FADRM ), which, as shown in Fig. 2, employs\na multi-resolution image recovery scheme that utilizes image resolution shrinkage and expansion\nin a residual manner, thereby capturing fine-grained details and facilitating the recovery of both\nglobal and local information. This balance between newly acquired knowledge through pixel space\n2\n\noptimization and the preservation of existing core local information within raw data modalities marks\na significant advancement in dataset distillation. By integrating these data-level residual connections,\nour approach enhances the generalization and robustness of the distilled datasets.\nExhaustive empirical evaluations of our proposed FADRM on CIFAR-100 [ 15], Tiny-ImageNet [ 41],\nImageNet-1k [ 8] and its subset demonstrate that it not only accelerates the dataset distillation process\nby 50% but also achieves superior accuracy that beats all previous state-of-the-art methods on both\naccuracy and generation speed. This approach effectively bridges the gap between model-centric\nand data-centric paradigms , providing a robust solution to the challenges inherent in high-quality\ndata generation. Our contributions in this paper are as follows:\n•We extend conventional residual connection from the model level to the data level area, and\npresent for the first time a simple yet effective, theoretically grounded residual connection\ndesign for data generation to enhance data-centric task.\n•We introduce a novel dataset distillation framework based on the proposed data residual\nmatching , incorporating multi-scale residual connections in data synthesis to improve both\nefficiency and accuracy.\n•Our approach achieves state-of-the-art results across multiple datasets, such as CIFAR-100,\nTiny-ImageNet and ImageNet-1k, while being more efficient and requiring less computa-\ntional cost than all previous methods.\n2 Related Work\nDataset Distillation aims to synthesize a compact dataset that retains the critical information of\na larger original dataset, enabling efficient training while maintaining performance comparable to\nthe full dataset. Overall, the matching criteria include Meta-Model Matching [38,24,22,49,9,12],\nGradient Matching [47,45,17,14,48],Trajectory Matching [4,7,5,10], Distribution Matching [ 46,\n37,20,16,26,31,40], and Uni-level Global Statistics Matching [43,29,30,42,6,39]. Dataset\ndistillation on large-scale datasets has recently attracted significant attention from the community.\nFor a detailed overview, it can be referred to the newest survey works [28, 19] on this topic.\nEfficient Dataset Distillation. Several methods improve the computational efficiency of dataset\ndistillation. TESLA [ 7] accelerates MTT [ 4] via batched gradient computation, avoiding full graph\nstorage and scaling to large datasets. DM [ 46] sidesteps bi-level optimization by directly matching\nfeature distributions. SRe2L [43] adopts a Uni-Level Framework that aligns synthetic data with\npretrained model statistics. G-VBSM [ 29] extends this by using lightweight model ensembles.\nEDC [30] further boosts efficiency through real data initialization, accelerating convergence.\nResidual Connection in Network Design. Residual connections have played a pivotal role in\nadvancing deep learning. Introduced in ResNet [ 11] to alleviate vanishing gradients, they enabled\ndeeper networks by improving gradient flow. This idea was extended in Inception-ResNet [ 33] through\nmulti-scale feature integration, and further generalized in DenseNet [ 13] via dense connectivity and\nfeature reuse. Residual designs have also been central to Transformer architectures [36].\n3 Approach\nPreliminaries. Let the original dataset be denoted by O={(xi, yi)}|O|\ni=1, and let the goal of dataset\ndistillation be to construct a compact synthetic dataset C={(˜xj,˜yj)}|C|\nj=1,with|C| ≪ |O| , such that\nthe model fθCtrained on Cexhibits similar generalization behavior to the model fθOtrained on O.\nThis objective can be formulated as minimizing the performance gap over the real data distribution:\narg min\nC,|C|sup\n(x,y)∼O|L(fθO(x), y)− L(fθC(x), y)| (1)\nwhere the parameters θOandθCare obtained via empirical risk minimization:\nθO= arg min\nθE(x,y)∼O[L(fθ(x), y)], θC= arg min\nθE(˜x,˜y)∼C[L(fθ(˜x),˜y)]. (2)\nThe goal is to generate Cin order to maximize model performance with minimal data. Among existing\nmethods, a notable class directly optimizes synthetic data without access to the original dataset,\n3\n\nreferred to as uni-level optimization . While effective, this approach faces two key limitations: (1)\nprogressive information loss during optimization, termed information vanishing , and (2) substantial\ncomputational and memory costs for large-scale synthesis, limiting real-world applicability.\nInformation Vanishing. In contrast to images distilled using bi-level frameworks, the information\ncontent in images generated by uni-level methods (e.g., EDC [ 30]) is fundamentally upper-bounded,\nas the original dataset is not utilized during synthesis (see Theorem 1). As optimization progresses,\nthe information density initially increases but eventually deteriorates due to the accumulation of local\nfeature loss. This degradation leads to information vanishing (see Fig. 3), which significantly reduces\nthe fidelity of the distilled images and limits their effectiveness in downstream tasks.\nTheorem 1 (Proof in Appendix A.2) .Letfθbe a pretrained neural network on original dataset Owith\nfixed parameters and BatchNorm layers’ mean and variance BNRMandBNRV. Let˜xdenote an im-\nage optimized by minimizing the following loss: L(C) =ℓCE(fθ(˜x),˜y)+λ(P\nl\r\rµl(˜x)−BNRM\nl\r\r\n2+P\nl\r\rσ2\nl(˜x)−BNRV\nl\r\r\n2). Define:\nH\u0000\nfθ\u0001\n= sup\nx∈supp(O)H\u0000\nfθ(x)\u0001\n, (3)\nas the maximum per-sample Shannon entropy of the network’s output. Then, the mutual information\nbetween the optimized distilled dataset C={(˜xj,˜yj)}|C|\nj=1and the original dataset Ois bounded by:\nI(C;O)≤ |C| H(fθ). (4)\nThe insight of this theorem is that if the pretrained model fθis overly confident on all inputs (low\nmaximum entropy), then H(fθ)is small, and thus the distilled set, no matter how we optimize it,\ncannot encode a large amount of information about O.\nARC Moments\n0 200 400 600 800 1000\nClass Index2.82.93.03.13.23.3Average Information ScoreSRe2L++\nRDED\nFADRM\nFigure 3: The above figures illustrate the phenomenon of Information Vanishing . The Left Figure\nshows the evolution of information density across optimization steps, quantified through feature-level\nentropy using a pretrained ResNet-18 [ 11], comparing uni-level optimization (W/O ARC ) with our\nFADRM (W/ARC ). The gray lines highlight the information density enhancement achieved through\nresidual connection. The Right Figure shows the comparison of information scores (higher is better)\nacross different classes, measured by pixel-level entropy , among FADRM , SRe2L++, and RDED. All\nexperiments are conducted on a distilled ImageNet-1k dataset with IPC=10.\nComputational Challenges. Although uni-level frameworks exhibit scalability to large-scale datasets,\nthe overall time required to generate a large distilled dataset remains prohibitively expensive. As\nillustrated in Fig. 1, EDC [ 30] requires nearly 70 hours to generate a 50 IPC distilled dataset, which\nlimits its applicability in contexts involving repeated runs, large-scale data synthesis, or comprehensive\nempirical analysis. This motivates the need for more computationally efficient optimization strategies.\n3.1 Overview of FADRM\nThe proposed FADRM framework, as illustrated in Fig. 2 and detailed in Algorithm 1, addresses the\nlimitations of existing uni-level optimization frameworks by integrating three proposed components:\n(1)MPT : a mixed-precision training scheme that accelerates optimization and reduces computation\n4\n\nby casting model parameters to lower-precision formats, (2) MRO : a multiple resolution optimization\nthat improves computational efficiency, and (3) ARC : an adjustable embedded residual mechanism\ndesigned to seamlessly integrate essential features from the original dataset. This framework ensures\nboth efficiency and generation fidelity in the optimization process.\nAlgorithm 1 FADRM : Residual Matching for Dataset Distillation\nRequire: Recover model fθ, budget B, real patches Ps, merge ratio α, downsampled resolutions\nDds, original resolutions Dorig, number of ARC sk\nEnsure: Distilled image ˜xB\n1:b← ⌊B /(k+1)⌋,˜x0←RESAMPLE (Ps,Dds)\n2:fori= 1tokdo\n3: fort= 1tobdo\n4: ˜x(i−1)b+t←GRADSTEP(fθ,˜x(i−1)b+t−1) ▷Optimize ˜xto align the property of fθ\n5: end for\n6: ˜xib←\u001aRESAMPLE (˜xib, Dorig),ifShape (˜xib) =Dds\nRESAMPLE (˜xib, Dds),otherwise\n7: ˜xib←α˜xib+ (1−α)·RESAMPLE (Ps,Shape (˜xib))\n8:end for\n9:fort= 1toB −kbdo\n10: ˜xkb+t←GRADSTEP(fθ,˜xkb+t−1)\n11:end for\n12:return ˜xB\n3.2 Mixed Precision Training for Data Generation\nPrevious uni-level frameworks typically retain a fixed training pipeline, seeking efficiency through\narchitectural or initialization-level changes. In contrast, we explicitly optimize the training process by\nincorporating Mixed Precision Training (MPT) [ 23]. Specifically, we convert the model parameters θ\nfrom FP32 to FP16 and utilize FP16 for both logits computation and cross-entropy loss evaluation.\nTo preserve numerical stability and ensure accurate distribution matching, we retain the computation\nof the divergence to the global statistics (Appendix B), as well as the gradients of the total loss with\nrespect to ˜xin FP32. By integrating MPT , our framework significantly reduces both GPU memory\nconsumption and training time by approximately 50%, thereby significantly enhancing efficiency.\n3.3 Multi-resolution Optimization\nMulti-Resolution Optimization ( MRO ) enhances computational efficiency by optimizing images\nacross multiple resolutions, unlike conventional methods that operate on a fixed input size. Naturally,\nlow-resolution inputs can reduce computational cost for the model, they often come at the expense\nof performance. To mitigate this, our method periodically increases the data resolution back at\nspecific stages, resulting in a mixed-resolution optimization process, as illustrated in Fig. 2 (bottom-\nright). This approach is particularly beneficial for large-scale datasets (e.g., ImageNet-1K), where\ndirect high-resolution optimization is computationally inefficient. Notably, optimization time scales\nsignificantly with input size for large datasets but remains stable for smaller ones (input size ≤64).\nThus, MRO is applied exclusively to large-scale datasets, as downscaling offers no efficiency gains\nfor smaller ones. Specifically, given an initialized image Ps∈RDorig×Dorig×C, we first downsample it\ninto a predefined resolution Ddsutilizing bilinear interpolation (detailed in Appendix C):\n˜x0=Resample (Ps, Dds), D ds< D orig (5)\nThe downscaled images ˜x0are then optimized within a total budget b, yielding the refined version ˜xb.\nSubsequently, ˜xbare upscaled to their original dimensions:\n˜xb=Resample (˜xb, Dorig) (6)\nThe upscaled image ˜xbis further optimized within the same budget bto recover information lost\nduring the downscaling and upscaling processes. This iterative procedure (downscaling optimization\nand upscaling optimization) is repeated until the total optimization budget Bis exhausted. To ensure\n5\n\nMRO achieves efficiency gain without compromising quality, selecting an appropriate Ddsis critical.\nExcessively small Ddsrisks significant information loss, degrading distilled data’s quality, while\noverly large Ddsoffers negligible efficiency benefits. Thus, Ddsmust be carefully calibrated to\nbalance efficiency and effectiveness.\nSaved Computation by MRO .Assume image-level convolution cost scales as O(D2C). The\nbaseline method performs all Bsteps at full resolution Dorig, yielding:\nCost baseline =B · O (D2\norigC) (7)\nFADRM performs kalternating-resolution stages of b=⌊B/(k+1)⌋steps, with approximately half\nat downsampled resolution Dds. Letr= (Dds/D orig)2. The normalized cost is:\nCost MRO\nCost baseline= 1−b\nB·\u0000\u0006k\n2\u0007\n(1−r)\u0001\n(8)\nUnder fixed B, k, the cost ratio decreases linearly with 1−r. Smaller r(i.e., more aggressive\ndownsampling) yields greater savings, but may compromise data fidelity. This trade-off highlights\nthe role of rin balancing efficiency and representation quality during distillation.\n3.4 Adjustable Residual Connection\nα = 0.5 α = 0.6 α = 0.7 α = 0.8 α = 0.9 α = 1.0Ice Bear Samoyede Airliner Volcano\nFigure 4: Visualization of the distilled images with\nvarying merge ratios using FADRM .In uni-level optimization, the absence of the\noriginal dataset leads to information vanishing\nwhich significantly degrades the feature rep-\nresentation of the distilled dataset. To miti-\ngate this issue, we introduce Adjustable Resid-\nual Connection ( ARC ), a core mechanism that\nmitigates information vanishing (see Fig. 3)\nand improves the robustness of the distilled\ndata (see Theroem 2). Essentially, ARC iter-\natively fuses the intermediate optimized image\n˜xt∈RDt×Dt×Cat iteration twith the resized\ninitialized data patches ˜Pt, which contain subtle\ndetails from the original dataset. Formally, the\nupdate rule is defined as:\n˜xt=α˜xt+ (1−α)Resample (Ps, Dt) (9)\nwhere α∈[0,1]is a tunable merge ratio gov-\nerning the contribution of original dataset infor-\nmation. A smaller αstrengthens the integration of details from Ps, whereas a larger αprioritizes\nthe preservation of the global features in the ˜xt. This trend is visualized in Fig. 4. ARC introduces a\nhyperparameter k, which determines the frequency of residual injections. Given a total optimization\nbudget of B, the training process is divided into k+ 1segments, where residual connections occur\nafter every b=⌊B/(k+ 1)⌋iterations. The update follows:\n˜Pib= Resample\u0000\nPs, Dib\u0001\n,˜xib=α˜xib+ (1−α)˜Pib. (10)\nwhere i∈ {1,2, . . . , k }denotes the index of the residual injection stage, and Dibindicates the spatial\nresolution of the intermediate image at the corresponding iteration t=ib. The final phase consists\nof purely optimization without additional residual injections. Notably, ARC performs a per-element\nweighted fusion of two image tensors with negligible overhead. With a complexity of O(HtWtC), it\nscales linearly with the number of pixels and channels, making it well-suited for high-resolution data.\nTheorem 2 (Proof in Appendix A.3) .LetHbe a class of functions h:Rd→R, and let hbe Lipschitz-\ncontinuous with constant Lh>0, and the loss function ℓbe Lipschitz-continuous with constant Ll>\n0and bounded within a finite range [0, B]. Consider: 1. Optimized perturbation added to the original\ndata: ˜Cres={˜xres\ni,˜yres\ni}n\ni=1. 2. residual injected dataset (FADRM): ˜CFADRM ={˜xi,˜yi}n\ni=1. 3.\npatches selected from the original dataset: O={xi, yi}n\ni=1. 4. discrepancy ∆ :=1\nnPn\ni=1∥˜xres\ni−\nxi∥. Lethres∈ H denote the hypothesis trained on ˜Cres, and hFADRM ∈ H be trained on ˜CFADRM .\n6\n\nDefine the corresponding empirical risks: bLres:=1\nnPn\ni=1ℓ(hres(˜xres\ni),˜yres\ni),bLFADRM :=\n1\nnPn\ni=1ℓ(hFADRM (˜xi),˜yi). Suppose the following conditions hold:\nRn(H ◦ O )−Rn(H ◦˜Cres)<−Lh∆(Ll+ 2Bα)\n2B(11)\nWhere αis the merge ratio. Under the condition specified in Equation (11), the generalization bound\nofhFADRM is rigorously shown to be tighter than that of hres, i.e.,\nbLFADRM + 2B·Rn(H ◦˜CFADRM )<bLres+ 2B·Rn(H ◦˜Cres). (12)\nThe insight of this theorem is that, when synthetic data is highly optimized and thus induces greater\nhypothesis complexity, combining it with the more structured and regular original data can lead to a\ntighter generalization bound, accounting for both empirical risk and Rademacher complexity.\n4 Experiments\n4.1 Datasets and Experimental Setup\nDatasets. We conduct experiments across datasets with varying resolutions, including CIFAR-100\n(32×32) [15], Tiny-ImageNet (64 ×64) [41], ImageNet-1K (224 ×224) [8], and their subsets.\nBaseline Methods. To evaluate the effectiveness of our proposed framework, we conduct a compre-\nhensive comparison against three state-of-the-art dataset distillation baselines. The first baseline is\nRDED [ 32], which selects cropped patches directly from the original dataset and is therefore catego-\nrized as involving full participation of the original data. The second method, EDC [ 30], retains a high\ndegree of original data participation by optimizing selected patches with an extremely small learning\nrate, producing synthetic images that are close to the original samples. The third method, CV-DD [ 6],\naligns global BatchNorm statistics with sufficient optimization by updating initialization, resulting in\nminimal original data involvement despite initialization from real patches. These baselines exhibit\nvarying degrees of original data involvement, providing a solid basis for evaluating FADRM .\n4.2 Main Results\nResults Analysis. As shown in Table 1, our framework consistently achieves state-of-the-art perfor-\nmance across various settings. For instance, on ImageNet-1K with IPC=10 and ResNet-101 as the\nstudent model, the ensemble-enhanced variant FADRM + attains an accuracy of 58.1%, outperform-\ning EDC and CV-DD by a substantial margin of +6.4%. Notably, RDED underperforms FADRM ,\nunderscoring the limitations of relying solely on the original dataset without further optimization.\nFurthermore, CV-DD is inferior to FADRM +, highlighting the drawbacks of largely excluding origi-\nnal data during synthesis. Lastly, the consistent outperformance of FADRM + over EDC validates the\nefficacy of our framework in harnessing original data via data-level residual connections.\nEfficiency Comparison. Table 2 (Left) highlights the superior efficiency of our framework compared\nto existing Uni-level frameworks. Bi-level frameworks are excluded from this comparison due to\ntheir inherent limitations in scalability for large-scale datasets. Specifically, FADRM + achieves a\nreduction of 3.9 seconds per image in optimization time compared to EDC [ 30], culminating in a\ntotal computational saving of 54 hours when applied to the 50 IPC ImageNet-1K dataset. Similarly,\nFADRM demonstrates a 28.5 hours reduction in training time relative to SRe2L++ for the same task.\nAdditionally, our framework significantly reduces peak memory usage compared to other frameworks,\nenabling efficient dataset distillation even in resource-constrained scenarios. These results underscore\nthe scalability and computational efficiency of our approach, which not only accelerates large-scale\ndataset distillation but also substantially lowers associated computational costs.\n4.3 Cross-Architecture Generalization\nA fundamental criterion for evaluating the quality of distilled data is its ability to generalize across\ndiverse network architectures, which significantly enhances its practical utility and adaptability in\nreal-world applications. As illustrated in Table 2 (Right), FADRM + consistently outperforms all\nexisting state-of-the-art methods across models of varying sizes and complexities, demonstrating\nsuperior generalization capabilities and robustness in diverse scenarios.\n7\n\nResNet18 ResNet50 ResNet101\nDataset IPC (Ratio) RDED EDC CV-DD FADRM FADRM +RDED EDC CV-DD FADRM FADRM +RDED EDC CV-DD FADRM FADRM +\nCIFAR-1001 (0.2%) 17.1 39.7 28.3 31.8 40.6 10.9 36.1 28.7 27.3 37.4 11.2 32.3 29.0 29.2 40.1\n10 (2.0%) 56.9 63.7 62.7 67.4 67.9 41.6 62.1 61.5 66.5 67.4 54.1 61.7 63.8 68.3 68.9\n50 (10.0%) 66.8 68.6 67.1 71.0 71.3 64.0 69.4 68.2 71.5 72.1 67.9 68.5 67.6 71.9 72.1\nWhole Dataset 78.9 79.9 79.5\nTiny-ImageNet1 (0.2%) 11.8 39.2 30.6 28.6 40.4 8.2 35.9 25.1 28.4 39.4 9.6 40.6 28.0 27.9 41.9\n10 (2.0%) 41.9 51.2 47.8 48.9 52.8 38.4 50.2 43.8 47.3 53.7 22.9 51.6 47.4 47.8 53.6\n50 (10.0%) 58.2 57.2 54.1 56.4 58.7 45.6 58.8 54.7 57.0 60.3 41.2 58.6 54.1 57.2 60.8\nWhole Dataset 68.9 71.5 70.6\nImageNette1 (0.1%) 35.8 - 36.2 36.2 39.2 27.0 - 27.6 31.1 31.9 25.1 - 25.3 26.3 29.3\n10 (1.0%) 61.4 - 64.1 64.8 69.0 55.0 - 61.4 64.1 68.1 54.0 - 61.0 61.9 63.7\n50 (5.2%) 80.4 - 81.6 83.6 84.6 81.8 - 82.0 84.1 85.4 75.0 - 80.0 80.3 82.3\nWhole Dataset 93.8 89.8 89.3\nImageWoof1 (0.1%) 20.8 - 21.4 21.0 22.8 17.8 - 19.1 19.5 19.9 19.6 - 19.9 20.0 21.8\n10 (1.1%) 38.5 - 49.3 44.5 57.3 35.2 - 47.8 44.9 54.1 31.3 - 42.6 40.4 51.4\n50 (5.3%) 68.5 - 71.9 72.3 72.6 67.0 - 71.2 71.0 71.7 59.1 - 69.9 70.3 70.6\nWhole Dataset 88.2 77.8 82.7\nImageNet-1k1 (0.1%) 6.6 12.8 9.2 9.0 14.7 8.0 13.3 10.0 12.2 16.2 5.9 12.2 7.0 6.8 14.1\n10 (0.8%) 42.0 48.6 46.0 48.4 50.9 49.7 54.1 51.3 54.5 57.5 48.3 51.7 51.7 54.8 58.1\n50 (3.9%) 56.5 58.0 59.5 60.1 61.2 62.0 64.3 63.9 65.4 66.9 61.2 64.9 62.7 66.0 67.0\nWhole Dataset 72.3 78.6 79.8\nTable 1: Post-evaluation performance comparison with SOTA baseline methods. All experiments\nfollow the training settings established in EDC [ 30]: 300 epochs for Tiny-ImageNet (IPC=10, 50),\nImageNet-1K, and its subsets, and 1,000 epochs for CIFAR-100, Tiny-ImageNet (IPC=1). For fair\ncomparison with single-model distillation (RDED) and ensemble-based methods (CV-DD, EDC), we\nevaluate both the single-model version ( FADRM only utilized ResNet18 [ 11] for distillation) and the\nensemble-enhanced version ( FADRM +). This evaluation strategy ensures equitable benchmarking\nwhile maintaining methodological consistency across all experiments.\nMethod Time Cost (s) Peak Memory (GB)\nSRe2L++ [6] 2.52 5.3\nFADRM 0.47 2.9\nG-VBSM [29] 17.28 21.4\nCV-DD [6] 8.20 23.4\nEDC [30] 4.99 17.9\nFADRM + 1.09 11.0Model #Params RDED EDC CV-DD FADRM +\nResNet18 [11] 11.7M 42.0 48.6 46.0 50.0\nResNet50 [11] 25.6M 49.7 54.1 51.3 57.5\nResNet101 [11] 44.5M 48.3 51.7 51.7 58.1\nEfficientNet-B0 [34] 39.6M 42.8 51.1 43.2 51.9\nMobileNetV2 [27] 3.4M 34.4 45.0 39.0 45.5\nShuffleNetV2-0.5x [44] 1.4M 19.6 29.8 27.4 30.2\nSwin-Tiny [21] 28.0M 29.2 38.3 – 39.1\nWide ResNet50-2 [11] 68.9M 50.0 – 53.9 59.1\nDenseNet121 [13] 8.0M 49.4 – 50.9 55.4\nDenseNet169 [13] 14.2M 50.9 – 53.6 58.5\nDenseNet201 [13] 20.0M 49.0 – 54.8 59.7\nTable 2: Left: Efficiency comparison between various optimization-based methods and our approach\nwhen distilling ImageNet-1k. The time cost is measured in seconds, representing the duration required\nto generate a single image on a single RTX 4090 GPU. Right: Top-1 accuracy (%) on ImageNet-1K\nfor cross-architecture generalization with IPC=10.\n4.4 Ablation Study\nIPC=10 IPC=50\nFADRM FADRM +FADRM FADRM +\n1×1 48.4 50.9 60.1 61.2\n2×2 47.7 50.0 59.8 60.1\nTable 3: Comparison of student model\n(ResNet-18) generalization performance\nwhen trained on distilled datasets generated\nusing 1×1and2×2patch configurations\nduring initialization and residual injection.Impact of Patch Numbers for Initialization and\nResiduals. To assess the effect of different patch\nconfigurations during both the initialization and resid-\nual injection stages, we conduct an ablation study,\nas shown in Table 3. The results suggest that both\n1×1and2×2patch settings are effective for gener-\nating distilled data. However, the 1×1configuration\nconsistently delivers the better overall performance,\nmaking it the preferred choice in practice.\nFADRM FADRM + SRe2L++ G-VBSM\nW/ MPT W/O MPT W/ MPT W/O MPT W/ MPT W/O MPT W/ MPT W/O MPT\nResNet-18 (Student) 47.7 % 47.8 % 50.0 % 49.6 % 43.1 % 43.1 % 30.5 % 30.7 %\nEfficiency 0.26 ms 0.63 ms 0.58 ms 0.96 ms 0.26 ms 0.63 ms 2.65 ms 4.32 ms\nPeak GPU Memory 2.9 GB 5.3 GB 12 GB 23 GB 2.9 GB 5.3 GB 11.8 GB 21.4 GB\nTable 4: Comparison of model generalization performance, optimization efficiency (milliseconds per\nimage per iteration, measured under 100 batch size and 224 as input size), and peak GPU memory\nusage with and without mixed precision training under ImageNet-1K IPC=10.\n8\n\nConfiguration Accuracy (%) Time Cost (s)\nFADRM (W/O ARC + W/O MRO ) 46.4 0.52\nFADRM (W/O ARC + W/ MRO ) 46.2 0.47\nFADRM (W/ARC (α= 0.9) + W/ MRO ) 45.7 0.47\nFADRM (W/ARC (α= 0.8) + W/ MRO ) 46.4 0.47\nFADRM (W/ARC (α= 0.7) + W/ MRO ) 47.6 0.47\nFADRM (W/ARC (α= 0.6) + W/ MRO ) 47.3 0.47\nFADRM (W/ARC (α= 0.5) + W/ MRO ) 47.7 0.47\nFADRM (W/ARC (α= 0.4) + W/ MRO ) 47.4 0.47Configuration Accuracy (%) Time Cost (s)\nFADRM+ (W/O ARC + W/O MRO ) 48.7 1.16\nFADRM+ (W/O ARC + W/ MRO ) 48.2 1.09\nFADRM+ (W/ARC (α= 0.9) + W/ MRO )) 48.5 1.09\nFADRM+ (W/ARC (α= 0.8) + W/ MRO ) 48.0 1.09\nFADRM+ (W/ARC (α= 0.7) + W/ MRO ) 48.9 1.09\nFADRM+ (W/ARC (α= 0.6) + W/ MRO ) 49.3 1.09\nFADRM+ (W/ARC (α= 0.5) + W/ MRO ) 50.0 1.09\nFADRM+ (W/ARC (α= 0.4) + W/ MRO ) 49.5 1.09\nTable 5: Performance comparison of ResNet-18 as the student model trained on distilled ImageNet-1K\n(IPC=10) datasets generated with different merge ratios ( α), fixed Dds=200 and k=3. The efficiency\nis measured in seconds per image generation. Left presents the ablation results for single-model\ndistillation, while Right shows the corresponding results for multi-model distillation.\nk 1 2 3 4 5 6\nImageNet-1k 47.1 47.6 47.8 47.6 47.4 47.3\nCIFAR-100 59.2 60.9 61.5 60.5 59.4 57.9Dds 160 180 200 224\nPost Eval (%) 47.2 47.5 47.7 47.7\nTime Cost (s) 0.42 0.44 0.47 0.52\nTable 6: Left presents the ablation results for k(frequency of residual connections) using FADRM\nwithDds=200, α=0.5, while Right shows the ablation results for Ddson ImageNet-1k IPC=10.\nEfficiency is measured as the total time required to optimize a single image under a fixed budget of\n2,000 optimization iterations using FADRM withα=0.5, k=3.\nImpact of Mixed Precision Training (MPT). Our ablation study in Table 4 shows that MPT\npreserves distilled dataset quality while significantly reducing peak memory usage and improving\noptimization efficiency, making it an effective strategy for accelerating distillation.\nImpact of Components in FADRM. To evaluate the impact of individual components ( MRO and\nARC ) in our framework, we conduct a comprehensive ablation study. As demonstrated in Table 5,\nthe results reveal that the initial integration of MRO results in a performance decline compared to\nthe baseline (W/O ARC and W/O MRO ). This decline is primarily attributed to the loss of crucial\ndetails during the resampling process. However, by incorporating ARC and reducing the merge\nratioα(thereby assigning higher priority to the original patches during merging), the performance\nis significantly enhanced compared to the baseline, which struggles with the issue of information\nvanishing. The optimal performance is achieved with a merge ratio of 0.5 for both FADRM and\nFADRM +, indicating that an equal combination of the original patch and the intermediate optimized\ninput produces the most favorable outcomes. Crucially, the results confirm that ARC effectively\nmitigates information vanishing and compensates for missing details during the resampling process,\nthereby facilitating the deployment of a fast yet highly effective framework.\nFADRM\nGolden Fish Shark Snow MobileCat ToySRe L++ FADRM\nGoat Lion Dog Car\nSRe L++\nFigure 5: Visualization of dataset distilled by\nFADRM and SRe2L++ on Tiny-ImageNet (top\ntwo rows) and ImageNet-1k (bottom two rows).Impact of Downsampled Input Size in MRO .\nTo determine the optimal downsampled input\nsize ( Dds) for MRO , we conduct an ablation\nstudy, as presented in Table 6 (Right). Our\nresults demonstrate that Dds=200 achieves the\nmost optimal performance. Notably, using other\nsizes leads to a degradation in the quality of the\ndistilled dataset compared to optimizing with\nthe original input size of 224.\nImpact of varying k.To investigate the impact\nofkon the quality of the distilled dataset, we\nconduct an ablation study as presented in Ta-\nble 6 (Left). The results demonstrate that k= 3\nyields the optimal configuration. Notably, we\nobserve a positive correlation between the kand\npost-evaluation performance when increasing k\nfrom one to three. However, beyond k= 3, per-\nformance decreases as kincreases, indicating\nthat excessive residual connections can intro-\nduce redundant local details over global struc-\ntures, which can lead to suboptimal results.\n9\n\n4.5 Distilled Image Visualization\nFig. 5 compares distilled data from FADRM and SRe2L++ [ 6], both using ResNet-18 with identical\ninitial patch images, differing only in FADRM ’s incorporation of residual connections. As shown,\nFADRM effectively preserves the critical features of the original patches and retains significantly\nmore details than SRe2L++. This highlights the advantage of residual connections in enhancing\ninformation density and improving the quality of distilled data.\n4.6 Application: Continual Learning\nLeveraging continual learning to verify the effectiveness of distilled dataset generalization has been\nwidely used in prior work [ 43,29,46]. Following these protocols and utilizing the class-incremental\nlearning framework as in DM [ 46], we conduct an evaluation on Tiny-ImageNet IPC=50 using a\n5-step and 10-step incremental, as shown in Fig. 6. The results indicate that FADRM consistently\nsurpasses RDED, demonstrating its effectiveness.\n40 80 120 160 200\nNumber of Classes24324048Top-1 Accuracy (%)\nFADRM\nRDED\n20 40 60 80 100 120 140 160 180 200\nNumber of Classes4044485256Top-1 Accuracy (%)\nFADRM\nRDED\nFigure 6: Five-step and Ten-step class-incremental learning on Tiny-ImageNet with IPC=50.\n5 Conclusion\nWe proposed FADRM , a novel framework for dataset distillation designed to generate high-quality dis-\ntilled datasets with significantly reduced computational overhead. Our work identifies and addresses\nthe critical challenge of vanishing information, a fundamental limitation in Uni-Level Framework\nthat heavily undermines the information density of distilled datasets. To address this, we introduce\ndata-level residual connections , a novel mechanism that balances the operations of preserving critical\noriginal features and integrating new information, enriching the distilled dataset with both original and\nnew condensed features and increasing its overall information density. Furthermore, by integrating\nparameter mixed precision training and input multi-resolution optimization, our framework achieves\nsignificant reductions in both Peak GPU memory consumption and training time. Extensive exper-\niments demonstrate that FADRM outperforms existing state-of-the-art methods in both efficiency\nand accuracy across multiple benchmark datasets. For future work, we aim to extend the idea of\ndata-level residual connections to broader modalities and applications of dataset distillation tasks.\nReferences\n[1]Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774 , 2023.\n[2]Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds\nand structural results. Journal of Machine Learning Research , 3(Nov):463–482, 2002.\n[3]Normand J Beaudry and Renato Renner. An intuitive proof of the data processing inequality.\narXiv preprint arXiv:1107.0740 , 2011.\n[4]George Cazenavette, Tongzhou Wang, Antonio Torralba, Alexei A Efros, and Jun-Yan Zhu.\nDataset distillation by matching training trajectories. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition , pages 4750–4759, 2022.\n10\n\n[5]Mingyang Chen, Bo Huang, Junda Lu, Bing Li, Yi Wang, Minhao Cheng, and Wei Wang.\nDataset distillation via adversarial prediction matching. arXiv preprint arXiv:2312.08912 , 2023.\n[6]Jiacheng Cui, Zhaoyi Li, Xiaochen Ma, Xinyue Bi, Yaxin Luo, and Zhiqiang Shen. Dataset\ndistillation via committee voting. arXiv preprint arXiv:2501.07575 , 2025.\n[7]Justin Cui, Ruochen Wang, Si Si, and Cho-Jui Hsieh. Scaling up dataset distillation to imagenet-\n1k with constant memory. In International Conference on Machine Learning , pages 6565–6590.\nPMLR, 2023.\n[8]Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition , pages 248–255. Ieee, 2009.\n[9]Zhiwei Deng and Olga Russakovsky. Remember the past: Distilling datasets into addressable\nmemories for neural networks. arXiv preprint arXiv:2206.02916 , 2022.\n[10] Ziyao Guo, Kai Wang, George Cazenavette, Hui Li, Kaipeng Zhang, and Yang You. To-\nwards lossless dataset distillation via difficulty-aligned trajectory matching. In The Twelfth\nInternational Conference on Learning Representations , 2024.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 770–778, 2016.\n[12] Yang He, Lingao Xiao, Joey Tianyi Zhou, and Ivor Tsang. Multisize dataset condensation.\nICLR , 2024.\n[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition , pages 4700–4708, 2017.\n[14] Jang-Hyun Kim, Jinuk Kim, Seong Joon Oh, Sangdoo Yun, Hwanjun Song, Joonhyun Jeong,\nJung-Woo Ha, and Hyun Oh Song. Dataset condensation via efficient synthetic-data param-\neterization. In International Conference on Machine Learning , pages 11102–11118. PMLR,\n2022.\n[15] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nTechnical report, University of Toronto, Toronto, ON, Canada, 2009.\n[16] Hae Beom Lee, Dong Bok Lee, and Sung Ju Hwang. Dataset condensation with latent space\nknowledge factorization and sharing. arXiv preprint arXiv:2208.10494 , 2022.\n[17] Saehyung Lee, Sanghyuk Chun, Sangwon Jung, Sangdoo Yun, and Sungroh Yoon. Dataset\ncondensation with contrastive signals. In International Conference on Machine Learning , pages\n12352–12364. PMLR, 2022.\n[18] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao,\nChengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint\narXiv:2412.19437 , 2024.\n[19] Ping Liu and Jiawei Du. The evolution of dataset distillation: Toward scalable and generalizable\nsolutions. arXiv preprint arXiv:2502.05673 , 2025.\n[20] Songhua Liu, Kai Wang, Xingyi Yang, Jingwen Ye, and Xinchao Wang. Dataset distillation via\nfactorization. Advances in Neural Information Processing Systems , 35:1100–1113, 2022.\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF international conference on computer vision , pages 10012–10022, 2021.\n[22] Noel Loo, Ramin Hasani, Alexander Amini, and Daniela Rus. Efficient dataset distillation using\nrandom feature approximation. arXiv preprint arXiv:2210.12067 , 2022.\n11\n\n[23] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740 , 2017.\n[24] Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with\ninfinitely wide convolutional networks. Advances in Neural Information Processing Systems ,\n34:5186–5198, 2021.\n[25] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. OpenAI Blog , 2018.\n[26] Ahmad Sajedi, Samir Khaki, Ehsan Amjadian, Lucy Z. Liu, Yuri A. Lawryshyn, and Kon-\nstantinos N. Plataniotis. Datadam: Efficient dataset distillation with attention matching. In\nProceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) , pages\n17097–17107, October 2023.\n[27] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 4510–4520, 2018.\n[28] Xinyi Shang, Peng Sun, Zhiqiang Shen, Tao Lin, and Jing-Hao Xue. Dataset distillation in the\nera of large-scale data: Methods, analysis, and future directions. 2025.\n[29] Shitong Shao, Zeyuan Yin, Muxin Zhou, Xindong Zhang, and Zhiqiang Shen. Generalized\nlarge-scale data condensation via various backbone and statistical matching. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16709–16718,\n2024.\n[30] Shitong Shao, Zikai Zhou, Huanran Chen, and Zhiqiang Shen. Elucidating the design space of\ndataset condensation. arXiv preprint arXiv:2404.13733 , 2024.\n[31] Donghyeok Shin, Seungjae Shin, and Il-Chul Moon. Frequency domain-based dataset distilla-\ntion. Advances in Neural Information Processing Systems , 36, 2024.\n[32] Peng Sun, Bei Shi, Daiwei Yu, and Tao Lin. On the diversity and realism of distilled dataset:\nAn efficient dataset distillation paradigm. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 9390–9399, 2024.\n[33] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4,\ninception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI\nconference on artificial intelligence , volume 31, 2017.\n[34] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In International\nconference on machine learning , pages 10096–10106. PMLR, 2021.\n[35] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut,\nJohan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.\n[37] Kai Wang, Bo Zhao, Xiangyu Peng, Zheng Zhu, Shuo Yang, Shuo Wang, Guan Huang, Hakan\nBilen, Xinchao Wang, and Yang You. Cafe: Learning to condense dataset by aligning features.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) , 2022.\n[38] Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A Efros. Dataset distillation.\narXiv preprint arXiv:1811.10959 , 2018.\n[39] Lingao Xiao and Yang He. Are large-scale soft labels necessary for large-scale dataset distilla-\ntion? arXiv preprint arXiv:2410.15919 , 2024.\n12\n\n[40] Eric Xue, Yijiang Li, Haoyang Liu, Yifan Shen, and Haohan Wang. Towards adversarially\nrobust dataset distillation by curvature regularization. arXiv preprint arXiv:2403.10045 , 2024.\n[41] Lian Yao, Yin Li, and Li Fei-Fei. Image classification using deep convolutional neural net-\nworks. https://cs231n.stanford.edu/reports/2015/pdfs/yle_project.\npdf, 2015. CS231n: Convolutional Neural Networks for Visual Recognition, Stanford Univer-\nsity, Course Project Report.\n[42] Zeyuan Yin and Zhiqiang Shen. Dataset distillation via curriculum data synthesis in large data\nera. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=PlaZD2nGCl .\n[43] Zeyuan Yin, Eric Xing, and Zhiqiang Shen. Squeeze, recover and relabel: Dataset condensation\nat imagenet scale from a new perspective. Advances in Neural Information Processing Systems ,\n36, 2024.\n[44] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient\nconvolutional neural network for mobile devices. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , pages 6848–6856, 2018.\n[45] Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In\nInternational Conference on Machine Learning , pages 12674–12685. PMLR, 2021.\n[46] Bo Zhao and Hakan Bilen. Dataset condensation with distribution matching. In IEEE/CVF\nWinter Conference on Applications of Computer Vision, WACV 2023, Waikoloa, HI, USA,\nJanuary 2-7, 2023 , 2023.\n[47] Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching.\nInInternational Conference on Learning Representations , 2021.\n[48] Binglin Zhou, Linhao Zhong, and Wentao Chen. Improve cross-architecture generalization on\ndataset distillation. arXiv preprint arXiv:2402.13007 , 2024.\n[49] Yongchao Zhou, Ehsan Nezhadarya, and Jimmy Ba. Dataset distillation using neural feature\nregression. Advances in Neural Information Processing Systems , 35:9813–9827, 2022.\n13\n\nAppendix of FADRM\nContents\nA Theoretical Derivation 15\nB Optimization Details 18\nC Resampling via Bilinear Interpolation 19\nD Limitations 19\nE Experimental Setup 19\nF Hyper-Parameters Setting 20\nG Additional Distilled Data Visualization 22\n14\n\nA Theoretical Derivation\nA.1 Preliminary\nLemma 1 (Data Processing Inequality [ 3]).LetX→Y→Zform a Markov chain. Then the mutual\ninformation between XandZis upper bounded by that between XandY:\nI(X;Z)≤I(X;Y). (13)\nIn particular, no post-processing of Ycan increase the information that Ycontains about X.\nTheorem 3 (Temperature-scaled KL divergence is bounded and Lipschitz-continuous) .Fix integers\nk≥2and a constant C >0. For any temperature T >0let\nz= (z1, . . . , z k)∈[−C, C]k, q(T)\ni=exp\u0000\nzi/T\u0001\nkX\nj=1exp\u0000\nzj/T\u0001(i= 1, . . . , k ). (14)\nLetp= (p1, . . . , p k)∈∆kbe an arbitrary target probability vector (e.g. it may come from another\nsoft-max with its own temperature). Define the loss\nℓ\u0000\np, q(T)(z)\u0001\n:= KL\u0000\np∥q(T)(z)\u0001\n=kX\ni=1pilogpi\nq(T)\ni. (15)\nThen the following hold:\n1. (Bounded range ) For every admissible pair (p, z),\n0≤ℓ\u0000\np, q(T)(z)\u0001\n≤B, B := log k+2C\nT. (16)\n2.(ℓ∞-Lipschitz continuity in logits ) The map z7→ℓ\u0000\np, q(T)(z)\u0001\nisL-Lipschitz w.r.t. the ℓ∞\nnorm with L=1\nT. Consequently, it is√\nk/T-Lipschitz w.r.t. the Euclidean norm.\nproof of Theorem 3. (i) Boundedness. Write\nKL\u0000\np∥q(T)\u0001\n=kX\ni=1pilogpi−kX\ni=1pilogq(T)\ni. (17)\nSince x7→xlogxis non-positive on [0,1], the first term is at most 0, so\nKL\u0000\np∥q(T)\u0001\n≤ −kX\ni=1pilogq(T)\ni. (18)\nFor the soft-max, logq(T)\ni=zi/T−logZ, where Z:=Pk\nj=1exp(zj/T). Hence\n−kX\ni=1pilogq(T)\ni=−1\nTkX\ni=1pizi+ log Z. (19)\nBecause each zi∈[−C, C]andP\nipi= 1,\n−1\nTX\nipizi≤C\nT. (20)\nMoreover, zi≤Cimplies Z≤kexp(C/T)and thus logZ≤logk+C\nT. Combining the two parts\nyields the desired upper bound logk+ 2C/T . Non-negativity of KL divergence gives the lower\nbound 0.\n15\n\n(ii) Lipschitz continuity. Differentiate ℓw.r.t. zi:\n∂ziℓ\u0000\np, q(T)(z)\u0001\n=−pi−q(T)\ni\nT. (21)\nBecause |pi−q(T)\ni| ≤1, we have |∂ziℓ| ≤1/Tfor every coordinate. Thus ∥∇zℓ∥∞≤1/T, and by\nthe mean-value theorem,\n\f\fℓ(p, q(T)(z))−ℓ(p, q(T)(z′))\f\f≤1\nT∥z−z′∥∞,∀z, z′∈[−C, C]k, (22)\nsoL= 1/Tin the ℓ∞norm. Since ∥v∥2≤√\nk∥v∥∞, the Euclidean Lipschitz constant is at most√\nk/T.\nLemma 2 (Generalization Bound via Rademacher Complexity [ 2]).LetHbe a class of functions\nmapping X → [0, B], and let S={x1, . . . , x n}be an i.i.d. sample from distribution D. Then, for\nanyδ >0, with probability at least 1−δ, the following inequality holds for all h∈ H:\nEx∼D[h(x)]≤1\nnnX\ni=1h(xi) + 2Rn(H) +Br\nlog(1/δ)\n2n(23)\nLemma 3 (Empirical Risk Proximity) .Let˜xi:=α˜xres\ni+ (1−α)xiwithα∈(0,1), and let the\ncorresponding datasets be ˜Cres:={(˜xres\ni, yi)}n\ni=1,˜CFADRM :={(˜xi, yi)}n\ni=1.Then for any model\nh∈ H, the empirical risk difference is bounded by a negligible value:\n\f\f\fbLres(h)−bLFADRM (h)\f\f\f≤LℓLh(1−α)·∆1,where ∆1:=1\nnnX\ni=1∥˜xres\ni−xi∥. (24)\nProof of Lemma 3. We begin by computing the pointwise difference in the loss:\n|ℓ(h(˜xres\ni), yi)−ℓ(h(˜xi), yi)|. (25)\nSince ℓisLℓ-Lipschitz in the model output, and hisLh-Lipschitz in the input, we have:\n|ℓ(h(˜xres\ni), yi)−ℓ(h(˜xi), yi)| ≤Lℓ· |h(˜xres\ni)−h(˜xi)| ≤LℓLh· ∥˜xres\ni−˜xi∥. (26)\nNote that:\n˜xi=α˜xres\ni+ (1−α)xi⇒ ˜xres\ni−˜xi= (1−α)(˜xres\ni−xi), (27)\nso:\n∥˜xres\ni−˜xi∥= (1−α)∥˜xres\ni−xi∥. (28)\nTherefore,\n|ℓ(h(˜xres\ni), yi)−ℓ(h(˜xi), yi)| ≤LℓLh(1−α)∥˜xres\ni−xi∥. (29)\nAveraging over nsamples:\n\f\f\fbLres(h)−bLFADRM (h)\f\f\f≤1\nnnX\ni=1LℓLh(1−α)∥˜xres\ni−xi∥=LℓLh(1−α)·∆1. (30)\nCorollary 1 (Lipschitz Convex Combination Bound) .Leth:Rd→Rbe an L-Lipschitz function.\nFor any x, y∈Rdandα∈(0,1), define z=αx+ (1−α)y. Then:\n|h(z)−(αh(x) + (1 −α)h(y))| ≤Lα(1−α)∥x−y∥ (31)\nIn particular, this implies:\nh(z)≤αh(x) + (1 −α)h(y) +Lα(1−α)∥x−y∥ (32)\nh(z)≥αh(x) + (1 −α)h(y)−Lα(1−α)∥x−y∥ (33)\n16\n\nA.2 Bounded Information in BN-Aligned Synthetic Data\nProof of Theorem 1. LetOdenote the original dataset. From it, a pretrained model fθis derived,\nwhich includes BatchNorm statistics {µl, σ2\nl}. Each synthetic image ˜xjin the distilled dataset Cis\ngenerated by minimizing an objective function depending only on fθand a fixed label ˜yj.\nWe assume that each ˜xjis generated independently given fθ, and that fθis a deterministic function\nofO. Then, for each sample (˜xj,˜yj), we have the Markov chain:\nO → fθ→˜xj, (34)\nBy applying Lemma 1, we get:\nI(˜xj;O)≤I(fθ;O) =H(fθ), (35)\nNow, by the chain rule of mutual information:\nI(C;O) =I({˜xj,˜yj}|C|\nj=1;O)≤|C|X\nj=1I(˜xj;O)≤ |C| · H(fθ), (36)\nwhere we used the fact that ˜yjis fixed and independent of Oand the independence assumption across\nsamples. Thus, the total information that the synthetic dataset Ccan retain about the original dataset\nOis bounded by the product of its size and the entropy of the model fθ.\nA.3 ARC improves the robustness of the distilled images\nProof of Theorem 2. Let˜xres\nibe a perturbation generated via distribution (running statistics) match-\ning and prediction (cross entropy) matching, and let xibe a real image from the original dataset.\nDefine the residual-injected sample ˜xias:\n˜xi:=α˜xres\ni+ (1−α)xi, α∈(0,1) (37)\nDefine the datasets:\n•˜Cres={˜xres\ni,˜yres\ni}n\ni=1: perturbation generated via distribution (running statistics) matching\nand prediction (cross entropy) matching,\n•O={xi, yi}n\ni=1: selected patches from the original dataset,\n•˜CFADRM ={˜xi,˜yi}n\ni=1: residual-injected dataset.\nWe begin by bounding the Rademacher complexity of the residual-injected dataset ˜CFADRM =\n{˜xi}n\ni=1, where ˜xi=α˜xres\ni+ (1−α)xi, using Lemma 2 and Corollary 1.\nFrom the definition:\nRn(H ◦˜CFADRM ) =Eσ\"\nsup\nh∈H1\nnnX\ni=1σih(˜xi)#\n(38)\nBy Corollary 1, we have for each term:\nh(˜xi)≤αh(˜xres\ni) + (1 −α)h(xi) +εi,where |εi| ≤Lh·α(1−α)∥˜xres\ni−xi∥ (39)\nTherefore:\nnX\ni=1σih(˜xi)≤nX\ni=1σi(αh(˜xres\ni) + (1 −α)h(xi)) +nX\ni=1|σiεi| (40)\nUsing|σi|= 1, we get:\nnX\ni=1|σiεi| ≤Lhα(1−α)nX\ni=1∥˜xres\ni−xi∥=n·Lhα(1−α)·∆ (41)\nDivide by n, take supremum and expectation:\n17\n\nRn(H ◦˜CFADRM )≤α·Rn(H ◦˜Cres) + (1 −α)·Rn(H ◦ O ) +Lhα(1−α)·∆ (42)\nRearrange the Inequality:\nRn(H◦˜CFADRM )−Rn(H◦˜Cres)≤(1−α)h\nRn(H ◦ O )−Rn(H ◦˜Cres)i\n+Lhα(1−α)·∆(43)\nMultiply 2Bon both sides and add a negligible positive value ϵto the LHS:\n2B·\u0002\nRn(H ◦˜CFADRM )−Rn(H ◦˜Cres)\u0003\n<2B(1−α)·\u0002\nRn(H ◦ O )−Rn(H ◦˜Cres)\u0003\n+ 2BLhα(1−α)·∆ +ϵ(44)\nAs validated in Theorem 3, when T > 0, KL-divergence becomes a bounded B-range loss, which we\nthen apply Lemma 2 to formulate generalization error:\nLgen(h)≤bL(h) + 2B·Rn(H ◦S) (45)\nApply to both models:\nLgen(hres)≤bLres+ 2B·Rn(H ◦˜Cres) (46)\nLgen(hFADRM )≤bLFADRM + 2B·Rn(H ◦˜CFADRM ) (47)\nRecall the lower bound for the difference of two ERMs established in Lemma 3, we then have:\nbLres(h)−bLFADRM (h)≥ −LℓLh(1−α)·∆,where ∆ :=1\nnnX\ni=1∥˜xres\ni−xi∥. (48)\nGiven the assumption (11), we can then derive:\n−LℓLh(1−α)·∆>2B(\n(1−α)h\nRn(H ◦ O )−Rn(H ◦˜Cres)i\n+L+hα(1−α)·∆)\n+ϵ(49)\nwhere the RHS in Equation (49) is the upper bound for the difference in Rademacher Complexity, we\nthen derive the following inequality:\nbLres−bLFADRM >2B·h\nRn(H ◦˜CFADRM )−Rn(H ◦˜Cres)i\n(50)\nwhich shows:\nbLres+ 2B·Rn(H ◦˜Cres)>bLFADRM + 2B·Rn(H ◦˜CFADRM ) (51)\nB Optimization Details\nFormally, the optimization process adheres to the principle of aligning the synthesized data with both\nthe predictive behavior and the statistical distribution captured by a pretrained model fθ. Specifically,\ngiven a synthesized image ˜xtat iteration t, the optimization objective is defined as:\narg min\n˜xtL(fθ(˜xt),˜y) +Dglobal(˜xt), (52)\nwhere l(fθ(˜xt),˜y)enforces consistency with the target predictions, while Dglobal(˜xt)ensures align-\nment with the statistical distribution. Importantly, the parameters of fθremain fixed throughout the\noptimization, and only ˜xtis updated.\nThe prediction alignment term is formulated as the cross-entropy loss computed over the synthesized\nbatch:\nL(fθ(˜xt),˜y) =−1\nNNX\nn=1CX\ni=1˜yn,ilogfθ(˜xt)n,i, (53)\n18\n\nwhere Ndenotes the batch size, and Crepresents the total number of classes. The alignment to the\ndistribution in pretrained model is calculated as follows:\nDglobal(˜xt) =X\nl∥µl(˜xt)−E[µl|O]∥2\n+X\nl\r\rσ2\nl(˜xt)−E[σ2\nl|O]\r\r\n2\n=X\nl\r\rµl(˜xt)−BNRM\nl\r\r\n2\n+X\nl\r\rσ2\nl(˜xt)−BNRV\nl\r\r\n2,\nwhere Odenotes the original dataset, and lindexes the layers of the model. The terms BNRM\nl\nandBNRV\nlcorrespond to the running mean and running variance of the Batch Normalization (BN)\nstatistics at layer l. By minimizing Dglobal(˜xt), the synthesized data is encouraged to exhibit statistical\ncharacteristics consistent with the original dataset, thereby preserving global information.\nC Resampling via Bilinear Interpolation\nGiven an original image I:Z2→RCdefined on discrete pixel coordinates, the continuous extension\n˜I:R2→RCat non-integer location (i′, j′)∈R2is computed via bilinear interpolation as follows:\n˜I(i′, j′) =1X\nm=01X\nn=0wm,n·I(i+m, j+n), (54)\nwhere i=⌊i′⌋,j=⌊j′⌋,α=i′−i∈[0,1),β=j′−j∈[0,1), and the interpolation weights are\ndefined by:\nwm,n= (1−m+ (−1)mα)(1−n+ (−1)nβ). (55)\nExplicitly, Equation (54) expands to:\n˜I(i′, j′) = (1 −α)(1−β)·I(i, j) +α(1−β)·I(i+ 1, j)\n+ (1−α)β·I(i, j+ 1) + αβ·I(i+ 1, j+ 1),(56)\nThis interpolation scheme can be viewed as a separable approximation to the continuous image\nfunction, with weights derived from tensor-product linear basis functions over the unit square. It\npreserves differentiability with respect to the fractional coordinates (i′, j′), making it particularly\namenable to gradient-based optimization frameworks.\nD Limitations\nWhile FADRM offers substantial improvements in computational efficiency and performance for\ndataset distillation, it also introduces several limitations. First, the method relies on the assumption\nthat residual signals between synthetic and real data capture critical learning dynamics, which may\nnot generalize across domains with highly abstract or non-visual modalities such as natural language\nor time-series data. Second, the use of distilled datasets can inadvertently reinforce biases present\nin the original data if not carefully audited, potentially leading to fairness concerns in downstream\napplications. From a broader societal perspective, while FADRM reduces the computation and\nresource demands of training large models, thereby contributing positively to sustainability, it may\nalso facilitate the deployment of powerful models in low-resource or surveillance scenarios without\nadequate ethical oversight. Thus, responsible deployment and continued research into bias mitigation\nand cross-domain generalization are essential to ensure the safe and equitable application of FADRM.\nE Experimental Setup\nOur method strictly follows the training configuration established in EDC to ensure a fair and\nconsistent comparison across all evaluated approaches. Additionally, we re-run RDED and CV-DD\n19\n\nunder the same configuration and report the highest performance obtained between their original\nsetup and the EDC configuration. This methodology guarantees a rigorous and equitable evaluation\nby accounting for potential variations in training dynamics across different settings.\nTo establish an upper bound on performance across different backbone architectures (representing\nthe results achieved when training models on the full original dataset) we adopt the hyperparameters\nspecified in Table 7. These hyperparameters are carefully chosen to ensure full model convergence\nwhile effectively mitigating the risk of overfitting, thereby providing a reliable reference for evaluating\nthe performance of distilled datasets.\nHyperparameters for Training the Original Dataset\nOptimizer SGD\nLearning Rate 0.1\nWeight Decay 1e-4\nMomentum 0.9\nBatch Size 128\nLoss Function Cross-Entropy\nEpochs 300\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 7: Hyperparameters for Training the Original Dataset.\nF Hyper-Parameters Setting\nIn summary, the synthesis of distilled data follows consistent hyperparameter configurations, as\noutlined in Table 8. Variations in hyperparameters are introduced exclusively during two phases: (1)\nthe model Pre-training phase. and (2) the post-evaluation phase. These adjustments are carefully\ntailored based on the scale of the models and the specific characteristics of the datasets used. During\nthe post-evaluation phase, we evaluate a total of four hyperparameter combinations, as detailed in\nTable 9. Among these, the parameter ηplays a critical role in controlling the decay rate of the learning\nrate, as defined by the cosine learning rate schedule in Equation 57. Specifically, a larger value of η\nresults in a slower decay rate, thereby preserving a higher learning rate for a longer duration during\ntraining.\nLearning Rate = 0.5×\u0012\n1 + cos\u0012\nπstep\nepochs ×η\u0013\u0013\n(57)\nF.1 CIFAR-100\nThis subsection outlines the hyperparameter configurations employed in the CIFAR-100 experiments,\nproviding the necessary details to ensure reproducibility in future research.\nPre-training phase. Table 10 provides a comprehensive summary of the hyperparameters employed\nfor training the models on the original CIFAR-100 dataset for generating the distilled dataset.\nEvaluation Phase. Table 11 outlines the hyperparameter configurations employed for the post-\nevaluation phase on the Distilled CIFAR-100 dataset.\nF.2 Tiny-ImageNet\nThis part describes the hyperparameter settings used in the Tiny-ImageNet experiments, offering\ncomprehensive details to facilitate reproducibility for future studies.\nPre-training phase. Table 12 presents a detailed overview of the hyperparameters used for model\ntraining on the original Tiny-ImageNet dataset.\nEvaluation Phase. Table 13 details the hyperparameter settings used during the post-evaluation\nphase on the Distilled Tiny-ImageNet dataset.\n20\n\nHyperparameter Value\nOptimizer Adam\nLearning rate 0.25\nBeta (0.5, 0.9)\nEpsilon 1×10−8\nBatch Size 100 or 10 (if C < 100)\nIterations Budgets ( B) 2,000\nMerge Ratio ( α) 0.5\nNumber of ARC (k) 3\nDownsampled Size ( Dds) 200 (ImageNet-1k and Its subsets),\nOriginal Input Size (CIFAR-100,\nTiny-ImageNet)\nFADRM Model ( R) ResNet18\nFADRM + Model ( R) ResNet18 DenseNet121 Shuf-\nfleNetV2 MobileNetV2\nScheduler Cosine Annealing\nAugmentation RandomResizedCrop, Horizontal\nFlip\nTable 8: Hyperparameters for generating the distilled datasets.\nSetting Learning Rate η\nS1 0.001 1\nS2 0.001 2\nS3 0.0005 1\nS4 0.0005 2\nTable 9: Hyperparameter settings with learning rate and η.\nF.3 ImageNette\nThis subsection describes the hyperparameter settings utilized in the ImageNette experiments, offering\ndetailed information to facilitate reproducibility for subsequent studies.\nPre-training phase. Table 14 summarizes the hyperparameters used for training models on the\noriginal ImageNette dataset to generate the distilled dataset, ensuring clarity and reproducibility.\nEvaluation Phase. Table 15 details the hyperparameter settings applied during the post-evaluation\nphase on the Distilled ImageNette dataset.\nF.4 ImageWoof\nThis section describes the hyperparameter settings used in the ImageWoof experiments, offering\ndetailed information to facilitate reproducibility for future studies.\nPre-training phase. Table 16 presents a detailed overview of the hyperparameters utilized for training\nmodels on the original ImageWoof dataset to produce the distilled dataset.\nEvaluation Phase. Table 17 presents the hyperparameter settings utilized during the post-evaluation\nstage on the Distilled Imagewoof dataset, detailing the configurations applied for performance\nassessment.\nF.5 ImageNet-1k\nThis subsection outlines the hyperparameter configurations employed in the ImageNet1k experiments,\nproviding the necessary details to ensure reproducibility in future research.\nPre-training phase. For ImageNet-1K, we employed the official PyTorch pretrained models, which\nhave been extensively trained on the full ImageNet-1K dataset.\n21\n\nHyperparameters for Model Pre-training\nOptimizer SGD\nLearning Rate 0.1\nWeight Decay 1e-4\nMomentum 0.9\nBatch Size 128\nEpoch 50\nScheduler Cosine Annealing\nAugmentation RandomCrop, Horizontal Flip\nLoss Function Cross-Entropy\nTable 10: Hyperparameters for CIFAR-100 Pre-trained Models.\nHyperparameters for Post-Eval on R18, R50 and R101\nOptimizer Adamw\nS1 IPC1 (R50), IPC50 (R18,R50)\nS2 IPC10 (R18, R50)\nS3 IPC1 (R101), IPC10 (R101),\nIPC50 (R101)\nS4 IPC1 (R18)\nSoft Label Generation BSSL\nLoss Function KL-Divergence\nBatch Size 16\nEpochs 1000\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 11: Hyperparameters for post-evaluation task on ResNet18, ResNet50 and ResNet101 for\nCIFAR-100.\nHyperparameters for Model Pre-training\nOptimizer SGD\nLearning Rate 0.1\nWeight Decay 1e-4\nMomentum 0.9\nBatch Size 64\nEpoch 150\nScheduler Cosine Annealing\nAugmentation RandomCrop, Horizontal Flip\nLoss Function Cross-Entropy\nTable 12: Hyperparameters for Tiny-ImageNet Pre-trained Models.\nEvaluation Phase. Table 18 provides a detailed overview of the hyperparameter settings used during\nthe post-evaluation phase on the Distilled ImageNet-1k dataset.\nG Additional Distilled Data Visualization\nAdditional visualizations of the distilled data generated by FADRM are provided in Fig. 7 (CIFAR-\n100), Fig. 9 (Tiny-ImageNet), Fig. 11 (ImageNette), Fig. 13 (ImageWoof), and Fig. 15 (ImageNet-1K).\nFurthermore, enhanced versions - FADRM + are presented in Fig. 8 (CIFAR-100), Fig. 10 (Tiny-\nImageNet), Fig. 12 (ImageNette), Fig. 14 (ImageWoof), and Fig. 16 (ImageNet-1K).\n22\n\nHyperparameters for Post-Eval on R18, R50 and R101\nOptimizer Adamw\nS1 IPC50 (R18)\nS2 IPC1 (R18) IPC10 (R18)\nS3 IPC50 (R50, R101)\nS4 IPC1 (R50, R101) IPC10 (R50,\nR101)\nSoft Label Generation BSSL\nLoss Function KL-Divergence\nBatch Size 16\nEpochs 300 (IPC10, IPC50), 1000\n(IPC1)\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 13: Hyperparameters for post-evaluation task on ResNet18, ResNet50 and ResNet101 for\nTiny-ImageNet.\nHyperparameters for Model Pre-training\nOptimizer SGD\nLearning Rate 0.01\nWeight Decay 1e-4\nMomentum 0.9\nBatch Size 128\nEpoch 300\nScheduler Cosine Annealing\nAugmentation RandomReizeCrop, Horizontal Flip\nLoss Function Cross-Entropy\nTable 14: Hyperparameters for ImageNette Pre-trained Models.\nHyperparameters for Post-Eval on R18, R50 and R101\nOptimizer Adamw\nS2 IPC50 (R101)\nS3 IPC10 (R18, R50) IPC50(R50)\nS4 IPC1(R18, R50, R101) IPC10\n(R101) IPC50 (R18)\nSoft Label Generation BSSL\nLoss Function KL-Divergence\nBatch Size 16\nEpochs 300\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 15: Hyperparameters for post-evaluation task on ResNet18, ResNet50 and ResNet101 for\nImageNette.\n23\n\nHyperparameters for Model Pre-training\nOptimizer SGD\nLearning Rate 0.1\nWeight Decay 1e-4\nMomentum 0.9\nBatch Size 128\nEpoch 50\nScheduler Cosine Annealing\nAugmentation RandomResizeCrop, Horizontal Flip\nLoss Function Cross-Entropy\nTable 16: Hyperparameters for ImageWoof Pre-trained Models.\nHyperparameters for Post-Eval on R18, R50 and R101\nOptimizer Adamw\nS1 IPC1 (R101)\nS2 IPC50 (R18)\nS3 IPC10 (R18, R50) IPC50 (R50,\nR101)\nS4 IPC1 (R18, R50) IPC10 (R101)\nSoft Label Generation BSSL\nLoss Function KL-Divergence\nBatch Size 16\nEpochs 300\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 17: Hyperparameters for post-evaluation task on ResNet18, ResNet50 and ResNet101 for\nImageWoof.\nHyperparameters for Post-Eval on R18, R50 and R101\nOptimizer Adamw\nS1 IPC50 (R18, R50)\nS2 IPC1 (R18) IPC10 (R18, R50,\nR101)\nS3 IPC50 (R101)\nS4 IPC1 (R50, R101)\nSoft Label Generation BSSL\nLoss Function KL-Divergence\nBatch Size 16\nEpochs 300\nAugmentation RandomResizedCrop,\nHorizontal Flip, CutMix\nTable 18: Hyperparameters for post-evaluation task on ResNet18, ResNet50 and ResNet101 for\nImageNet-1k.\n24\n\nFigure 7: Visualization of synthetic data on CIFAR-100 generated by FADRM .\n25\n\nFigure 8: Visualization of synthetic data on CIFAR-100 generated by FADRM +.\n26\n\nFigure 9: Visualization of synthetic data on Tiny-ImageNet generated by FADRM .\n27\n\nFigure 10: Visualization of synthetic data on Tiny-ImageNet generated by FADRM +.\n28\n\nFigure 11: Visualization of synthetic data on ImageNette generated by FADRM .\n29\n\nFigure 12: Visualization of synthetic data on ImageNette generated by FADRM +.\n30\n\nFigure 13: Visualization of synthetic data on ImageWoof generated by FADRM .\n31\n\nFigure 14: Visualization of synthetic data on ImageWoof generated by FADRM +.\n32\n\nFigure 15: Visualization of synthetic data on ImageNet-1k generated by FADRM .\n33\n\nFigure 16: Visualization of synthetic data on ImageNet-1k generated by FADRM +.\n34\n\n",
    "source": "http://arxiv.org/abs/2506.24125v1",
    "authors": [
      "Jiacheng Cui",
      "Xinyue Bi",
      "Yaxin Luo",
      "Xiaohan Zhao",
      "Jiacheng Liu",
      "Zhiqiang Shen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "type": "content"
  },
  {
    "id": "2506.24124v1_abstract",
    "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives",
    "content": "Title: Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives\n\nAbstract: Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.",
    "source": "http://arxiv.org/abs/2506.24124v1",
    "authors": [
      "Dong Sixun",
      "Fan Wei",
      "Teresa Wu",
      "Fu Yanjie"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24124v1_content",
    "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives",
    "content": "arXiv:2506.24124v1  [cs.LG]  30 Jun 2025Teaching Time Series to See and Speak: Forecasting\nwith Aligned Visual and Textual Perspectives\nSixun Dong1Wei Fan2Teresa Wu1Yanjie Fu1∗\n1Arizona State University2University of Oxford\n{sixun.dong, teresa.wu, yanjie.fu}@asu.edu ,wei.fan@ox.ac.uk\nAbstract\nTime series forecasting traditionally relies on unimodal numerical inputs, which\noften struggle to capture high-level semantic patterns due to their dense and un-\nstructured nature. While recent approaches have explored representing time series\nas text using large language models (LLMs), these methods remain limited by\nthe discrete nature of token sequences and lack the perceptual intuition humans\ntypically apply, such as interpreting visual patterns. In this paper, we propose a\nmultimodal contrastive learning framework that transforms raw time series into\nstructured visual and textual perspectives. Rather than using natural language\nor real-world images, we construct both modalities directly from numerical se-\nquences. We then align these views in a shared semantic space via contrastive\nlearning, enabling the model to capture richer and more complementary represen-\ntations. Furthermore, we introduce a variate selection module that leverages the\naligned representations to identify the most informative variables for multivari-\nate forecasting. Extensive experiments on fifteen short-term and six long-term\nforecasting benchmarks demonstrate that our approach consistently outperforms\nstrong unimodal and cross-modal baselines, highlighting the effectiveness of mul-\ntimodal alignment in enhancing time series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP .\n1 Introduction\nTime series data, consisting of records captured over time, are prevalent in our lives and various\napplication domains [ 55]. As one of the essential tasks in time series data, time series forecasting,\ninvolves predicting future data points by analyzing historical data. Given its widespread demand\nin real-world scenarios, time series forecasting has attracted considerable research interest from\nthe artificial intelligence and deep learning community, particularly in fields such as meteorology\n[132, 117, 133], energy [112, 55, 85], healthcare [44, 11], and financial investment [17, 74].\nClassic deep learning-based approaches to time series forecasting usually rely on designing unique\narchitectures as different end-to-end forecasting models. Researchers have explored various architec-\ntures, including Transformer-based [ 77,70], CNN-based [ 97], MLP-based [ 79,131], and RNN-based\nmodels [ 59,71], to accomplish forecasting. However, these methods mostly treats time series data\npurely as numerical sequences; in other words, they can be depicted as unimodal frameworks, as\nshown in Figure 1(a). Unfortunately, unimodal frameworks are limited in modeling complex patterns,\ncontextual semantics, and long-term dependencies, and, thus, often result into fragile models.\nWith the success of Large Language Models (LLMs), researchers in time series have brought up the\nnew perspective of cross modality . There are studies that treat time series as a “foreign language” that\ncan be decoded using the language representation strategies of large language models through fine-\n∗Corresponding author.\nPreprint.\n\nMethod Input LM Train Emb. LM Use Vision MM Feat.\nTime-LLM Prompted LLaMA ✗ LM-based TSF. ✗ ✗\nChronos (T5) Tokenized T5 ✓ LM-based TSF. ✗ ✗\nPatchTST Numerical ✗ ✓ ✗ ✗ ✗\niTransformer Numerical ✗ ✓ ✗ ✗ ✗\nTimesCLIP Numerical CLIP-Text ✓ Extract Feat. ✓ ✓\n(c) Comparison of input modalities and modeling strategies.\nFigure 1: Comparison of frameworks and modeling strategies. Overview of modeling paradigms for\nTime Series Forecasting(TSF). The left figure contrasts unimodal and multimodal frameworks, while\nthe right table summarizes representative methods across core modeling dimensions. TimesCLIP\nuniquely constructs and aligns visual and textual features directly from numerical time series, enabling\nmultimodal learning without requiring external modalities.\ntuning [ 90,62,71] or reprogramming dedicated time series encoders [ 41]. Despite these advances,\nrepresenting time series as a language is still less intuitive, as humans typically interpret time series\ndata visually rather than textually [92]. For example, doctors usually adopt the visual information in\nECG to diagnose heart conditions [ 83]; financial professionals would conduct analysis based on the\nstock market charts instead of pure numbers [ 9]. Recent studies have suggested that directly applying\nLLM techniques to time series forecasting may not always yield effective results [ 92,43]. Recent\ncross-modality studies [ 56,86,7,57,47] have converted time series data into images for diverse\ntasks, but are not performant in time series forecasting [ 86,56]. More recent studies have utilized\nFourier-transformed spectrograms [ 7,111], which, however, don’t provide visually interpretative\ntime series representations.\nBesides, efforts have been made to extract multimodal knowledge for time series related domain-\nspecific applications. The studies in [ 66,12] attempted to classify different sensors; however,\nthe sensors capture the same type of data-human motion, which does not conform to the true\nmultimodality. In healthcare, the integration of medical time series and disease description texts to\nconstruct multimodal frameworks has been explored [ 83]. However, these applied studies benefit\ndomain-specific data with annotated multimodal information, but are limited in generalized time\nseries analysis and forecasting.\nIn response to the above challenges, we introduce TimesCLIP , a novel Multimodal Contrastive\nLearning approach for time series modeling and forecasting. Inspired by the effective modeling of\nCLIP [ 81] that bridges the gap between text and visual understanding, TimesCLIP introduces a novel\nidea to jointly learn both numeric time series and visual representations. Specifically, it involves\nconverting time series data into images and employing a multimodal contrastive learning framework.\nFigure 1 (b) shows that, in the vision branch, original numerical variates are transformed into distinct\nfigures using different colors. In the language branch, similar to LLM-based approaches for time\nseries, we treat numerical time series data as a \"foreign\" language. However, instead of employing a\ncomplex projection module to align time series with \"real\" language, we employ a simpler learning\ntokenizer to align time series with the feature space of pretrained language model. We further\nintroduce a modified contrastive loss specifically designed for multimodal time series, aiming to align\nthe vision representation with the language representation within a multimodal space. Subsequently,\nwe introduce an innovative variate selection module. We designate the aligned classification feature as\n“Query” and use a cross-attention layer to identify the most correlative features for downstream tasks\nin the multivariate feature sequence. This module enables the model to effectively utilize multimodal\nfeatures and classification information to identify the most significant one among multiple variables\nfor downstream tasks. In brief, we summarize our contribution in threefold:\n•We present a novel concept for converting numerical multivariate time series into vision and\nlanguage representations. We propose an innovative multimodal contrastive learning framework to\nalign time series data with a vision-language multimodal space.\n•We introduce a variate selection module designed to leverage aligned multimodal features, enabling\nthe identification of the most relevant variable among the multivariate time series for time series\nforecasting.\n•Extensive experiments demonstrate that our proposed model, TimesCLIP, achieves strong perfor-\nmance on both short-term and long-term forecasting tasks.\n2\n\nFigure 2: Overview of the TimesCLIP framework. TimesCLIP converts numerical multivariate time\nseries into colorized figures and patchified sequences, which are then processed by pretrained vision\nand language backbones. A learnable CLS token is appended on the language side to serve as a\nunified representation for contrastive alignment. A multimodal InfoNCE loss Lalignaligns vision and\nlanguage features into a shared space. The aligned CLStoken is then used to identify task-relevant\nvariates via cross-attention, and passed to a generator for forecasting.\n2 Related Work\nMultimodal Learning for Time Series. Contrastive learning has significantly advanced the process-\ning and understanding of multimodal data, especially CLIP [ 81], which illustrates the generalization\nability of multimodal contrastive learning. [ 119] built deep multimodal representation learning from\ntemporal data, which utilizes different modality information, such as audio, image, and language.\n[87] introduced the Dual Swin Transformer (DuST), a multimodal classification framework, that\nintegrates video with synchronous time series data for driving risk assessment. [ 50] explored a frozen\nlanguage model for zero-shot learning on ECG data. [ 22] integrated structured and unstructured data\nas different modalities. However, these works require the original dataset to contain more than one\nmodality or do not require aligning different modalities. The benchmarks for time series forecasting\n[115,70], only contain recorded numerical data, cannot work with these methods. We summarize\nrepresentative forecasting paradigms and their use of language models, input modalities, and multi-\nmodal strategies in Figure 1(c). [ 66,12] claim that they attempt to utilize multimodal learning for\nhuman motion classification, a type of time series data. However, they remain constrained by the use\nof various sensors placed at different positions on the human body for action recognition. They do\nnot adhere to the commonly accepted definition of multimodality in the AI field, which traditionally\ninvolves integrating distinct modalities such as vision, audio, language, and depth, rather than using\ndifferent sensors to record the same modality information. While recent work has explored applying\nvision-language models (VLMs) to time series data [ 135], we instead propose a contrastive alignment\nmethod specifically designed for numerical time series.\nIn addition, we include more related work on time series forecasting with language models, vision-\nlanguage contrastive learning, and time series foundation models in Appendix H.\n3 Method\nIn this section, we first present our proposed model in Section 3.1 for time series forecasting.\nWe explain the modified vision representation module and language module in Section 3.2 and\ncontrastive learning multimodal alignment for time series in Section 3.3, followed by the designed\nvariate selection module in Section 3.4. The aligned language representation of time series data is\npassed into the generator to forecast future time series data in Section 3.5.\n3.1 Overview\nFigure 1 shows our general-purpose architecture of multi-modal time series contrastive learning\nincludes two modules: (i) vision module and (ii) language module. To implement the general purpose\n3\n\narchitecture, Figure 2 shows our proposed model TimesCLIP. Compared with the proposed general\nmultimodal architecture in Figure 1, TimesCLIP consists of three parts: (i) a multimodal vision\nmodule, (ii) a multimodal language module, and (iii) a variate selection module.\nThe left part of Figure 2 shows the vision module of TimesCLIP contains three blocks: the Visual-\nization Preprocess for time series, a pretrained and frozen multimodal vision backbone to extract\ntime series figure features, and a trainable multimodal projection layer. First, the Visualization\nPreprocess converts the original time series signals into figures, by using a designed normalization\nstep to normalize the original numerical values and visualize them in the figures with different colors.\nBy converting the original time series into images, we can utilize existing image feature encoders\nto extract time series features from a visual perspective. Additionally, following [ 21,127,81], we\nfeed vision features to a learnable multimodal projection to transform and represent time series\nvision features into a multimodal time series feature space. Overall, this vision branch plays the most\nimportant role in our proposed multimodal framework, which successfully converts the numerical data\nof the time series into a visual representation space that is to be aligned with language representation\nspace.\nThe right part of Figure 2 shows that we patchify each variate of time series data and leverage an online\nlearnable time series tokenizer to project the patchified time series signal into language tokens. After\nconcatenating classification class tokens for each variate, we pass them into the pretrained language\nencoder to obtain the language representation of time series data. Additionally, the introduced class\ntokens are utilized for multimodal alignment. More explanation about the modules above is in\nSection 3.2. Specifically, we introduce a special variate selection module, which utilizes classification\ninformation from cross-modal space to select the most significant correlative variate feature from\nmulti-variate time series data. Finally, we feed the aligned language representation into the generator\nto produce the final forecast.\n3.2 Vision-Language Module for Time Series\nIn time series forecasting, given a length- Thistorical multivariate time series observation X1:T=\n{v1:T\n1, . . . , v1:T\nN} ∈RT×N,v1:T\ni={v1\ni, . . . , vT\ni}, i∈[1,2, . . . , N ].Nrepresents different recorded\nvariates, and Tis the known historical time step. We predict the future Stime steps of the time series\nY=X1+S:T+S={v1+S:T+S\n1 , . . . , v1+S:T+S\nN }.\nFigure 2 shows that, with different preprocessing procedures for time series data, its vision rep-\nresentation and language representation are produced by the vision module and language module,\nrespectively.\nVision module . Since original time series data is numerical signals with multivariate, differences\nin numerical ranges exist. To visualize these data with reasonable value ranges in the figure, we\nnormalize every variable in a fixed window size Lwindow instead of normalizing in all time series\ndata or mini-batch data. In this way, we reduce the impact of outlier maxima or minima compared to\nnormalizing the values in the minibatch or all data. Next, we plot each time series variable with a\nspecific color to help discriminate variable types to align in the multimodal feature space with the\nlanguage representation of time series. This process can be denoted as Equation (1).\nIi=Visualize (v1:T\ni), i∈[1, . . . , N ] (1)\nAs a result, we convert time series into a sequence of images Ximg={I1, I2, . . . , I N}. Inspired by\nthe prior works [ 81,127], we input the image sequence of the time series XImginto a pretrained\nand frozen multimodal vision encoder Ev. This process produces a sequence of feature maps\n{f1, f2, . . . , f N}, where each feature map fiis generated by fimg\ni=Eimg(Ii), i∈[1,2, . . . , N ].\nSince the multimodal visual backbone is kept frozen during training, we introduce an additional\nprojection layer Projimgto align the visual features of the time series with the multimodal feature\nspace. Consequently, similar to conventions established in [ 81,127,129], the vision representation\nis referred to as the vision classification token. We denote this vision representation as CLSimg\ni=\nProjimg(fi), i∈[1,2, . . . , N ].\nFinally, for length- Tmultiple variates time series data X1:T\ni, we have its vision representation as\nXimg\nfeat={CLSimg\n1, . . . , CLSimg\nN}.\n4\n\nLanguage module . Inspired by ViT [ 21] and Bert [ 45], PatchTST [ 77] introduces a patchify strategy\nthat segments time series into subseries-level patches, serving as input tokens for the Transformer.\nOur method applies this strategy to segment the time series X1:T={v1:T\n1, . . . , v1:T\nN}into patches\nas detailed in Equations (2) to (4). Prior to applying the patchify strategy, we follow the studies\nin [77,70,110] and implement layer normalization [ 2] to address the problem of non-stationarity in\ntime series data.\nxpatch=Patchify (LayerNorm (x1:T)) (2)\nxpatch={v1:PL\npatch,1, v1:PL\npatch,2, . . . , v1:PL\npatch,N} (3)\nv1:PL\npatch,i={v1\npatch,i, . . . , vM\npatch,i}, i∈[1,2, . . . , N ], (4)\nwhere PLrepresents the patch length, Sis the stride, and M=⌊T−PL\nS⌋+ 2is the number of patches.\nUnlike previous time series forecasting models based on the Transformer, such as PatchTST [ 77],\nwe utilize a multimodal pretrained model instead of training the Transformer from scratch. In prior\nstudies [ 81,127], words must be tokenized using a pretrained tokenizer before being fed into the\nlanguage encoder. However, due to the substantial domain gap between time series data and natural\nlanguage, we cannot use a pretrained language tokenizer, such as CLIPTokenizer [81]. To address\nthis issue, inspired by [ 77,70], we introduce a random initialized learnable linear layer that functions\nas an embedding layer, also referred to as the Tokenizer, for time series.\nv1:PL\ntoken,i=Tokenizer (v1:PL\npatch,i), i∈[1,2, . . . , N ] (5)\nv1:PL\ntoken,i={v1\ntoken,i, v2\ntoken,i, . . . , vM\ntoken,i} (6)\nSubsequently, following [ 127], to adopt multimodal language encoder to time series domain, we\nprepend a learnable embedding vclsto each token sequence of variate v1:PL\ntoken,i, which we call [class]\ntoken [ 21,127,81]. Equation (7) shows that our method learns language representation by utiliz-\ning pretrained multimodal language model Etextto embed time series into multimodal language\nrepresentation space.\nftext\ni=Etext(\u0002\nvcls, v1\ntoken,i, v2\ntoken,i, . . . , vM\ntoken,i\u0003\n+epos) (7)\nwhere [., .]represents the concatenation of time series token and [class ]token, and eposrepresents the\nlearnable position embedding. Notably, we use the same [class ]token for each given variate v1:PL\npatch,i.\nConsequentially, we obtain language representations of time series as Equation (8)\nXtext\nfeat={ftext\n1, ftext\n2, . . . , ftext\nN} (8)\nFollowing the contrastive learning strategies in [ 21,81,127], we regard the first token of ftext\nias the\n[class]token, denoted by CLSitext, to align with the vision representation. Consequently, we define\nXtext\ncls={CLStext\n1, . . . , CLStext\nN}.\n3.3 Multimodal Contrastive Loss\nOur proposed vision-language module (Section 3.2) processes a length- Tmultivariate time series\nX1:Tto obtain the vision representation Ximg\nfeatand the language classification representation Xtext\ncls\nof the time series, where Ximg\nfeat, Xtext\ncls∈RN×D,Nrepresents the number of variables, and D\nis the feature dimension. Then, by using a batch of data to calculate the loss, we obtain the\nvision representations V={Ximg\nfeat,1, . . . , Ximg\nfeat,B}and language classification representations L=\n{Xtext\ncls,1, . . . , Xtext\ncls,B}, where Bdenotes the batch size.\nAfter that, we incorporate the multimodal contrastive alignment for time series into the standard\ncontrastive framework [81], based on InfoNCE loss [78] as follows:\nLcont(V, L) =−1\nBPB\ni=1logexp(φ(Ximg\ncls,i,Xtext\ncls,i)/τ)\nPB\nj=1exp(φ(Ximg\ncls,i,Xtext\ncls,j)/τ)(9) φ(Ximg\ncls,i, Xtext\ncls,i) =Ximg\ncls,i\r\r\rXimg\ncls,i\r\r\r·Xtext\ncls,iT\n∥Xtext\ncls,iT∥(10)\nwhere τis a learnable temperature parameter [ 81,127] and φ(·,·)denotes cosine similarity. Each\nmultimodal pair represents Nvariates from a length- Ttime series. Positive pairs are formed between\nCLSimg\niandCLStext\nifrom the same sample, while negatives come from different samples in the batch.\nWe compute the contrastive loss bidirectionally:\nLalign=Lcont(V, L) +Lcont(L, V). (11)\n5\n\n3.4 Variate Selection\nWe design a novel variate selection module to effectively utilize information through the variates of\noriginal time series data across different views. Inspired by [ 127,51], we leverage the [class ]token\nfrom the language module as the \"Query\" to identify the most correlative time series variate-level\ntoken via a cross-attention mechanism. Time series tokens are generated by tokenizing the time\nseries data [ 70] and then encoding them with the shared multimodal language model described in\nSection 3.2. Although this feature extraction step is similar to the approach in [ 70], we utilize a\nshared pretrained multimodal Transformer model, instead of training one from scratch.\nVariate Tokenizer . Note that in our model, given a multivariate time series X1:Tof length\nT, the approach diverges from the patchify strategy used in the language module, as described\nin Section 3.2. Each variable vi∈R1×Tis treated as an individual token, without apply-\ning the patchification process, where Ndenotes the number of variables, and Tdenotes the\ntime steps. As a result, X1:Tcan be represented as X∈RN×T. Post-tokenization, the\ntime series is transformed to X1:T\nvar∈RN×D, where Dis the hidden dimension. We pass\nX1:T\nvar={v1\nvar, v2\nvar, . . . , vN\nvar}to the pretrained multimodal language encoder Evar\ntextand produce variate-\nlevel time series representations H={h1, h2, . . . , h N}. This process is denoted as Equation (12).\nhi=Evar\ntext(vi\nvar), i∈[1,2, . . . , N ] (12) vi\nCLS=CLStext\ni+Softmax\u0012\n(WqCLStext\ni)(WkH)T√\ndk\u0013\nWvH (13)\nThe variate-level time series representation HandCLStext\ni, named Query in Figure 2, from the\nmultimodal language model discussed in Section 3.2, are normalized using layer normalization [ 2].\nSubsequently, we select the most correlative variate feature by employing a cross-attention layer, an\nadaptation of Transformer decoder[ 95], with CLStext\niserving as the Query and Has both the Key and\nValue in the cross-attention layer (Figure 2). Formally, this procedure is defined as:\nwhere Wq,Wk, andWvare the weight matrices for the Query, Key, and Value projections, respectively,\nin the cross-attention mechanism. dkrepresents the dimensionality of the per-head dimension of\nmulti-head attention[ 95,51]. Finally, vi\nCLSis passed through a LayerNorm and a two-layer FFN with\nnonlinearity. Together with the cross-attention layers in Equation (13), these components form a\nmodified Transformer decoder [95].\n3.5 Generator\nAfter obtaining vi\nCLSfor each variate from the variate selection module, we combine them with the\nmultimodal language representation of the time series. Rather than simply concatenating them at\nthe end of the feature sequence, we replace the last token of the multimodal language representation\nwithvi\nCLS. This replacement is motivated by the padding strategy used in the Patchify process\ndescribed in Equation (2). Further details and ablation studies on this fusion strategy can be found\nin Appendix D.2. Finally, following [ 77], we apply a flattening layer followed by a linear head to\ngenerate the forecasting result ˆY=ˆX1+S:T+S.\n3.6 Training Loss\nWe adopt the mean squared error (MSE) loss for all forecasting tasks, except for the M4 dataset\nwhere we follow prior work[ 116] and use the Symmetric Mean Absolute Percentage Error (SMAPE)\nloss:LSMAPE =200\nBPB\ni=1|Yi−ˆYi|\n|Yi|+|ˆYi|. We refer to both LMSEandLSMAPE as the predictive loss Lgen.\nThe overall training objective combines predictive and contrastive terms: L=λ1Lgen+λ2Lalign,\nwhere λ1andλ2are weighting coefficients.\n4 Experiments\nWe introduce the implementation details and evaluation metrics in Section 4.1. We then evaluate\nthe effectiveness of our proposed method for short-term forecasting in Section 4.2 and long-term\nforecasting in Section 4.3.\n6\n\nTable 1: Full results for the short-term forecasting task in the M4 dataset. X.in the Transformers\nindicates the name of X-former. Stationary means the Non-stationary Transformer.\nModelsMM-Based LLM-Based Uni-modality Models (End to End)\nTimesCLIP Time-LLM *GPT4TS *iTrans. TimesNet N-HiTS N-BEATS∗ETS. LightTS DLinear FED. Stationary Auto. Pyra. In. LogTrans Re. LSTM TCN S4\n(Ours ) [2024] [2023] [2024] [2022] [2023] [2019] [2022] [2023] [2023] [2022] [2022] [2021] [2022] [2021] [2019] [2020] [1997] [2019] [2021]YearlySMAPE ↓13.188 13.419 15.110 14.141 13.387 13.418 13.436 18.009 14.247 16.965 13.728 13.717 13.974 15.530 14.727 17.107 16.169 176.040 14.920 61.675\nMASE ↓ 2.95 3.005 3.565 3.179 2.996 3.045 3.043 4.487 3.109 4.283 3.048 3.078 3.134 3.711 3.418 4.177 3.800 31.033 3.364 19.953\nOWA↓ 0.775 0.789 0.911 0.833 0.786 0.793 0.794 1.115 0.827 1.058 0.803 0.807 0.822 0.942 0.881 1.049 0.973 9.290 0.880 4.397QuarterlySMAPE ↓10.007 10.110 10.597 10.739 10.100 10.202 10.124 13.376 11.364 12.145 10.792 10.958 11.338 15.449 11.360 13.207 13.313 172.808 11.122 65.999\nMASE ↓ 1.166 1.178 1.253 1.282 1.182 1.194 1.169 1.906 1.328 1.520 1.283 1.325 1.365 2.350 1.401 1.827 1.775 19.753 1.360 17.662\nOWA↓ 0.880 0.889 0.938 0.955 0.890 0.899 0.886 1.302 1.000 1.106 0.958 0.981 1.012 1.558 1.027 1.266 1.252 15.049 1.001 9.436MonthlySMAPE ↓12.502 12.980 13.258 13.727 12.670 12.791 12.677 14.588 14.014 13.514 14.260 13.917 13.958 17.642 14.062 16.149 20.128 143.237 15.626 64.664\nMASE ↓ 0.929 0.963 1.003 1.082 0.933 0.969 0.937 1.368 1.053 1.037 1.102 1.097 1.103 1.913 1.141 1.660 2.614 16.551 1.274 16.245\nOWA↓ 0.870 0.903 0.931 0.984 0.878 0.899 0.880 1.149 0.981 0.956 1.012 0.998 1.002 1.511 1.024 1.340 1.927 12.747 1.141 9.879OthersSMAPE ↓ 4.587 4.795 6.124 5.569 4.891 5.061 4.925 7.267 15.880 6.709 4.954 6.302 5.485 24.786 24.460 23.236 32.491 186.282 7.186 121.844\nMASE ↓ 3.116 3.178 4.116 4.052 3.302 3.216 3.391 5.240 11.434 4.953 3.264 4.064 3.865 18.581 20.960 16.288 33.355 119.294 4.677 91.650\nOWA↓ 0.974 1.006 1.259 1.225 1.035 1.040 1.053 1.591 3.474 1.487 1.036 1.304 1.187 5.538 5.879 5.013 8.679 38.411 1.494 27.273Weighted\nAverageSMAPE ↓11.642 11.983 12.690 12.699 11.829 11.927 11.851 14.718 13.525 13.639 12.840 12.780 12.909 16.987 14.086 16.018 18.200 160.031 13.961 67.156\nMASE ↓ 1.560 1.595 1.808 1.761 1.585 1.613 1.599 2.408 2.111 2.095 1.701 1.756 1.771 3.265 2.718 3.010 4.223 25.788 1.945 21.208\nOWA↓ 0.837 0.859 0.940 0.929 0.851 0.861 0.855 1.172 1.051 1.051 0.918 0.930 0.939 1.480 1.230 1.378 1.775 12.642 1.023 8.021\n∗For N-BEATS[ 79], we follow the experiment result in [ 115], which remove the special ensemble method. We adopt the performance of\nTime-LLM and GPT4TS from [41].\nTable 2: Average results on PEMS and illness\ndatasets. Full results are provided in Table 10.\nModelTimesCLIP iTransformer PatchTST\nOurs [2024] [2022]\nMetric MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓\nPEMS03 0.176 0.261 0.250 0.306 0.299 0.363\nPEMS04 0.098 0.216 0.121 0.232 0.312 0.377\nPEMS03 0.115 0.226 0.128 0.237 0.202 0.293\nPEMS07 0.358 0.773 1.334 1.480 1.009 1.351\nillness 1.986 0.878 2.261 0.961 2.137 0.892Table 3: Forecasting results on EPF datasets\n(multivariate, 168 →24).\nModelTimesCLIP iTransformer PatchTST\nOurs [2024] [2022]\nMetric MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓\nNord Pool 0.293 0.325 0.338 0.352 0.307 0.336\nPJM 0.076 0.178 0.079 0.179 0.081 0.187\nEPEX-BE 0.142 0.162 0.149 0.174 0.158 0.179\nEPEX-FR 0.143 0.151 0.148 0.154 0.159 0.167\nEPEX-DE 0.190 0.245 0.212 0.261 0.210 0.258\n4.1 Experimental Details\nWe employ the pretrained CLIP vision and language encoders, both based on ViT-B [ 81], as our\nvision and language backbones, respectively. In our experiments, we freeze the vision backbone\nwithin our vision module and fine-tune the pretrained CLIP text encoder during training. Additionally,\nwe replace CLIP’s original token embedding with a new embedding trained from scratch. Unlike\nend-to-end models [ 77,70,115], which require modifications for different benchmarks, our proposed\nmethod maintains the same architecture, including hidden dimensions and module layers. To ensure a\nfair evaluation of method effectiveness, we implement our model without any fine-tuning techniques,\nsuch as Parameter-Efficient Fine-Tuning (PEFT) [ 34], which are commonly used to accelerate training\nand enhance performance. More implementation details are provided in the Appendix A.\nEvaluation Metrics . Following [ 70], we adopt mean absolute error(MAE) and mean square er-\nror(MSE) for long-term forecasting. Following [ 115,79], we use symmetric mean absolute percent-\nage error(SMAPE), mean absolute scaled error(MASE) and overall weighted average (OWA) for\nshort-term forecasting. More details are provided in Appendix B.\n4.2 Short-term Forecasting\nM4 dataset. Following [ 70,41], we select the M4 dataset as the benchmark for short-term forecasting.\nMore detailed dataset description and model hyperparameters are provided in the Appendix E.1.\nBaselines . Following [ 116], we extensively compared 19 models, categorized into three architectures:\n(1) Multimodal (MM)-based model, our proposed TimesCLIP (2) LLM-based models such as Time-\nLLM [ 41] and GPT4TS [ 140]. (3) Unimodal end-to-end models: LSTM [1997], S4 [2021], TCN\n[2019], TimesNet [ 115], N-HiTS [2023], N-BEATS [2019], LightTS [2023], , DLinear [2023],\niTransformer [2024], Reformer [2020], Informer [2021], Pyraformer [2022], Autoformer [2021],\nFEDformer [2022], Non-stationary Transformer [2022], and ETSformer [2022].\nAs shown in Table 1, our MM-based model, TimesCLIP, achieves the best performance across all\ndatasets and metrics compared to all baselines.\nEPF dataset. We follow [ 65] in using the EPF dataset, but extend the original 2 →1 setting to\na multivariate-to-multivariate configuration (3 inputs →3 outputs) to better evaluate short-term\nmultivariate forecasting. Only two baselines are included due to the revised setup. Additionally, we\n7\n\ntreat multi-scenario datasets (e.g., M4, EPF, PEMS) as multiple independent short-term forecasting\ntasks, resulting in a total of fifteen short-term benchmarks in our evaluation.\n4.3 Long-term Forecasting\nWe conduct experiments on six standard long-term forecasting datasets: Electricity (ECL), Exchange,\nTraffic, Weather, Illness, ETTm1, and ETTm2. While our setup is based on the widely-adopted\nbenchmark of TimesNet [ 115], we intentionally omit prediction lengths of 336 and 720, as these\nrepresent forecasting horizons up to 7.5 times longer than the context length. Instead, we fix the input\nsequence length to 96 and evaluate on horizons of 96 and 192, which reflect more realistic forecasting\nscenarios. This choice is motivated by recent studies [ 8,1,77] that emphasize the importance of\naligning context and prediction ranges in practical applications.\nBaselines . Following [ 70], we choose 13 time series forecasting models as our benchmark, in-\ncluding: (1) Our method, TimesCLIP, is the only MM-based method. (2) LLM-based methods:\nTime-LLM[ 42] and GPT4TS[ 139]. (3) Uni-modality models: iTransformer[ 70], Autoformer[ 114],\nFEDformer[ 137], Stationary[ 69], Crossformer[ 134], PatchTST[ 77], DLinear[ 128], TiDE[ 18],\nRLinear[ 58], SCINet[ 65], and TimesNet[ 116]. The results of uni-modal models and GPT4TS\nfrom [ 70,139], however, the results of Time-LLM[2024] from [ 92]. More details and discussion are\nin Appendix E.2.\nThe results are illustrated in Table 4, Bold indicates the best performance, and Underline represents\nthe second-best. Our multimodal(MM)-based model, TimesCLIP, achieves the best performance on\nsix datasets and secures the most first-place rankings in long-term forecasting tasks.\nTable 4: Full results of the long-term forecasting task. The prediction lengths are set to {96,192},\nfollowing the setting of TimesNet [2022]. The input sequence length is set to 96 for all baselines.\nAvgdenotes the average results across all four prediction lengths.\nModelsMM-Based LLM-Based Uni-Models (End-to-End)\nTimesCLIP Time-LLM GPT4TS iTransformer RLinear PatchTST Crossformer TiDE TimesNet DLinear SCINet FEDformer Stationary Autoformer\nOurs [2024] [2023] [2024] [2023] [2022] [2023] [2023] [2022] [2023] [2022] [2022] [2022] [2021]\nMetric MSE↓MAE↓MSE↓ MAE↓ MSE↓ MAE↓ MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓Exch.96 0.083 0.204 0.123 0.251 0.096 0.218 0.086 0.206 0.093 0.217 0.088 0.205 0.256 0.367 0.094 0.218 0.107 0.234 0.088 0.218 0.267 0.396 0.148 0.278 0.111 0.237 0.197 0.323\n192 0.172 0.298 0.224 0.344 0.182 0.307 0.177 0.299 0.184 0.307 0.176 0.299 0.470 0.509 0.184 0.307 0.226 0.344 0.176 0.315 0.351 0.459 0.271 0.315 0.219 0.335 0.300 0.369\nAvg 0.128 0.251 0.174 0.298 0.139 0.263 0.132 0.253 0.139 0.262 0.132 0.252 0.363 0.438 0.139 0.263 0.167 0.289 0.132 0.267 0.309 0.428 0.210 0.297 0.165 0.286 0.249 0.346Traffic96 0.390 0.262 0.392 0.267 0.388 0.282 0.395 0.268 0.649 0.389 0.462 0.295 0.522 0.290 0.805 0.493 0.593 0.321 0.650 0.396 0.788 0.499 0.587 0.366 0.612 0.338 0.613 0.388\n192 0.403 0.274 0.409 0.271 0.407 0.290 0.417 0.276 0.601 0.366 0.466 0.296 0.530 0.293 0.756 0.474 0.617 0.336 0.598 0.370 0.789 0.505 0.604 0.373 0.613 0.340 0.616 0.382\nAvg 0.397 0.268 0.401 0.269 0.398 0.286 0.406 0.272 0.625 0.378 0.464 0.296 0.526 0.292 0.781 0.484 0.605 0.329 0.624 0.383 0.789 0.502 0.596 0.370 0.613 0.339 0.615 0.385Weather96 0.153 0.199 0.155 0.199 0.162 0.212 0.174 0.214 0.192 0.232 0.177 0.218 0.158 0.230 0.202 0.261 0.172 0.220 0.196 0.255 0.221 0.306 0.217 0.296 0.173 0.223 0.266 0.336\n192 0.202 0.246 0.223 0.261 0.204 0.248 0.221 0.254 0.240 0.271 0.225 0.259 0.206 0.277 0.242 0.298 0.219 0.261 0.237 0.296 0.261 0.340 0.276 0.336 0.245 0.285 0.307 0.367\nAvg 0.178 0.223 0.189 0.230 0.183 0.230 0.198 0.234 0.216 0.252 0.201 0.239 0.182 0.254 0.222 0.280 0.196 0.241 0.217 0.276 0.241 0.323 0.247 0.316 0.209 0.254 0.287 0.352ETTm196 0.319 0.358 0.294 0.341 0.300 0.340 0.334 0.368 0.355 0.376 0.329 0.367 0.404 0.426 0.364 0.387 0.338 0.375 0.345 0.372 0.418 0.438 0.379 0.419 0.386 0.398 0.505 0.475\n192 0.367 0.387 0.341 0.369 0.343 0.000 0.377 0.391 0.391 0.392 0.367 0.385 0.450 0.451 0.398 0.404 0.374 0.387 0.380 0.389 0.439 0.450 0.426 0.441 0.459 0.444 0.553 0.496\nAvg 0.343 0.373 0.318 0.355 0.322 0.170 0.356 0.380 0.373 0.384 0.348 0.376 0.427 0.439 0.381 0.396 0.356 0.381 0.363 0.381 0.429 0.444 0.403 0.430 0.423 0.421 0.529 0.486ETTm296 0.176 0.258 0.162 0.248 0.163 0.249 0.180 0.264 0.182 0.265 0.175 0.259 0.287 0.366 0.207 0.305 0.187 0.267 0.193 0.292 0.286 0.377 0.203 0.287 0.192 0.274 0.255 0.339\n192 0.242 0.302 0.235 0.304 0.222 0.291 0.250 0.309 0.246 0.304 0.241 0.302 0.414 0.492 0.290 0.364 0.249 0.309 0.284 0.362 0.399 0.445 0.269 0.328 0.280 0.339 0.281 0.340\nAvg 0.209 0.280 0.199 0.276 0.193 0.270 0.215 0.287 0.214 0.285 0.208 0.281 0.351 0.429 0.249 0.335 0.218 0.288 0.239 0.327 0.343 0.411 0.236 0.308 0.236 0.307 0.268 0.340Solar.96 0.203 0.248 - - - - 0.206 0.237 0.322 0.339 0.208 0.251 0.310 0.331 0.312 0.399 0.250 0.292 0.290 0.378 0.237 0.344 0.242 0.342 0.215 0.249 0.884 0.711\n192 0.244 0.261 - - - - 0.241 0.264 0.359 0.356 0.239 0.272 0.734 0.725 0.339 0.416 0.296 0.318 0.320 0.398 0.280 0.380 0.285 0.380 0.254 0.272 0.834 0.692\nAvg 0.224 0.254 - - - - 0.224 0.250 0.341 0.348 0.224 0.262 0.522 0.528 0.326 0.408 0.273 0.305 0.305 0.388 0.259 0.362 0.264 0.361 0.235 0.261 0.859 0.702\nFew-shot Learning . We conduct experiments with few-shot learning on {5%, 10%, 20%} of the\ndata from the Exchange dataset. The results are shown in Table 8.\n5 Analysis\nIn this section, we first analyze the impact of multimodal alignment and variate selection module in\nSections 5.1 and 5.2. To comprehensively evaluate our proposed multimodal contrastive learning\nframework, we conduct ablation studies of different vision and language backbones in Section 5.3.\nFurthermore, we discuss the limitations and broader impact of our work.\n5.1 Ablation of Multimodal Alignment\nWe perform a comprehensive ablation study on both short-term (M4) and long-term (Exchange)\nforecasting benchmarks to evaluate the contributions of key components in TimesCLIP. Since the\nM4 dataset contains only a single variate, colorization is not applicable there; thus, the ablation of\ncolorization is conducted on Exchange.\nAs shown in Table 5, the multimodal contrastive loss Lalignand Variate Selection module are essential\nfor strong performance. In particular, colorization, the assignment of fixed colors to each variate,\nenables the vision encoder to visually distinguish variables, which is crucial for effective contrastive\nlearning. Without colorization, the selection mechanism based on the visual ’Query’ becomes\n8\n\nsignificantly less effective. Further analysis of the normalization and colorization steps is provided in\nAppendix D.1.\nWe further validate the general effectiveness of multimodal alignment across different vision and\nlanguage backbones in Section 5.3 and Table 6, where the alignment consistently improves forecasting\nperformance regardless of the specific encoder choice.\nTable 5: Ablation study of our proposed module.\nMethodVision Module Variable M4 Weighted Avg. Exchange\nLalign Col. Selection SMAPE ↓MASE ↓OWA↓ MSE↓MAE↓\nOurs✗ ✗\n✗11.782 1.578 0.847 0.389 0.414\n✓ ✗ - - - 0.383 0.413\n✓ ✓ 11.725 1.572 0.843 0.376 0.410\n✓ ✗ ✓ - - - 0.383 0.424\n✓ ✓ ✓ 11.642 1.560 0.837 0.335 0.394Table 6: Results with different pretrained\nvision and language backbones on M4.\nLanguage Vision\nMM Pretrained LalignM4 Weighted Avg.\nBackbone Backbone SMAPE ↓MASE ↓OWA↓\nBERT-base -\n✗✗ 16.356 2.234 1.187\nBERT-base ResNet50 ✓ 17.898 2.611 1.342\nBERT-base ViT-B/16 ✓ 17.927 2.634 1.350\nBERT-base SwinT-B ✓ 17.954 2.643 1.353\nT5 -\n✗✗ 11.798 1.579 0.848\nT5 CLIP-ViT ✓ 11.775 1.579 0.847\nCLIP-Text -\n✗✗ 11.782 1.578 0.847\nCLIP-Text ResNet50 ✓ 11.681 1.565 0.840\nCLIP-Text ViT-B/16 ✓ 11.640 1.557 0.836\nCLIP-Text SwinT-B ✓ 11.676 1.566 0.840\nCLIP-Text CLIP-Vision ✓ ✓ 11.642 1.560 0.8375.2 Ablation of Variate Selection\nWe present ablation studies to assess the effectiveness of variate selection module within our proposed\nframework. Because our method needs a vision branch to obtain the class token, which will serves\nas the \"Query\" for the Variable Selection module, we only evaluate variable selection when the vision\nmodule is working. The results in Table 5 indicate that variable selection can help our method achieve\nbetter performance. It also benefits from colorization. Since we leverage a shared language backbone\nfor the Variable Selection module, enabling the variable selection module does not introduce extra\nparameters except for additional learnable positional embedding.\n5.3 Ablation of Backbone\nWe investigate the impact of different pretrained backbones on forecasting performance, including\nvision encoders such as ViT-B/16 [2020], Swin Transformer [2021], ResNet50 [2016], and CLIP-\nVision [ 81], as well as language encoders like BERT [2019], T5 [2020], and CLIP-Text [ 81]. As\nshown in Table 6, CLIP-Text consistently outperforms BERT and T5, even without the vision module.\nThis suggests that language representations learned in a multimodal space are more effective for time\nseries forecasting than those trained on textual corpora alone.\nPrior work [ 93] shows that prompting-based methods like Time-LLM, which use frozen LLMs (e.g.,\nLLaMA) for autoregressive generation, perform poorly in forecasting tasks. More recent approaches\nsuch as Chronos [ 1] improve performance by re-learning the embedding layer of T5 and formulating\nforecasting as a token-level generation problem.\nIn contrast, our method takes a fundamentally different approach: instead of treating forecasting as a\nlanguage modeling task, we align time series with a multimodal representation space via contrastive\nlearning. This avoids the limitations of both prompting and generation, and leads to more robust and\ngeneralizable forecasting performance. This alignment-based strategy avoids reliance on discrete\ntoken sequences and preserves continuous semantics across views.\nBroader Impact and Limitations . In multivariate time series forecasting, our proposed model\nrequires converting each variate into an image and requires significant GPU memory for extracting\nimage features. Additionally, GPU memory requirements increase as the token length and the number\nof variables grow. Moreover, similar to CLIP [2021], our proposed multimodal framework can\nalign numerical data with both vision and language representations. It has the potential to extend to\nvision-based large language models and time series foundation models.\n6 Conclusion\nWe aim to address the limitations of previous works that solely relied on unimodal approaches for time\nseries forecasting. We present a novel multimodal contrastive learning model, called TimesCLIP. This\napproach successfully aligns time series data with a multimodal vision-language space by converting\nthe original numerical data points into colorized images and interpreting the time series data as a\n\"foreign\" language. Moreover, to efficiently leverage the aligned multimodal features, we designed a\nvariate selection module to identify the most correlative variate feature from the multivariate time\nseries feature sequence. Extensive experiments demonstrate that our proposed method surpasses\nestablished baselines in both short-term and long-term forecasting.\n9\n\nReferences\n[1]Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin\nShen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, Shubham\nKapoor, Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari\nTorkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. Chronos:\nLearning the language of time series, 2024. URL https://arxiv.org/abs/2403.07815 .\n[2] Jimmy Lei Ba. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016.\n[3]Haoyue Bai, Wangyang Ying, Nanxu Gong, Xinyuan Wang, Hao Liu, and Yanjie Fu. Privacy\npreserving generative feature transformation.\n[4]Haoyue Bai, Min Hou, Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, and Meng Wang.\nGorec: a generative cold-start recommendation framework. In Proceedings of the 31st ACM\ninternational conference on multimedia , pages 1004–1012, 2023.\n[5]Haoyue Bai, Le Wu, Min Hou, Miaomiao Cai, Zhuangzhuang He, Yuyang Zhou, Richang\nHong, and Meng Wang. Multimodality invariant learning for multimedia-based new item\nrecommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research\nand Development in Information Retrieval , pages 677–686, 2024.\n[6]Haoyue Bai, Guodong Chen, Wangyang Ying, Xinyuan Wang, Nanxu Gong, Sixun Dong,\nGiulia Pedrielli, Haoyu Wang, Haifeng Chen, and Yanjie Fu. Brownian bridge augmented\nsurrogate simulation and injection planning for geological co _2storage. arXiv preprint\narXiv:2505.18204 , 2025.\n[7]Silvio Barra, Salvatore Mario Carta, Andrea Corriga, Alessandro Sebastian Podda, and\nDiego Reforgiato Recupero. Deep learning and time series-to-image encoding for finan-\ncial forecasting. IEEE/CAA Journal of Automatica Sinica , 7(3):683–692, 2020.\n[8]Christoph Bergmeir. Fundamental limitations of foundational forecasting models: The need\nfor multimodality and rigorous evaluation. Invited Talk at the NeurIPS 2024 Workshop on\nTime Series in the Age of Large Models, December 2024. https://neurips.cc/virtual/\n2024/108471 .\n[9]Konstantinos Bisiotis, Stelios Psarakis, and Athanasios N Yannacopoulos. Control charts in\nfinancial applications: An overview. Quality and Reliability Engineering International , 38(3):\n1441–1462, 2022.\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. Advances in neural information processing systems , 33:1877–\n1901, 2020.\n[11] C Bui, N Pham, A V o, A Tran, A Nguyen, and T Le. Time series forecasting for healthcare\ndiagnosis and prognostics with the focus on cardiovascular diseases. In 6th International\nConference on the Development of Biomedical Engineering in Vietnam (BME6) 6 , pages\n809–818. Springer, 2018.\n[12] Ruichu Cai, Zhifang Jiang, Zijian Li, Weilin Chen, Xuexin Chen, Zhifeng Hao, Yifan Shen,\nGuangyi Chen, and Kun Zhang. From orthogonality to dependency: Learning disentangled\nrepresentation for multi-modal time-series sensing signals. arXiv preprint arXiv:2405.16083 ,\n2024.\n[13] David Campos, Miao Zhang, Bin Yang, Tung Kieu, Chenjuan Guo, and Christian S Jensen.\nLightts: Lightweight time series classification with adaptive ensemble distillation. Proceedings\nof the ACM on Management of Data , 1(2):1–27, 2023.\n[14] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, and Yuexian Zou. Locvtp:\nVideo-text pre-training for temporal localization. arXiv preprint arXiv:2207.10362 , 2022.\n10\n\n[15] Cristian Challu, Kin G Olivares, Boris N Oreshkin, Federico Garza Ramirez, Max Mergenthaler\nCanseco, and Artur Dubrawski. Nhits: Neural hierarchical interpolation for time series\nforecasting. In Proceedings of the AAAI conference on artificial intelligence , volume 37, pages\n6989–6997, 2023.\n[16] Zhenrui Chen, Zhibo Dai, Huiyan Xing, Junyu Chen, Menghao Huo, and Kuan Lu. Multi-\nmodel approach for stock price prediction and trading recommendations. Preprints , May\n2025. doi: 10.20944/preprints202501.1003.v3. URL https://doi.org/10.20944/\npreprints202501.1003.v3 .\n[17] Dawei Cheng, Fangzhou Yang, Sheng Xiang, and Jin Liu. Financial time series forecasting\nwith multi-modality graph neural network. Pattern Recognition , 121:108218, 2022.\n[18] Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu.\nLong-term forecasting with tide: Time-series dense encoder. arXiv preprint arXiv:2304.08424 ,\n2023.\n[19] Jiaxiang Dong, Haixu Wu, Yuxuan Wang, Yunzhong Qiu, Li Zhang, Jianmin Wang, and\nMingsheng Long. Timesiam: A pre-training framework for siamese time-series modeling.\narXiv preprint arXiv:2402.02475 , 2024.\n[20] Sixun Dong, Huazhang Hu, Dongze Lian, Weixin Luo, Yicheng Qian, and Shenghua Gao.\nWeakly supervised video representation learning with unaligned text for sequential videos. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n2437–2447, 2023.\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929 , 2020.\n[22] Sayna Ebrahimi, Sercan O Arik, Yihe Dong, and Tomas Pfister. Lanistr: Multimodal learning\nfrom structured and unstructured data. arXiv preprint arXiv:2305.16556 , 2023.\n[23] Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable repre-\nsentation learning for multivariate time series. Advances in neural information processing\nsystems , 32, 2019.\n[24] Nanxu Gong, Sixun Dong, Haoyue Bai, Xinyuan Wang, Wangyang Ying, and Yanjie Fu.\nAgentic feature augmentation: Unifying selection and generation with teaming, planning, and\nmemories. arXiv preprint arXiv:2505.15076 , 2025.\n[25] Nanxu Gong, Zijun Li, Sixun Dong, Haoyue Bai, Wangyang Ying, Xinyuan Wang, and Yanjie\nFu. Sculpting features from noise: Reward-guided hierarchical diffusion for task-optimal\nfeature transformation. arXiv preprint arXiv:2505.15152 , 2025.\n[26] Nanxu Gong, Chandan K Reddy, Wangyang Ying, Haifeng Chen, and Yanjie Fu. Evolutionary\nlarge language model for automated feature transformation. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 39, pages 16844–16852, 2025.\n[27] Nanxu Gong, Xinyuan Wang, Wangyang Ying, Haoyue Bai, Sixun Dong, Haifeng Chen, and\nYanjie Fu. Unsupervised feature transformation via in-context generation, generator-critic llm\nagents, and duet-play teaming. arXiv preprint arXiv:2504.21304 , 2025.\n[28] Nanxu Gong, Wangyang Ying, Dongjie Wang, and Yanjie Fu. Neuro-symbolic embedding\nfor short and effective feature selection via autoregressive generation. ACM Transactions on\nIntelligent Systems and Technology , 16(2):1–21, 2025.\n[29] Mononito Goswami, Konrad Szafer, Arjun Choudhry, Yifu Cai, Shuo Li, and Artur\nDubrawski. Moment: a family of open time-series foundation models.. 2024. arXiv preprint\narXiv:2402.03885 , 2024.\n11\n\n[30] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew G Wilson. Large language models are\nzero-shot time series forecasters. Advances in Neural Information Processing Systems , 36,\n2024.\n[31] Albert Gu, Karan Goel, and Christopher Ré. Efficiently modeling long sequences with\nstructured state spaces. arXiv preprint arXiv:2111.00396 , 2021.\n[32] Andrey Guzhov, Federico Raue, Jörn Hees, and Andreas Dengel. Audioclip: Extending clip to\nimage, text and audio. In ICASSP 2022-2022 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) , pages 976–980. IEEE, 2022.\n[33] Tengda Han, Weidi Xie, and Andrew Zisserman. Temporal alignment networks for long-\nterm video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 2906–2916, 2022.\n[34] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, and Sai Qian Zhang. Parameter-efficient\nfine-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608 ,\n2024.\n[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npages 770–778, 2016.\n[36] Zhuangzhuang He, Yifan Wang, Yonghui Yang, Peijie Sun, Le Wu, Haoyue Bai, Jinqi Gong,\nRichang Hong, and Min Zhang. Double correction framework for denoising recommendation.\nInProceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining , pages 1062–1072, 2024.\n[37] S Hochreiter. Long short-term memory. Neural Computation MIT-Press , 1997.\n[38] Huazhang Hu, Sixun Dong, Yiqun Zhao, Dongze Lian, Zhengxin Li, and Shenghua Gao.\nTransrac: Encoding multi-scale temporal correlation with transformers for repetitive action\ncounting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 19013–19022, 2022.\n[39] Menghao Huo, Kuan Lu, Yuxiao Li, and Qiang Zhu. Ct-patchtst: Channel-time patch\ntime-series transformer for long-term renewable energy forecasting. arXiv preprint\narXiv:2501.08620 , 2025. URL https://arxiv.org/abs/2501.08620 .\n[40] Furong Jia, Kevin Wang, Yixiang Zheng, Defu Cao, and Yan Liu. Gpt4mts: Prompt-based\nlarge language model for multimodal time-series forecasting. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 38, pages 23343–23351, 2024.\n[41] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y . Zhang, Xiaoming Shi, Pin-Yu\nChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series\nforecasting by reprogramming large language models, 2024. URL https://arxiv.org/\nabs/2310.01728 .\n[42] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y . Zhang, Xiaoming Shi, Pin-Yu\nChen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, and Qingsong Wen. Time-llm: Time series\nforecasting by reprogramming large language models, 2024. URL https://arxiv.org/\nabs/2310.01728 .\n[43] Ming Jin, Yifan Zhang, Wei Chen, Kexin Zhang, Yuxuan Liang, Bin Yang, Jindong Wang,\nShirui Pan, and Qingsong Wen. Position: What can large language models tell us about time\nseries analysis. In Forty-first International Conference on Machine Learning , 2024.\n[44] Shruti Kaushik, Abhinav Choudhury, Pankaj Kumar Sheron, Nataraj Dasgupta, Sayee Natara-\njan, Larry A Pickett, and Varun Dutt. Ai in healthcare: time-series forecasting using statistical,\nneural, and ensemble architectures. Frontiers in big data , 3:4, 2020.\n[45] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In Proceedings of naacL-HLT ,\nvolume 1, page 2. Minneapolis, Minnesota, 2019.\n12\n\n[46] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\narXiv preprint arXiv:2001.04451 , 2020.\n[47] Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, and Huaxiu Yao. Lite:\nModeling environmental ecosystems with multimodal large language models. arXiv preprint\narXiv:2404.01165 , 2024.\n[48] Haozhou Li, Qinke Peng, Xinyuan Wang, Xu Mou, and Yonghao Wang. Sehf: A summary-\nenhanced hierarchical framework for financial report sentiment analysis. IEEE Transactions\non Computational Social Systems , 11(3):4087–4101, 2023.\n[49] Haozhou Li, Xinyuan Wang, Hongkai Du, Wentong Sun, and Qinke Peng. Sade: A speaker-\naware dual encoding model based on diagbert for medical triage and pre-diagnosis. In\nICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Process-\ning (ICASSP) , pages 12712–12716. IEEE, 2024.\n[50] Jun Li, Che Liu, Sibo Cheng, Rossella Arcucci, and Shenda Hong. Frozen language model\nhelps ecg zero-shot learning. In Medical Imaging with Deep Learning , pages 402–415. PMLR,\n2024.\n[51] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-\nimage pre-training with frozen image encoders and large language models. In International\nconference on machine learning , pages 19730–19742. PMLR, 2023.\n[52] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,\nLimin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355 , 2023.\n[53] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu,\nMichael Zeng, Heng Ji, and Shih-Fu Chang. Clip-event: Connecting text and images with\nevent structures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 16420–16429, 2022.\n[54] Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng\nYan. Enhancing the locality and breaking the memory bottleneck of transformer on time series\nforecasting. Advances in neural information processing systems , 32, 2019.\n[55] Wenxiang Li and KL Eddie Law. Deep learning models for time series forecasting: a review.\nIEEE Access , 2024.\n[56] Xixi Li, Yanfei Kang, and Feng Li. Forecasting with time series imaging. Expert Systems with\nApplications , 160:113680, 2020.\n[57] Zekun Li, Shiyang Li, and Xifeng Yan. Time series as images: Vision transformer for\nirregularly sampled time series. Advances in Neural Information Processing Systems , 36,\n2024.\n[58] Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu. Revisiting long-term time series forecasting: An\ninvestigation on linear mapping. arXiv preprint arXiv:2305.10721 , 2023.\n[59] Shengsheng Lin, Weiwei Lin, Wentai Wu, Feiyu Zhao, Ruichao Mo, and Haotong Zhang.\nSegrnn: Segment recurrent neural network for long-term time series forecasting. arXiv preprint\narXiv:2308.11200 , 2023.\n[60] Hanghang Liu, Linyi Liu, and Clifford J Rosen. Pth and the regulation of mesenchymal cells\nwithin the bone marrow niche. Cells , 13(5):406, 2024.\n[61] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\nAdvances in neural information processing systems , 36, 2024.\n[62] Haoxin Liu, Zhiyuan Zhao, Jindong Wang, Harshavardhan Kamarthi, and B Aditya Prakash.\nLstprompt: Large language models as zero-shot time series forecasters by long-short-term\nprompting. arXiv preprint arXiv:2402.16132 , 2024.\n13\n\n[63] Linyi Liu, Sha Leng, Junli Yue, Qian Lu, Weizhe Xu, Xiaowei Yi, Dingming Huang, and Lan\nZhang. Edta enhances stromal cell–derived factor 1 α–induced migration of dental pulp cells\nby up-regulating chemokine receptor 4 expression. Journal of Endodontics , 45(5):599–605,\n2019.\n[64] Linyi Liu, Phuong T Le, J Patrizia Stohn, Hanghang Liu, Wangyang Ying, Roland Baron, and\nClifford J Rosen. Calorie restriction in mice impairs cortical but not trabecular peak bone mass\nby suppressing bone remodeling. Journal of Bone and Mineral Research , 39(8):1188–1199,\n2024.\n[65] Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang\nXu. Scinet: Time series modeling and forecasting with sample convolution and interaction.\nAdvances in Neural Information Processing Systems , 35:5816–5828, 2022.\n[66] Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Suhas Diggavi,\nMani Srivastava, and Tarek Abdelzaher. Focal: Contrastive learning for multimodal time-\nseries sensing signals in factorized orthogonal latent space. Advances in Neural Information\nProcessing Systems , 36, 2024.\n[67] Shizhan Liu, Hang Yu, Cong Liao, Jianguo Li, Weiyao Lin, Alex X Liu, and Schahram Dustdar.\nPyraformer: Low-complexity pyramidal attention for long-range time series modeling and\nforecasting. In # PLACEHOLDER_PARENT_METADATA_VALUE# , 2022.\n[68] Xu Liu, Junfeng Hu, Yuan Li, Shizhe Diao, Yuxuan Liang, Bryan Hooi, and Roger Zim-\nmermann. Unitime: A language-empowered unified model for cross-domain time series\nforecasting. In Proceedings of the ACM on Web Conference 2024 , pages 4095–4106, 2024.\n[69] Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long. Non-stationary transformers: Ex-\nploring the stationarity in time series forecasting. Advances in Neural Information Processing\nSystems , 35:9881–9893, 2022.\n[70] Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng\nLong. itransformer: Inverted transformers are effective for time series forecasting, 2024. URL\nhttps://arxiv.org/abs/2310.06625 .\n[71] Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingsheng Long. Auto-\ntimes: Autoregressive time series forecasters via large language models. arXiv preprint\narXiv:2402.02370 , 2024.\n[72] Yong Liu, Haoran Zhang, Chenyu Li, Xiangdong Huang, Jianmin Wang, and Mingsheng\nLong. Timer: Generative pre-trained transformers are large time series models. In Forty-first\nInternational Conference on Machine Learning , 2024.\n[73] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision , pages 10012–10022, 2021.\n[74] Wenjie Lu, Jiazheng Li, Jingyang Wang, and Lele Qin. A cnn-bilstm-am method for stock\nprice prediction. Neural Computing and Applications , 33(10):4741–4753, 2021.\n[75] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. Howto100m: Learning a text-video embedding by watching hundred million\nnarrated video clips. In Proceedings of the IEEE/CVF International Conference on Computer\nVision , pages 2630–2640, 2019.\n[76] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu,\nShiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general\nvideo recognition. In European Conference on Computer Vision , pages 1–18. Springer, 2022.\n[77] Yuqi Nie, Nam H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is\nworth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730 ,\n2022.\n14\n\n[78] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive\npredictive coding. arXiv preprint arXiv:1807.03748 , 2018.\n[79] Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats:\nNeural basis expansion analysis for interpretable time series forecasting. arXiv preprint\narXiv:1905.10437 , 2019.\n[80] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\nstyle, high-performance deep learning library. Advances in neural information processing\nsystems , 32, 2019.\n[81] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning ,\npages 8748–8763. PMLR, 2021.\n[82] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research , 21(140):1–67, 2020.\n[83] Antônio H Ribeiro, Manoel Horta Ribeiro, Gabriela MM Paixão, Derick M Oliveira, Paulo R\nGomes, Jéssica A Canazart, Milton PS Ferreira, Carl R Andersson, Peter W Macfarlane,\nWagner Meira Jr, et al. Automatic diagnosis of the 12-lead ecg using a deep neural network.\nNature communications , 11(1):1760, 2020.\n[84] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\nHigh-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 10684–10695, 2022.\n[85] Jason Runge and Radu Zmeureanu. A review of deep learning techniques for forecasting\nenergy use in buildings. Energies , 14(3):608, 2021.\n[86] Artemios-Anargyros Semenoglou, Evangelos Spiliotis, and Vassilios Assimakopoulos. Image-\nbased time series forecasting: A deep convolutional neural network approach. Neural Networks ,\n157:39–53, 2023.\n[87] Liang Shi, Yixin Chen, Meimei Liu, and Feng Guo. Dust: Dual swin transformer for multi-\nmodal video and time-series modeling. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 4537–4546, 2024.\n[88] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Learning video representa-\ntions using contrastive bidirectional transformer. arXiv preprint arXiv:1906.05743 , 2019.\n[89] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A\njoint model for video and language representation learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 7464–7473, 2019.\n[90] Chenxi Sun, Hongyan Li, Yaliang Li, and Shenda Hong. Test: Text prototype aligned\nembedding to activate llm’s ability for time series. arXiv preprint arXiv:2308.08241 , 2023.\n[91] Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu, Huan Yang, and Jianlong Fu. Long-form\nvideo-language pre-training with multimodal temporal contrastive learning. arXiv preprint\narXiv:2210.06031 , 2022.\n[92] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are\nlanguage models actually useful for time series forecasting? In The Thirty-eighth Annual\nConference on Neural Information Processing Systems , 2024.\n[93] Mingtian Tan, Mike A Merrill, Vinayak Gupta, Tim Althoff, and Thomas Hartvigsen. Are\nlanguage models actually useful for time series forecasting? In The Thirty-eighth Annual\nConference on Neural Information Processing Systems , 2024.\n15\n\n[94] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nthée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\n[95] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems , 30, 2017.\n[96] Dongjie Wang, Yanyong Huang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang,\nSixun Dong, Tao Zhe, Kunpeng Liu, Meng Xiao, et al. Towards data-centric ai: A com-\nprehensive survey of traditional, reinforcement, and generative approaches for tabular data\ntransformation. arXiv preprint arXiv:2501.10555 , 2025.\n[97] Huiqiang Wang, Jian Peng, Feihu Huang, Jince Wang, Junhui Chen, and Yifei Xiao. Micn:\nMulti-scale local and global context modeling for long-term series forecasting. In The eleventh\ninternational conference on learning representations , 2023.\n[98] Jue Wang, Gedas Bertasius, Du Tran, and Lorenzo Torresani. Long-short temporal contrastive\nlearning of video transformers. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 14010–14020, 2022.\n[99] Mengmeng Wang, Jiazheng Xing, and Yong Liu. Actionclip: A new paradigm for video action\nrecognition. arXiv preprint arXiv:2109.08472 , 2021.\n[100] Xinyuan Wang, Qinke Peng, Xu Mou, Haozhou Li, and Ying Wang. A hierarchal bert structure\nfor native speaker writing detection. In 2022 China Automation Congress (CAC) , pages\n3705–3710. IEEE, 2022.\n[101] Xinyuan Wang, Haozhou Li, Dingfang Zheng, and Qinke Peng. Lcmdc: Large-scale chinese\nmedical dialogue corpora for automatic triage and medical consultation. arXiv preprint\narXiv:2410.03521 , 2024.\n[102] Xinyuan Wang, Dongjie Wang, Wangyang Ying, Rui Xie, Haifeng Chen, and Yanjie Fu.\nKnockoff-guided feature selection via a single pre-trained reinforced agent. arXiv preprint\narXiv:2403.04015 , 2024.\n[103] Xinyuan Wang, Liang Wu, Liangjie Hong, Hao Liu, and Yanjie Fu. Llm-enhanced user-item\ninteractions: Leveraging edge information for optimized recommendations. arXiv preprint\narXiv:2402.09617 , 2024.\n[104] Xinyuan Wang, Haoyue Bai, Nanxu Gong, Wangyang Ying, Sixun Dong, Xiquan Cui, and\nYanjie Fu. Llm-ml teaming: Integrated symbolic decoding and gradient search for valid and\nstable generative feature transformation. arXiv preprint arXiv:2506.09085 , 2025.\n[105] Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu,\nYanjie Fu, and Haifeng Chen. Mixllm: Dynamic routing in mixed large language models.\narXiv preprint arXiv:2502.18482 , 2025.\n[106] Xinyuan Wang, Dongjie Wang, Wangyang Ying, Haoyue Bai, Nanxu Gong, Sixun Dong,\nKunpeng Liu, and Yanjie Fu. Efficient post-training refinement of latent reasoning in large\nlanguage models. arXiv preprint arXiv:2506.08552 , 2025.\n[107] Xinyuan Wang, Liang Wu, and Yanjie Fu. Enhanced whole page optimization via mixed-\ngrained reward mechanism-adapted language models. arXiv preprint arXiv:2506.09084 , 2025.\n[108] Ying Wang, Qinke Peng, Xu Mou, Xinyuan Wang, Haozhou Li, Tian Han, Zhao Sun, and\nXiao Wang. A successful hybrid deep learning model aiming at promoter identification. BMC\nbioinformatics , 23(Suppl 1):206, 2022.\n[109] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Yong Liu, Mingsheng Long, and Jianmin Wang.\nDeep time series models: A comprehensive survey and benchmark. 2024.\n[110] Yuxuan Wang, Haixu Wu, Jiaxiang Dong, Guo Qin, Haoran Zhang, Yong Liu, Yunzhong Qiu,\nJianmin Wang, and Mingsheng Long. Timexer: Empowering transformers for time series\nforecasting with exogenous variables. arXiv preprint arXiv:2402.19072 , 2024.\n16\n\n[111] Zhiguang Wang and Tim Oates. Imaging time-series to improve classification and imputation.\narXiv preprint arXiv:1506.00327 , 2015.\n[112] Rafał Weron. Electricity price forecasting: A review of the state-of-the-art with a look into the\nfuture. International journal of forecasting , 30(4):1030–1081, 2014.\n[113] Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven Hoi. Etsformer: Expo-\nnential smoothing transformers for time-series forecasting. arXiv preprint arXiv:2202.01381 ,\n2022.\n[114] Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. Autoformer: Decomposition\ntransformers with auto-correlation for long-term series forecasting. Advances in neural\ninformation processing systems , 34:22419–22430, 2021.\n[115] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times-\nnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint\narXiv:2210.02186 , 2022.\n[116] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. Times-\nnet: Temporal 2d-variation modeling for general time series analysis. arXiv preprint\narXiv:2210.02186 , 2022.\n[117] Haixu Wu, Hang Zhou, Mingsheng Long, and Jianmin Wang. Interpretable weather forecasting\nfor worldwide stations with a unified deep model. Nature Machine Intelligence , 5(6):602–611,\n2023.\n[118] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and Juan Pablo Bello. Wav2clip: Learning\nrobust audio representations from clip. In ICASSP 2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) , pages 4563–4567. IEEE, 2022.\n[119] Xitong Yang, Palghat Ramesh, Radha Chitta, Sriganesh Madhvanath, Edgar A Bernal, and\nJiebo Luo. Deep multimodal representation learning from temporal data. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 5447–5455, 2017.\n[120] Wangyang Ying, Lei Zhang, and Hongli Deng. Sichuan dialect speech recognition with deep\nlstm network. Frontiers of Computer Science , 14(2):378–387, 2020.\n[121] Wangyang Ying, Dongjie Wang, Kunpeng Liu, Leilei Sun, and Yanjie Fu. Self-optimizing fea-\nture generation via categorical hashing representation and hierarchical reinforcement crossing.\nIn2023 IEEE International Conference on Data Mining (ICDM) , pages 748–757. IEEE, 2023.\n[122] Wangyang Ying, Dongjie Wang, Haifeng Chen, and Yanjie Fu. Feature selection as deep\nsequential generative learning. ACM Transactions on Knowledge Discovery from Data , 18(9):\n1–21, 2024.\n[123] Wangyang Ying, Dongjie Wang, Xuanming Hu, Yuanchun Zhou, Charu C Aggarwal, and\nYanjie Fu. Unsupervised generative feature transformation via graph contrastive pre-training\nand multi-objective fine-tuning. In Proceedings of the 30th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining , pages 3966–3976, 2024.\n[124] Wangyang Ying, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Sixun Dong, Haifeng Chen, and\nYanjie Fu. Bridging the domain gap in equation distillation with reinforcement feedback.\narXiv preprint arXiv:2505.15572 , 2025.\n[125] Wangyang Ying, Cong Wei, Nanxu Gong, Xinyuan Wang, Haoyue Bai, Arun Vignesh\nMalarkkan, Sixun Dong, Dongjie Wang, Denghui Zhang, and Yanjie Fu. A survey on\ndata-centric ai: Tabular learning from reinforcement learning and generative ai perspective.\narXiv preprint arXiv:2502.08828 , 2025.\n[126] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli,\nXiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization\nfor deep learning: Training bert in 76 minutes. arXiv preprint arXiv:1904.00962 , 2019.\n17\n\n[127] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui\nWu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint\narXiv:2205.01917 , 2022.\n[128] Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu. Are transformers effective for time series\nforecasting? In Proceedings of the AAAI conference on artificial intelligence , volume 37,\npages 11121–11128, 2023.\n[129] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for\nlanguage image pre-training. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision , pages 11975–11986, 2023.\n[130] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng\nGao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , pages 8552–8562, 2022.\n[131] Tianping Zhang, Yizhuo Zhang, Wei Cao, Jiang Bian, Xiaohan Yi, Shun Zheng, and Jian\nLi. Less is more: Fast multivariate time series forecasting with light sampling-oriented mlp\nstructures. arXiv preprint arXiv:2207.01186 , 2022.\n[132] Wenqing Zhang, Junming Huang, Ruotong Wang, Changsong Wei, Wenqian Huang, and\nYuxin Qiao. Integration of mamba and transformer-mat for long-short range time series\nforecasting with application to weather dynamics. In 2024 International Conference on\nElectrical, Communication and Computer Engineering (ICECCE) , pages 1–6. IEEE, 2024.\n[133] Yuchen Zhang, Mingsheng Long, Kaiyuan Chen, Lanxiang Xing, Ronghua Jin, Michael I\nJordan, and Jianmin Wang. Skilful nowcasting of extreme precipitation with nowcastnet.\nNature , 619(7970):526–532, 2023.\n[134] Yunhao Zhang and Junchi Yan. Crossformer: Transformer utilizing cross-dimension depen-\ndency for multivariate time series forecasting. In The eleventh international conference on\nlearning representations , 2023.\n[135] Siru Zhong, Weilin Ruan, Ming Jin, Huan Li, Qingsong Wen, and Yuxuan Liang. Time-vlm:\nExploring multimodal vision-language models for augmented time series forecasting, 2025.\nURL https://arxiv.org/abs/2502.04395 .\n[136] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai\nZhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In\nProceedings of the AAAI conference on artificial intelligence , volume 35, pages 11106–11115,\n2021.\n[137] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\nFrequency enhanced decomposed transformer for long-term series forecasting. In International\nconference on machine learning , pages 27268–27286. PMLR, 2022.\n[138] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer:\nFrequency enhanced decomposed transformer for long-term series forecasting. In International\nconference on machine learning , pages 27268–27286. PMLR, 2022.\n[139] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series\nanalysis by pretrained lm. Advances in neural information processing systems , 36:43322–\n43355, 2023.\n[140] Tian Zhou, Peisong Niu, Liang Sun, Rong Jin, et al. One fits all: Power general time series\nanalysis by pretrained lm. Advances in neural information processing systems , 36:43322–\n43355, 2023.\n[141] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n8746–8755, 2020.\n18\n\nA Implementation Details\nFor model initialization, we adopt Kaiming uniform initialization for 1D convolutional layers and\nXavier initialization for linear layers. Layer normalization layers are initialized with the bias set to\n0 and the weights set to 1. Additionally, the introduced class token is normalized from a normal\ndistribution, N(0,0.02). We set the weights in λ1Lgen+λ2Lalign asλ1= 1andλ2= 0.1, unless\notherwise specified. We implement our experiments using on PyTorch[ 80] and Time Series Library\n(TSLib)[ 109,115]. All experiments are conducted on a single NVIDIA RTX 6000 Ada Generation\nwith a fixed random seed of 2024.\nB Benchmarks and Evaluation Metrics\nDim. denotes the number of variables. The dataset sizes are presented as in (Train, Validation, Test).\nThe dataset descriptions are in Table 7, adopted from [ 115]. For evaluation, following[ 115], we\nTable 7: Dataset statistics from [115].\nTasks Dataset Dim Context →Predict Length Dataset Size Information (Frequency)\nETTm1, ETTm2 7\n96→{96, 192 }(34465, 11521, 11521) Electricity (15 mins)\nForecasting Traffic 862 (12185, 1757, 3509) Transportation (Hourly)\n(Long-term) Weather 21 (36792, 5271, 10540) Weather (10 mins)\nExchange 8 (5120, 665, 1422) Exchange rate (Daily)\nSolar-Energy 137 (36601,5161,10417) Energy(10 mins)\nM4-Yearly 1 6 (23000, 0, 23000) Demographic\nM4-Quarterly 1 8 (24000, 0, 24000) Finance\nForecasting M4-Monthly 1 18 (48000, 0, 48000) Industry\n(short-term) M4-Weakly 1 13 (359, 0, 359) Macro\nM4-Daily 1 14 (4227, 0, 4227) Micro\nM4-Hourly 1 48 (414, 0, 414) Other\nillness 7 36→{24,36,48, 60 } (617,74,170) Illness(Weekly)\nPEMS-03 358\n96→{12,24,48, 96 }(15617, 5135, 5135) Transportation(5 mins)\nPEMS-04 307 (10172, 3375, 3375) Transportation(5 mins)\nPEMS-07 883 (16911, 5622, 5622) Transportation(5 mins)\nPEMS-08 170 (10690, 3548, 3548) Transportation(5 mins)\nEPF-NordPool 3\n168→24(36500, 5219, 10460) Electricity Price (1 Hour)\nEPF-PJM 3 (36500, 5219, 10460) Electricity Price (1 Hour)\nEPF-EPEX-BE 3 (36500, 5219, 10460) Electricity Price (1 Hour)\nEPF-EPEX-FR 3 (36500, 5219, 10460) Electricity Price (1 Hour)\nEPF-EPEX-DE 3 (36500, 5219, 10460) Electricity Price (1 Hour)\nutilize the commonly used metrics in previous work[ 115,70], including mean square error (MSE)\nand mean absolute error (MAE) for long-term forecasting. For short-term forecasting, following\nN-BEATS [ 79,115], we adopt the symmetric mean absolute percentage error (SMAPE), mean\nabsolute scaled error (MASE), and overall weighted average (OWA) as the metrics. The detailed\n19\n\ncalculations are as follows:\nMAE =1\nHHX\ni=1|Xi−bXi|, MSE =1\nHHX\ni=1(Xi−bXi)2,\nSMAPE =200\nHHX\ni=1|Xi−bXi|\n|Xi|+|bXi|, MAPE =100\nHHX\ni=1|Xi−bXi|\n|Xi|,\nMASE =1\nHHX\ni=1|Xi−bXi|\n1\nH−mPH\nj=m+1|Xj−Xj−m|,OWA =1\n2\u0014SMAPE\nSMAPE Naïve2+MASE\nMASE Naïve2\u0015\n,\nwhere mis the periodicity of the data. X,bX∈RH×Care the ground truth and prediction results of\nthe future with Htime pints and Cdimensions. Ximeans the i-th future time point.\nC Few-shot Learning\nThe experiments in [ 93] indicate that LLM-based models do not benefit from pretrained large\nlanguage models. To further evaluate the effectiveness of our multimodal model, we select three\nwell-known end-to-end models as baselines. The results indicate that our method achieves comparable\nperformance in 10%-shot learning. However, due to the feature space gap between the language\ndomain and time series data, our model fails in 5%-shot learning. Additionally, we observe that our\nmodel, when using a pretrained backbone, tends to overfit on the training and validation sets when\ntrained with limited data, leading to lower performance on the test set.\nTable 8: Few-shot learning on Exchange dataset.\nModel TimesCLIP PatchTST iTransformer TimesNet\nFew-shot Metric MSE ↓MAE↓MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓\n20%96 0.121 0.241 0.087 0.208 0.100 0.226 0.153 0.275\n192 0.192 0.314 0.174 0.300 0.196 0.319 0.260 0.366\n336 0.368 0.447 0.290 0.393 0.370 0.446 0.417 0.472\n720 0.938 0.733 0.717 0.647 0.926 0.734 0.962 0.747\nAvg. 0.405 0.434 0.317 0.387 0.398 0.431 0.448 0.465\n10%96 0.102 0.227 0.089 0.210 0.114 0.241 0.152 0.275\n192 0.187 0.316 0.194 0.315 0.220 0.339 0.272 0.383\n336 0.322 0.417 0.325 0.417 0.388 0.458 0.400 0.466\nAvg. 0.204 0.320 0.203 0.314 0.241 0.346 0.275 0.375\n5%96 0.141 0.235 0.108 0.231 0.133 0.262 0.187 0.303\n192 0.671 0.619 0.237 0.351 0.239 0.356 0.271 0.376\nAvg. 0.406 0.427 0.173 0.291 0.186 0.309 0.229 0.340\nD Extra Ablation Study\nD.1 Ablation of Visualization Preprocess\nAs shown in Figure 3, we visualize the results under two conditions: Norm and Colorize, as introduced\nin Section 3.2. Figure 3(a) indicates that the original numerical time series data spans a wide range.\nAfter normalization, as illustrated in Figure 3(b), we observe that variates with smaller values are\nmagnified, making their periodicity more apparent. While the bottom three variates exhibit similar\npatterns, colorization helps the model distinguish different variates despite their structural similarity.\nD.2 Variate Fusion Strategy\nIn this section, we conducted an ablation study on the fusion strategy for fusing the obtained vCLS to\ntime series representation sequence as referred to in Section 3.5.\n20\n\nFigure 3: The visualization of Visualization Preprocess. Sample data is from Weather dataset.\nStrategy explanation . As introduced in Section 3.5, we propose a new variate fusion strategy to\nintegrate the extracted correlative variate into the language representation of the time series. In\ndetail, following [ 77], the patchify process includes a padding step, as detailed in Equation (2). The\noriginal length- Ttime series is padded to T+Padding , where the padding length is half of the stride.\nConsequently, the last patch contains XT−S\n2:Tand half padding data, while the second to last patch\ncontains XT−S:T. Replacing the last token with vi\nCLShelps mitigate the influence of padding on the\nprocessing and avoids introducing additional parameters in the linear head. We conduct an ablation\nstudy of this specific fusion process in Table 9.\nAblation of fusion strategy . We conducted ablation studies on three different fusion strategies\ndesigned to integrate the extracted correlative variate into the language representation of time series\nas the generator’s input. These strategies include concatenating at the end of the feature sequence,\nreplacing the class token, and replacing the last feature of the language representation. As shown in\nTable 9, the results demonstrate that replacing the last feature of the language representation sequence\nis the most effective strategy for integrating extracted correlative variable features. As explained in\nSection 3.5, we think the padding step, introduced by [ 77], employs \"same\" padding, and replicating\nthe last length- Padding of the time series features may compromise the model’s forecasting accuracy.\nTable 9: Ablation study on variate fusion strategy\nMethodVariate Fusion M4 Weighted Avg.\nStrategy Position SMAPE ↓MASE ↓OWA↓\nOursReplace First 11.687 1.563 0.840\nConcat End 11.682 1.562 0.839\nReplace Last 11.642 1.560 0.837\nE Experimental Details\nIn this section, we provide more experimental details and analysis. For experiments result in Tables 1\nand 4, if not otherwise stated, we adopt the baselines results from original paper and official code,\nexcept Time-LLM[ 42]. We noticed the short term forecasting results of other baselines in Time-\nLLM[ 42] are different with baselines’ original paper, such as TimesNet have lower performance\nin [42], although [ 42] claim their experimental setups followed [ 115]. For fair comparison, we\nadopt Time-LLM results for long-term forecasting from [ 93], which re-implements three LLM-based\nmodels and introduce a novel view that LLMs may not help time series analysis better. Additionally,\nwe adopt the experiment results of Time-LLM for short-term forecasting from [ 42], because these\nresults have no advantage compared with other baselines’ original results. Additionally, all the\nbaselines that we reproduced are implemented based on the configurations of the original paper and\nofficial code. It is also notable that no multimodal-based methods are proposed for general time series\nanalysis. Please refer to our code, which shows more experimental details.\nE.1 short-term forecasting\nWe implement our model for short-term forecasting with the hyper-parameters in Table 14. For\nM4 datasets, we conduct 17 uni-modality end-to-end baselines for short-term forecasting, which\n21\n\nare classified into four architectures: (1)RNN-based models like LSTM [1997] and S4 [2021]; (2)\nCNN-based models, specifically TCN [2019] and TimesNet [ 115]; (3) MLP-based models such as N-\nHiTS [2023], N-BEATS [2019], LightTS [2023], and DLinear [2023]; (4) Transformer-based models\nincluding iTransformer [2024], Reformer [2020], Informer [2021], Pyraformer [2022], Autoformer\n[2021], FEDformer [2022], Non-stationary Transformer [2022], and ETSformer [2022]. For PEMS,\nillness, and EPF datasets, we compared TimesCLIP with iTransformer and PatchTST. The full results\nare shown in Tables 1, 3 and 10.\nTable 10: PEMS and illness, input 96 for PEMS, input 36 for illness\nModelTimesCLIP iTransformer PatchTST\nOurs [2024] [2022]\nMetric MSE↓MAE↓MSE↓MAE↓MSE↓MAE↓PEMS0812 0.094 0.207 0.088 0.193 0.107 0.221\n24 0.130 0.244 0.138 0.243 0.167 0.279\n48 0.212 0.276 0.324 0.348 0.322 0.394\n96 0.269 0.319 0.450 0.439 0.599 0.557\nAvg 0.176 0.261 0.250 0.306 0.299 0.363PEMS0412 0.080 0.201 0.081 0.189 0.109 0.223\n24 0.088 0.207 0.100 0.212 0.179 0.293\n48 0.100 0.216 0.135 0.248 0.344 0.414\n96 0.124 0.240 0.167 0.279 0.615 0.580\nAvg 0.098 0.216 0.121 0.232 0.312 0.377PEMS0312 0.069 0.181 0.069 0.174 0.082 0.192\n24 0.091 0.206 0.098 0.209 0.125 0.235\n48 0.142 0.258 0.167 0.279 0.220 0.318\n96 0.157 0.257 0.178 0.287 0.381 0.428\nAvg 0.115 0.226 0.128 0.237 0.202 0.293PEMS0712 0.063 0.175 0.066 0.164 0.087 0.207\n24 0.074 0.183 0.087 0.192 0.145 0.264\n48 0.106 0.204 0.262 0.356 0.284 0.374\n96 0.116 0.210 0.918 0.768 0.493 0.506\nAvg 0.358 0.773 1.334 1.480 1.009 1.351illness24 2.007 0.869 2.317 0.941 2.199 0.889\n36 2.160 0.910 2.199 0.950 2.362 0.917\n48 1.849 0.845 2.243 0.964 2.028 0.876\n60 1.927 0.889 2.287 0.990 1.960 0.888\nAvg 1.986 0.878 2.261 0.961 2.137 0.892\nE.2 long-term forecasting\nTo evaluate our method on the Traffic and ECL datasets, our CLIP-based model results in GPU\nmemory consumption increasing with the number of variables. For instance, if we input 800 variates,\nthe resulting language feature will have the shape [B×800,12,512], requiring the processing of\nB×800time series figures, which can lead to memory overflow. To address this issue, we modify our\nmodel to embed numerical time series using the tokenizer, as described in Section 3.4, and exclude\nthe multimodal contrastive learning loss. The hyper-parameters are shown in Table 11. As a result,\nthe language feature shape becomes [B,800,512], reducing the feature size by 12 times compared\nto the original design. Furthermore, our model relies solely on the pretrained multimodal language\nmodel for long-term forecasting.\nAs discussed in Section 5.3, multimodal language models exhibit strong generalization ability for\ntime series forecasting tasks. The experimental results, shown in Table 4, demonstrate that our\nsimplified multimodal pretrained model still achieves the best performance on the Traffic dataset. We\nbelieve that with an efficient fine-tuning strategy, training our proposed model with contrastive loss\nwould further improve its performance.\nFor Weather and Exchange dataset, we have different hyper-parameters setting depend on prediction\nlength. The details are shown in Tables 12 and 13.\n22\n\nTable 11: Hyper-parameters on Traffic and ECL\nHyper-parameterLong-term Forecasting\nTraffic ECL\nOptimizer AdamW[126]\nLR decay schedule Cosine Decaying to 0\nPred Length 96, 192, 336, 720\ntrain epoch 50\nBatch Size 64\nPretrained Encoder LR 1e-4\nOthers LR 1e-4\nearly stop 20Table 12: Hyper-parameters on Weather\nHyper-parameter Weather (long-term)\nOptimizer AdamW[126]\nLR decay schedule Exponential Decay, γ= 0.5\nPred Length 96 192 336 720\ntrain epoch 10\nBatch Size 8 16 64 4\nPretrained Encoder LR 1e-6 5e-6 1e-6 1e-6\nOthers LR 1e-4\nearly stop 3\nλ1(weight of Lgen) 1\nλ2(weight of Lalign ) 0.1\nTable 13: Hyper-parameters on Exchange\nHyper-parameter Exchange (long-term)\nOptimizer AdamW[126]\nLR decay schedule Exponential Decay, γ= 0.5\nPred Length 96 192 336 720\ntrain epoch 10\nBatch Size 4 8 8 4\nPretrained Encoder LR 1e-5 1e-6 5e-5 5e-6\nOthers LR 1e-4\nearly stop 3\nλ1(weight of Lgen) 1\nλ2(weight of Lalign ) 0.5 0.1 0.1 0.9Table 14: Hyper-parameters on short forecasting.\nHyper-parameter M4, PMES, illness, EPF\nOptimizer AdamW[126]\nLR decay schedule Cosine Decay to 0\ntrain epoch 100\nBatch Size 64\nPretrained Encoder LR 1e-4\nOthers LR 1e-3\nλ1(weight of Lgen) 1\nλ2(weight of Lalign ) 0.1\nearly stop 30\nF FLOPs and Parameters Analysis\nWe compare the efficiency of our model in terms of FLOPs, trainable parameters, and total parameters\nagainst two Transformer-based models, one CNN-based model, and two LLM-based models. Note\nthat we implement the same model architecture, based on CLIP-ViT-B [ 81], across all datasets. The\nslight increase in parameters is due to the longer learnable position embeddings and the additional\nclass token. In contrast, other baselines are modified for different datasets, leading to variations\nin model parameters. It is important to note that LLM-based models have a large number of total\nparameters; however, through Parameter-Efficient Fine-Tuning(PEFT) [ 34], they fine-tune only a\nsmall subset of parameters. This fine-tuning technique is also can be applied to our Transformer-based\nmodel to reduce the number of trainable parameters. However, since this work focuses on multimodal\ncontrastive learning, we do not employ such techniques to accelerate training.\nTable 15: Comparison of FLOPs and parameters across different methods. Evaluate models with\nthe prediction length set to 96. We adopt the results of Time-LLM and GPT4TS from [ 93]. Other\nbaselines are evaluated based on their official codes and papers. The dim/Channel denotes the hidden\ndimension of attention layer or CNN’s channel depend on model’s architecture. Due to Transformer-\nbased model have different dimension of FFN, denoted as Dim. of FFN , for instance, PatchTST has\n2028 for Weather and 512 for Traffic, which cause different parameters.\nMethod Dataset Enc. Layers Dim/Channel Dim. of FFN FLOPs(G) Trainable Param.(M) Total Param.(M)\nTimesNet [2022]Exchange 2 64 64 4.54 4.71 4.71\nWeather 2 32 32 1.13 1.19 1.19\nTraffic 2 512 512 288.12 301.69 301.69\nPatchTST [2022]Exchange 2 512 2048 0.61 6.90 6.90\nWeather 2 512 2048 1.61 6.90 6.90\nTraffic 2 512 512 33.52 3.76 3.76\niTransformer [2024]Exchange 2 128 128 2.77 0.22 0.22\nWeather 3 512 512 0.123 4.83 3.83\nTraffic 4 512 512 8.63 6.41 6.41\nGPT4TS [2023]* Weather 6 Layers of GPT2 - - 86\nTime-LLM [2024]* Weather Llama-7B - - 6642\nTimesCLIP (Ours)Exchange\nCLIP-ViT-B (12 layers)39.44 65.85 153.31\nWeather 103.21 65.87 153.32\nTraffic 262.93 66.14 153.59\n23\n\nG Limitation and Further Improvement\nOur method is based on pretrained multimodal language encoder, a standard Transformer-based\nframework. However, the computational complexity of traditional Transformers, which scales\nasO(n2), poses a significant challenge when handling multiple variates as extremely long token\nsequences. This issue became apparent during our evaluation of the model on a traffic dataset, where\nwe were compelled to omit the vision module to conserve GPU resources. Consequently, the number\nof trainable parameters and the demand for GPU resources increase as the token length grows with\nthe number of variables. Fortunately, recent advancements have addressed these efficiency challenges;\nstudies such as those on the Linear Transformer and lower parameter fine-tuning strategies like LoRA\noffer promising solutions. We think that designing a more efficient time series imaging strategy and\nalignment method to reduce resource requirements will extend this work as a foundational multimodal\ntime series representation framework for more downstream tasks.\nFurthermore, mapping time-series signals into a unified vision–language embedding space can benefit\nother modalities as well. For instance, repetitive action counting in videos can be framed as a time-\nseries analysis problem [ 38]. The proposed multimodal alignment framework can also be applied\nto other downstream applications such as data-centric AI [ 96,125,3,24–28,102,104,121–123],\nlinguistics [ 120,100], business [ 36,103,4,5,48,107], and medicine [ 63,108,64,101,60,49].\nAdditionally, the reinforcement learning strategy [124], reasoning process [106], and the rule-based\nenhancement [6] could also be applied to enhance the robustness of the alignment process.\nH Related work\nTime Series Forecasting with Language Models. Recent developments in multimodal large\nlanguage models (LLMs), such as [ 94,61], demonstrate the growing multimodal understanding\ncapabilities of LLMs. For instance, [ 30] directly leverages the sequential processing capabilities\nof LLMs for numerical forecasting. Furthermore, several researchers have treated time series as\na language [ 40,139,42,90,62], employing pretrained large language models [ 10,94] to enhance\nforecasting methodologies. Notably, [ 90] introduced TEST, which employs text prototype-aligned\nembeddings to augment the ability of LLMs to process time series data. Similarly, [ 41] proposed\nTime-LLM, utilizing a novel training strategy named ’reprogramming,’ designed to tailor LLMs’\ncapabilities specifically for time series forecasting. However, a recent study [ 92] suggests that\nLLM-based time series forecasting models might achieve better performance by replacing their LLM\nmodules with a simpler encoder.\nVision-Text Multimodal Learning In recent years, vision-text multimodal learning [ 75,88,81,89,\n141,14,33,99,98,127,16] has attracted increasing attention within the computer vision community.\nOne of the most notable contributions is CLIP [ 81], which successfully learns multimodal visual\nrepresentations guided by natural language. CLIP utilizes a contrastive learning loss, specifically\nthe InfoNCE loss [ 78], to align visual representations with language representations within a shared\nmultimodal space. The zero-shot ability of multimodal contrastive learning has spurred a multitude of\nfollow-up works across various domains to align different modalities to a unified multimodal space,\nsuch as image domain [ 51,127,76], video domain [ 89,99,20,33,53,91], 3D domain [ 84,130,39],\nand audio domain [ 32,118]. Moreover, recent multimodal large language models[ 61,52] continue to\nemploy similar frameworks to integrate diverse modal representations with language representation\nspaces, unlocking the potential of LLMs. LLM routing can be applied to leverage the power of\ndifferent LLMs [105].\nTime Series Foundation Modals . Inspired by the concept of foundation models, such as LLMs,\nand moving away from training each model per task, some studies have also explored building\nfoundational models for time series. For instance, [ 29] introduced MOMENT, a family of pretrained\ntime-series foundation models, and collected a new time series benchmark. [ 68] introduced a unified\nmodel trained for cross-domain learning with natural language as domain instructions to provide\ndomain-specific information. In contrast to [ 77], which adapts pre-training strategies from neutral\nlanguage processing and computer vision, [ 72,19] introduced effective pre-training frameworks for\ntime series forecasting. These efforts aim to construct robust time series foundation models.\n24\n\n",
    "source": "http://arxiv.org/abs/2506.24124v1",
    "authors": [
      "Dong Sixun",
      "Fan Wei",
      "Teresa Wu",
      "Fu Yanjie"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "type": "content"
  },
  {
    "id": "2506.24123v1_abstract",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "content": "Title: Calligrapher: Freestyle Text Image Customization\n\nAbstract: We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.",
    "source": "http://arxiv.org/abs/2506.24123v1",
    "authors": [
      "Yue Ma",
      "Qingyan Bai",
      "Hao Ouyang",
      "Ka Leong Cheng",
      "Qiuyu Wang",
      "Hongyu Liu",
      "Zichen Liu",
      "Haofan Wang",
      "Jingye Chen",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24123v1_content",
    "title": "Calligrapher: Freestyle Text Image Customization",
    "content": "arXiv:2506.24123v1  [cs.CV]  30 Jun 2025Calligrapher: Freestyle Text Image Customization\nYUE MA∗and QINGYAN BAI∗,Hong Kong University of Science and Technology, China\nHAO OUYANG, Ant Group, China\nKA LEONG CHENG, Hong Kong University of Science and Technology, China\nQIUYU WANG, Ant Group, China\nHONGYU LIU and ZICHEN LIU, Hong Kong University of Science and Technology, China\nHAOFAN WANG, InstantX, Independent Research Team\nJINGYE CHEN, Hong Kong University of Science and Technology, China\nYUJUN SHEN†,Ant Group, China\nQIFENG CHEN†,Hong Kong University of Science and Technology, China\nSelf-referenceCross-reference\nFig. 1. Photorealistic text image customization results produced by our proposed Calligrapher , which allows users to perform customization with\ndiverse stylized images and text prompts. The input and reference images are shown in the lower left corner of the generated results, respectively for the\nsetting of self-reference and cross-reference text image customization.\nWe introduce Calligrapher, a novel diffusion-based framework that inno-\nvatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges\nof precise style control and data dependency in typographic customization,\nour framework incorporates three key technical contributions. First, we\n∗Both authors contributed equally to this research.\n†Corresponding authors.\nAuthors’ Contact Information: Yue Ma; Qingyan Bai, Hong Kong University of Science\nand Technology, Hong Kong, China; Hao Ouyang, Ant Group, Hangzhou, China; Ka\nLeong Cheng, Hong Kong University of Science and Technology, Hong Kong, China;\nQiuyu Wang, Ant Group, Hangzhou, China; Hongyu Liu; Zichen Liu, Hong Kong\nUniversity of Science and Technology, Hong Kong, China; Haofan Wang, InstantX,\nIndependent Research Team; Jingye Chen, Hong Kong University of Science and Tech-\nnology, Hong Kong, China; Yujun Shen, Ant Group, Hangzhou, China; Qifeng Chen,\nHong Kong University of Science and Technology, Hong Kong, China.develop a self-distillation mechanism that leverages the pre-trained text-\nto-image generative model itself alongside the large language model to\nautomatically construct a style-centric typography benchmark. Second, we\nintroduce a localized style injection framework via a trainable style encoder,\nwhich comprises both Qformer and linear layers, to extract robust style\nfeatures from reference images. An in-context generation mechanism is also\nemployed to directly embed reference images into the denoising process,\nfurther enhancing the refined alignment of target styles. Extensive quanti-\ntative and qualitative evaluations across diverse fonts and design contexts\nconfirm Calligrapher’s accurate reproduction of intricate stylistic details and\nprecise glyph positioning. By automating high-quality, visually consistent\ntypography, Calligrapher surpasses traditional models, empowering creative\npractitioners in digital art, branding, and contextual typographic design. The\ncode, model, and data can be found at the Project Page .\nAdditional Key Words and Phrases: Text image customization, style transfer,\ndiffusion models\n\n2•\n1 Introduction\nThe advertising and promotion industry, encompassing digital me-\ndia, branding, packaging, and printed materials, relies on vivid and\nmeticulously crafted typography to effectively communicate mes-\nsages and solidify brand identity. Currently, designers often dedicate\nsubstantial time to manually fine-tuning fonts to achieve specific\naesthetic objectives. This process is not only labor-intensive but\ncan also introduce inconsistencies. Therefore, an automated method\ncapable of generating text that emulates a reference style while\nensuring precise character positioning would significantly stream-\nline the design workflow and enhance overall visual consistency, as\ndemonstrated in Fig. 1.\nModern typography design, as illustrated in Fig. 2, predominantly\nemploys two main categories of methods. The first category centers\non the use of standardized font libraries [mdn web docs 1996]. While\nthese libraries offer considerable accessibility, they often present\nchallenges in seamless integration with diverse backgrounds and\ntypically necessitate substantial manual adjustment to achieve spe-\ncific aesthetic or artistic outcomes. The second branch of methods\nemploys neural generative models [Huang et al .2023; Mou et al .\n2024; Yang et al .2024; Zhang et al .2023b] to enable typography\ngeneration, editing, automating text modification, and font creation.\nAlthough promising, this technique frequently fails to capture the\nprecise nuances of specific font styles or handle styles different from\nthose in the source image, which are difficult to express through\ntextual description. Our work bridges these gaps by enhancing\ngenerative techniques to automate the typography customization\nprocess while ensuring that the final output closely adheres to the\ndesired visual style.\nSpecifically, we propose a diffusion-based framework to address\ndata dependency and precise style control through three technical\ncontributions. Firstly, we introduce a self-distillation framework to\nconstruct a style-oriented typography training dataset. This frame-\nwork leverages the pre-trained text-to-image generative model in\nconjunction with a large language model to synthesize a compre-\nhensive set of text images. These are processed and paired with cor-\nresponding reference images, prompts, and masks, thereby creating\nself-supervised training data that facilitates style learning without\nrequiring manual annotation. Based on the aforementioned data\ngeneration pipeline, we propose a style-centric text customization\nbenchmark. This benchmark, inlcuding training and test sets, is ex-\npected to further boost the development of the typography research\ncommunity. Secondly, a local style injection mechanism is designed\nto employ a trainable style encoder, including both Qformer [Li\net al.2023] and linear layers, to extract robust style-related features\nfrom references. By replacing cross-attention features in the denois-\ning transformer network with these style embeddings, the method\nachieves granular typographic control in the latent space. Thirdly,\nan in-context generation mechanism directly integrates reference\nimages into the denoising process. This integration significantly\nenhances the fidelity of style alignment between the generated out-\nput and the target references. These design elements enable our\nmethod to uniquely generate highly desirable text images that accu-\nrately reflect the style of reference inputs, even with arbitrary text\nor non-text images.\nStandardfontGenerative methodOurs\nInput\nManualworkandInflexible&unrealstyleInaccurate stlye controlAutomaticAccurate stlyecontrolandAutomatic\nFig. 2. Motivation and technical differentiation of our approach. Ex-\nisting typography design methods face critical limitations: (1) Standard\nfont libraries prioritize accessibility but require extensive manual adjust-\nments for integration into diverse backgrounds, resulting in inflexible and\nunrealistic outputs. (2) Neural generative models automate typography but\noften fail to capture precise font style nuances, especially when relying\non textual descriptions. In contrast, the proposed method addresses these\nchallenges by enabling fully automated typography generation with precise\nstyle control and various kinds of references, including non-text images.\nOverall, we construct and propose a style-centric text customiza-\ntion benchmark based on the self-distillation strategy, specifically\naddressing the critical need for the model learning and standardized\nevaluation in this field. The proposed model learned on the training\nset of this benchmark achieves success in text customization with\nvarious kinds of references, and has been extensively evaluated\nusing both qualitative and quantitative methods, including user\nstudies, demonstrating superior performance across multiple met-\nrics compared to existing approaches. Further results also suggest\nthe model also could be applied to the task of reference-based gener-\nation without tuning. Our work represents a significant step toward\nautomated, efficient, and artistically driven typography design, with\nsubstantial potential applications in both design and branding pro-\ncesses, potentially revolutionizing workflows in creative industries\nby reducing manual labor while maintaining artistic integrity.\n2 Related work\nVisual text rendering. Visual text rendering has been a longstand-\ning research problem in the era of generative AI. Traditional image\ngeneration models such as Stable Diffusion [Rombach et al .2022a],\nImagen [Saharia et al .2022], and DALL-E [Ramesh et al .2021] have\nfallen short in accurately rendering text. Consequently, some re-\nsearchers have resorted to incorporating additional conditions into\nthe generation process [Chen et al .2023b,a, 2024, 2025; Ji et al .2023;\nJiang et al .2025; Koo et al .2025; Liu et al .2022, 2024a,b; Ma et al .\n2024, 2023; Tuo et al .2024, 2023; Wang et al .2025; Zhao and Lian\n2023; Zhao et al .2024]. For instance, GlyphDraw [Ma et al .2023]\nintroduces two diffusion branches: one for determining the text loca-\ntion and another for the actual text generation. While this approach\nsomewhat alleviates the issues associated with text rendering, it\nremains limited to single-line text generation. TextDiffuser series\n[Chen et al .2023a, 2024] seeks to address this limitation by using\nTransformers and Large Language Models (LLMs) to handle text\npositioning tasks, thereby extending capabilities to multi-line text\nrendering. AnyText [Tuo et al .2024, 2023] introduces the generation\nof multi-language text images. Brush Your Text [Zhang et al .2024]\nutilizes the canny map of a text template as a condition, achieving a\n\nCalligrapher: Freestyle Text Image Customization •3\nGenerateHuman-writtenInstructionAugmented prompt\nGenerated image\nThe  text is \"NGANGUR.\nGeneratedimageOCR(X1, Y1, X2, Y2)Bonding boxStep 1Step 2Step 3T2I\nSimplifyThe textis“Apple”.…The textis“Maila”.\n(X1, Y1, X2, Y2)DatasetAugmented promptStep 4\n🔥TuningmodelPrompt\nFig. 3. Self-distillation pipeline for style-oriented typography dataset construction and model training. We emulate natural language processing practices\nby leveraging pre-trained text-to-image generative models and large language models to synthesize stylized text images, paired with reference prompts and\nmasks. This generates self-supervised training pairs for robust style learning without manual annotation.\nhigher accuracy in multi-language rendering, though it falls short\nin terms of diversity. Overall, while existing efforts have focused on\nimproving the accuracy of text rendering, the challenge of render-\ning controllable text remains substantial. Overall, although existing\nwork has focused on the accuracy of text rendering, there is still a\nsignificant need to create more visually appealing and controllable\ntext. This is the main focus of our research.\nText attributes customization. Early work has focused on font\nattribute customization [Gal et al .2022; Hayashi et al .2019; He\net al.2024a,a,c, 2023; Kondo et al .2024; Wang et al .2020; Yang et al .\n2024]. For example, Attribute2Font [Wang et al .2020] can automati-\ncally generate font styles by synthesizing visually pleasing glyph\nimages based on user-specified attributes with corresponding val-\nues. However, compared to font stylizing, the task of customizing\nscene text attributes [He et al .2024b; Paliwal et al .2024; Su et al .\n2023; Tuo et al .2024] is more challenging due to complex factors\nsuch as perspective distortions and unique textures in scenes. For\ninstance, MetaDesigner [He et al .2024b] is a system that uses LLMs\nto facilitate the creation of customized artistic typography. It em-\nploys a multi-agent framework enhanced by a feedback loop from\nmultimodal models and user evaluations, to produce aesthetically\npleasing and contextually relevant WordArt that adapts to user pref-\nerences. AnyText2 [Tuo et al .2024] explicitly designs a font encoder\nand a color encoder, providing additional style-related guidance\nduring the rendering process. However, it is typically limited to gen-\nerating simple fonts and struggles in producing artistic typography\nnor following the given styles, and occasionally generates blurred\nresults. We believe that accommodating free-style fonts can make\nthe visuals more dynamic and engaging.\nImage style transfer. Image style transfer has been a longstanding\nresearch problem [Bai et al .2024; Chen et al .2017, 2021; Chung\net al.2024; Frenkel et al .2024; Gal et al .2022; Hertz et al .2024; Isola\net al.2017; Karras et al .2019, 2020; Kotovenko et al .2019; Kwon and\nYe 2022; Ouyang et al .2025; Patashnik et al .2021; Shah et al .2024;\nSohn et al .2023; Wang et al .2021, 2023; Zhang et al .2023a, 2022;\nZhu et al .2017]. Within this domain, various methods have been\nproposed to tackle the challenge of transferring the style from one\nimage to another. Pix2Pix [Isola et al .2017] models style transfer as\na low-level CNN prediction task, treating it as an image-to-image\ntranslation within a conditional GAN framework, where the model is\ntrained on paired images to directly predict the transformation from\ncontent to style at the pixel level. On the other hand, CycleGAN [Zhu\net al.2017] employs a cycle consistency loss to enable style transfer\nin scenarios with unpaired images, using a GAN-based loss thatencourages the model to generate images that are indistinguishable\nfrom real images in the target style domain. InstructPix2Pix [Brooks\net al.2023] is built on top of the Stable Diffusion [Rombach et al .\n2022a], enabling the usage of text prompt to conduct style transfer\ntrained on curated paired images. Our main focus is to transfer the\nstyle of text based on a reference image and additional guidance such\nas color palette. Generally, text areas are relatively small, and there\nare also some minor stroke details that require precise rendering,\nwhich makes this task particularly challenging.\n3 Methodology\nThe data generation and training pipeline of our method are shown\nin Fig. 3 and Fig. 4. Given the input image with mask, reference\nstyle image, and prompt, the purpose of our approach is to gener-\nate the text following the font style and customize it to the input\nsource image, even for reference fonts of uncommon styles (i.e.,\ncartoon, handwriting, and 3D style). In this section, we first discuss\nthe motivation in Section 3.1, followed by the three carefully de-\nsigned design components: The self-distillation learning strategy to\ncope with data scarcity is introduced in Section 3.2. Then, we de-\nscribe the localized style injection mechanism in Section 3.3. Finally,\nwe demonstrate the design of In-context inference for finer style\nconsistency in Section 3.4.\n3.1 Motivation\nIn this subsection, we identify several key limitations in current\nstate-of-the-art approaches for real-world typography design and\npresent corresponding motivations and solutions.\nScarcity of artistic typography data. A significant challenge\nin this domain is the limited availability of large-scale datasets\ndedicated to artistic typography. Our observations indicate that\ncurrent diffusion models [Black-Forest-Labs 2024a] are capable of\nsynthesizing high-quality stylized text when paired with robust\npost-processing and careful selection. We propose incorporating\nthe model to generate a synthesized artistic typography benchmark\nand employ a self-distillation training strategy that leverages a high-\nquality synthesized dataset to effectively transfer artistic styles, as\nelaborated in Section 3.2.\nFailure to capture subtle font details. Existing methods often rely\non global stylization techniques that are insufficient for capturing\nshapes and textures, focusing only on the task of self-reference\ninpainting. To address this limitation, we introduce a novel training\npipeline that emphasizes localized style injection. This pipeline\nconcentrates on fine-grained detail refinement, and yields a more\n\n4•\nencoderVisual...Masked imageQformer Style encoder\nDenoised latent\n🔥Image latent\nConcat\nVAE\n❄Text embedding Text is ”gic”\nDouble block\n❄\n❄Self attnQKV\n❄Style attnModulation\n❄MLP\nLoss computation\nFig. 4. Training framework of Calligrapher , demonstrating the integration of localized style injection and diffusion-based learning. The framework\nprocesses masked images through a Variational Auto-Encoder (VAE) to obtain latent representations, concatenated with mask and noise latents. A style\nencoder comprising a visual encoder, Qformer, and linear layers is designed to extract style-related features from the reference style image, while text\nembeddings (e.g., “ gic” in the case) modulate the denoising transformer. In the denoising block, style attention predicted from the style features replaces the\noriginal cross-attention, injecting style embeddings ( 𝐾E,𝑉E) with the denoiser’s query 𝑄to enable granular typographic control in the latent space. The\nmodel is optimized under the flow-matching learning objective with the self-distillation typography dataset.\nfaithful reproduction with the in-context generation techniques, as\noutlined in Section 3.3 and Section 3.4.\n3.2 Self distillation & stylized typography benchmark\nUnlike image translation tasks in ControlNet [Zhang et al .2023b],\nacquiring high-quality supervised training data for text style trans-\nfer remains challenging [Black-Forest-Labs 2024a,b; Ye et al .2023]\ndue to the prohibitive cost and effort required to manually curate\nlarge-scale datasets of text pairs, which exhibit identical semantic\ncontent but distinct stylistic attributes. Furthermore, such datasets\nrequire diverse and sufficiently rich stylistic variations to enable\nmodels to robustly capture nuanced style features and adapt to\ncomplex style transfer scenarios. With the finding that modern\ngenerative models [Black-Forest-Labs 2024a] could produce text\nimages with desirable quality, we draw inspiration from recent\nadvances in self-training paradigms within large language model\n(LLM) research [Huang et al .2022], where a robust generative model\nis adopted to yield data to train itself. As in Fig. 3, our proposed\nframework introduces a novel methodology where the pretrained\ngenerative model is employed to: (1) synthesize stylistically consis-\ntent training data through controlled generation, and (2) refine the\nstyle transfer model using the self-generated corpus. This approach\nestablishes a learning system that effectively leverages the internal\nknowledge representation of generative models while circumvent-\ning the dependency on human-annotated paired examples.\nSpecifically, as in Fig. 3, we first leverage large language models\n(LLMs) to generate a diverse set of semantic-coherent prompts 𝑝an-\nnotated with explicit typographic style descriptors (e.g., “3D metallictext, ” “watercolor calligraphy”). These style-conditioned prompts are\nsubsequently fed into the flow-matching diffusion model G𝜃[Black-\nForest-Labs 2024a], to synthesize high-fidelity stylized text images\nthrough iterative denoising processes. To construct training pairs\nfrom the synthesized corpus, we first adopt the neural text under-\nstanding method [AI 2023] to detect the text locations and employ\na strategic cropping mechanism that preserves typographic consis-\ntency while enabling effective self-supervision. For each generated\nimage, we randomly crop a local region containing stylized charac-\nters as the reference style exemplar, while maintaining the remain-\ning text region as the target for style transfer learning. Based on the\naforementioned data generation pipeline, we establish and propose\na style-centric text customization benchmark to benefit the devel-\nopment of the community. The details of this stylized typography\nbenchmark can be found in the data webpage.\nTo formalize the task, let xrepresent the main inputs of the text\ncustomization task that include the image latent, mask, and noise\nlatent, while ystands for the reference image. The proposed data\ngeneration strategy allows the model to efficiently learn to capture\nlocalized stylistic patterns and generate target text images from\nGaussian noise 𝜀∼N( 0,I), via the flow matching objective [Esser\net al. 2024]:\nmin\n𝜃Ex0∼𝑝(x0),𝜀∼N( 0,I)\u0002\n𝜆(𝑡)∥𝐷(x𝑡,𝑡,𝑝, y)−x0∥2\n2\u0003\n, (1)\nwhere𝐷(x𝑡,𝑡,𝑝, y)=x𝑡−𝑡·G𝜃(x𝑡,𝑡,𝑝, y)as in [Esser et al .2024],\n𝑡stands for the timestep, xtdenotes noisy inputs at 𝑡, and𝜆(𝑡)\nindicates the loss weighting.\n\nCalligrapher: Freestyle Text Image Customization •5\nSource image\nEugenia→InfatuateWillie →  GardenCAJERA →SISTER\nBaby→DollyPANDORA →ROBINSONRachelle →BEAUTYEdited result\nRef. 1Ref.2\nPLUSVALIA →  LOSANGELESJosie→DuckSource imageEdited resultSource imageEdited result\nInput(a)(b)(c)InputRef.1Ref.2\n(d)An art gallery wall with an painting next to a sign saying 'Art speaks the soul'A shop with a sign saying 'Calligrapher'A girl holding a sign saying ‘Hello World’\nFig. 5. Qualitative results of Calligrapher under various settings. We demonstrate text customization results respectively under settings of (a) self-\nreference, (b) cross-reference, and (c) non-text reference. Reference-based image generation results are also incorporated in (d).\n3.3 Localized style injection\nIn order to achieve text customization, we follow ControlNet [Zhang\net al.2023b] and IP-Adapter [Ye et al .2023] to learn another control-\nlable branch (namely the style encoder E) to encode the conditional\ncontrol signals while the original denoiser serves as the main branch,\nmaking the denoising formulation as follows:\nG(x𝑡,𝑡,𝑝, y)=F(D(x𝑡,𝑡,𝑝),E(y)), (2)\nwhere Findicates the fusion function for the features of the style\nencoderEand the main denoising network D. To extract initial\nfeatures from the reference, we instantiate the style encoder with a\npre-trained multi-modal visual encoder [Zhai et al .2023] and an-\nother encoder composed of linear layers and Qformer [Li et al .2023]\nwith learnable query parameters to transform these features into\nthe key and value matrices. The fusion function Fis instantiated as\nfeature replacement and cross-attention. The key and value matrices\npredicted from the style encoder are then injected into the mainbranchD, by replacing the original Key and Value matrices in the\nstyle attention module of the single block, as in Fig. 4:\nStyleAttention(𝑄D,𝐾E,𝑉E)=softmax \n𝑄D𝐾⊤\nE√︁\n𝑑𝐾!\n𝑉E,(3)\nwhere𝑑𝐾indicates the tensor dimension. These features from style\nattention would be added to the original attention activations for\nmodulation. We follow prior art [Rombach et al .2022b] to perform\ntraining and inference in the Variational Auto-Encoder (VAE) latent\nspace for efficiency.\n3.4 In-context generation\nMotivated by recent works [Huang et al .2024; Zhang et al .2025]\ndemonstrating the strong contextual capabilities of diffusion-based\n\n6•\nTable 1. Quantitative comparisons with SOTA baselines including TextDiffuser-2 [Chen et al .2024], AnyText [Tuo et al .2023] and FLUX-Fill [Black-\nForest-Labs 2024b]. Our method demonstrates the best or comparable performance across multiple metrics. The metrics for the best-performing method are\nhighlighted in bold.\nMethodMetrics User Study\nFID↓CLIP↑DINO↑OCR Acc↑Style Sync↑Text Matching↑Aesthetic↑Overall↑\nTextDiffuser-2 66.68 0.7097 0.8914 0.81 2.42 2.40 2.37 0.10\nAnyText 69.72 0.7041 0.8821 0.45 1.98 1.68 1.95 0.04\nFLUX-Fill 67.79 0.7090 0.8984 0.61 2.20 2.52 2.35 0.14\nOurs 38.09 0.7401 0.9474 0.84 3.40 3.40 3.32 0.72\ngenerative models [Black-Forest-Labs 2024a], we explore if reference-\nbased text customization enables to be improved by in-context infer-\nence. Specifically, our approach explicitly embeds contextual infor-\nmation - serving as a style reference - into the denoising trajectory\nby means of spatial concatenation at the pixel level. This composite\nimage is then encoded through the shared VAE to yield a unified and\ncontextualized latent representation. This latent feature, together\nwith a correspondingly constructed binary mask that zeroes out the\nregion occupied by the reference, is then sent to DiT to condition\nthe denoising of Gaussian noise. The resulting context-aware latent\nencapsulates both the semantic content to be edited and the stylistic\ncues from the reference, forming a holistic conditioning signal for\nthe subsequent diffusion process. As a result, this design enables\nfine-grained style coherence while preserving structural fidelity in\nthe generated text.\n4 Experiment\n4.1 Implementation Details\nIn our experiment, we adopt FLUX-Fill [Black-Forest-Labs 2024b]\nand FLUX [Black-Forest-Labs 2024a] as the base model for cus-\ntomization and generation. The visual encoder is based on siglip-\npatch14 [Zhai et al .2023] and Qformer [Liu et al .2023]. In the\ntraining phase, we freeze the FLUX model parameters to maintain\nits powerful generation ability. The localized style injection module\nis trained for 100,000 steps using 8 Tesla A800 GPUs, taking approx-\nimately 10 days. The AdamW optimizer is employed with a learning\nrate of 2×10−5and a batch size of 32. In the inference phase, we\nemploy the flow-matching Euler scheduler [Esser et al .2024] with\nsampling steps of 50 and a guidance scale of 30.0.\n4.2 Settings and Applications\nSelf-reference text image customization. One of the applica-\ntions of our method is to modify the text content in the input image\nfollowing the original text style. As shown in Fig. 5(a), our approach\nallows for the editing of text content while preserving the original\ntext style, achieved by simply modifying the relevant descriptions\nin the input text prompt. For example, given an input image, our ap-\nproach manages to inpaint the “ Eugenia ” to “ Infatuate ”, “Willie ”\nto “Garden ” ( Fig. 5(a)). The background of the input and output\nimages remains consistent. Considering previous works are only en-\nable to perform this mentioned task of self-reference customization\n(inpainting), we conduct quantitative and qualitative comparisons\nunder this setting in the latter sections. We also demonstrate the\nfurther unique capabilities of our model beyond this setting.\nOursSource imageFLUX-ﬁllTextDiffuser-2AnyTextThe text is ’OPTIMISTIC’.The text is ’Ninja’.The text is ’Magnify’.The text is ’Unicorn’.\nFig. 6. Qualitative comparisons on self-reference customization . Cal-\nligrapher achieves better performance in terms of style sync and quality.\nCross-reference text image customization. Cross-reference text\ncustomization aims to edit the text content using the reference with\ndifferent style, which has never been demonstrated in previous\nmethods [Chen et al .2023a; Tuo et al .2023]. In Fig. 5(b), we present\nvarious customization results given different styles of reference\ntext images. Our approach is capable of generating style-aligned\nimages while ensuring the controllability of the text. On the other\nhand, we empirically find that, the text customization model also\nworks well when non-text images serve as the reference, such as\nimages of fire, rainbows, and lightning. As in Fig. 5(c), our approach\ngenerates text that well aligns with these styles. The generated\nimage also maintains a high level of background consistency and\nachieves impressive aesthetic quality.\n\nCalligrapher: Freestyle Text Image Customization •7\nSource imageW/o self-distillW/ self-distill\nSource imageW/o in-contextW/ in-context\nSIMAZARIAS  →  CELEBRATION\nKamalbati →  Karaoke\nK.Eunice  →  BEACHMaita  →  Cloud\nFig. 7. Ablation studies on the self-distillation (left) and in-context generation (right) to validate their effectiveness.\nReference-based text image generation. Furthermore, we make\nefforts to achieve additional global controllable tasks of reference-\nbased text image generation, where the input xin Eq. (2) only\nincludes the noise latent. We find the style encoder trained based\non the original main branch (FLUX-fill [Black-Forest-Labs 2024b])\nworks with the new main branch of FLUX [Black-Forest-Labs 2024a],\nwhich could enable reference-based text image generation without\nfurther training and suggests the generalization of the learned model\nas in Fig. 5(d). This may be attributed to the parameter similarity\nbetween these two base models.\n4.3 Comparison with baselines\nQuantitative results. For quantitative evaluation, we compare\nour method with state-of-the-art methods on the test set of our\ntypography benchmark, which includes 100 text images with masks,\nprompts, and corresponding references. FID [Heusel et al .2017] is\nadopted to evaluate the general quality and similarity of the whole\nimages following prior arts. We also compute the style similarity of\nthe text images within masked regions respectively, with the CLIP\nViT-base [Radford et al .2021] and DINO-v2 [Oquab et al .2023]\nmodels. For the OCR metrics, we utilize the Google Cloud text de-\ntection API [Google-Cloud-API 2025] to recognize the content and\ncalculate the accuracy of generated text. Results shown in Table 1\ndemonstrate that the proposed method achieves the best in terms of\nall metrics. The user study, conducted with 30 participants yielding\nover 1000 votes, provides results including three sub-domain scores\n(on a scale of 1-4) and an overall preference percentage, which fur-\nther demonstrate that our approach achieves the best performance.\nQualitative results. Qualitative comparisons with TextDiffuser-\n2 [Chen et al .2024], AnyText [Tuo et al .2023], and FLUX-fill [Black-\nForest-Labs 2024b] are shown in Fig. 6, TextDiffuser-2 struggles in\nsynthesizing the correct characters and styles. AnyText also gen-\nerates text images in an undesirable style and low visual quality.\nIt occasionally generates incorrect characters such as “ Ninja ” and\n“Magnify ”. FLUX-fill [Black-Forest-Labs 2024b] demonstrates compe-\ntent lexical accuracy but suffers from stylistic inconsistency, whereas\nthe proposed method achieves substantial superiority in both di-\nmensions. Compared to existing methods, Calligrapherdemonstratessignificant advantages in terms of textual correctness and style con-\nsistency. A notable example is the distinctive pattern of the “ D” letters\nin the reference word “ SPELLBOUND ” where our method maintains\nsuperior glyph integrity and stylistic coherence during generation.\n4.4 Ablation studies\nEffectiveness of self-distillation. We evaluate the impact of self-\ndistillation on style similarity in text image customization. For com-\nparison, we show generated results from the model with and with-\nout the self-distillation training method. As shown in Fig. 7 (left),\nthe model with self-distillation achieves significantly higher style\nconsistency between generated images. This demonstrates that self-\ndistillation leverages the generative model’s internal knowledge\nto create stylistically coherent training pairs, circumventing the\nscarcity of manually curated paired data and enabling the model to\nrobustly learn and transfer nuanced style characteristics.\nEffectiveness of in-context generation. We also evaluate the\neffectiveness of the in-context strategy during the inference stage.\nAs shown in the right subfigure in Fig. 7, it is clear to observe\nthat the generated results achieves better style consistency the in-\ncontext strategy. We analyze that this is because the DiT structure\nincorporates self-attention, which is calculated on all tokens. The in-\ncontext strategy helps enhance the interaction of attention between\nreference text images and generated results.\n5 Conclusion\nAutomating typography customization is critical for advertising.\nThis work addresses labor-intensive manual font tuning by propos-\ning a diffusion-based framework for automated typography cus-\ntomization with style consistency. Our key contributions include a\nself-distillation dataset construction pipeline, local style injection\nvia trainable encoders, and in-context generation integrating refer-\nences. A style-centric benchmark is also constructed to facilitate text\ncustomization. Experiments show our model enables accurate style\nreplication for arbitrary text or non-text inputs of diverse styles.\nThis advances efficient, artistic typography design, reducing manual\neffort and enhancing workflow consistency in creative industries.\n\n8•\nReferences\nJaided AI. 2023. EasyOCR. https://github.com/JaidedAI/EasyOCR.\nQingyan Bai, Hao Ouyang, Yinghao Xu, Qiuyu Wang, Ceyuan Yang, Ka Leong Cheng,\nYujun Shen, and Qifeng Chen. 2024. Edicho: Consistent Image Editing in the Wild.\narXiv preprint arXiv:2412.21079 (2024).\nBlack-Forest-Labs. 2024a. FLUX. https://github.com/black-forest-labs/flux.\nBlack-Forest-Labs. 2024b. FLUX.1 Tools. https://blackforestlabs.ai/flux-1-tools/.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning\nto follow image editing instructions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition .\nDongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang Hua. 2017. Stylebank: An\nexplicit representation for neural image style transfer. In Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition .\nHaibo Chen, Zhizhong Wang, Huiming Zhang, Zhiwen Zuo, Ailin Li, Wei Xing, Dong-\nming Lu, et al .2021. Artistic style transfer with internal-external learning and\ncontrastive learning. Advances in Neural Information Processing Systems (2021).\nHaoxing Chen, Zhuoer Xu, Zhangxuan Gu, Yaohui Li, Changhua Meng, Huijia Zhu,\nWeiqiang Wang, et al .2023b. Diffute: Universal text editing diffusion model. Ad-\nvances in Neural Information Processing Systems (2023).\nJingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. 2023a.\nTextdiffuser: Diffusion models as text painters. Advances in Neural Information\nProcessing Systems (2023).\nJingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. 2024.\nTextdiffuser-2: Unleashing the power of language models for text rendering. In\nEuropean Conference on Computer Vision .\nSiXiang Chen, Jianyu Lai, Jialin Gao, Tian Ye, Haoyu Chen, Hengyu Shi, Shitong Shao,\nYunlong Lin, Song Fei, Zhaohu Xing, Yeying Jin, Junfeng Luo, Xiaoming Wei, and\nLei Zhu. 2025. PosterCraft: Rethinking High-Quality Aesthetic Poster Generation\nin a Unified Framework. arXiv preprint arXiv:2506.10741 (2025).\nJiwoo Chung, Sangeek Hyun, and Jae-Pil Heo. 2024. Style injection in diffusion: A\ntraining-free approach for adapting large-scale diffusion models for style transfer.\nInProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\nPatrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry\nSaini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al .2024. Scal-\ning rectified flow transformers for high-resolution image synthesis. In Forty-first\nInternational Conference on Machine Learning .\nYarden Frenkel, Yael Vinker, Ariel Shamir, and Daniel Cohen-Or. 2024. Implicit style-\ncontent separation using B-LoRA. In European Conference on Computer Vision .\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. 2022. Stylegan-nada: Clip-guided domain adaptation of image generators.\nACM Transactions on Graphics (TOG) (2022).\nGoogle-Cloud-API. 2025. Google Cloud text detection API. https://cloud.google.com/\nvision/docs/ocr.\nHideaki Hayashi, Kohtaro Abe, and Seiichi Uchida. 2019. GlyphGAN: Style-consistent\nfont generation based on generative adversarial networks. Knowledge-Based Systems\n(2019).\nHaibin He, Xinyuan Chen, Chaoyue Wang, Juhua Liu, Bo Du, Dacheng Tao, and Qiao Yu.\n2024a. Diff-font: Diffusion model for robust one-shot font generation. International\nJournal of Computer Vision (2024).\nJun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Qi He, Wangmeng Xiang,\nHanyuan Chen, Jin-Peng Lan, Xianhui Lin, Kang Zhu, et al .2024b. MetaDesigner:\nAdvancing artistic typography through AI-Driven, user-Centric, and multilingual\nwordArt synthesis. arXiv preprint arXiv:2406.19859 (2024).\nJun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu,\nXianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, et al .2024c. WordArt Designer\nAPI: User-Driven artistic typography synthesis with large language models on\nmodelScope. arXiv preprint arXiv:2401.01699 (2024).\nJun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Xianhui Lin,\nXiaoyang Kang, Zengke Jin, Yusen Hu, Bin Luo, et al .2023. WordArt designer: user-\ndriven artistic typography synthesis using large language models. arXiv preprint\narXiv:2310.18332 (2023).\nAmir Hertz, Andrey Voynov, Shlomi Fruchter, and Daniel Cohen-Or. 2024. Style aligned\nimage generation via shared attention. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition .\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp\nHochreiter. 2017. GANs trained by a two time-scale update rule converge to a local\nnash equilibrium. In Advances in Neural Information Processing Systems .\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu,\nand Jiawei Han. 2022. Large language models can self-improve. arXiv preprint\narXiv:2210.11610 (2022).\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. 2023.\nComposer: Creative and controllable image synthesis with composable conditions.\narXiv preprint arXiv:2302.09778 (2023).\nLianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang,\nYutong Feng, Yu Liu, and Jingren Zhou. 2024. In-Context LoRA for Diffusion\nTransformers. arXiv preprint arxiv:2410.23775 (2024).Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. 2017. Image-to-image\ntranslation with conditional adversarial networks. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition .\nJiabao Ji, Guanhua Zhang, Zhaowen Wang, Bairu Hou, Zhifei Zhang, Brian Price, and\nShiyu Chang. 2023. Improving diffusion models for scene text editing with dual\nencoders. arXiv preprint arXiv:2304.05568 (2023).\nBowen Jiang, Yuan Yuan, Xinyi Bai, Zhuoqun Hao, Alyson Yin, Yaojie Hu, Wenyu Liao,\nLyle Ungar, and Camillo J Taylor. 2025. ControlText: Unlocking controllable fonts in\nmultilingual text rendering without font annotations. arXiv preprint arXiv:2502.10999\n(2025).\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\nfor generative adversarial networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition .\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. 2020. Analyzing and improving the image quality of stylegan. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nTetta Kondo, Shumpei Takezaki, Daichi Haraguchi, and Seiichi Uchida. 2024. Font\nstyle interpolation with diffusion models. In International Conference on Document\nAnalysis and Recognition .\nMyungkyu Koo, Subin Kim, Sangkyung Kwak, Jaehyun Nam, Seojin Kim, and Jinwoo\nShin. 2025. FontAdapter: Instant Font Adaptation in Visual Text Generation. arXiv\npreprint arXiv:2506.05843 (2025).\nDmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, Sabine Lang, and Bjorn Ommer.\n2019. A content transformation block for image style transfer. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition .\nGihyun Kwon and Jong Chul Ye. 2022. Clipstyler: Image style transfer with a single\ntext condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition .\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\nInInternational Conference on Machine Learning .\nGongye Liu, Menghan Xia, Yong Zhang, Haoxin Chen, Jinbo Xing, Xintao Wang,\nYujiu Yang, and Ying Shan. 2023. StyleCrafter: Enhancing Stylized Text-to-Video\nGeneration with Style Adapter. arXiv preprint arXiv:2312.00330 (2023).\nRosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sha-\nran Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant.\n2022. Character-aware models improve visual text rendering. arXiv preprint\narXiv:2212.10562 (2022).\nZeyu Liu, Weicong Liang, Zhanhao Liang, Chong Luo, Ji Li, Gao Huang, and Yuhui Yuan.\n2024a. Glyph-byt5: A customized text encoder for accurate visual text rendering. In\nEuropean Conference on Computer Vision .\nZeyu Liu, Weicong Liang, Yiming Zhao, Bohan Chen, Lin Liang, Lijuan Wang, Ji Li,\nand Yuhui Yuan. 2024b. Glyph-byt5-v2: A strong aesthetic baseline for accurate\nmultilingual visual text rendering. arXiv preprint arXiv:2406.10208 (2024).\nJian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, and Zhenyu Yang. 2024.\nGlyphdraw2: Automatic generation of complex glyph posters with diffusion models\nand large language models. arXiv preprint arXiv:2407.02252 (2024).\nJian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong\nLin. 2023. Glyphdraw: Seamlessly rendering text with intricate spatial structures in\ntext-to-image generation. arXiv preprint arXiv:2303.17870 (2023).\nmdn web docs. 1996. https://developer.mozilla.org/en-US/docs/Web/CSS/font-family.\nhttps://developer.mozilla.org/en-US/docs/Web/CSS/font-family\nChong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and\nYing Shan. 2024. T2i-adapter: Learning adapters to dig out more controllable ability\nfor text-to-image diffusion models. In Proceedings of the AAAI conference on artificial\nintelligence , Vol. 38. 4296–4304.\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\net al.2023. Dinov2: Learning robust visual features without supervision. arXiv\npreprint arXiv:2304.07193 (2023).\nZiheng Ouyang, Zhen Li, and Qibin Hou. 2025. K-LoRA: Unlocking training-free fusion\nof any subject and style loras. arXiv preprint arXiv:2502.18461 (2025).\nShubham Paliwal, Arushi Jain, Monika Sharma, Vikram Jamwal, and Lovekesh Vig.\n2024. CustomText: Customized textual image generation using diffusion models.\narXiv preprint arXiv:2405.12531 (2024).\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski.\n2021. Styleclip: Text-driven manipulation of stylegan imagery. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision .\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al .2021.\nLearning transferable visual models from natural language supervision. In Interna-\ntional conference on machine learning .\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad-\nford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In\nInternational Conference on Machine Learning .\n\nCalligrapher: Freestyle Text Image Customization •9\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\n2022a. High-resolution image synthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\n2022b. High-resolution image synthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition .\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al.2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems (2022).\nViraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li,\nand Varun Jampani. 2024. Ziplora: Any subject in any style by effectively merging\nloras. In European Conference on Computer Vision .\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang,\nJarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al .2023. Styledrop: Text-to-\nimage generation in any style. arXiv preprint arXiv:2306.00983 (2023).\nTonghua Su, Fuxiang Yang, Xiang Zhou, Donglin Di, Zhongjie Wang, and Songze Li.\n2023. Scene style text editing. arXiv preprint arXiv:2304.10097 (2023).\nYuxiang Tuo, Yifeng Geng, and Liefeng Bo. 2024. AnyText2: Visual text generation and\nediting with customizable attributes. arXiv preprint arXiv:2411.15245 (2024).\nYuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. 2023. Any-\ntext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054\n(2023).\nAlex Jinpeng Wang, Dongxing Mao, Jiawei Zhang, Weiming Han, Zhuobai Dong, Linjie\nLi, Yiqi Lin, Zhengyuan Yang, Libo Qin, Fuwei Zhang, et al .2025. TextAtlas5M: A\nlarge-scale dataset for dense text image generation. arXiv preprint arXiv:2502.07870\n(2025).\nPei Wang, Yijun Li, and Nuno Vasconcelos. 2021. Rethinking and improving the\nrobustness of image style transfer. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition .\nYizhi Wang, Yue Gao, and Zhouhui Lian. 2020. Attribute2font: Creating fonts you want\nfrom attributes. ACM Transactions on Graphics (2020).\nZhizhong Wang, Lei Zhao, and Wei Xing. 2023. Stylediffusion: Controllable disentangled\nstyle transfer via diffusion models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision .\nZhenhua Yang, Dezhi Peng, Yuxin Kong, Yuyi Zhang, Cong Yao, and Lianwen Jin.\n2024. Fontdiffuser: One-shot font generation via denoising diffusion with multi-\nscale content aggregation and style contrastive learning. In Proceedings of the AAAI\nConference on Artificial Intelligence .\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-Adapter: Text com-\npatible image prompt adapter for text-to-image diffusion models. arXiv preprint\narxiv:2308.06721 (2023).\nXiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. 2023. Sigmoid\nLoss for Language Image Pre-Training. arXiv preprint arXiv:2303.15343 (2023).\nLingjun Zhang, Xinyuan Chen, Yaohui Wang, Yue Lu, and Yu Qiao. 2024. Brush your\ntext: Synthesize any scene text on images via diffusion model. In Proceedings of the\nAAAI Conference on Artificial Intelligence .\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023b. Adding conditional control\nto text-to-image diffusion models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision .\nYuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong,\nand Changsheng Xu. 2023a. Inversion-based style transfer with diffusion models. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition .\nYuxin Zhang, Fan Tang, Weiming Dong, Haibin Huang, Chongyang Ma, Tong-Yee Lee,\nand Changsheng Xu. 2022. Domain enhanced arbitrary image style transfer via\ncontrastive learning. In ACM SIGGRAPH 2022 Conference Proceedings .\nZechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. 2025. In-Context Edit:\nEnabling Instructional Image Editing with In-Context Generation in Large Scale\nDiffusion Transformer. arXiv preprint arxiv:2410.23775 (2025).\nYiming Zhao and Zhouhui Lian. 2023. Udifftext: A unified framework for high-quality\ntext synthesis in arbitrary images via character-aware diffusion models. arXiv\npreprint arXiv:2312.04884 (2023).\nZhen Zhao, Jingqun Tang, Binghong Wu, Chunhui Lin, Shu Wei, Hao Liu, Xin Tan,\nZhizhong Zhang, Can Huang, and Yuan Xie. 2024. Harmonizing visual text compre-\nhension and generation. arXiv preprint arXiv:2407.16364 (2024).\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired image-to-\nimage translation using cycle-consistent adversarial networks. In Proceedings of the\nIEEE International Conference on Computer Vision .\n\n10 •\nSource imageEdited resultSource imageEdited resultSource imageEdited result\nExotische Tiere →  Groundhog DayLoroxganjo →  TechnologyADRINALE →  AT FINALE\nBREAKER →  AMAZINGSATURDAY →  BIRTHDAYMAIJA →  Beach\nMORTA →  TRAINLUCIFER →LEGIONUNIQUE →  BRAVE\nVviana →  VanillaMARICARMEN →  MAGICQUILLAUSSIE POP! →  TAKE A WALK!\nWARRNAMBOOL →  LIGHTHOUSETELENAPOTA →  ADVENTUROUSTHANK →  LOVE\nBRAVETTI→ChocoHavenZONA MORTA →  DESTINATIONRELIQUIAE →  THE FOREST\nFig. 8. Self-reference text image customization results.\n\nCalligrapher: Freestyle Text Image Customization •11\nSource imageEdited resultSource imageEdited resultSource imageEdited result\nMaria→ActionRELIQUIAE →  THE FORESTPANDORA →ROBINSON\nEugenia→InfatuateDelia →  BalloonAzir →  Rose\nDARK NIGHT →  EDGERUNNERPLUSVALLA →  LOSANGELESMARICARMEN →MAGICQUILL\nMARIA →  ACTIONWARRNAMBOOL →LIGHTHOUSE\nThankyou! →Carpe Diem!Rachelle  → BEAUTY\nJORDAN  →  JORGEZONA MORTA  → DESTINATION\n(a)(b)\nFig. 9. Cross-style customization based on (a) text reference and (b) non-text reference images.\n\n",
    "source": "http://arxiv.org/abs/2506.24123v1",
    "authors": [
      "Yue Ma",
      "Qingyan Bai",
      "Hao Ouyang",
      "Ka Leong Cheng",
      "Qiuyu Wang",
      "Hongyu Liu",
      "Zichen Liu",
      "Haofan Wang",
      "Jingye Chen",
      "Yujun Shen",
      "Qifeng Chen"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "content"
  },
  {
    "id": "2506.24121v1_abstract",
    "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation",
    "content": "Title: TextMesh4D: High-Quality Text-to-4D Mesh Generation\n\nAbstract: Recent advancements in diffusion generative models significantly advanced\nimage, video, and 3D content creation from user-provided text prompts. However,\nthe challenging problem of dynamic 3D content generation (text-to-4D) with\ndiffusion guidance remains largely unexplored. In this paper, we introduce\nTextMesh4D, a novel framework for high-quality text-to-4D generation. Our\napproach leverages per-face Jacobians as a differentiable mesh representation\nand decomposes 4D generation into two stages: static object creation and\ndynamic motion synthesis. We further propose a flexibility-rigidity\nregularization term to stabilize Jacobian optimization under video diffusion\npriors, ensuring robust geometric performance. Experiments demonstrate that\nTextMesh4D achieves state-of-the-art results in terms of temporal consistency,\nstructural fidelity, and visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead-requiring only a single 24GB GPU-offering a\ncost-effective yet high-quality solution for text-driven 4D mesh generation.\nThe code will be released to facilitate future research in text-to-4D\ngeneration.",
    "source": "http://arxiv.org/abs/2506.24121v1",
    "authors": [
      "Sisi Dai",
      "Xinxin Su",
      "Boyan Wan",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24121v1_content",
    "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation",
    "content": "arXiv:2506.24121v1  [cs.CV]  30 Jun 2025TextMesh4D: High-Quality Text-to-4D Mesh Generation\nSisi Dai1Xinxin Su1Boyan Wan1Ruizhen Hu2Kai Xu1\n1National University of Defense Technology2Shenzhen University\nA knight in shining armor holding a sword and shield fighting.\nA dalmatian is running fast.\n A robotic arm at work.\nA burning pile of firewood with dancing flames.\nFigure 1. Given a text prompt, our method TextMesh4D can generate high-fidelity dynamic 3D mesh with the most preservation of\ngeometry and appearance among realistic and continuous motion.\nAbstract\nRecent advancements in diffusion generative models signif-\nicantly advanced image, video, and 3D content creation\nfrom user-provided text prompts. However, the challeng-\ning problem of dynamic 3D content generation (text-to-4D)\nwith diffusion guidance remains largely unexplored. In this\npaper, we introduce TextMesh4D, a novel framework for\nhigh-quality text-to-4D generation. Our approach lever-\nages per-face Jacobians as a differentiable mesh repre-\nsentation and decomposes 4D generation into two stages:\nstatic object creation and dynamic motion synthesis. We\nfurther propose a flexibility-rigidity regularization term tostabilize Jacobian optimization under video diffusion pri-\nors, ensuring robust geometric performance. Experiments\ndemonstrate that TextMesh4D achieves state-of-the-art re-\nsults in terms of temporal consistency, structural fidelity,\nand visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead—requiring only a single 24GB\nGPU—offering a cost-effective yet high-quality solution for\ntext-driven 4D mesh generation. The code will be released\nto facilitate future research in text-to-4D generation.\n1\n\n1. Introduction\n3D content generation has garnered significant attention\nwith the popularity of various applications such as virtual\nreality, augmented reality, gaming and robotics simulation,\netc. Recent advancements in text-to-image/video diffusion\nmodels [6, 17, 42, 47, 49, 50, 55], along with the pioneering\ntechnique of Score Distillation Sampling (SDS) [37], have\ndriven significant progress in text-to-3D generation. How-\never, this development has primarily focused on static rep-\nresentations, leaving dynamic 3D content generation, also\nknown as 4D generation, comparatively underexplored.\nText-to-4D generation poses significant challenges due\nto the conflict between the simplicity of input text prompts\nand the complexity of dynamic 3D outputs, which re-\nquires natural motion and maintaining temporal consis-\ntency in both appearance and geometry. Some approaches\n[3, 48, 63] address this by adopting dynamic neural radi-\nance fields (NeRF) [34] as the 4D representation, while oth-\ners [29] employ dynamic 3D Gaussian Splatting (3DGS)\n[24]. However, these representations lack sufficient sur-\nface or shape constraints and often favor view-specific fi-\ndelity over precise geometry, leading to geometric distor-\ntion and appearance inconsistencies in the generated mo-\ntion. In contrast, 3D triangle meshes offer explicit topolog-\nical constraints that ease geometry distortion and naturally\ndecouple shape from appearance during rendering, thereby\nenabling more consistent and higher-quality 4D generation.\nMoreover, compared to NeRF or Gaussian representations,\nmesh-based optimization requires significantly less GPU\nmemory. The generated results can be directly integrated\ninto standard CG pipelines, providing non-expert users with\na straightforward method for complex 4D mesh workflows.\nTo this end, we introduce TextMesh4D, a novel frame-\nwork for high-quality text-to-4D generation with mesh rep-\nresentations, using distilled priors from pre-trained diffu-\nsion models in a zero-shot manner. We tackle this com-\nplex task by splitting it into two stages: (1) generating a\nhigh-quality static 3D textured mesh, and (2) deforming\nthat mesh to produce high-quality motion. The challenge\nthus lies in identifying a mesh representation that effec-\ntively supports both stages. Our key insight is that per-face\nJacobians [2], which inherently exhibit low-frequency sig-\nnals, mitigate shape collapse—compared to vertex displace-\nments—and enable continuous, natural motion generation.\nBuilding on this observation, we develop a 4D parame-\nterization framework based on Jacobians in two parts: (1)\nstatic parameterization, which leverages Jacobians to pro-\nduce a high-quality 3D textured mesh, laying the foundation\nfor subsequent motion, and (2) dynamic parameterization,\nwhich incorporates local deformations through delta Jaco-\nbians in conjunction with global transformations. We first\noptimize the static parameters via Score Distillation Sam-\npling (SDS) using a carefully evaluated combination of im-age and 3D diffusion models, resulting in the high-quality\n3D textured mesh. Next, we optimize the dynamic param-\neters with SDS for video diffusion priors, yielding natural\nand consistent motion.\nMoreover, although Jacobians, as a smooth representa-\ntion, provide an elegant solution and continuous perfor-\nmance under video diffusion priors, their high degree of\nfreedom can make optimization challenging without direct\nsupervision. To address this, we introduce a tailored regu-\nlarization term that strikes a balance between rigidity and\nflexibility, thereby activating geometric performance under\nvideo score distillation sampling and ensuring robust out-\ncomes. Experiments demonstrate that our method achieves\ntext-driven 4D generation with state-of-the-art quality in\nterms of geometry, appearance, and motion.\nOur contributions can be summarized as follows:\n•TextMesh4D: we propose a novel text-to-4D genera-\ntion framework that employs Jacobians as a differentiable\nmesh representation, introducing our Jacobian-based 4D\nparameterization for generating a static 3D textured ob-\nject followed by vivid dynamic motion.\n•Flexibility-Rigidity Regularization: We introduce a tai-\nlored regularization term that balances flexibility and\nrigidity to fully exploit the Jacobian representation under\nvideo distillation sampling.\n•Superior Performance on Commodity Hardware: Our\nframework delivers state-of-the-art 4D generation re-\nsults—achieving high temporal consistency, structural\npreservation, and visual fidelity—while running on a sin-\ngle 24GB GPU with low memory overhead.\n2. Related Work\nText-to-Image/Video Generation. In recent years, dif-\nfusion models have achieved significant advancements in\nimage and video generation, including text-to-image (T2I)\nmodels [40, 42, 43], as well as text-to-video (T2V) mod-\nels [1, 9, 55]. These models are trained on large-scale\nopen-domain datasets, typically including LAION-5B [44],\nWebVid-10M [5], HD-VG-130M [57]. Recent advance-\nments in text-image-to-video generation (TI2V) have incor-\nporated image-based semantic conditions into T2V mod-\nels [12, 15, 58]. The latest model DynamiCrafter [60], em-\nploys a learnable image encoding network and dual cross-\nattention layers to effectively integrate text and image in-\nformation, achieving impressive open-domain TI2V gener-\nation. Our work distills the generative power of video diffu-\nsion models for motion generation, with the belief that our\nmethod will evolve accordingly with he continued advance-\nment of video generation technology.\nText-to-3D Generation. Early methods [10, 22, 30] for\ntext-to-3D generation require paired data of 3D data and\n2\n\nextraction\nRigid\ntransformationInput text: A goldfish swimmingPoisson \nsolve\nUV texture map𝑀𝑀𝑆𝑆(𝑉𝑉𝑆𝑆,𝐹𝐹)\n𝐽𝐽:JacobiansStatic Object Generation Dynamic Motion Generation\n𝑀𝑀0(𝑉𝑉0,𝐹𝐹)\n𝑀𝑀𝑖𝑖(𝑉𝑉i,𝐹𝐹)\nCombined Diffusion Guidance\n𝐿𝐿𝑆𝑆𝑆𝑆𝑆𝑆−3𝑆𝑆+ 𝐿𝐿𝑆𝑆𝑆𝑆𝑆𝑆−2𝑆𝑆Video  Diffusion Guidance\n𝐿𝐿𝑉𝑉𝑆𝑆𝑆𝑆Delta jacobians\nview\n𝑓𝑓(𝑝𝑝)\nc(𝑝𝑝)\n c(𝑣𝑣)\ntime\nFigure 2. Overview of our TextMesh4D. Given a text prompt, TextMesh4D aims to generate high-quality 4D mesh in line with the prompt.\nTo achieve this, TextMesh4D buids upon differentiable Jacobians for static and dynamic mesh representations. In the first static stage\n(Sec. 3.2), we generate a high-quality static 3D with Jacobians, initialized by NeuS with supervision provided by combined 2D and 3D\ndiffusion priors. In the second dynamic stage (Sec. 3.3), we generate the dynamic motion with delta Jacobians for local deformation and\nglobal transformation, with tailored flexibility-rigidity geometric regularizer.\ncorresponding textual descriptions to learn the joint embed-\nding space of shape and text for supervision, which lim-\nits their generality to unseen object categories. Benefiting\nfrom large pre-trained text-to-image models and differen-\ntiable rendering techniques, breakthroughs [18, 23, 26, 35]\nin text-to-3D content generation have been achieved. Re-\ncently, the technique SDS (Score Distillation Sampling)\nhas been introduced in the pioneering work DreamFusion\n[37], enabling 3D generation by distilling guidance from\npre-trained T2I diffusion models, achieving impressive re-\nsults and becoming popular. There are a lot of follow-up\nworks to improve DreamFusion. Some focus on 3D rep-\nresentation [11, 28]: Magic3D [28] proposes a coarse-to-\nfine pipeline to generate the fine-grained mesh; TextMesh\n[53] extends the geometry representation from NeRF to an\nSDF framework, enhancing detailed mesh extraction and\nphotorealistic rendering; DreamGauissian [51] proposes to\nadopt 3D Gaussian Splatting to increase efficiency. Some\nworks focus on improving SDS: SJC [54] proposes a variant\nof SDS while VSD are proposed in ProlificDreamer [59];\nDreamTime [19] improves the generation quality by modi-\nfying the timestep sampling strategy. Others focus on in-\nducing 3D prior into the guidance source, which effec-\ntively alleviates the Janus problem. Additional 3D prior\nare introduced in shape [8, 13, 20, 33, 36, 62], provid-\ning geometric initial values for optimizing NeRF. MV-\nDream [46] proposes to fine-tune the diffusion model to\ngenerate multi-view images and as so explicitly embeds 3D\ninformation into a 2D diffusion model. Moreover, workson image-based generation [32, 38, 52] and text-based edit-\ning [16, 27, 45, 64] are also boosted by utilizing these capa-\nblities. Our first static stage performs text-to-3D generation\nwith our Jacobian-based representation.\nText-to-4D Generation. Our research focuses on gener-\nating dynamic 4D content from textual descriptions. A pi-\noneering effort, MA V3D [48], combines a T2V diffusion\nmodel with dynamic Neural Radiance Fields (NeRF) and\nHexPlane [7] to optimize both scene appearance and mo-\ntion consistency. Building on this foundation, 4D-fy [3] em-\nploys a hybrid SDS approach that integrates T2I, 3D-aware\nT2I, and T2V diffusion models to achieve high-fidelity 4D\ngeneration. Align Your Gaussians (AYG)[29] employs dy-\nnamic 3D Gaussian splatting to reduce optimization time\nwhile enhancing temporal consistency. TC4D [4] intro-\nduces trajectory conditioning to maintain coherence be-\ntween global and local motion. Although these methods\nhave demonstrated effectiveness, they often rely heavily\non NeRF and video models, leading to substantial compu-\ntational costs. Comp4D [61] employs a Large Language\nModel (LLM) to segment input prompts into distinct en-\ntities, generating 4D objects independently and then com-\nbining them based on trajectory data provided by the LLM.\nOur work performs text-to-4D generation using a mesh rep-\nresentation, consisting of decomposed static and dynamic\nstages, which include both local deformation and global\ntransformation. Additionally, incorporating geometry and\ntexture disentanglement additionally.\n3\n\n3. Method\nOur goal is to generate 4D mesh from text prompts, us-\ning distilled priors from pre-trained diffusion models in a\nzero-shot manner. The input is a given text prompt de-\nscribing both the desired object and motion. The out-\nput is a sequence of textured 3D meshes, formulated as\nM={Mi= (Vi,F,T)}L\ni=1, where Videnotes the ver-\ntices of the i-th mesh, which vary across sequence to cap-\nture the motion. Frepresents the faces, Tindicates the UV\ntexture map, and Lis the length of the sequence.\nWe now first introduce our Jacobian-based parameteri-\nzation (Sec. 3.1), and then explain how the parameters are\noptimized from static (Sec. 3.2) to dynamic (Sec. 3.3).\n3.1. Jacobian-based 4D Parameterization\nJacobians. At each triangle fjof mesh M, the Jacobian\nJj∈R3×3is a linear transformation from the triangle’s\ntangent space to vertex space V ∈R3. Defining the defor-\nmation as vertex displacement ∆VviaΦ, a linear operator\n∇jis yield to associate each Φwith its corresponding Jaco-\nbian matrix ∇j(Φ). Thus, the Jacobian ∇j(Φ)restricts Φ\nwithin each triangle fj, inherently providing low-frequency,\nsmooth signals for deformation as vertex positions.\nGiven an target assignment of Jacobian Jj, a deforma-\ntion map Φ∗can be solved following Poisson equation in a\nleast-squares sense:\nΦ∗= min\nΦX\nfj∈F|fj|\r\r∇j(Φ)−Jj\r\r2\n2,\nwhere |fj|is the area of triangle fj. With deformation map\nΦ∗embedding the entire mesh, Φcan be optimized indi-\nrectly by optimizing the Jacobian matrices J={Jj}for\neach face. We then leverage a differentiable Poisson solver\nlayer [2] for our optimization.\n4D Parameterization. The total 4D parametrization con-\nsists of decomposed parts: 1) static parameters for a tex-\ntured 3D mesh, Ms={Vs,F,T }; 2) a sequence of dy-\nnamic parameters Θ ={θi={∆Vi,Ri}}L\ni=1, compris-\ning the desired motion including both deformation ∆Vi,\nand rigid transformation Riwith global translation and ro-\ntation, where Lis the length of the sequence. Therefore,\nthe output mesh sequence is M={Mi= (Ri(Vs+\n∆Vi),F,T)}L\ni=1.\nTo achieve high-quality generation, rather than basing on\ndirect vertex positions, we bulid the parametrization upon\nJacobians J={Jj}at each triangle as the mesh represen-\ntation, thereby facilitating smooth, continuous, and globally\nconsistent deformations. Thus, as illustrated in Fig. 2, our\nmethod consists of two stages: 1) First, we optimize for a\nhigh-quality static 3D model Ms, the parameters are sub-\nstituted by Jacobians as Ms={V0+ ∆V,F,T }={V0+\n1) Initialization 2) Optimized via Jacobians\nFigure 3. Comparison of geometry and texture between initializa-\ntion and subsequent jacobian-based generation.\nJ,F,T }, where V0andFare initialized parameters by\nSDF network (Sec. 3.2); 2) With the static parameters fixed,\ndynamic parameters Θ ={θi={∆Vi,Ri}}L\ni=1are also\nrepresented by Jacobians as Θ ={θi={∆Ji,Ri}}L\ni=1,\nthen to be optimized (Sec. 3.3).\n3.2. Static Object Generation\nMesh Initialization. Recall that at this stage, our objective\nis to generate a high-quality, textured 3D mesh solely from\nan input text prompt. We observe that learning large topo-\nlogical changes via direct mesh initialization (e.g., spot) is\nchallenging for arbitrary inputs and often results in unsat-\nisfactory mesh quality. To this end, we adopt NeuS [56]\nfor mesh initialization. NeuS [56] is a volume rendering\nmethod that integrates the advantages of signed distance\nfunctions (SDF) and Neural Radiance Fields (NeRF) [34],\nbetter for extracting a 3D geometry and obtaining a mesh\nas the initialization for further high-quality generation. We\ndenote the NeuS N={f(p), c(p)}, both f(p)andc(p)are\nnetworks implemented using MLPs, outputting the SDF and\ncolor at point p, respectively. The NeuS is then optimized\nwith supervision provided by combined diffusion priors un-\nder input text conditioning. After optimization, we extract\nthe surface at the zero-level set of SDF as the initial mesh\nM0using the marching cubes algorithm [31].\nOptimization via Jacobians. The extracted mesh M0\nconsisted by a set of vertices V0∈RN×3, faces F ∈\nRM×3, which is converted to the representation of differen-\ntiable Jacobians, {V0+J,F}, where the Jacobians are ini-\ntialized as identity matrices for optimization. We inherit the\nweights of the color network from the initialization phase\nand continue to refine them. However, unlike the initial-\nization phase, which utilizes random sampling points, the\nsampling in this phase is focused on regions near the ini-\ntialized surface. Thus we denote the color network at this\nphase as c(v). This concentrated sampling strategy allows\nfor a more precise refinement of the color generation.\nTo achieve the high-quality generation, we employ com-\nbined diffusion priors from both 3D-aware and 2D-image\ndiffusion models, following [3, 63]. the 3D-aware diffu-\nsion model, e.g. MVDream [46], is trained with multi-view\nembeddings along with camera parameters, providing a 3D\nprior and alleviating the Janus problem. As for the 2D-\n4\n\nimage diffusion priors, we incorporate an additional loss\nterm based on the variational score distillation (VSD) for\nappearance improvement. Combined SDS with them lever-\nages the complementary strengths of 3D-aware and 2D-\nimage diffusion models, resulting in a better generation for\nour static object:\nLstatic =λ3DLSDS−3D+λ2DLSDS−2D, (1)\nThe loss weights {λ3D, λ2D}are carefully tuned for better\ngeneration. They are set to {0.7,0.3}during the initializa-\ntion stage and adjusted to {0.5,0.5}subsequently. Please\nrefer to the supplementary material for loss details.\nTo sum up, during the initialization stage, we optimize\nthe networks {f(p), c(p)}. After initialization, with the ini-\ntialized M0, parameters that further need to be optimized\nare{J, c(v)}. Finally, the UV-space texture map Tis\nextracted from c(v)for the subsequent motion generation.\nFig. 3 illustrates the evolution from the initialization phase\nto the final generation at this stage.\n3.3. Dynamic Motion Generation\nWith the static parameters fixed, we then optimize the dy-\nnamic parameters Θ ={θi={∆Ji,Ri}}L\ni=1to produce\nthe vivid 3D motion. Note that a differentiable renderer is\nrequired to project the textured mesh sequence with Tto\nthe image space, thus enabling gradient steps during opti-\nmization at this stage. We implement the renderer based on\nNVdiffrast [25] as follows:\nR(·|C) :=Miθi,MS7→IMi∈RH×W, (2)\nwhere HandWdenote the height and width of the rendered\nimage, with Crepresenting the camera extrinsics.\nObjective Function. The overall objective function at this\nstage is:\nLdynamic =LVDS+Lflex+Lrig+Lother (3)\nFirst of all, we use video diffusion priors to provide se-\nmantic motion guidance by video score distillation sam-\npling (VDS). This procedure queries a video diffusion\nmodel, e.g. [1], to see how a rendered video from our repre-\nsentation aligns with input prompt, through the noise sam-\npling of video diffusion process. The gradients are then\nbackpropagated to the dynamic parameters. Please refer\nto the supplementary material for the corresponding loss\nLVDS computation.\nHowever, the stochastic nature of SDS and Jacobians’\nhigh degree of freedom introduce distortions and instable\nconvergence into the optimization. To address this issue,\nwe design the tailored regularization term, with a synergy\nof flexibility and rigidity, for Jacobians’ robust optimization\nunder guidance from video score distillation sampling.Lflexis a geometric regularization term on the optimized\nJacobians to prevent the divergence too far from the static\nobject’s geometry during dynamic optimization, inspired by\n[14]. This term ensures the global geometry is preserved\nwhile still allowing for flexible deformations that capture\nmotion semantics. Specifically, the term penalizes the dif-\nference between the dynamic Jacobians {J+ ∆Ji}and the\nstatic identity with a weight:\nLflex=ℓ−1X\ni=0|f|−1X\nj=0e∥ˆJj−I∥∥ˆJj−I∥2, (4)\nWe then further employ As-Rigid-As-Possible (ARAP) en-\nergy [21] as the rigidity regularization term Lrig:\nLrig=ℓ−1X\ni=0n−1X\nj=0X\nk∈Nvjwj,k||(vi\nj−vi\nk)−Rj(vs\nj−vs\nk)||2,\n(5)\nwhere Nvjrepresents the one-ring neighborhoods of vertex\nvj.wj,k= (cot αjk+ cot βjk)/2, measuring the impact of\nvkonvj.αjkandβjkare the angles on the faces adjacent\nto the edge ( vj,vk), which are opposite the edge itself. Rj\nrepresents the optimal rotation estimated by Singular Value\nDecomposition (SVD) [21]. This term encourages the gen-\neration to maintain locally rigid during the deformation.\nThe rigidity-flexibility integrated regularizer strikes a\nbalance between preserving rigid motion and maintaining\ndeformation fidelity, thereby enhancing the optimization re-\nsults, as demonstrated in our ablation studies. We leave ad-\nditional details of the other regularizers, e.g. smoothness\nterm and Jacobian’s dof regularization, in the supplemen-\ntary material.\n4. Experiments\nIn this section, we first compare our method with several\nstate-of-the-art baselines and then conduct extensive abla-\ntion studies to verify the design choices of our method.\n4.1. Results and Comparisons\nBaselines. We compare with the publicly available 4D-\nfy [3] in text-to-4D generation. Additionally, since the\nsource code for AYG [29] is not available, we further de-\nsigned a variant of DreamGaussian4D [41], by inputting\nthe text prompt to Kling to generate a high-quality image,\nwhich then serves as the input for DreamGaussian4D.\nEvaluation metrics. It is a common difficulty to quan-\ntitatively evaluate text-to-4D generation results due to the\nlack of ground truth. We thus adopt the evaluation metrics\nfollowing [3]: CLIP score and user study. The CLIP score\nassesses how well the generated output aligns with the input\ntext prompt by computing the cosine similarity between the\n5\n\nAbutterfly fluttering wings in the air A snake gracefully slithering\nA cute fluffy dog jumping\nA cow running fast\nA monkey riding a bikeA fox dressed in a suit dancing\nAn emoji of a baby panda reading a bookA corgi is running fastFigure 4. Text-to-4D generation results of our method, demonstrating high quality, consistency and realistic motion. Dynamic motions are\npresented in the supplementary video.6\n\nAn astronaut riding a horse4D-fy Ours\n DreamGaussian4DAn octopus is underwater\nFigure 5. We compare our method with 4D-fy and the variation of DreamGaussian4D. Our method achieves significantly higher quality in\nboth texture and geometry, while maintaining consistency during motion.\nTable 1. Quantitative comparisons with 4d-fy and variations of\nDreamGaussian4D. The methods are evaluated in terms of CLIP\nScore (CLIP) and metrics of user study, where the numbers are the\npercentages of users who voted for the corresponding method.\nUser Study\nMethod CLIP AQ SQ MQ TA Overall\n4D-fy 31.03 6.9% 4.4% 1.0% 13.4% 4.7%\nDreamGaussian4D 29.17 0.3% 0.5% 0.8% 3.1% 1.8%\nOurs 32.32 92.7% 95.1% 98.2% 83.5% 93.5%\ntextual CLIP embedding and visual CLIP embedding [39].\nMultiple camera views and frames over time are sampled\nduring the computation of the CLIP score. Moreover, we\nconduct a user preference study to evaluate sample quality\nalong the dimensions of appearance quality (AQ) ,3D struc-\nture quality (SQ) ,motion quality (MQ) ,text alignment (TA) ,\nandoverall preference (Overall) , as in 4D-fy [3]. There are\n31 participants involved to rate the 20 prompts’ results from\nour method as well as from the baselines. Please refer to the\nsupplementary document for more details.Comparison results. As shown in Fig. 5, our method\nsignificantly outperforms the baselines in terms of motion\nvividness, geometric preservation, and appearance consis-\ntency. The implicit representation of baselines, such as\nNeRF in 4D-fy and 3DGS in DreamGaussian4D, results\nin most of the generated motion manifesting as floating ar-\ntifacts in the geometric field, making it visually challeng-\ning to interpret. Furthermore, since these baselines do not\nmodel global transformations, the generated motion is con-\nfined to local regions of the object, further limiting their\nperformance. In contrast, our method leverages a mesh\nrepresentation that incorporates both local deformation and\nglobal transformation, enabling successful spatial move-\nment with only an input text. Additionally, the mesh rep-\nresentation allows our method to run efficiently on a 24GB\nGPU, achieving motion convergence within 1.5 hours. In\ncomparison, 4D-fy requires an 80GB GPU and over 15\nhours to complete, while DreamGaussian4D needs a 48GB\nGPU. The quantitative results presented in Table 1 further\ndemonstrate our superiority. The user study indicates that\nusers generally favor our method when directly comparing\nwith the results obtained by the baselines, with the number\nindicating the percentage of participants choosing the corre-\n7\n\nVertex + 𝐿𝐿𝑟𝑟𝑟𝑟𝑟𝑟 Jacobian +𝐿𝐿𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓 Jacobian +𝐿𝐿𝑟𝑟𝑟𝑟𝑟𝑟+𝐿𝐿𝑓𝑓𝑓𝑓𝑓𝑓𝑓𝑓 Jacobian + 𝐿𝐿𝑟𝑟𝑟𝑟𝑟𝑟 Vertex + 1000𝐿𝐿𝑟𝑟𝑟𝑟𝑟𝑟Figure 6. Qualitative ablations with the given text prompt “a flag\nfluttering in the air”. The top two rows showcase two frames,\nwhile the bottom row illustrates the geometry corresponding to\nthe second row.\nsponding method. More of our text-to-4D generation results\nare shown in Fig. 4. Please refer to the supplementary video\nfor a better viewing experience.\n4.2. Ablation Study\nWe conduct ablation studies to verify the necessity of de-\nsigned choices. Qualitative comparisons are shown in Fig. 6\nand quantitative results are reported in Tab. 2.\nVertex displacements. To verify the importance of the\nper-face Jacobians, we substituted them with vertex dis-\nplacement optimization, which is the most straightforward\napproach for mesh optimization. Under the same VDS\nguidance, in addition to ARAP, we introduced regulariza-\ntion constraints, including normal consistency and Lapla-\ncian regularization.\nWith a light ARAP energy weight (the same as ours,\nshown in the first column of Fig. 6), vertex displacements\nalign motion with the text semantics but lead to a large num-\nber of self-intersections, resulting in a loss of shape fidelity.\nWith a heavier ARAP energy weight (shown in the second\ncolumn of Fig. 6), although the shape is preserved, there is\nstill some shape collapse, and no motion is generated.\nThe results under vertex displacement representation ex-\nhibit a granular effect, similar to the results obtained using\nNeRF [3] or Gaussian-based methods [41]. These methods\nfocus on optimizing spatial points, and due to the random-\nness of VSD score distillation sampling method, discrete\nspatial point representations are unstable under VDS guid-\nance, leading to difficulties in convergence and introduc-\ning instability and artifacts in the final results. In contrast,\nwith the same ARAP weight (shown in the third column of\nFig. 6), the continuous per-face Jacobian representation en-\nsures smooth optimization by mitigating the noisy gradients\nproduced by VSD, thus demonstrating its superiority.\nFlexibility regularizer. To evaluate the effectiveness ofTable 2. Ablation Study. The choices are also evaluated in terms\nof metrics of user study, where the numbers are the percentages of\nusers who voted for the corresponding method.\nUser Study\nSettings AQ SQ MQ TA Overall\nVertex+ Lrig 1.9% 1.0% 1.5% 1.8% 1.1%\nVertex+1000 Lrig 1.1% 7.3% 0.5% 1.3% 3.9%\nJacobian+ Lrig 15.2% 9.0% 3.4% 5.6% 7.4%\nJacobian+ Lflex 5.6% 6.9% 39.7% 26.3% 10.3%\nOurs 76.2% 75.8% 55.0% 65.0% 77.3%\nour designed geometry regularizers, we perform dynamic\nmotion optimization by omitting the flexibility regulariza-\ntion term and leaving only the rigidity term. As shown in\nthe third column of Fig. 6, the motion optimization becomes\ntrapped in a local optimum due to the sole rigidity con-\nstraint. For example, to simulate fluttering in the air, the\nflagpole undergoes bending, causing the flag to appear to\nmove in an unnatural way.\nRigidity regularizer. We also perform dynamic motion\noptimization by omitting the rigidity regularization terms,\nleaving only the flexibility term. As shown in the fourth\ncolumn of Fig. 6, although the motion is most extensive, the\nshape appears to be stretched (similar to clay), highlighting\nthe necessity of the local rigidity loss. With our integrated\nflexibility-rigidity regularizer, the realistic motion of “a flag\nfluttering in the air” is achieved.\n5. Conclusion\nWe propose TextMesh4D, a novel framework for high-\nfidelity 4D mesh generation from a text prompt. By building\nour 4D differentiable mesh representation based on per-face\nJacobians, our method first generates a high-quality static\n3D mesh, and then learns dynamic motion, including both\nlocal deformation and global transformation, through super-\nvision provided by video diffusion priors. Furthermore, we\nintroduce flexibility and rigidity regularizers to stabilize Ja-\ncobian optimization under video diffusion priors, ensuring\nrobust geometric performance. TextMesh4D achieves supe-\nrior results in text-to-4D generation, with realistic motion,\ngeometry preservation, and appearance consistency.\nLimitations & Future Work. As our framework is de-\nsigned to first generate static content and then dynamic mo-\ntion, the dynamic generation may fail if the static generation\nis unsatisfactory, leading to incorrect accumulation. We be-\nlieve this issue could be addressed with further advances\nin 2D and 3D diffusion models. Moreover, the optimiza-\ntion space for global transformations in our method is still\nconstrained by the camera space of differentiable rendering.\nWe believe that combining our method with the latest video\ndiffusion models under camera control offers a promising\ndirection for future work.\n8\n\nReferences\n[1] Zeroscope text-to-video model. https : / /\nhuggingface . co / cerspense / zeroscope _\nv2_576w . Accessed: 2023-10-31. 2, 5\n[2] Noam Aigerman, Kunal Gupta, Vladimir G Kim, Siddhartha\nChaudhuri, Jun Saito, and Thibault Groueix. Neural jaco-\nbian fields: Learning intrinsic mappings of arbitrary meshes.\narXiv preprint arXiv:2205.02904 , 2022. 2, 4\n[3] Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon\nWetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov,\nJeong Joon Park, Andrea Tagliasacchi, and David B Lin-\ndell. 4d-fy: Text-to-4d generation using hybrid score dis-\ntillation sampling. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n7996–8006, 2024. 2, 3, 4, 5, 7, 8\n[4] Sherwin Bahmani, Xian Liu, Wang Yifan, Ivan Sko-\nrokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon\nPark, Sergey Tulyakov, Gordon Wetzstein, et al. Tc4d:\nTrajectory-conditioned text-to-4d generation. In European\nConference on Computer Vision , pages 53–72. Springer,\n2025. 3\n[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proc. ICCV , 2021. 2\n[6] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2\n[7] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 130–141, 2023. 3\n[8] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. Dreamavatar: Text-and-shape guided 3d hu-\nman avatar generation via diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 958–968, 2024. 3\n[9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al. Videocrafter1: Open\ndiffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512 , 2023. 2\n[10] Kevin Chen, Christopher B Choy, Manolis Savva, An-\ngel X Chang, Thomas Funkhouser, and Silvio Savarese.\nText2shape: Generating shapes from natural language by\nlearning joint embeddings. In Computer Vision–ACCV 2018:\n14th Asian Conference on Computer Vision, Perth, Australia,\nDecember 2–6, 2018, Revised Selected Papers, Part III 14 ,\npages 100–116. Springer, 2019. 2\n[11] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. In Proceedings of the\nIEEE/CVF international conference on computer vision ,\npages 22246–22256, 2023. 3\n[12] I2VGen-XL contributors. I2vgen-xl. Accessed October 15,2023 [Online] https://modelscope.cn/models/\ndamo/Image-to-Video/summary . 2\n[13] Sisi Dai, Wenhao Li, Haowen Sun, Haibin Huang,\nChongyang Ma, Hui Huang, Kai Xu, and Ruizhen Hu. In-\nterfusion: Text-driven generation of 3d human-object inter-\naction. arXiv preprint arXiv:2403.15612 , 2024. 3\n[14] William Gao, Noam Aigerman, Thibault Groueix, V ova\nKim, and Rana Hanocka. Textdeformer: Geometry manipu-\nlation using text guidance. In ACM SIGGRAPH 2023 Con-\nference Proceedings , pages 1–11, 2023. 5\n[15] Xianfan Gu, Chuan Wen, Weirui Ye, Jiaming Song, and Yang\nGao. Seer: Language instructed video prediction with latent\ndiffusion models. arXiv preprint arXiv:2303.14897 , 2023. 2\n[16] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 19740–19750, 2023. 3\n[17] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels.arXiv preprint arXiv:2210.02303 , 2022. 2\n[18] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535 , 2022. 3\n[19] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422 , 2023. 3\n[20] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. Dreamwaltz:\nMake a scene with complex 3d animatable avatars. Advances\nin Neural Information Processing Systems , 36, 2024. 3\n[21] Takeo Igarashi, Tomer Moscovich, and John F Hughes. As-\nrigid-as-possible shape manipulation. ACM transactions on\nGraphics (TOG) , 24(3):1134–1141, 2005. 5\n[22] Tansin Jahan, Yanran Guan, and Oliver Van Kaick.\nSemantics-guided latent space exploration for shape gener-\nation. In Computer Graphics Forum , pages 115–126. Wiley\nOnline Library, 2021. 2\n[23] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n867–876, 2022. 3\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk ¨uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Trans. Graph. , 42(4):139–1,\n2023. 2\n[25] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila. Modular primitives for\nhigh-performance differentiable rendering. ACM Transac-\ntions on Graphics (ToG) , 39(6):1–14, 2020. 5\n[26] Han-Hung Lee and Angel X Chang. Understanding pure\nclip guidance for voxel grid nerf models. arXiv preprint\narXiv:2209.15172 , 2022. 3\n9\n\n[27] Yuhan Li, Yishun Dou, Yue Shi, Yu Lei, Xuanhong Chen, Yi\nZhang, Peng Zhou, and Bingbing Ni. Focaldreamer: Text-\ndriven 3d editing via focal-fusion assembly. In Proceed-\nings of the AAAI Conference on Artificial Intelligence , pages\n3279–3287, 2024. 3\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-\nresolution text-to-3d content creation. arXiv preprint\narXiv:2211.10440 , 2022. 3\n[29] Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fi-\ndler, and Karsten Kreis. Align your gaussians: Text-to-4d\nwith dynamic 3d gaussians and composed diffusion models.\nInProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 8576–8588, 2024. 2,\n3, 5\n[30] Zhengzhe Liu, Yi Wang, Xiaojuan Qi, and Chi-Wing Fu. To-\nwards implicit text-guided 3d shape generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition , pages 17896–17906, 2022. 2\n[31] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. ACM\nsiggraph computer graphics , 21(4):163–169, 1987. 4\n[32] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\npages 8446–8455, 2023. 3\n[33] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. arXiv preprint arXiv:2211.07600 ,\n2022. 3\n[34] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM , 65(1):99–106, 2021. 2,\n4\n[35] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 Conference Papers , pages 1–8, 2022. 3\n[36] Ryan Po and Gordon Wetzstein. Compositional 3d scene\ngeneration using locally conditioned diffusion. In 2024 In-\nternational Conference on 3D Vision (3DV) , pages 651–663.\nIEEE, 2024. 3\n[37] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv\npreprint arXiv:2209.14988 , 2022. 2, 3\n[38] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843 ,\n2023. 3\n[39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learningtransferable visual models from natural language supervi-\nsion. In International conference on machine learning , pages\n8748–8763. PMLR, 2021. 7\n[40] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea V oss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International confer-\nence on machine learning , pages 8821–8831. Pmlr, 2021. 2\n[41] Jiawei Ren, Liang Pan, Jiaxiang Tang, Chi Zhang, Ang Cao,\nGang Zeng, and Ziwei Liu. Dreamgaussian4d: Genera-\ntive 4d gaussian splatting. arXiv preprint arXiv:2312.17142 ,\n2023. 5, 8\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bjorn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In Proc. CVPR , 2022. 2\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in neural information\nprocessing systems , 35:36479–36494, 2022. 2\n[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Proc. NeurIPS , 2022. 2\n[45] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. V ox-e: Text-guided voxel editing of 3d ob-\njects. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision , pages 430–440, 2023. 3\n[46] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512 , 2023. 3, 4\n[47] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792 ,\n2022. 2\n[48] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual,\nIurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea\nVedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dy-\nnamic scene generation. arXiv preprint arXiv:2301.11280 ,\n2023. 2, 3\n[49] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International confer-\nence on machine learning , pages 2256–2265. PMLR, 2015.\n2\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 , 2020. 2\n[51] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653 ,\n2023. 3\n[52] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity 3d\ncreation from a single image with diffusion prior. In Pro-\n10\n\nceedings of the IEEE/CVF international conference on com-\nputer vision , pages 22819–22829, 2023. 3\n[53] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts. In 2024\nInternational Conference on 3D Vision (3DV) , pages 1554–\n1563. IEEE, 2024. 3\n[54] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 12619–12629, 2023. 3\n[55] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571 , 2023. 2\n[56] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\narXiv preprint arXiv:2106.10689 , 2021. 4\n[57] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv preprint arXiv:2305.10874 , 2023. 2\n[58] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Videocomposer: Compositional video\nsynthesis with motion controllability. arXiv preprint\narXiv:2306.02018 , 2023. 2\n[59] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. Advances in Neural Information Processing Systems ,\n36, 2024. 3\n[60] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen,\nWangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying\nShan, and Tien-Tsin Wong. Dynamicrafter: Animating\nopen-domain images with video diffusion priors. In Eu-\nropean Conference on Computer Vision , pages 399–417.\nSpringer, 2025. 2\n[61] Dejia Xu, Hanwen Liang, Neel P Bhatt, Hezhen Hu, Hanxue\nLiang, Konstantinos N Plataniotis, and Zhangyang Wang.\nComp4d: Llm-guided compositional 4d scene generation.\narXiv preprint arXiv:2403.16993 , 2024. 3\n[62] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying\nShan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot\ntext-to-3d synthesis using 3d shape prior and text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n20908–20918, 2023. 3\n[63] Yufeng Zheng, Xueting Li, Koki Nagano, Sifei Liu, Otmar\nHilliges, and Shalini De Mello. A unified approach for text-\nand image-guided 4d scene generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 7300–7309, 2024. 2, 4\n[64] Jingyu Zhuang, Chen Wang, Liang Lin, Lingjie Liu, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. In SIGGRAPH Asia 2023 Conference Papers ,\npages 1–10, 2023. 3\n11\n\n",
    "source": "http://arxiv.org/abs/2506.24121v1",
    "authors": [
      "Sisi Dai",
      "Xinxin Su",
      "Boyan Wan",
      "Ruizhen Hu",
      "Kai Xu"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "content"
  },
  {
    "id": "2506.24120v1_abstract",
    "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime",
    "content": "Title: Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime\n\nAbstract: Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.",
    "source": "http://arxiv.org/abs/2506.24120v1",
    "authors": [
      "Yuqing Wang",
      "Shangding Gu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24120v1_content",
    "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime",
    "content": "arXiv:2506.24120v1  [cs.LG]  30 Jun 2025Data Uniformity Improves Training Efficiency and\nMore, with a Convergence Framework Beyond the\nNTK Regime\nYuqing Wang\nDepartment of Applied Mathematics and Statistics\nJohns Hopkins University\nywan1050@jh.eduShangding Gu\nDepartment of EECS\nUC Berkeley\nshangding.gu@berkeley.edu\nABSTRACT\nData selection plays a crucial role in data-driven decision-making, including in\nlarge language models (LLMs), and is typically task-dependent. Properties such\nas data quality and diversity have been extensively studied and are known to en-\nhance model performance. However, it remains unclear whether there exist other\nquantitative and general principles of data selection that can consistently improve\nperformance, especially for complex tasks with limited prior knowledge. In this\npaper, we demonstrate that selecting more uniformly distributed data can improve\ntraining efficiency while enhancing performance. Specifically, we establish that\nmore uniform (less biased) distribution leads to a larger minimum pairwise dis-\ntance between data points, denoted by hmin, and prove that a smaller hmincan\nslow down the training dynamics of gradient descent (GD). Moreover, we theo-\nretically show that the approximation error of neural networks decreases as hmin\nincreases. Our analysis introduces a convergence framework for GD beyond the\nNeural Tangent Kernel (NTK) regime, applicable to a broad class of architectures,\nincluding transformers, without requiring Lipschitz smoothness. This framework\nfurther provides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct com-\nprehensive experiments for supervised fine-tuning across various settings, includ-\ning different optimization strategies, model sizes, and training datasets. The re-\nsults consistently demonstrate that selecting data by maximizing pairwise distance\nsignificantly accelerates training and achieves comparable or better performance\nin LLMs across diverse datasets. Code and Datasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity .\n0.31 0.6\nLoss Threshold Limit0100200300400500600Time to Reach (minutes)148.2\n46.6281.2\n139.6561.6\n280.010k Uniform\n10k Random\n20k Full\n(a) Training Time Comparison.\nOverall ARC Challenge TruthfulQA MC0102030405060Accuracy (%)39.3853.07\n25.7043.2056.91\n29.5043.0955.46\n30.7243.6356.66\n30.60Base Model\n10k Uniform\n10k Random\n20k Full (b) Performance Comparison.\nFigure 1: Comparison of LLaMA-1-13B training efficiency and evaluation performance across dif-\nferent data selection strategies. (a) Time to reach loss thresholds (0.31 and 0.6), measured using a\n3-step moving average and reported based on wall-clock time using a single A100 GPU. The y-axis\nshows the time (in minutes) required to reach each target loss for the first time. Uniform selection\nconsistently leads to faster convergence, indicating improved training efficiency. (b) Evaluation\nperformance on ARC Challenge and TruthfulQA MC , showing that the 10k Uniform subset out-\nperforms the 10k Random subset and matches the performance of the full 20k dataset.\n1\n\n1 I NTRODUCTION\nData selection is fundamental to a lot of applications including large language models (LLMs) (Al-\nbalak et al., 2024; Zhao et al., 2023; Chang et al., 2024), such as TeaMs-RL (Gu et al., 2024) and\nWizardLM (Xu et al., 2024). One main difficulty of data selection is that it is often tailored to spe-\ncific tasks. Therefore, it is crucial to establish general and applicable rules to guide the data selection\nprocess. Existing rules include improving data quality and data diversity (Muennighoff et al., 2025;\nAlbalak et al., 2024; Gu et al., 2024), which have been widely studied and are recognized for their\npositive impact on model performance. However, data selection is still a challenging problem that\ncalls for more quantitative and broadly applicable principles.\nRegarding the role of data, existing results from approximation theory suggest that uniform sampling\nexhibits near-optimal behavior under many circumstances. Krieg et al. (2022) showed that i.i.d. ran-\ndom sampling achieves optimal convergence rates, and Krieg & Sonnleitner (2024) further proved\nthat uniform sampling is asymptotically as effective as optimal deterministic schemes for approx-\nimating Sobolev functions. In the context of learning, Dong et al. (2024) further demonstrated in\nProposition 2.1 that uniform sampling yields near-optimal generalization in low-dimensional linear\nprobing scenarios. These results collectively highlight that uniform sampling, despite its simplicity,\nis efficient for function approximation and learning.\nMotivated by the above theoretical results, we wonder whether selecting data to be more uniformly\ndistributed can benefit training and lead to improved generalization performance, particularly in the\nabsence of prior information for complicated tasks.\nIn this paper, we unveil that selecting more uniform data would accelerate training and and achieve\nperformance comparable to using a larger dataset. More precisely, we characterize the effect of\ndata uniformity through the minimum distance between input data points, denoted by hmin, both\ntheoretically and experimentally:\n• We prove the relationship between the uniformity of data and the minimum distance hmin(Sec-\ntion 4.1): more uniform (less biased) distribution leads to larger hmin.\n• We develop a general framework of convergence analysis under gradient descent (GD) beyond\nthe neural tangent kernel (NTK) regime (Remark 1), which works for a family of complicated\narchitectures including feedforward layers and attention layers, and including residual connec-\ntion, normalization, softmax, tanh, GELU, SiLU, etc. (Section 4.2). This framework is based on\npolynomial generalized smoothness (Definition 5) and local relaxed dissipativity (Definition 6),\ncombined with ideas in measure theory and differential topology. It is also a useful tool for non-\nconvex optimization without Lipschitz smoothness.\n• Based on this convergence analysis, we show that as hmindecreases, the training potentially slows\ndown (Corollary 3).\n• We also show theoretical insights on the benefits of residual connections and function composi-\ntions in neural networks (Remark 2).\n• We prove that larger hminand smaller local maximum distance hmaxd+1lead to smoother ap-\nproximation and smaller approximation error between the neural network and ground truth (Sec-\ntion 4.3). This is achieved by establishing a data-dependent Bramble-Hilbert lemma, combined\nwith ideas in computational geometry.\n• We experimentally demonstrate the benefits of selecting more uniformly distributed data by max-\nimizing pairwise distances between data points in supervised fine-tuning. This uniformity-driven\nsampling strategy consistently leads to faster convergence and comparable or improved perfor-\nmance across both ℓ2-SGD and cross-entropy loss with Adam optimization (e.g., as shown in\nFigure 1). Our results generalize across multiple data sources, dataset sizes, and model scales,\nincluding LLaMA-1 7B and 13B models.\nThe paper is organized as follows: Section 2 discusses more related works and the differences\nbetween them and our work. Section 3 introduces notations and concepts including analyticity,\npolynomial generalized smoothnesses, and local relaxed dissipativity. Section 4 contains the main\ntheory. Specifically, Section 4.1 relates larger hminwith more uniform (less biased) distribution;\nSection 4.2 and Section 4.3 discuss the effect of hminfrom approximation and optimization perspec-\ntives respectively. Section 5 introduces the experimental settings and results. Specifically, Section\n2\n\n5.1 introduces the data sources, sampling strategies, and visualizations comparing random, uniform,\nand full datasets; Section 5.2 demonstrates the ℓ2-SGD training setup along with findings on the\nbenefits of data uniformity; Section 5.3 presents experiments using cross-entropy loss with Adam\noptimization, highlighting how uniform data selection improves training efficiency and accelerates\nconvergence.\n2 R ELATED WORKS\nData Diversity. Data diversity has been explored across a wide range of domains for enhancing\ngeneralization, efficiency, and robustness. For example, Muennighoff et al. (2025) investigate the\nrole of data diversity in improving test-time scaling generalization. In WizardLM (Xu et al., 2024),\ndiverse queries are obtained by sampling inputs randomly for distillation from larger LLMs. TeaMs-\nRL (Gu et al., 2024) leverages reinforcement learning (RL) to teach LLMs for diverse instructional\ndata generalization, improving downstream model generalization. Beyond language modeling, data\ndiversity has shown benefits for generalization in supervised learning (Yu et al., 2022), software se-\ncurity (Nguyen-Tuong et al., 2008), fault tolerance in software systems (Ammann & Knight, 2002),\nand medical image segmentation (Hofmanninger et al., 2020). Particularly, in RL, diverse data\nsupports more efficient offline training (Nguyen-Tang & Arora, 2023) and improves exploration\nand policy learning (Eysenbach et al., 2019). Other domains, such as mobile infostation networks\n(Yuen et al., 2003) and privacy protection (Machanavajjhala et al., 2007), also leverage diversity for\nimproved robustness. More broadly, Gong et al. (2019) demonstrate that data diversity enhances\nmachine learning performance across various stages, such as model diversification and inference\ndiversification.\nConvergence of Neural Networks. There is many existing literature focusing on understanding the\nconvergence of neural networks, particularly in the overparameterized regime. The neural tangent\nkernel (NTK) framework, introduced by Jacot et al. (2018), has led to a series of convergence results\nshowing that when the network width is sufficiently large—typically a high-order polynomial in the\nnumber of samples N, network depth L, and inverse of angle between data—(stochastic) gradient\ndescent can drive the training loss to zero exponentially (Allen-Zhu et al., 2019; Du et al., 2019;\nLee et al., 2019; Zou et al., 2020; Zou & Gu, 2019; Ji & Telgarsky, 2019; Chen et al., 2020b;\nSong & Yang, 2019; Oymak & Soltanolkotabi, 2020). However, in the NTK regime, networks\nessentially behave as linear models and do not exhibit meaningful feature learning. To address\nthis limitation, recent work has moved toward analyzing feature learning beyond the NTK regime.\nYang & Hu (2020) introduced the µP framework and the tensor program methodology in infinite-\nwidth networks. Building on this, Chen et al. (2022) studied feature learning under gradient flow,\nwhile Chen et al. (2025) extended the analysis to stochastic gradient descent, establishing global\nconvergence in the µP limit. Ba et al. (2022) focused on the very first step of gradient descent in two-\nlayer networks. There are also many work investigating feature learning in structured data settings\nAllen-Zhu & Li (2020; 2022); Cao et al. (2022); Shi et al. (2021); Telgarsky (2022). Another line of\nwork adopts a mean-field perspective (Song et al., 2018; Chizat & Bach, 2018; Rotskoff & Vanden-\nEijnden, 2018; Wei et al., 2019; Chen et al., 2020a; Sirignano & Spiliopoulos, 2020; Fang et al.,\n2021) and dynamical mean-field theory perspective (Bordelon & Pehlevan, 2022; 2023). Recently,\nHan & Imaizumi (2025) developed a distributional view of GD under feedforward network with\nfinite width under non-infinitesimal learning rates. In contrast to these approaches, which typically\nassume fixed architectures and/or infinitesimal learning rates, our work introduces tools applicable\nto a broad class of analytic and poly-smooth networks and remains effective under larger learning\nrates, offering a theoretical foundation for convergence in more practical, complex architectures.\nResidual Connections. Residual connections have been widely studied for their theoretical and\npractical benefits in deep learning. Hardt & Ma (2016) showed that deep linear residual networks\nexhibit no spurious local optima and maintain strong expressivity, a result further supported by Liu\net al. (2019), who demonstrated the absence of spurious minima in residual architectures. Huang\net al. (2020) highlighted the role of residual connections in preserving learnability across depth,\nwhile Scholkemper et al. (2024) found that they help mitigate oversmoothing in graph neural net-\nworks. In the context of transformers, Qin et al. (2025) showed that residual connections improve\nthe conditioning of output matrices in single-layer architectures with feedforward network and at-\ntention. In contrast to these works, our study analyzes general and more complex architectures and\n3\n\nprovides a theoretical explanation based on measure theory and differential topology for the intrinsic\nnon-degeneracy of the layerwise mapping induced by residual connections throughout training.\nApproximation of Neural Networks. A lot of literature has investigated the universal approxima-\ntion properties of neural networks through the lens of Sobolev spaces. Early work by Andoni et al.\n(2014) considered the learnability of a two-layer neural network with analytic activation functions.\nBuilding on classical approximation theory, Lu et al. (2021) applied the Bramble–Hilbert theorem\nto characterize the expressivity of deep ReLU networks for smooth functions, deriving error bounds\nin the L∞norm. Extending the analysis to Sobolev norms, G ¨uhring et al. (2020) established approx-\nimation rates for ReLU networks in Ws,p. Similarly, De Ryck et al. (2021) derived approximation\nerror bounds in Wk,∞norm for tanh networks, while Shen et al. (2022) focused on convolutional\narchitectures and their ability to approximate functions in Sobolev spaces. More recently, Jiang &\nLi (2024) analyzed the approximation capabilities of transformer architectures in the L∞norm. Re-\ngarding the general Lpnorm for 1≤p≤∞, there are works studying, for example, transfromers\nYun et al. (2019); Kajitsuka & Sato (2023); Kratsios et al. (2021); Edelman et al. (2022); Luo et al.\n(2022), residual networks (ResNets) Lin & Jegelka (2018); Tabuada & Gharesifard (2022), feedfor-\nward networks (Hanin & Sellke, 2017; Kidger & Lyons, 2020; Park et al., 2020; Cai, 2022, etc.).\n3 P RELIMINARY\nWe use∥⋅∥to denote ℓ2norm for vectors and matrices, use ∥⋅∥pto denote ℓpnorm for vectors and\nLp(Ω)norm for functions, use ∥⋅∥r,p,Ωto denote the Sobolev norm of Wr,p(Ω). For the function\nf∶Rm→Rn, We use∇f∈Rn×mto denote the Jacobian of the map fw.r.t. all variables, and use\n∇xfto represent the Jacobian w.r.t. x. For a vector α, we denote ∣α∣=∣(α1,⋯, αd)∣=∑d\ni=1αi; for\na setΩ, we use ∣Ω∣to denote the area of the set. The closure and interior of a set Ωare denoted as\n¯ΩandΩo. For two measures νandµ, we use ν≪µto denote the absolute continuity of νw.r.t. µ.\nWe denote Lebd(⋅)to be the Lebesgue measure of a set in Rd. We use conv{v1,⋯, vn}to denote\nthe convex hull over v1,⋯, vn. We denote Bx(r)to be the open ball of radius rcentered at xunder\nEuclidean distance. We use R≥0to denote all the non-negative real values.\nWe use normalization in building the neural networks (2) that is defined as follows\nτϵ(x)=x√\n∥x∥2+ϵ2, for some ϵ>0. (1)\nTo analyze the approximation properties of neural networks, we use the Sobolev space and Sobolev\nnorm, which are defined as follows:\nDefinition 1 (Sobolev space) .The Sobolev space Wm,p(Ω)is defined as Wm,p(Ω)={ϕ∈\nLp(Ω)∶Dαϕ∈Lp(Ω),∀∣α∣≤k},with the corresponding Sobolev norm ∥ϕ∥m,p=\n⎧⎪⎪⎨⎪⎪⎩(∑∣α∣≤m∥Dαϕ∥p\np)1/p,1≤p<∞\nmax∣α∣≤m∥Dαϕ∥∞, p=∞where∥ϕ∥∞∶=ess supx∈Ω∥ϕ(x)∥, and Dαϕ=∂∣α∣ϕ\n∂α1x1⋯∂αdxd.\nNext, we introduce analyticity, polynomial boundedness, polynomial generalized continuity and\nsmoothness, and local relaxed dissipativity as key properties for proving the convergence of neural\nnetworks.\n3.1 A NALYTIC FUNCTION\nAnalyticity is a useful property in both measure theory and differential topology. For a compact do-\nmainD, real-analytic functions are dense in the space of continuous function C(D)under Whitney\nC0-topology (Grauert, 1958), i.e., with respect to uniform convergence on compact sets. Therefore,\nassuming analyticity is reasonable in many settings. It is defined as follows.\nDefinition 2 (Analytic function) .A function f(x)=⎛\n⎝f1(x)\n⋮\nfm(x)⎞\n⎠∶U→Rm, with an open subset\nU⊆Rm, is called real-analytic on U, if for each x0∈U, the function fi(x)can be represented by a\nconvergent power series in some neighborhood of x0, for all i=1,⋯, m.\n4\n\nMany activation functions and components in neural networks are real-analytic. For example:\nCorollary 1. Any product, sum, and composition of softmax, tanh, sigmoid, GELU, SiLU, polyno-\nmial, τϵ, and exponential functions, is real-analytic.\nThe above corollary implies that feedforward layers using tanh, GELU, or SiLU, as well as atten-\ntion layers with softmax or linear activation, are real-analytic. Consequently, architectures such as\ntransformers, residual networks, and feedforward networks built with real-analytic activations and\nnormalizations are themselves real-analytic.\n3.2 P OLYNOMIAL BOUND ,POLYNOMIAL GENERALIZED CONTINUITY AND SMOOTHNESS\nClassical non-convex optimization theory typically assumes Lipschitz smoothness of the objective\nfunction, a condition that is rarely satisfied in neural network training. One direction of generaliza-\ntion stems from the concept (L0, L1)-smoothness (Zhang et al., 2019). Building on this, Li et al.\n(2023) introduced the generalized smoothness for a broader class of non-Lipschitz smooth functions.\nMotivated by these developments, we propose a new formulation based on polynomial boundedness,\npolynomial generalized continuity and smoothness.\nDefinition 3 (Poly-boundedness) .The function f(x)∶Ω⊆Rn→Rdis polynomially bounded if\n∥f(x)∥≤S(∥x1∥,⋯,∥xnx∥),∀x∈Ω\nwhere xi∈Rni,1×ni,2, and dimx=n=∑nx\ni=1ni,1ni,2;S(⋅)is some polynomial whose coefficients\nare all positive.\nDefinition 4 (Poly-continuity) .The function f(x)∶Ω⊆Rn→Rdsatisfies polynomial generalized\ncontinuity if\n∥f(x)−f(x′)∥≤S(∥xmax,1∥,⋯,∥xmax,d∥)∥x−x′∥, ,∀x, x′∈Ω\nwhere∥xmax,i∥=max{∥xi∥,∥x′\ni∥}, and S(⋅)is some polynomial whose coefficients are all positive.\nDefinition 5 (Poly-smoothness) .The function f(x)∶Ω⊆Rn→RdinC1satisfies polynomial\ngeneralized smoothness if ∇fsatisfies polynomial generalized continuity.\nNote such S(⋅)is monotonically increasing in R≥0×⋯×R≥0.\nThe differences between the above poly-smoothness and generalized smoothness (Li et al., 2023)\nlies in two parts. 1) Li et al. (2023) defined the bound of Hessian first, and therefore evaluate the\nsmoothness function at one fixed point for any two points in its neighbourhood, while we directly\nconsider the continuity of the gradient and evaluate the smoothness function S(⋅)at the two points.\n2) Regarding the smoothness function, Li et al. (2023) employed a function of ∥∇f(x)∥, while we\nuse a polynomial of ∥xi∥,∥x′\ni∥.\nThe three definitions above are easily satisfied in typical neural network settings. Specifically:\nCorollary 2. Any product, sum, and composition of polynomial, softmax, tanh, sigmoid, τϵ, GELU,\nSiLU, satisfies polynomial boundedness for both the functions and their gradients, polynomial gen-\neralized continuity, and polynomial generalized smoothness.\nThe above corollary implies that many neural networks, including transformers, residual networks,\nand feedforward networks that employ the aforementioned functions, satisfy the three properties\ndiscussed above.\n3.3 L OCAL RELAXED DISSIPATIVE CONDITION\nSince the objective function in neural network training typically does not satisfy Lipschitz smooth-\nness, additional conditions are needed to guarantee convergence. Inspired by some weak dissipative\nconditions from operator theory (for example, hypomonotonicity (Moudafi, 2004)), we define the\nfollowing local relaxed dissipative condition:\nDefinition 6 ((x, x∗, r, ρ, ϵ)-dissipativity) .A function f(x)∈C1satisfies the (x, x∗, r, ρ, ϵ)-\ndissipative condition near xif there exists some stationary point x∗and some constant r>∥x−x∗∥,\ns.t.,∀y∈Bx∗(r)/{x∶∥∇f(x)∥≤ϵ},\n∇f(y)⊺(y−x∗)≥−ρ∥∇f(y)∥2\nwhere ϵ≥0, and ρ∈Ris a constant depending on x, x∗, ϵ, but independent of y.\n5\n\nThe above condition holds trivially with ρ=1\nϵmax∥y−x∗∥=r\nϵby Cauchy-Schwartz inequality.\nNote that ρcan be negative, and a function fwith more favorable dissipative properties leads to a\nsmaller ρ. For example, if fis locally convex near x∗with some r>0, then ρ=0.\n4 T HEORY\nIn this section, we consider the data {(xi, yi)}N\ni=1, and define\nhij=∥xi−xj∥, and hmin=min\ni,jhij.\nWe mainly use hminto characterize the effect of data distributions. More precisely, we show that\nan increase in hmincorresponds to a more uniform (less biased) distributions (Section 4.1). We\nthen prove that a larger hminleads to faster training (Section 4.2) and smaller approximation error\n(Section 4.3). Additionally, we present a convergence framework for a family of neural networks\nunder GD beyond the NTK regime (Remark 1). This framework provides theoretical support for\nthe practical effectiveness of structures such as residual connections, function composition, and\nnormalization (Remark 2).\nWe assume that the data satisfy the following properties:\nAssumption 1 (Data) .Assume x1,⋯, xNi.i.d.∼π(x)dx, where π(x)∈L∞is supported on an open\nsetΩwith∣Ω∣<∞, and π≪Leb onΩ. Assume the ground truth function g(x)∈Wr,p(Ω)for\n1≤p<∞andr≥1, and yi=g(xi).\nIn the above assumption, the input data are sampled i.i.d. from some distribution. If we modify\nthe data, for example, by increasing the minimum distance between xi, the underlying distribution\nchanges accordingly, but the modified data remain i.i.d. samples from this new distribution. We\nfurther assume that the density is absolutely continuous w.r.t. Lebesgue measure on its support and\nlies in L∞to exclude degenerate cases like point masses. For the ground truth function, we assume\nit belongs to the Sobolev space Wr,p(Ω). Notably, Wr,p(Ω)functions are dense in Lp(Ω)(see\nLemma 14), making it a reasonable assumption.\nUnlike analyses in the NTK regime (Allen-Zhu et al., 2019; Zou et al., 2020; Zou & Gu, 2019; Chen\net al., 2020b; Oymak & Soltanolkotabi, 2020), which typically assume normalized input data xiso\nthathminreflects only angular separation between points, hminin our framework encodes both the\nangular separation and the norm differences between input data points, as it is defined directly via\ntheℓ2distance without data normalization.\n4.1 M INIMUM DISTANCE hmin VS MORE UNIFORM DISTRIBUTION\nIn this section, we discuss the relationship between hminand the sampling density function of input\ndataπ(⋅): less biased density results in larger hmin, and consequently admits faster convergence\n(Theorem 2) and smaller approximation error (Theorem 3).\nBefore stating the result, we introduce the following notations. Let πmax=∥π(x)∥∞. For some\n0<¯πmax≤πmax, we define Ωmax={x∈Ω∣π(x)≥¯πmax}. Below is the main theorem showing\nthe relationship between hminandπmax,¯πmax.\nTheorem 1. Suppose Assumption 1 holds. For any biased distribution such that there exists a ball\nB(C(−logδ\n¯πmax(N−1)Vd)1/d\n)⊆Ωmax, andπmax\n¯πmax=O(1), we have with probability at least 1−2δ,\n(2δ\nπmaxN(N−1)Vd)1/d\n≤hmin≤C(−logδ\n¯πmax(N−1)Vd)1/d\nwhere Vd=πd/2\nΓ(d/2+1)is the volumn of d-dimensional unit ball, and C>0is some universal constant.\nThe above theorem provides both the lower and the upper bounds of hminin terms of πmaxand¯πmax.\nThe assumptions on the biased distribution require that the density should be high over a region that\nis not too small, i.e., avoiding some narrow peaks with near-zero support. When π(⋅)is biased, both\nπmaxand¯πmaxcan be large, leading to smaller upper and lower bounds of hmin. Conversely, if\n6\n\nπ(⋅)becomes more “flattened”, i.e., more uniform and less biased, then πmaxdecreases, and ¯πmax\nwill also decrease under certain conditions where the distribution is significantly more uniform than\nbefore. In particular, when πmaxbecomes smaller than the previous ¯πmax, then ¯πmaxnecessarily\ndecreases. As a result, both the lower and the upper bounds of hminincrease. In summary, a less\nbiased (i.e., more uniform) density yields a larger hmin.\n4.2 O PTIMIZATION : CONVERGENCE OF NEURAL NETWORKS BEYOND NTK REGIME\nIn this section, we establish the convergence of a general class of neural networks under gradient\ndescent beyond the NTK regime (Theorem 2). Moreover, if hmindecreases, the convergence will be\nslower (Corollary 3).\nWe consider a family of neural networks in the following form:\nu0,i=xi\nuℓ+1,i=uℓ,i+¯φℓ(θ;uℓ,i), ℓ=0,⋯, L−1 (2)\nf(θ;xi)=¯φL(θ;uL,i)\nwhere ¯φℓ(θ;u)=φℓ(θ;τϵ(u))with the normalization function τϵ(1).\nWe then make the following assumptions:\nAssumption 2. Assume φℓ(θ;u)=φℓ(θℓ;u),∀ℓ=0⋯, L.\nThe above assumption shows that the ℓth-layer weights are not used in other layers for all ℓ=0⋯, L.\nAssumption 3. Assume φℓ(θ;u)are real-analytic (Definition 2) for all θandu, where ℓ=1,⋯, L,\nandφ0(θ;u)is real-analytic for all θanduin some bounded open neighbourhood of ¯Ω.\nAssumption 4. Assume for all ℓ=0,⋯, L,φℓ(θ;u)satisfies polynomial general-\nized continuity (Definition 4) and smoothness (Definition 5) for both θand u. Also,\nφℓ(θ;u),∇θφℓ(θ;u),∇uφℓ(θ;u)are polynomially bounded (Definition 3).\nThe above assumptions hold true for many practical structures, including transformers, feedforward\nnetworks, and residual networks. See more discussions in Section 3.1 and 3.2.\nAssumption 5. Let˜fℓ(θ;uℓ,i)=f(θ;xi). Assume for any θL−1, θLwithdimθL−1>Ndexcept for\na measure-zero set, there exists one (u1,⋯, uN), s.t.,⎛\n⎜\n⎝∇θL−1˜fL−1(θ;u1)\n⋮\n∇θL−1˜fL−1(θ;uN)⎞\n⎟\n⎠has full row rank Nd.\nThe above assumption is very mild. It requires the non-degeneracy of the matrix at only one point,\nwhich essentially guarantees the nonlinearity of the architecture. The following is one example.\nLemma 1. The following structure satisfies Assumption 5: φL(θ;u)=θLuandφL−1(θ;u)=\nθL−1,2σ(θL−1,1u), where σ(⋅)is GELU, θL∈Rd×dθL−1,1∈RmL−1×d,θL−1,2∈Rd×mL−1, and\nmL−1>Nd.\nThe above lemma considers an architecture in which the (L−1)th layer is a two-layer feedforward\nnetwork, and the final layer is linear. It further shows that the width mL−1>Ndis sufficient to\nsatisfy Assumption 5. Indeed, since we only require mL−1>Naccording to Assumption 5, this\nlower bound Ndmay not be tight and can potentially be improved.\nWe define ℓ2loss function of each data point and the total loss as follows\nl(θ;xi, yi)=1\n2∥yi−f(θ;xi)∥2,andL(θ;{xi, yi}N\ni=1)=1\nNN\n∑\ni=1l(f(xi), yi).\nConsider minimizing L(θ)by gradient descent\nθk+1=θk−η∇θL(θk)\nwhere θis the vectorized version of (θ1,⋯, θnθ)and each θiis a vector or matrix.\nBelow is the main theorem showing the convergence of neural networks (2) under gradient descent.\n7\n\nTheorem 2. Suppose Assumptions 1 to 5 hold. Consider the initialization θ0inRdimθexcept\nfor a measure-zero set. Let R=√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0)for some stationary point θ∗. Let\nηbe in some dense set of (0,min{2−δ\nL,1})for some small constant 0<δ<2, and L=\nS(⋯,∥θ∗\ni∥+R+max θ∈Bθ∗(R)∥∇θiL(θ)∥,⋯), where S(⋅)is some polynomial whose coefficients\nare all positive. Assume L(θ)satisfies the (θ0, θ∗, R, ρ, ϵ L)-dissipative condition in Definition 6.\nThen, under some arbitrarily small adjustment on the scale of φℓforℓ=0,⋯, L−1, with probability\n1 over the joint distribution of the input data x1,⋯, xN, GD converges to ∥∇L(θ)∥≤ϵL, and\nL(θk)≤k−1\n∏\ns=0(1−η(1−ηL\n2)µlow,s,X\nN)L(θ0),∀k≥1,and∥∇L(θk)∥>ϵL. (3)\nwhere µlow,s,X>0is some strictly positive constant depending on θsandx1,⋯, xN.\nIn the above theorem, GD is proved to convergence to the neighbourhood of some stationary points,\nand the assumptions are practical as is discussed in earlier this section. A complete version of the\nabove theorem can be found in Theorem 5, Appendix C.\nThe unusual density requirement of the learning rate ηis the consequence of the parameteric\ntransversality theorem (Theorem 6 (Hirsch, 2012) and Lemma 10 in Appendix C.3). The “bad”\nηwithin this intervel, which is excluded by the theorem, may cause the neural network to converge\nto a degenerate map (i.e., its Jacobian ∇fis not full rank) whose image lies in a lower-dimensional\nsubspace, and thus prone to lose full expressivity1. However, such values can be avoided via a\nsmall perturbation of η, which suffices to yield a “good” learning rate. The assumption on the small\nadjustment to the scale of φℓfollows the same reasoning.\nTheθ∗can be any stationary point that satisfies the dissipativity (Definition 6) and therefore its exis-\ntence does not directly imply the convergence of GD; moreover, θ∗is not necessarily the limit of GD\niteration. Therefore, the radius Rfor local relaxed dissipativity is determined by the initialization θ0\nand does not assume any information about the subsequent GD iterations.\nBy Assumption 4, L(θ)can be shown to satisfy polynomial generalized smoothness with some\npolynomial S(⋅), which can be explicitly computed once the architecture is fixed (see Lemma 8).\nThe number of terms in S(⋅)depends on the depth Lof this network (see Lemma 8). Furthermore,\nsuchS(⋅)is proven to be bounded by Lalong the GD trajectory, where Ldepends on the initial\ncondition θ0, initial loss L(θ0), maximum ∇L(θ)in some bounded region, and some stationary\npoint θ∗and its landscape nearby determined solely by θ0. In other words, the Lipschitz smoothness\nalong the GD trajectory is proved instead of assumed.\nThe constant µlow,s,X is indeed a lower frame bound of⎛\n⎝∇θf(x1)\n⋮\n∇θf(xN))⎞\n⎠, which is proven to form\na frame with probability 1 for almost all θ, and consequently µlow,s,X>0(see more details in\nAppendix C.1). The inequality (3) implies the decay of the loss L(θk). We remark that the above\nresult provides a general framework for analyzing a broad class of neural networks. When the\narchitecture is not explicitly specified, the result does not identify the specific minimizer to which\nGD converges—for example, it could be a spurious local minimum. Moreover, it does not guarantee\nexponential convergence, as the constant µlow,s,Xcan become arbitrarily small during GD iterations.\nHowever, finer quantifications can potentially be obtained for specific architectures. For instance, if\nthe network is known to admit only global minima within the ball Bθ∗(R), then convergence to a\nneighborhood of a global minimum is ensured.\nBased on the convergence of the neural network, we derive the following corollary, which highlights\nthe relationship between data spacing, particularly hmin, and the convergence rate.\nCorollary 3. Under the same assumptions as Theorem 2, fix some xi. Let X=(x1,⋯, xN), and\nDi,H={j∣hij=∥xi−xj∥≤H,∀j≠i}. Then for any k≥1, there exists rk,i,H>0andLk,i,H>0,\n1Note that the image of the ground truth function gmay lie in a lower-dimensional subspace. However, the\nlower-dimensional subspace induced by certain ηmay not align with the desired subspace.\n8\n\ns.t., when√\n∑j∈Di,Hh2\nij≤rk,i,H , we have\nµlow,s,X≤Lk,i,H√\n∑\nj∈Di,Hh2\nij,∀s≤k.\nSpecifically, if H=hminandDi,H≠∅, there exists Lk,i, rk,i>0, s.t., when hmin≤rk,i, we have\nµlow,s,X≤Lk,ihmin,∀s≤k.\nThe above corollary shows that, for any fixed data point xi, when nearby points become even closer\ntoxi, the training process potentially slows down, which is verified experimentally in Section 5.\nIndeed, not all data uniformity schemes have the same effect, even when all the hijin the theorem\nare identical across different schemes. Therefore, µlow,s,Xcan only be upper bounded in terms of\nhij. However, there is a general trend that a decrease in hijtends to slow down the dynamics. More\nprecisely, there exists a subsequence of the value of {µlow,s,Xt}tdepending on the data Xt, s.t., as\nthe nearby distances√\n∑j∈Di,Hh2\nijdecreases for some Xt, the value of µlow,s,Xtalso monotonically\ndecreases. Consequently, the value of 1−η(1−ηL\n2)µlow,s,Xt\nNincreases, and thus slows down the\nconvergence. The dependence on hminis a special case of this phenomenon and follows the same\nreasoning.\nRemark 1 (Convergence beyond the NTK regime) .Theorem 2 provides a general framework of\nconvergence for a family of neural networks beyond the NTK regime in the following two senses\nregarding both the local relaxed dissipativity and the learning rate:\n1) When the loss landscape is unfavorable around the initial condition (for example, in the worst\ncaseρ=R\nϵfor some ϵ>0, which is valid for any landscape; see Section 3.3), then Lis large,\nand although the learning rate may be very small, it does not necessarily decrease as the width\nincreases. Moreover, such a large ρmay indicate insufficient overparameterization, i.e., the width of\nthe neural network is too small, which falls outside the NTK regime. Our theory demonstrates that\nconvergence to a stationary point is still possible in this non-NTK setting.\n2) If the loss landscape is favorable near the initial condition, i.e., ρis small, then the learning rate\nηin Theorem 2 can be much larger than that required by the infinitesimal (gradient flow) regime\nassumed in NTK analysis, since the number of terms in S(⋅)depends on the depth L.\nApart from the local relaxed dissipativity condition, the only width requirement appears in Assump-\ntion 5. As is discussed below Lemma 1, the width is required to be greater than Nin Assumption 5,\nand it suffices for the width of the (L−1)th layer to be greater than Ndfor the feedforward layer;\nno constraints are imposed on the widths of other layers. This lower bound Nd, which is not sharp,\nis much milder than the NTK-type convergence conditions that typically require a high-degree poly-\nnomial dependence on d, N, L , etc. (See Section 2).\nRemark 2 (Theoretical insights on architectures: residual connection, composition, and normaliza-\ntion) .When constructing a neural network, it is important to ensure that the model is expressive\nenough. Theoretically, this corresponds to requiring the neural network mapping fto be non-\ndegenerate, i.e., its Jacobian ∇fshould be full rank in some region. Otherwise, the mapping may\ncollapse onto a lower-dimensional subspace, and thus lose full expressivity.\nIn fact, non-degeneracy is a generic property among real-analytic functions. If a function is degen-\nerate, we can typically restore non-degeneracy by adding a small identity component, i.e., by incor-\nporating a residual connection, and then the modified mapping is non-degenerate (see Lemma 10).\nThis helps explain the empirical success of residual architectures in practice.\nMoreover, neural networks are constructed as compositions of layer-wise transformations. If each\nlayer has a Jacobian with full rank (e.g., ensured via residual connections), then the Jacobian of\nthe overall composition remains full rank due to the chain rule: ∇(f○g)(x)=∇f(g(x))∇g(x).\nThis shows that the compositional (iterative) structure of neural networks preserves non-degeneracy.\nIn contrast, other combinations, such as summations of functions, do not necessarily preserve this\nproperty, even if each individual function is non-degenerate. For example, consider f1(x)=x⊺x\nandf2(x)=−x⊺x; while each is non-degenerate, their sum f1+f2=0is degenerate.\nRegarding the normalization used in (2), it differs from practical schemes such as batch normal-\nization and layer normalization. Nonetheless, they share a common goal, which is to reduce the\n9\n\nmagnitude of neurons and restore desirable regularity properties in the network. Such normaliza-\ntion helps flatten the optimization landscape, thereby facilitating more stable and efficient training.\nSee more discussion in Wang et al. (2023).\nThe proof of Theorem 2 can be divided into three parts: 1) Establishing a lower bound on the gradi-\nent in terms of the loss values, using tools from measure theory and differential topology, especially\nthe parametric transversality theorem (see Theorem 6; Hirsch (2012)); 2) Verifying the polynomial\ngeneralized smoothness of the loss function L, which is primarily derived via Gronwall’s inequality;\n3) Deriving bounded Lipschitz smoothness along the gradient descent trajectory, using the local re-\nlaxed dissipativity condition together with polynomial generalized smoothness. Combining the three\ncomponents, we obtain the convergence to the neighbourhood of some stationary point. The proof\nof Corollary 3 mainly relies on the local Lipschitz continuity of eigenvalues for real-analytic matrix\nfunctions (see Theorem 4.1 in Kurdyka & Paunescu (2008)). Detailed proofs are in Appendix C.\n4.3 A PPROXIMATION : DATA-DEPENDENT ERROR IN THE INTERPOLATION REGIME\nIn this section, we characterize the approximation error between the ground truth function and any\nneural network that serves as a universal approximator in the interpolation regime, in terms of the\npairwise distances among data points. We show that a larger hminand smaller local maximal dis-\ntances hmaxd+1lead to smaller approximation error.\nWe define the interpolation regime as I=conv{x1,⋯, xN}. The following theorem characterizes\nthe data-dependent approximation error.\nTheorem 3. Consider a set of data {(xi, yi)}N\ni=1under Assumption 1. Let Ii=conv{xi1,⋯, xid+1}\nwhere xj∉Ii,∀j≠i1,⋯, id+1, and define hmaxd+1=max imax xq,xj∈Iihqj. Suppose there is a\nfamily of neural networks that can approximate polynomial of degree nd=nd(N, d), i.e., for any\nϵ2>0andψ∈Pnd, there exists some fs.t.∥f−ψ∥p≤ϵ2. Then, for some C1>0andϕ∈Wm,p(Ω)\nwithm>max{r, nd}, with probability 1 over the joint distribution of x1,⋯, xN, we have\n∥f−g∥p,Io≤C1hm\nmaxd+1h−r\nmin∥ϕ∥m,p,Io.\nThe above theorem differs from classical results in approximation theory. It fixes a specific approxi-\nmation function and examines how the distances between data points affect its error. In the theorem,\ntheIidenotes a d−simplex in the triangulation of the interpolation regime I(see detailed descrip-\ntion in Appendix D.1). The neural networks used in the theorem are some universal approximators,\nincluding transformer, feedforward networks, residual networks, etc. (see Section 2).\nRegarding the dependence on data, the error bound is determined by two quantities: hmaxd+1, which\nwill decrease as the number of samples increases, and hmin, which will increase if the samples are\nmore uniformly distributed (see Section 4.1). The parameter rquantifies a more refined aspect of\nfunction approximation: being in the Sobolev space Wr,p(Ω)not only controls function values but\nalso their derivatives, allowing for smoother approximations, and thus larger hminalso contributes\nto smoother approximation. Indeed, if the neural network f(x)is real-analytic (Assumption 3),\nthenf(x)∈Wr,p(Ω)(see Lemma 15). Moreover, if the network is a universal approximator under\nSobolev norm (G ¨uhring et al., 2020; De Ryck et al., 2021; Shen et al., 2022), the above results can\nbe extended to hold in the Sobolev norm as well.\nThe above theorem also indicates that increasing the number of data points—i.e., decreasing hmaxd+1\nwhile keeping hminnon-increasing—can achieve the same approximation error as using fewer\nbut more uniformly distributed data points—i.e., increasing hminwhile keeping hmaxd+1non-\ndecreasing. This finding is also validated in our experiments (see Section 5).\nThe proof of this theorem mainly involves establishing a data-dependent version of Bramble-Hilbert\nlemma (Bramble & Hilbert, 1970) in numerical analysis. This is achieved by proving the Bramble-\nHilbert lemma for each d−simplex in the Delaunay triangulation. See details in Appendix D.\n5 E XPERIMENTS\nIn our experiments, we first introduce the data uniformity strategy—iteratively selecting data points\nthat maximize the distances among all previously chosen points—and present visualizations com-\nparing the selected uniform data, randomly selected data of the same size, and the full dataset. The\n10\n\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2(a) 9k full data points.\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2 (b) 4.5k uniform data points.\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2 (c) 4.5k random data points.\nFigure 2: Visualization of data point distributions for datasets selected using different methods from\nTeaMs-RL (Gu et al., 2024).\npurpose of comparing the selected uniform data with randomly selected data of the same size is to\nisolate the effect of the minimum distance hminwhile keeping the dataset size constant. We then\nconduct supervised fine-tuning using ℓ2loss with SGD for theoretical validation and cross-entropy\nloss with Adam for practical evaluation, assessing both performance and training time across differ-\nent source datasets, dataset scales, and model sizes to demonstrate the effectiveness of our method.\nThe experiments validate our theoretical results by showing that training with the selected uniform\ndata, which implies a larger hmin, accelerates convergence (see Corollary 3) while maintaining com-\nparable performance to using the full dataset (see Theorem 3 and the discussions below). Detailed\nexperimental settings are provided in Appendix G.\n5.1 D ATA SELECTION AND DATA VISUALIZATION\nData Selection via Distance Maximization. To construct compact yet representative instruction-\ntuning datasets, we apply a greedy uniformity selection strategy based on data distance. Specifically,\nwe represent each instruction–input–output triple as an average Word2Vec embedding trained on the\nfull corpus. Starting from a randomly selected seed, we iteratively add the data point that maximizes\nthe minimum cosine distance to all previously selected points, ensuring maximal coverage of the\ndata space. We apply this strategy to two corpora: TeaMs-RL (Gu et al., 2024) and WizardLM (Xu\net al., 2024). The TeaMs-RL and WizardLM datasets are both distilled from ChatGPT-3.5/4, using\ndifferent strategies to generate diverse instructional data. From the 9k TeaMs-RL dataset, we select\na 4.5k maximized distance subset and compare it to a 4.5k random sample, which is composed of\nhalf randomly sampled data points and half selected to minimize pairwise distances from the 9k full\ndataset2; from the 20k WizardLM dataset, we select a 10k maximized distance subset comparing\nit against a 10k random sample, and the random subset consists of the first 10k samples from the\noriginal 20k dataset, which is already randomly shuffled. This approach allows us to fairly evaluate\nthe benefits of uniformity-oriented sampling across different dataset scales.\nData Visualization via PCA Projection. To qualitatively assess the distributional coverage of dif-\nferent data selection methods, we project high-dimensional sentence embeddings into two dimen-\nsions using PCA (Ma ´ckiewicz & Ratajczak, 1993). For consistency, PCA is fit on the full dataset\n(either TeaMs-RL 9k or WizardLM 20k) and applied to both the full set and its corresponding uni-\nform and random subsets (the uniform data is selected based on maximizing pairwise distance). The\nresulting 2D coordinates are normalized using MinMax scaling to [0,1] and plotted for visualization.\nAs shown in Figures 2 and 3, the uniform subsets from TeaMs-RL (Gu et al., 2024) and WizardLM\n(Xu et al., 2024) (4.5k for TeaMs-RL, 10k for WizardLM) exhibit broader and more dispersed cov-\nerage across the embedding space compared to the random subsets and the full dataset, which cluster\nmore tightly and redundantly. These patterns highlight the ability of uniformity-based sampling to\nretain semantic coverage with reduced sample sizes, supporting its utility for efficient training.\n2This choice of random data is made because the dataset size is relatively small, and we aim to highlight the\neffect of different minimum distances between data points hmin.\n11\n\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2(a) 20k full data points.\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2 (b) 10k uniform data points.\n0.0 0.2 0.4 0.6 0.8 1.0\nPrincipal Component 10.00.20.40.60.81.0Principal Component 2 (c) 10k random data points.\nFigure 3: Visualization of data point distributions for datasets selected using different methods from\nWizardLM (Xu et al., 2024).\n5.2 ℓ2-SGD T RAINING EXPERIMENTS\nWe present training dynamics under ℓ2loss with SGD to validate our theoretical insights. While\nℓ2-(S)GD provides a useful controlled setting for analysis, it is generally less efficient in practice\ncompared to cross-entropy loss optimized with Adam, which is the standard approach for training\nLLMs. Due to the limited practical use and lack of precedent for ℓ2-SGD in LLM training, we do\nnot evaluate generalization performance in this section. For broader comparisons involving cross-\nentropy and Adam, see Section 5.3.\nTraining Comparison with Uniform Subsets. Figure 4 shows the training loss curves of\nLLaMA-1-7B (Touvron et al., 2023)3using ℓ2loss with SGD (Bottou, 2010; Cortes et al., 2012)\non the TeaMs-RL dataset under three data configurations: the full dataset (9k), a 4.5k subset se-\nlected by maximizing pairwise distance, and a 4.5k randomly sampled subset. The figure includes\nboth unsmoothed (left) and smoothed (right) loss curves, where unsmoothed curves reflect raw step-\nwise fluctuations in loss, while smoothed curves (e.g., via moving average) highlight overall training\ntrends more clearly. In both plots, we zoom in the early and middle stage of the training for clearer\ncomparison (see full training in Appendix F), and observe that the model trained on the uniform\nsubset consistently converges faster than the one trained on the random subset across training steps.\nMoreover, despite using only half the number of samples, the uniform subset achieves convergence\nbehavior comparable to or better than the full dataset. This validates our theory (see Section 4)\nand suggests that uniformity-aware data selection can substantially enhance training efficiency and\nconvergence, even under limited data budgets. More experiments are provided in Appendix F.\n0 5 10 15 20 25\nStep11121314151617Loss\nModel\nL2-sgd-llama-1-random-4.5k\nL2-sgd-llama-1-uniform-4.5k\nL2-sgd-llama-1-full-9k\n(a) without smoothness.\n0 5 10 15 20 25\nStep121314151617Loss\nModel\nL2-sgd-llama-1-random-4.5k\nL2-sgd-llama-1-uniform-4.5k\nL2-sgd-llama-1-full-9k (b) with smoothness.\nFigure 4: ℓ2SGD Training Experiments: Training loss comparison of TeaMs-RL data point distri-\nbutions for different dataset sizes: 4.5k uniform (maximized distance) dataset, 4.5k random dataset,\n9k full dataset.\n3https://huggingface.co/huggyllama/llama-7b\n12\n\n0 5 10 15 20 25 30 35\nStep0.91.01.11.21.3LossModel\nllama-1-7b-random-4.5k\nllama-1-7b-uniform-4.5k\nllama-1-7b-full-9k(a) without smoothness.\n0 5 10 15 20 25 30 35\nStep0.91.01.11.21.3LossModel\nllama-1-7b-random-4.5k\nllama-1-7b-uniform-4.5k\nllama-1-7b-full-9k (b) with smoothness.\nFigure 5: Training loss comparison of TeaMs-RL data point distributions for different dataset\nsizes using cross entropy loss and Adam: 4.5k uniform (maximized distance) dataset, 4.5k ran-\ndom dataset, 9k full dataset.\n5.3 C ROSS ENTROPY LOSSADAM TRAINING EXPERIMENTS\nCross-Entropy Loss and Adam Optimization with maximized-distance Subsets. Figures 5 (a)\nand 5 (b) compare training loss curves under cross-entropy loss (Mao et al., 2023) using the Adam\noptimizer (Kingma & Ba, 2014) for three TeaMs-RL datasets: the full 9k dataset, a 4.5k maximized-\ndistance subset selected via maximized distance, and a 4.5k random subset. Figure 5 (a) presents raw\ntraining loss, while Figure 5 (b) shows smoothed loss for clarity. In both cases, the model trained on\nthe maximized-distance based subset consistently converges faster and achieves significantly lower\nloss than the one trained on the random subset, despite both using the same number of samples. No-\ntably, the maximized-distance 4.5k subset also outperforms the full 9k dataset in both convergence\nspeed and final loss, highlighting the efficiency gains from data uniformity and well-distributed\ndata. These results reaffirm that maximized-distance data selection not only improves optimization\nbut also reduces training redundancy under standard cross-entropy objectives with modern optimiz-\ners. Notably, models trained on uniform data exhibit lower initial loss compared to those trained on\nrandom or full datasets. This may be attributed to the higher diversity and broader coverage of the\nuniform subset, which is more aligned with the base model’s existing knowledge—likely due to its\npretraining on large-scale corpora exceeding 1T tokens (Touvron et al., 2023).\nScalability of Uniform Data Benefits with Larger Datasets. Figures 6 (a) and 6 (b) show the\ntraining dynamics of models trained on WizardLM data using cross-entropy loss with the Adam op-\ntimizer across three dataset settings: the full 20k dataset, a 10k maximized-distance subset selected\nby maximizing distance, and a 10k random subset. In both plots, the maximized-distance subset\nconsistently outperforms the random counterpart throughout the training process, achieving lower\nand more stable loss values. Notably, the maximized-distance 10k subset even surpasses the full 20k\ndataset regarding convergence, suggesting that data uniformity can compensate for and even outper-\nform brute-force data scaling. This pattern, observed across both TeaMs-RL and WizardLM data,\nhighlights the robustness and scalability of uniform-based sampling strategies, offering a compelling\napproach for reducing computational costs without sacrificing learning quality.\nTraining on large-scale models. Figures 7 through 8 present training loss comparisons for the\nLLaMA-1 13B model (Touvron et al., 2023)4using full datasets, random subsets, and uniformity-\nselected subsets across TeaMs-RL (4.5k/9k) and WizardLM (10k/20k) data. In all settings, the mod-\nels trained on uniformity-maximized subsets exhibit consistently lower or comparable loss compared\nto those trained on full datasets, and significantly outperform the randomly sampled subsets. This\ntrend holds both with and without smoothing applied to the loss curves. Particularly in Figures 8 (a)\nand 8 (b), the 10k uniform subset achieves better final loss than even the full 20k dataset, reinforc-\ning the efficiency of data uniformity in training large-scale models. These results demonstrate that\nuniformity-aware sampling remains effective even at larger model scales, offering substantial gains\nin training efficiency without compromising performance.\n4https://huggingface.co/huggyllama/llama-13b\n13\n\n0 50 100 150 200 250 300\nStep0.30.40.50.60.70.80.91.0LossModel\nllama-1-7b-random-10k\nllama-1-7b-uniform-10k\nllama-1-7b-full-20k(a) without smoothness.\n0 50 100 150 200 250 300\nStep0.30.40.50.60.70.80.91.0LossModel\nllama-1-7b-random-10k\nllama-1-7b-uniform-10k\nllama-1-7b-full-20k (b) with smoothness.\nFigure 6: Training loss comparison of WizardLM data point distributions for different dataset sizes\nusing cross entropy and adam: 10k uniform (maximized distance) dataset, 10k random dataset, 20k\nfull dataset.\n0 5 10 15 20 25 30 35\nStep0.80.91.01.11.21.3LossModel\nllama-1-13b-random-4.5k\nllama-1-13b-uniform-4.5k\nllama-1-13b-full-9k\n(a) without smoothness.\n0 5 10 15 20 25 30 35\nStep0.80.91.01.11.2LossModel\nllama-1-13b-random-4.5k\nllama-1-13b-uniform-4.5k\nllama-1-13b-full-9k (b) with smoothness.\nFigure 7: Training loss comparison of TeaMs-RL data point distributions for different dataset sizes\nusing cross entropy loss and Adam on llama-1-13b models: 4.5k uniform (maximized distance)\ndataset, 4.5k random dataset, 9k full dataset.\n0 20 40 60 80 100 120 140\nStep0.30.40.50.60.70.80.9LossModel\nllama-1-13b-random-10k\nllama-1-13b-uniform-10k\nllama-1-13b-full-20k\n(a) without smoothness.\n0 20 40 60 80 100 120 140\nStep0.30.40.50.60.70.80.9LossModel\nllama-1-13b-random-10k\nllama-1-13b-uniform-10k\nllama-1-13b-full-20k (b) with smoothness.\nFigure 8: Training loss comparison of WizardLM data point distributions for different dataset sizes\nusing cross entropy and adam on llama-1-13b models: 10k uniform (maximized distance) dataset,\n10k random dataset, 20k full dataset.\n5.4 P ERFORMANCE AND TRAINING TIMECOMPARISON\nEvaluation Performance with Diverse Subsets. Figures 9 and 10 present evaluation results of\nLLaMA-1-13B on the ARC Challenge (Clark et al., 2018) and TruthfulQA MC benchmarks (Lin\net al., 2021), using datasets from TeaMs-RL and WizardLM, respectively. In both settings, models\ntrained on data selected via maximum pairwise distance (orange bars) consistently outperform those\ntrained on random subsets (green) and the original baselines (blue). Notably, the 4.5k and 10k uni-\nform subsets achieve performance comparable to or slightly better than the full datasets (red) while\n14\n\nOverall ARC Challenge TruthfulQA MC0102030405060Accuracy (%)39.3853.07\n25.7047.5258.19\n36.8447.3359.30\n35.3748.5859.22\n37.94Base Model\n4.5k Uniform\n4.5k Random\n9k FullFigure 9: ARC and MC performance on Llama-1-13b using TeaMs-RL diverse datasets.\nOverall ARC Challenge TruthfulQA MC0102030405060Accuracy (%)39.3853.07\n25.7043.2056.91\n29.5043.0955.46\n30.7243.6356.66\n30.60Base Model\n10k Uniform\n10k Random\n20k Full\nFigure 10: ARC and MC performance on Llama-1-13b using WizardLM diverse datasets.\nusing significantly fewer samples. This is particularly evident on the ARC Challenge task (e.g., Fig-\nure 11), where the uniform subsets closely match or exceed the accuracy of the full datasets. These\nresults further validate the effectiveness of uniformity-based data selection in improving model gen-\neralization and sample efficiency for LLM training.\nTime to Threshold: Faster Convergence with Uniform Data Selection. As shown in Figures11\n(a) and (b), we compare the convergence efficiency of three data selection strategies—Uniform, Ran-\ndom, and Full, under different loss thresholds using LLaMA-1-7B and LLaMA-1-13B. All times are\nmeasured based on wall-clock time using a single A100 GPU. For LLaMA-1-7B, Uniform reaches\nthe 0.34 and 0.65 loss thresholds in 116.1 and 21.3 minutes, respectively, outperforming Random\n(197.4, 111.7) and Full (231.3, 148.8). Similarly, LLaMA-1-13B trained on the 10k Uniform subset\nreaches the 0.31 and 0.6 thresholds in 148.2 and 46.6 minutes, significantly faster than Random\n(281.2, 139.6) and 20k Full (561.6, 280.0). These results highlight that Uniform selection not only\nreduces the dataset size but also accelerates training, indicating a more efficient optimization path\nand improved data quality.\n15\n\n0.34 0.65\nLoss Threshold Limit050100150200Time to Reach (minutes)116.1\n21.3197.4\n111.7231.3\n148.810k Uniform\n10k Random\n20k Full(a) LLaMA-1-7B.\n0.31 0.6\nLoss Threshold Limit0100200300400500Time to Reach (minutes)\n148.2\n46.6281.2\n139.6561.6\n280.010k Uniform\n10k Random\n20k Full\n(b) LLaMA-1-13B.\nFigure 11: Training time to reach loss thresholds using a 3-step moving average, measured on\na single A100 GPU. For both (a) LLaMA-1-7B (0.34, 0.65) and (b) LLaMA-1-13B (0.31, 0.6), we\ncompare the convergence speed of Uniform, Random, and Full data selection strategies. Uniform\nsampling consistently achieves faster convergence, indicating more efficient training.\n6 C ONCLUSION\nIn this paper, we demonstrate the importance and advantages of data uniformity, i.e., selecting more\nuniform data, both theoretically and empirically. On the theoretical side , we first show that when\nthe distribution is more uniformly distributed, the minimum pairwise distance between data points,\nhmin, becomes larger. We then prove that a smaller hmincan slow down the training dynamics\nunder GD, and lead to larger approximation error. To prove these results, we develop a conver-\ngence framework of neural networks, that is valid for a family of architectures that is analytic and\npolynomially smooth under ℓ2norm. We remark that it is possible to obtain finer quantification\n16\n\nof the polynomial generalized smoothness with respect to each θirather than the full parameter θ.\nMoreover, it is also possible to extend the analysis beyond ℓ2norm, which would allow additional\noperators, such as convolutions, to be incorporated into the framework. We will leave these direc-\ntions for future exploration. We also highlight the advantages of residual connections and function\ncompositions, which help preserve the expressivity of neural networks. This insight is supported\nby our theoretical analysis, particularly from the perspective of differential topology. In terms\nof experiments , we conducted extensive evaluations across multiple settings, including different\noptimization algorithms (L2-SGD and Adam), model sizes (LLaMA-1 7B and 13B), and datasets\n(TeaMs-RL and WizardLM). The results consistently show that selecting data by maximizing pair-\nwise distance significantly accelerates training and achieves comparable or even better performance\nthan full datasets—despite using fewer samples. This improvement is observed across both train-\ning loss and downstream evaluation tasks such as ARC and TruthfulQA. Our findings reinforce the\npractical value of data uniformity as an effective strategy for efficient and scalable LLM fine-tuning.\nACKNOWLEDGMENT\nWe would like to thank Laixi Shi, Yusu Wang, and Yijun Dong for useful discussions and refer-\nences. We also thank NVIDIA for generously providing the computational resources used in our\nexperiments. This work was partially done when YW was at the Simons Institute for the Theory of\nComputing.\nREFERENCES\nAlon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang,\nNiklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang,\nTatsunori Hashimoto, and William Yang Wang. A survey on data selection for language mod-\nels. Transactions on Machine Learning Research , 2024. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=XfHWcNTSHp . Survey Certification.\nZeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and\nself-distillation in deep learning. arXiv preprint arXiv:2012.09816 , 2020.\nZeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust\ndeep learning. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science\n(FOCS) , pp. 977–988. IEEE, 2022.\nZeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-\nparameterization. In International Conference on Machine Learning , pp. 242–252. PMLR, 2019.\nPaul Eric Ammann and John C Knight. Data diversity: An approach to software fault tolerance.\nIeee transactions on computers , 37(4):418–425, 2002.\nAlexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning polynomials with\nneural networks. In International conference on machine learning , pp. 1908–1916. PMLR, 2014.\nJimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-\ndimensional asymptotics of feature learning: How one gradient step improves the representation.\nAdvances in Neural Information Processing Systems , 35:37932–37946, 2022.\nBlake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution\nin wide neural networks. Advances in Neural Information Processing Systems , 35:32240–32256,\n2022.\nBlake Bordelon and Cengiz Pehlevan. Dynamics of finite width kernel and prediction fluctuations in\nmean field neural networks. Advances in Neural Information Processing Systems , 36:9707–9750,\n2023.\nL´eon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of\nCOMPSTAT’2010: 19th International Conference on Computational StatisticsParis France, Au-\ngust 22-27, 2010 Keynote, Invited and Contributed Papers , pp. 177–186. Springer, 2010.\n17\n\nJames H Bramble and SR Hilbert. Estimation of linear functionals on sobolev spaces with applica-\ntion to fourier transforms and spline interpolation. SIAM Journal on Numerical Analysis , 7(1):\n112–124, 1970.\nYongqiang Cai. Achieve the minimum width of neural networks for universal approximation. arXiv\npreprint arXiv:2209.11395 , 2022.\nYuan Cao, Zixiang Chen, Mikhail Belkin, and Quanquan Gu. Benign overfitting in two-layer con-\nvolutional neural networks. arXiv preprint arXiv:2202.06526 , 2022.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan\nYi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language models. ACM\ntransactions on intelligent systems and technology , 15(3):1–45, 2024.\nZhengdao Chen, Eric Vanden-Eijnden, and Joan Bruna. On feature learning in neural networks with\nglobal convergence guarantees. arXiv preprint arXiv:2204.10782 , 2022.\nZixiang Chen, Yuan Cao, Quanquan Gu, and Tong Zhang. A generalized neural tangent kernel\nanalysis for two-layer neural networks. Advances in Neural Information Processing Systems , 33:\n13363–13373, 2020a.\nZixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is suf-\nficient to learn deep relu networks? In International Conference on Learning Representations ,\n2020b.\nZixiang Chen, Greg Yang, Qingyue Zhao, and Quanquan Gu. Global convergence and rich fea-\nture learning in l-layer infinite-width neural networks under µp parametrization. arXiv preprint\narXiv:2503.09565 , 2025.\nLenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-\nparameterized models using optimal transport. Advances in neural information processing sys-\ntems, 31, 2018.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457 , 2018.\nCorinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. L2 regularization for learning kernels.\narXiv preprint arXiv:1205.2653 , 2012.\nMark De Berg. Computational geometry: algorithms and applications . Springer Science & Business\nMedia, 2000.\nTim De Ryck, Samuel Lanthaler, and Siddhartha Mishra. On the approximation of functions by tanh\nneural networks. Neural Networks , 143:732–750, 2021.\nYijun Dong, Viet Hoang Phan, Xiang Pan, and Qi Lei. Sketchy moment matching: Toward fast and\nprovable data selection for finetuning. Advances in Neural Information Processing Systems , 37:\n43367–43402, 2024.\nSimon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global\nminima of deep neural networks. In International conference on machine learning , pp. 1675–\n1685. PMLR, 2019.\nBenjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable\ncreation in self-attention mechanisms. In International Conference on Machine Learning , pp.\n5793–5831. PMLR, 2022.\nEtienne Emmrich. Discrete versions of Gronwall’s lemma and their application to the numerical\nanalysis of parabolic problems . Techn. Univ., 1999.\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need:\nLearning skills without a reward function. In International Conference on Learning Representa-\ntions , 2019. URL https://openreview.net/forum?id=SJx63jRqFm .\n18\n\nCong Fang, Jason Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field\nframework for over-parameterized deep neural networks. In Conference on learning theory , pp.\n1887–1936. PMLR, 2021.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang\nSutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model\nevaluation harness, 07 2024. URL https://zenodo.org/records/12608602 .\nZhiqiang Gong, Ping Zhong, and Weidong Hu. Diversity in machine learning. Ieee Access , 7:\n64323–64350, 2019.\nHans Grauert. On levi’s problem and the imbedding of real-analytic manifolds. Annals of Mathe-\nmatics , 68(2):460–472, 1958.\nShangding Gu, Alois Knoll, and Ming Jin. Teams-rl: Teaching llms to generate better instruction\ndatasets via reinforcement learning. Transactions on Machine Learning Research , 2024.\nIngo G ¨uhring, Gitta Kutyniok, and Philipp Petersen. Error bounds for approximations with deep\nrelu neural networks in w s, p norms. Analysis and Applications , 18(05):803–859, 2020.\nQiyang Han and Masaaki Imaizumi. Precise gradient descent training dynamics for finite-width\nmulti-layer neural networks. arXiv preprint arXiv:2505.04898 , 2025.\nBoris Hanin and Mark Sellke. Approximating continuous functions by relu nets of minimal width.\narXiv preprint arXiv:1710.11278 , 2017.\nMoritz Hardt and Tengyu Ma. Identity matters in deep learning. arXiv preprint arXiv:1611.04231 ,\n2016.\nMorris W Hirsch. Differential topology , volume 33. Springer Science & Business Media, 2012.\nJohannes Hofmanninger, Forian Prayer, Jeanny Pan, Sebastian R ¨ohrich, Helmut Prosch, and Georg\nLangs. Automatic lung segmentation in routine imaging is primarily a data diversity problem, not\na methodology problem. European radiology experimental , 4:1–13, 2020.\nKaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize\nbetter than deep feedforward networks?—a neural tangent kernel perspective. Advances in neural\ninformation processing systems , 33:2698–2709, 2020.\nArthur Jacot, Franck Gabriel, and Cl ´ement Hongler. Neural tangent kernel: Convergence and gen-\neralization in neural networks. Advances in neural information processing systems , 31, 2018.\nZiwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve ar-\nbitrarily small test error with shallow relu networks. In International Conference on Learning\nRepresentations , 2019.\nHaotian Jiang and Qianxiao Li. Approximation rate of the transformer architecture for sequence\nmodeling. Advances in Neural Information Processing Systems , 37:68926–68955, 2024.\nTokio Kajitsuka and Issei Sato. Are transformers with one layer self-attention using low-rank weight\nmatrices universal approximators? arXiv preprint arXiv:2307.14023 , 2023.\nPatrick Kidger and Terry Lyons. Universal approximation with deep narrow networks. In Conference\non learning theory , pp. 2306–2327. PMLR, 2020.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 , 2014.\nAnastasis Kratsios, Behnoosh Zamanlooy, Tianlin Liu, and Ivan Dokmani ´c. Universal approxima-\ntion under constraints is possible with transformers. arXiv preprint arXiv:2110.03303 , 2021.\nDavid Krieg and Mathias Sonnleitner. Random points are optimal for the approximation of sobolev\nfunctions. IMA Journal of Numerical Analysis , 44(3):1346–1371, 2024.\n19\n\nDavid Krieg, Erich Novak, and Mathias Sonnleitner. Recovery of sobolev functions restricted to iid\nsampling. Mathematics of Computation , 91(338):2715–2738, 2022.\nKrzysztof Kurdyka and Laurentiu Paunescu. Hyperbolic polynomials and multiparameter real-\nanalytic perturbation theory. 2008.\nJaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-\nDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models\nunder gradient descent. Advances in neural information processing systems , 32, 2019.\nHaochuan Li, Jian Qian, Yi Tian, Alexander Rakhlin, and Ali Jadbabaie. Convex and non-convex\noptimization under generalized smoothness. Advances in Neural Information Processing Systems ,\n36:40238–40271, 2023.\nHongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approxi-\nmator. Advances in neural information processing systems , 31, 2018.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 , 2021.\nTianyi Liu, Minshuo Chen, Mo Zhou, Simon S Du, Enlu Zhou, and Tuo Zhao. Towards understand-\ning the importance of shortcut connections in residual networks. Advances in neural information\nprocessing systems , 32, 2019.\nJianfeng Lu, Zuowei Shen, Haizhao Yang, and Shijun Zhang. Deep network approximation for\nsmooth functions. SIAM Journal on Mathematical Analysis , 53(5):5465–5506, 2021.\nShengjie Luo, Shanda Li, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. Your transformer\nmay not be as powerful as you expect. Advances in Neural Information Processing Systems , 35:\n4301–4315, 2022.\nAshwin Machanavajjhala, Daniel Kifer, Johannes Gehrke, and Muthuramakrishnan Venkitasubra-\nmaniam. l-diversity: Privacy beyond k-anonymity. Acm transactions on knowledge discovery\nfrom data (tkdd) , 1(1):3–es, 2007.\nAndrzej Ma ´ckiewicz and Waldemar Ratajczak. Principal components analysis (pca). Computers &\nGeosciences , 19(3):303–342, 1993.\nAnqi Mao, Mehryar Mohri, and Yutao Zhong. Cross-entropy loss functions: Theoretical analysis\nand applications. In International conference on Machine learning , pp. 23803–23828. PMLR,\n2023.\nBoris Mityagin. The zero set of a real analytic function. arXiv preprint arXiv:1512.07276 , 2015.\nAbdellatif Moudafi. A remark on the convergence of the tikhonov regularization without mono-\ntonicity. MATHEMATICAL INEQUALITIES AND APPLICATIONS , 7:283–288, 2004.\nNiklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke\nZettlemoyer, Percy Liang, Emmanuel Cand `es, and Tatsunori Hashimoto. s1: Simple test-time\nscaling. arXiv preprint arXiv:2501.19393 , 2025.\nThanh Nguyen-Tang and Raman Arora. On sample-efficient offline reinforcement learning: Data\ndiversity, posterior sampling and beyond. Advances in neural information processing systems , 36:\n61115–61157, 2023.\nAnh Nguyen-Tuong, David Evans, John C Knight, Benjamin Cox, and Jack W Davidson. Security\nthrough redundant data diversity. In 2008 IEEE International Conference on Dependable Systems\nand Networks With FTCS and DCC (DSN) , pp. 187–196. IEEE, 2008.\nSamet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global con-\nvergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in\nInformation Theory , 1(1):84–105, 2020.\nSejun Park, Chulhee Yun, Jaeho Lee, and Jinwoo Shin. Minimum width for universal approximation.\narXiv preprint arXiv:2006.08859 , 2020.\n20\n\nZhen Qin, Jinxin Zhou, and Zhihui Zhu. On the convergence of gradient descent on learning trans-\nformers with residual connections. arXiv preprint arXiv:2506.05249 , 2025.\nTrout Rader. Nice demand functions. Econometrica: Journal of the Econometric Society , pp. 913–\n935, 1973.\nGrant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems:\nAsymptotic convexity of the loss landscape and universal scaling of the approximation error.\nstat, 1050:22, 2018.\nKamron Saniee. A simple expression for multivariate lagrange interpolation. SIAM undergraduate\nresearch online , 1(1):1–9, 2008.\nMichael Scholkemper, Xinyi Wu, Ali Jadbabaie, and Michael T Schaub. Residual connections and\nnormalization can provably prevent oversmoothing in gnns. arXiv preprint arXiv:2406.02997 ,\n2024.\nGuohao Shen, Yuling Jiao, Yuanyuan Lin, and Jian Huang. Approximation with cnns in sobolev\nspace: with applications to classification. Advances in neural information processing systems , 35:\n2876–2888, 2022.\nZhenmei Shi, Junyi Wei, and Yingyu Liang. A theoretical analysis on feature learning in neural\nnetworks: Emergence from inputs and advantage over fixed features. In International Conference\non Learning Representations , 2021.\nJustin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A law of\nlarge numbers. SIAM Journal on Applied Mathematics , 80(2):725–752, 2020.\nMei Song, Andrea Montanari, and P Nguyen. A mean field view of the landscape of two-layers\nneural networks. Proceedings of the National Academy of Sciences , 115(33):E7665–E7671, 2018.\nZhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff bound.\narXiv preprint arXiv:1906.03593 , 2019.\nPaulo Tabuada and Bahman Gharesifard. Universal approximation power of deep residual neural\nnetworks through the lens of control. IEEE Transactions on Automatic Control , 68(5):2715–\n2728, 2022.\nMatus Telgarsky. Feature selection with gradient descent on two-layer networks in low-rotation\nregimes. arXiv preprint arXiv:2208.02789 , 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth ´ee\nLacroix, Baptiste Rozi `ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971 , 2023.\nYuqing Wang, Minshuo Chen, Tuo Zhao, and Molei Tao. Large learning rate tames homogeneity:\nConvergence and balancing effect. In International Conference on Learning Representations ,\n2022. URL https://openreview.net/forum?id=3tbDrs77LJ5 .\nYuqing Wang, Zhenghao Xu, Tuo Zhao, and Molei Tao. Good regularity creates large learning rate\nimplicit biases: edge of stability, balancing, and catapult. arXiv preprint arXiv:2310.17087 , 2023.\nDavid S Watkins. A generalization of the bramble-hilbert lemma and applications to multivariate\ninterpolation. Journal of approximation theory , 26(3):219–231, 1979.\nColin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and\noptimization of neural nets vs their induced kernel. Advances in Neural Information Processing\nSystems , 32, 2019.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei\nLin, and Daxin Jiang. WizardLM: Empowering large pre-trained language models to follow\ncomplex instructions. In The Twelfth International Conference on Learning Representations ,\n2024. URL https://openreview.net/forum?id=CfXh93NDgH .\n21\n\nGreg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint\narXiv:2011.14522 , 2020.\nYu Yu, Shahram Khadivi, and Jia Xu. Can data diversity enhance learning generalization? In\nProceedings of the 29th international conference on computational linguistics , pp. 4933–4945,\n2022.\nWing Ho Yuen, Roy D Yates, and S-C Mau. Exploiting data diversity and multiuser diversity in non-\ncooperative mobile infostation networks. In IEEE INFOCOM 2003. Twenty-second Annual Joint\nConference of the IEEE Computer and Communications Societies (IEEE Cat. No. 03CH37428) ,\nvolume 3, pp. 2218–2228. IEEE, 2003.\nChulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.\nAre transformers universal approximators of sequence-to-sequence functions? arXiv preprint\narXiv:1912.10077 , 2019.\nJingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates\ntraining: A theoretical justification for adaptivity. arXiv preprint arXiv:1905.11881 , 2019.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. arXiv\npreprint arXiv:2303.18223 , 1(2), 2023.\nDifan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural\nnetworks. Advances in neural information processing systems , 32, 2019.\nDifan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-\nparameterized deep relu networks. Machine Learning , 109(3):467–492, 2020.\n22\n\nAPPENDIX\nA P RELIMINARY PROPERTIES AND ASSUMPTION JUSTIFICATION\nA.1 A NALYTIC FUNCTION\nReal-analyticity is preserved under many operations. Specifically:\nProposition 1. Real-analytic functions have the following properties:\n1. The sums, products, divisions where the denominators are not zero, and compositions of real-\nanalytic functions are real-analytic.\n2. The derivative and integral of real-analytic functions are real-analytic.\n3. Real-analytic function is C∞.\nWe then prove that a lot of neural network architectures are real-analytic.\nProof of Corollary 1. Since exponential function admits Taylor series in some neighbourhood of\nany point in Rd, and the denominators of softmax and sigmoid functions are strictly positive at any\npoint, by Proposition 1, we have that softmax, sigmoid, GELU, SiLU are analytic functions on Rd.\nSimilarly, polynomial functions admit Taylor series in some neighbourhood of any point in Rd, and\nthe denominator of τϵis strictly positive at any point, we have τϵand polynomial functions are\nanalytic.\nAlso, tanh can be Taylor expanded at any point and thus analytic.\nBy Proposition 1, any product, sum, compositions of the above functions are still analytic.\nA.2 P OLYNOMIAL BOUNDEDNESS ,POLYNOMIAL GENERALIZED CONTINUITY AND\nSMOOTHNESS\nPolynomial boundedness, polynomial generalized continuity and smoothness are preserved under\nthe following operations:\nProposition 2. Assume fandgsatisfies polynomial boundedness for both the functions and their\ngradients, polynomial generalized continuity, and polynomial generalized smoothness. Then\n1.f(x)+g(x), where f, g∶Rd→Rm,\n2.f(x)g(x), where f∈Rm×d, and g∈Rd×n,\n3.(f○g)(x), where f∶Rd→Rm, and g∶Rn→Rd,\nsatisfy the same properties under different polynomials.\nProof. LetSf,0, Sg,0be the corresponding polynomials in polynomial generalized continuity of f\nandg,Sf,1, Sg,1be the ones in polynomial generalized smoothness, Sf, Sgbe the corresponding\npolynomials in polynomial bound, and S∇f, S∇gbe the corresponding polynomials in polynomial\nbound of their gradients. For notational simplicity, we use S(x)to denote S(∥x1∥,⋯,∥xd∥), and\nS(xmax)to denote S(∥xmax,1∥,⋯,∥xmax,d∥).\nThen\n∥f(x)+g(x)−f(x′)−g(x′)∥≤∥f(x)−f(x′)∥+∥g(x)−g(x′)∥≤(Sf,0+Sg,0)(xmax)∥x−x′∥.\nSimilarly,∇f+∇gfollows the same idea with (Sf,1+Sg,1)(xmax). Moreover, f+gis upper\nbounded by Sf(xmax)+Sg(xmax), and∇f+∇gis upper bounded by S∇f(xmax)+S∇g(xmax).\n23\n\nAlso,\n∥f(x)g(x)−f(x′)g(x′)∥=∥f(x)g(x)−f(x)g(x′)+f(x)g(x′)−f(x′)g(x′)∥\n≤∥f(x)∥∥g(x)−g(x′)∥+∥f(x)−f(x′)∥∥g(x′)∥\n≤(Sf(x)Sg,0(xmax)+Sf,0(xmax)Sg(x′))∥x−x′∥\n≤(Sf(xmax)Sg,0(xmax)+Sf,0(xmax)Sg(xmax))∥x−x′∥.\nObviously, (Sf(xmax)Sg,0(xmax)+Sf,0(xmax)Sg(xmax))is a polynomial with positive coeffi-\ncients, and therefore fgsatisfies polynomial generalized continuity.\nBased on the above derivation, ∇(fg)=∇f⋅g+f∇gis a sum of two product, and thus satisfies\npolynomial continuity with (S∇fSg,0+Sf,1Sg+S∇gSf,0+Sg,1Sf).\nAlso, fgis upper bounded by SfSg, and∇(fg)=∇f⋅g+f∇gis upper bounded by S∇fSg+S∇gSf.\nNext, since all the Shave positive coefficients, we have\n∥f(g(x))−f(g(x′))∥≤Sf,0(⋯,∥g(x)max,i∥,⋯)∥g(x)−g(x′)∥\n≤Sf,0(Sg(xmax),⋯, Sg(xmax))Sg,0(xmax)∥x−x′∥.\nAlso,∇xf(g(x))=∇gf(g(x))∇g(x). Similar to the above derivations, we have\n∇xf(g(x))is polynomial continuous with Sf,1(Sg(xmax),⋯, Sg(xmax))Sg,0(xmax)S∇g(xmax)+\nS∇f(Sf(xmax),⋯, Sf(xmax))Sg,1(xmax).\nIn the end, it can be shown that f(g(x))is upper bounded by Sf(Sg(xmax),⋯, Sg(xmax)), and\n∇f(g(x))is upper bounded by S∇g(xmax)S∇f(Sg(xmax),⋯, Sg(xmax)).\nCorollary 4. The functions, polynomials, softmax, tanh, sigmoid, τϵ, GELU, SiLU, and the prod-\nuct, sum, compositions of them, satisfies polynomial boundedness for both the functions and their\ngradients, polynomial generalized continuity, and polynomial generalized smoothness.\nProof. By Proposition 2, we only need to show that polynomials, softmax, tanh, sigmoid, τϵ, GELU,\nSiLU satisfy all the properties.\nFirst, polynomials obviously satisfies all the properties. It can be shown by simple calculations that\nsoftmax, tanh, sigmoid, τϵ, and their derivatives are all upper bounded by constant, and therefore\nsatisfies all the above properties.\nFor SiLU, it is the product of xand sigmoid, and therefore by Proposition 2 satisfies all the above\nproperties.\nFor GELU, σ(x)=xΦ(x), where Φ(x)=∫x\n−∞e−s2/2\n√\n2πds. Also, Φ′(x)=ϕ(x)=e−x2/2\n√\n2π. Then\nΦ(x)andΦ′(x)are both upper bounded by constants, and thus satisfies all the above properties. By\nProposition 2, GELU, the product of xandΦ(x), also satisfies these properties.\nSimilar to Lipschitz smoothness and generalized smoothness (Li et al., 2023), the polynomial gen-\neralized smoothness has another interpretation as follows:\nLemma 2. If the function f(x)∈C1satisfies the polynomial generalized smoothness, then the\nfollowing inequalities hold\nf(x′)≤f(x)+∇f(x)⊺(x′−x)+S(∥xmax,1∥,⋯,∥xmax,d∥)\n2∥x−x′∥2(4)\nProof. Since f(x)∈C1satisfies the polynomial generalized smoothness, we have\n∥∇f(x)−∇f((1−t)x+tx′)∥≤S(⋯,max{∥xi∥,∥(1−t)xi+tx′\ni∥,⋯})∥x−((1−t)x+tx′)∥\n≤t S(⋯,max{∥xi∥,∥x′\ni∥},⋯)∥x−x′)∥\n=t S(∥xmax,1∥,⋯,∥xmax,d∥)∥x−x′∥\n24\n\nThen\nf(x′)−f(x)=∫1\n0∇f((1−t)x+tx′)⊺(x′−x)dt\n=∫1\n0∇f(x)⊺(x′−x)dt+∫1\n0(∇f((1−t)x+tx′)−∇f(x))⊺(x′−x)dt\n≤∇f(x)⊺(x′−x)+∥x−x′∥∫1\n0∥∇f((1−t)x+tx′)−∇f(x)∥dt\n≤∇f(x)⊺(x′−x)+S(∥xmax,1∥,⋯,∥xmax,d∥)∥x−x′∥2∫1\n0t dt\n=∇f(x)⊺(x′−x)+S(∥xmax,1∥,⋯,∥xmax,d∥)\n2∥x−x′∥2\nA.3 E XAMPLE OF ASSUMPTION 5\nAssumption 5 is used in the convergence of neural networks. Before introducing the convergence\nproof, we first verify that the Assumption 5 can be easily satisfied:\nProof of Lemma 1. First, we compute the Jacobian of the neural network w.r.t. θL−1,1\n∇θL−1,1f(θ;xi)\n=∇τϵφL(τϵ(uL,i))∇uτϵ(uL,i)∇θL−1,2¯φ(uL−1,i)\n=θL⎛\n⎝1√\n∥uL,i∥2+ϵ2I−1\n(√\n∥uL,i∥2+ϵ2)3uL,iu⊺\nL,i⎞\n⎠\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\nPi⎛\n⎝σ(θL−1,1uL−1,i)⊺\n⋱\nσ(θL−1,1uL−1,i)⊺⎞\n⎠\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\nQi\nwhere in∇θL−1,2¯φ(uL−1,i), we vectorize θL−1,2in row.\nAlso, σ(x)=xΦ(x), where Φ(x)=∫x\n−∞e−s2/2\n√\n2πds. Letϕ(x)=e−x2/2\n√\n2π. Then we have\nσ′(x)=Φ(x)+xϕ(x)\nσ(n)(x)=2ϕ(n−2)(x)+ϕ(n−1)(x)\n=(2(−1)n−2Hn−2(x)+(−1)n−1Hn−1(x))ϕ(x)\nwhere Hn(x)is the probabilist’s Hermite polynomial.\nBy Taylor expansion,\nσ(v⊺u+a)=∞\n∑\nn=0σ(n)(a)\nn!(v⊺u)n\nConsider σ(tθu)neart=0. Choose 0<t1<t2<⋯<tN≪1. For probabilist’s Hermite\npolynomials,\nH2n(0)≠0, H2n−1(0)=0,∀n≥1.\nThus,\nσ(0)=0, σ(n)(0)≠0,∀n≥1.\nLetθL−1,1,jbe the jth row of θL−1,1. Then\n⎛\n⎝σ(t1θL−1,1,ju)\n⋮\nσ(tNθL−1,1,ju)⎞\n⎠=⎛\n⎜⎜\n⎝∑∞\nn=0σ(n)(0)\nn!(t1θL−1,1,ju)n\n⋮\n∑∞\nn=0σ(n)(0)\nn!(tNθL−1,1,ju)n⎞\n⎟⎟\n⎠=N\n∑\nn=1σ(n)(0)\nn!(θL−1,1,ju)n⎛\n⎝tn\n1\n⋮\ntn\nN⎞\n⎠+O⎛\n⎜\n⎝tN+1\n1\n⋮\ntN+1\nN⎞\n⎟\n⎠\n25\n\nNote\ndet⎛\n⎜\n⎝t1⋯tN\n1\n⋮ ⋮\ntN⋯tN\nN⎞\n⎟\n⎠=N\n∏\ni=1ti∏\n1≤j<k≤N(tk−tj)>0.\nThus the Nvectors⎛\n⎝tn\n1\n⋮\ntn\nN⎞\n⎠are linearly independent.\nWe then choose us.t.θL−1,1uhas at least Ndistinct non-zero elements. We claim that the set of all\nθL−1,1such that there is no us.t.θL−1,1uhas at least Ndistinct non-zero elements, is measure-zero\ninR∣θL−1,1. We denote the set to be BθL−1,1.\nConsider v∈Rmin the column space of θL−1,1. Fix a partition of melements into at most N−1\nblocks, where in each block of vis constant. Such set of vectors is of dimensional N−1. Moreover,\nthe number of such partition is finite. Therefore, Leb∣θL−1,1∣(BθL−1,1)=0.\nWOLG, assume the first Nelements are distinct. Then, similarly, we have that the Nvectors\n⎛\n⎝(θL−1,1,1u)n\n⋮\n(θL−1,1,Nu)n⎞\n⎠are linearly independent. Thus\ndet⎛\n⎜⎜\n⎝σ(1)(0)\n1!(θL−1,1,1u)⋯σ(N)(0)\nN!(θL−1,1,1u)N\n⋮ ⋮\nσ(1)(0)\n1!(θL−1,1,Nu)⋯σ(N)(0)\nN!(θL−1,1,Nu)N⎞\n⎟⎟\n⎠\n=N\n∏\ni=1σ(i)(0)\ni!det⎛\n⎜\n⎝(θL−1,1,1u)⋯(θL−1,1,1u)N\n⋮ ⋮\n(θL−1,1,Nu)⋯(θL−1,1,Nu)N⎞\n⎟\n⎠≠0.\nTherefore,⎛\n⎝σ(t1θL−1,1u)⊺\n⋮\nσ(tNθL−1,1u)⊺⎞\n⎠is full rank N, and then⎛\n⎝Q1\n⋮\nQN⎞\n⎠is full rank Nd.\nAlso, since Piis invertible for any uL,i, we have⎛\n⎝B1\n⋱\nBN⎞\n⎠is invertible. Thus\n⎛\n⎜\n⎝∇τϵφL(τϵ(uL,1))∇uτϵ(uL,1)∇θL−1,2¯φ(uL−1,1)\n⋮\n∇τϵφL(τϵ(uL,N))∇uτϵ(uL,N)∇θL−1,2¯φ(uL−1,N)⎞\n⎟\n⎠=⎛\n⎝B1\n⋱\nBN⎞\n⎠⎛\n⎝Q1\n⋮\nQN⎞\n⎠\nis full rank Nd, where uL−1,i=tiu.\nB M INIMUM DISTANCE AND BIASED DISTRIBUTION :PROOF OF THEOREM 1\nWe now present the complete version of Theorem 1.\nTheorem 4. Suppose Assumption 1 holds.\n1. For any biased distribution such that there exists a ball B(C(−logδ\n¯πmax(N−1)Vd)1/d\n)⊆Ωmax, and\nπmax\n¯πmax=O(1), we have with probability at least 1−2δ,\n(2δ\nπmaxN(N−1)Vd)1/d\n≤hmin≤C(−logδ\n¯πmax(N−1)Vd)1/d\nwhere Vd=πd/2\nΓ(d/2+1)is the volumn of d-dimensional unit ball, and C>0is some universal\nconstant.\n2. For general distribution, we have with probability at least 1−2δ,\n(2δ\nπmaxN(N−1)Vd)1/d\n≤hmin≤(−logδ\nπmin(N−1)Vd)1/d\n.\n26\n\nProof. First fix any point xi=x. For this proof, we consider some small enough r∈(0,1), and then\nπminVdrd≤P(∥xj−xi∥<r∣xi=x)=∫B(x,r)π(z)dz≤πmaxVdrd(5)\nwhere Vd=πd/2\nΓ(d/2+1)is the volumn of d-dimensional unit ball.\nWe would like to lower bound\nP(hmin≥r)=P(∥xi−xj∥≥r,∀1≤i<j≤N)\n=E[ 1{∥xi−xj∥≥r,∀1≤i<j≤N}]\n=E[ 1{∥x1−x2∥≥r} 1{∥x3−xi∥≥r,∀i=1,2}⋯ 1{∥xN−xi∥≥r,∀i=1,⋯,N−1}]\n=E[E[ 1{∥x1−x2∥≥r}∣x1]⋅E[ 1{∥x3−xi∥≥r,∀i=1,2}∣x1, x2]⋯E[ 1{∥xN−xi∥≥r,∀i=1,⋯,N−1}∣x1,⋯, xN−1]]\nwhere the last inequality follows from the properties of conditional expectation.\nAlso, we have\nE[ 1{∥x1−x2∥≥r}∣x1]=P(∥x1−x2∥≥r∣x1)\n≥1−πmaxVdrd\nwhere the inequality follows from (5), and similarly,\nE[ 1{∥xs−xi∥≥r,∀i=1,⋯,s−1}∣x1,⋯, xs−1]=P(∥xs−xi∥≥r,∀i=1,⋯, s−1∣x1,⋯, xs−1)\n≥1−(s−1)πmaxVdrd.\nTherefore, for some small r, we have\nP(hmin≥r)≥E[N−1\n∏\ni=1(1−i πmaxVdrd)]=N−1\n∏\ni=1(1−i πmaxVdrd)\n≥1−N−1\n∑\ni=1i πmaxVdrd=1−N(N−1)\n2πmaxVdrd.\nThus with probability at least 1−δ, we have\nhmin≥(2δ\nπmaxN(N−1)Vd)1/d\n.\nFor the other side, for biased distribution satisfying the assumptions stated in the theorem, we con-\nsider\nr≤2C(−logδ\n¯πmax(N−1)Vd)1/d\n, and Ωmax,B=B(r\n2)⊆Ωmax.\nThen,\nP(hmin≤r)=P(∃i, j, s.t.∥xi−xj∥≤r)\n≥P(∃at least two points xi, xj∈Ωmax,B, i.e.,∥xi−xj∥≤r)\n=1−P(xi∉Ωmax,B,∀i)−P(∃i, s.t.x i∈Ωmax,B, xj∉Ωmax,B,∀j≠i)\n≥1−((1−¯πmax∣Ωmax,B∣)N+N(1−¯πmax∣Ωmax,B∣)N−1πmax∣Ωmax,B∣)\n=1−(1−¯πmax∣Ωmax,B∣)N−1(1−¯πmax∣Ωmax,B∣+Nπmax∣Ωmax,B∣)\n≥1−e−(N−1)¯πmax∣Ωmax,B∣(1−¯πmax∣Ωmax,B∣+Nπmax∣Ωmax,B∣)\nwhere the last inequality follows from (1−x)N≤e−Nx.\nThen, we have with probability at least 1−δ,\nhmin≤C(−logδ\n¯πmax(N−1)Vd)1/d\n,\n27\n\nwhere this inequality follows from ∣Ωmax,B∣=Vd(r\n2)dandC>0is some universal constant.\nThe last conclusion follows from taking union bound of the above two results.\nFor general distribution,\nE[ 1{∥x1−x2∥≥r}∣x1]=P(∥x1−x2∥≥r∣x1)\n≤1−πminVdrd\nwhere the inequality follows from (5), and similarly,\nE[ 1{∥xs−xi∥≥r,∀i=1,⋯,s−1}∣x1,⋯, xs−1]=P(∥xs−xi∥≥r,∀i=1,⋯, s−1∣x1,⋯, xs−1)\n≤1−πminVdrd.\nTherefore, for some small r, we have\nP(hmin≥r)≤(1−πminVdrd)N−1\nThus with probability at least 1−δ, we have\nhmin<(1−δ1\nN−1\nπminVd)1/d\n≤(−logδ\nπminVd(N−1))1/d\nwhere the last inequality follows from 1−e−x≤xforx>0.\nC O PTIMIZATION :PROOF OF THEOREM 2AND COROLLARY 3\nWe recall that the architecture of neural network is defined as follows\nu0,i=xi\nuℓ+1,i=uℓ,i+¯φℓ(θ;uℓ,i), ℓ=0,⋯, L−1\nf(xi)=¯φL(θ;uL,i).\nThe loss function of each data point is\nl(f(xi), yi)=1\n2∥yi−f(xi)∥2\nand the total loss is\nL(f(xi), yi)=1\nNN\n∑\ni=1l(f(xi), yi).\nWe then compute the Jacobian of the following maps that will be used in the proof:\n∇fl(xi)=(f(xi)−yi)⊺\n∇θjf(xi)=∑\nℓ∈Jj∇u¯φL(uL,i)(Im+∇u¯φL−1(uL−1,i))⋯(Im+∇u¯φℓ+1(uℓ+1,i))∇θj¯φℓ(uℓ,i;θ)\nwhere∇u¯φℓ(uℓ)is the Jacobian of the map ¯φℓ(uℓ)w.r.t. uℓ,∇θℓf(xi)is also the Jacobian; the set\nJjdenotes the indices of layers in which θjappears.\nThen\n∇θjl(xi)=∇fl(xi)∇θℓf(xi)\n=∑\nℓ∈Jj∇fl(xi)∇u¯φL(uL,i)(Im+∇u¯φL−1(uL−1,i))⋯(Im+∇u¯φℓ+1(uℓ+1,i))∇θj¯φℓ(uℓ,i;θ)\nand we have\n∇θjL=1\nNN\n∑\ni=1∇θjl(xi).\n28\n\nIn this section, we also define the following function for any matrix function M(x)∈Rn×mwith\nm≥n\ndetr(M(x))=(m\nn)\n∑\ni=1(detMi(x))2,\nwhere Mi(x)∈Rn×nis the matrix with ncolumns of M(x). Then detr(M(x))≠0if and only if\nM(x)is full rank at x.\nThe following is a more complete version of the convergence result in Theorem 2, combining Corol-\nlary 3.\nTheorem 5. Suppose Assumptions 1 to 5 hold. Consider the initialization θ0inRdimθexcept\nfor a measure-zero set. Then, under some arbitrarily small adjustment on the scale of φℓforℓ=\n0,⋯, L−1, with probability 1 over the joint distribution π×⋯× πof the input data x1,⋯, xN, we\nhave the following results:\n1. When ηis in some dense set of (0,min{2−δ\nL1,1}), where\nL1=S⎛\n⎜\n⎝⋯,∥θ0\ni∥+√\n2C\nδL(θ0)+η max\n∥θ∥≤∥θ0∥+√\n2C\nδL(θ0)∥∇θiL(θ)∥,⋯)⎞\n⎟\n⎠\nfor some polynomial S(⋅)with positive coefficients, we have\nL(θk)≤k−1\n∏\ns=0(1−η(1−ηL1\n2)µlow,s,X\nN)L(θ0),∀k≤T=C\nη,\nwhere µlow,s,X>0is some strictly positive constant depending on θsandx1,⋯, xN.\n2. Furthermore, let R=√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0)for some stationary point θ∗. Assume L(θ)\nsatisfies the (θ0, θ∗, R, ρ, ϵ L)-dissipative condition in Definition 6. When ηis in some dense set\nof(0,min{2−δ\nL2,1}), where\nL2=S(⋯,∥θ∗\ni∥+R+max\nθ∈Bθ∗(R)∥∇θiL(θ)∥,⋯),\nGD will converge to ∥∇L(θ)∥≤ϵL, and\nL(θk)≤k−1\n∏\ns=0(1−η(1−ηL2\n2)µlow,s,X\nN)L(θ0),∀k≥1and∥∇L(θk)∥>ϵL.\n3. Fix xi, and let Di,H={j∣hij=∥xi−xj∥≤H,∀j≠i}. Then for both the above cases, for any\nk≥1, there exists rk,i,H>0andLk,i,H>0, s.t., when√\n∑j∈Di,Hh2\nij≤rk,i,H , we have\nµlow,s,X≤Lk,i,H√\n∑\nj∈Di,Hh2\nij,∀s≤k.\nSpecifically, if H=hminandDi,H≠∅, there exists Lk,i, rk,i>0, s.t., when hmin≤rk,i, we have\nµlow,s,X≤Lk,ihmin,∀s≤k.\nProof. Define the GD iteration map to be\nΨ(θ)=θ−η∇θL(θ),andΨk(θ)=Ψ○⋯○ Ψ\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\nk(θ).\nFirst, consider fixed x1,⋯, xN. Let\nBθ,rank={θ∶∇θΨ(θ)is not full rank }\n29\n\nConsider applying GD, where ηis in a dense set of (0,min{2\nL,1}). By Lemma 10, we know\nLebdimθ(Bθ,rank)=0.\nNow consider any measure-zero set Bθ,0inRdimθ. By Theorem 8,\nLebdimθ(Ψ−1(Bθ,0))=0.\nNext we would like to prove that (Ψk)−1(Bθ,0)is measure-zero for any k≥1. Suppose\n(Ψk)−1(Bθ,0)is measure-zero. We would like to show that (Ψk+1)−1(Bθ,0)is measure zero. This\nis obvious since (Ψk+1)−1(Bθ,0)=Ψ−1((Ψk)−1(Bθ,0)).\nLetBθ,0be the union of all the bad measure-zero sets of θin Assumption 5 and Lemmas 4, including\nBθ,rank, and there are finite union of such sets, which implies Lebdimθ(Bθ,0)=0. Let\nBθ=∞\n⋃\nk=1(Ψk)−1(Bθ,0).\nThen\nLebdimθ(Bθ)=Lebdimθ(∞\n⋃\nk=1(Ψk)−1(Bθ,0))≤∞\n∑\nk=10=0,\nnamely, the initial condition set of θwhere there exists some iteration ks.t.θk∈Bθ,0, is measure-\nzero. In the rest of the proof, we just consider θ0∈Rdimθ/Bθ.\nNext we consider the bad set of (x1,⋯, xN). By Lemma 4, for any θ0, the bad set of (x1,⋯, xN),\ndenoted as BX,0, is measure-zero,\nLebNd(BX,0)=0.\nAlso, there are at most countable θkin the GD trajectory, and thus all such bad sets ⋃∞\nk=0BX,0\nsatisfy\nLebNd(∞\n⋃\nk=0BX,k)≤∞\n∑\nk=00=0.\nSince(x1,⋯, xN)∼π×⋯× π, and π≪Leb, we have\n(π×⋯× π)(BX)=0,\nnamely, with probability 0, (x1,⋯, xN)will be in this set.\nThen, consider some initial conditions θ0except for a measure-zero set, and consider some dataset\n{(xi, yi)}N\ni=1chosen with probability 1 over the joint distribution. By Lemma 8 and Lemma 2, we\nhave for i=1,2,\nL(θk+1)≤L(θk)+∇L(θk)⊺(θk+1−θk)+Sk\n2∥θk−θk+1∥2\n=L(θk)−η(1−ηSk\n2)∥∇L(θk)∥2\n≤(1−η(1−ηSk\n2)µlow,k,X\nN)L(θk)\n≤(1−η(1−ηLi\n2)µlow,k,X\nN)L(θk)\n≤k\n∏\ns=0(1−η(1−ηLi\n2)µlow,s,X\nN)L(θ0)\nwhere the second inequality follows from Lemma 4, the third inequality follows from Lemma 3 and\nS(a1,⋯, anθ)≤Lby the monotonicity of S(⋅)inR≥0×⋯× R≥0;µlow,k,X>0is some strictly\npositive constant depending on θkandx1,⋯, xN.\n30\n\nBy Lemma 3, when i=1, the above inequality holds for k+1≤T=C\nη. When i=2with\n(θ0, θ∗, R, ρ, ϵ L)-dissipative condition, it holds for any k≥1.\nThen, under (θ0, θ∗, R, ρ, ϵ L)-dissipative condition, GD will converge to the region ∥∇L(θ)∥≤ϵL.\nIf not, we have µlow,k,X>0because by Assumption 5 and Lemma 4, the θs.t.µlow,θ,X=0lies\nin some measure-zero set and we remove any finite-step convergence to this set at the beginning of\nthis proof. Thus (1−η(1−ηL2\n2)µlow,k,X\nN)<1, which means L(θk)will keep decreasing. Contra-\ndiction.\nThe last statement follows from Lemma 4, and take Lk,i,H=max s≤kLs,i,H andrk,i,H=\nmins≤krs,i,H .\nThe following lemma shows that the loss function is Lipschitz smooth along the GD iteration.\nLemma 3. Assume L(θ)satisfies polynomial generalized smoothness.\n1. Without (θ0, θ∗, R, ρ, ϵ L)-dissipative condition, let η≤min{2−δ\nL1,1}, where\nL1=S⎛\n⎜\n⎝⋯,∥θ0\ni∥+√\n2C\nδL(θ0)+ max\n∥θ∥≤∥θ0∥+√\n2C\nδL(θ0)∥∇θiL(θ)∥,⋯)⎞\n⎟\n⎠.\nThen\nSk≤L1,∀k≤T=C\nη.\n2. With (θ0, θ∗, R, ρ, ϵ L)-dissipative condition, let η≤min{2−δ\nL2,1}, where\nL2=S⎛\n⎜⎜\n⎝⋯,∥θ∗\ni∥+√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0)+ max\nθ∈Bθ∗(√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0))∥∇θiL(θ)∥,⋯⎞\n⎟⎟\n⎠.\nThen\nSk≤L2,∀k≥0and∥∇L(θk)∥≥ϵL.\nProof. First, note\nmax{∥θk\ni∥,∥θk+1\ni∥}≤∥θk\ni∥+η∥∇Lθi(θk)∥.\nBy the monotonicity of S(⋅), we only need to show ∥θk\ni∥is bounded under the two cases.\nWithout the dissipative assumption, we would like to show that\n∥θk\ni∥≤∥θ0\ni∥+√\n2C\nδL(θ0),∀k≤T=C\nη.\nSuppose the above inequality holds for all θj\ni, where j≤kandi=1,⋯, nθ. Then\nSk≤¯Sk=S(⋯,∥θk\ni∥+η∥∇θiL(θk)∥,⋯))\n≤L1=S(⋯,∥θ0\ni∥+√\n2C\nδL(θ0)+ max\n∥θ∥≤∥θ0∥+√\n2C\nδL(θ0)∥∇θiL(θ)∥,⋯))\nnamely,\nη≤2−δ\nL1≤2−δ\n¯Sk.\n31\n\nBy Lemma 8 and Lemma 2, we have\n0≤L(θk+1)≤L(θk)+∇L(θk)⊺(θk+1−θk)+Sk\n2∥θk−θk+1∥2\n=L(θk)−η(1−ηSk\n2)∥∇L(θk)∥2\n≤L(θ0)−ηk\n∑\nj=0(1−ηSj\n2)∥∇L(θj)∥2\n≤L(θ0)−δ\n2ηk\n∑\nj=0∥∇L(θj)∥2,\nnamely,\nk\n∑\nj=0∥∇L(θj)∥2≤2\nδη(L(θ0)−L(θk+1))≤2\nδηL(θ0). (6)\nThen\n∥θk+1\ni∥=∥θk\ni−η∇θiL(θk)∥=⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪θ0\ni−ηk\n∑\nj=0∇θiL(θj)⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪\n≤∥θ0\ni∥+ηk\n∑\nj=0∥∇L(θj)∥\n≤∥θ0\ni∥+η⌟roo⟪⟪op\n⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪kk\n∑\nj=0∥∇L(θj)∥2\n≤∥θ0\ni∥+√\n2\nδkηL(θ0)\n≤∥θ0\ni∥+√\n2C\nδL(θ0)\nwhere the second inequality follows from Cauchy-Schwartz inequality, the third inequality follows\nfrom (6), and last inequality follows from k≤T=C\nη.\nThe above derivation also holds for θk+1. Therefore Sk+1≤L1.\nNext, under the dissipative condition, we would like to show that\nθk\ni∈Bθ∗\ni⎛\n⎝√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0)⎞\n⎠, i.e.,∥θk\ni−θ∗\ni∥2≤∥θ0−θ∗∥2+4ρ+2\nδL(θ0)\nSuppose the above inequality holds for all j≤k. Then we have\nSk≤¯Sk=S(⋯,∥θk\ni∥+η∥∇θiL(θk)∥,⋯)\n≤L2=S(⋯,∥θ∗\ni∥+√\n∥θ0−θ∗∥2+4ρ+2\nδL(θ0)+ max\nθ∈Bθ∗(∥θ0−θ∗∥2+4ρ+2\nδL(θ0))∥∇θiL(θ)∥,⋯)\nnamely,\nη≤2−δ\nL2≤2−δ\n¯Sk.\n32\n\nThen, consider the k+1th iteration\n∥θk+1−θ∗∥2=∥θk−θ∗−η∇L(θk)∥2\n=∥θk−θ∗∥2−2η∇L(θk)⊺(θk−θ∗)+η2∥∇L(θk)∥2\n≤∥θk−θ∗∥2+η(2ρ+η)∥∇L(θk)∥2\n≤∥θ0−θ∗∥2+η(2ρ+η)k\n∑\nj=0∥∇L(θj)∥2\n≤∥θ0−θ∗∥2+4ρ+2η\nδL(θ0)\nwhere the first inequality follows from Definition 6, and the last inequality follows from (6), which\nalso holds true when L1is replaced by L2.\nThen together with η≤1and\n∥θk\ni∥≤∥θ∗\ni∥+∥θk\ni−θ∗\ni∥≤∥θ∗\ni∥+∥θk−θ∗∥,\nwe obtain Sk≤L2for all k.\nC.1 L OWER BOUND OF GRADIENT\nIn this section, we use subscripts on θandθL−1to denote their scalar elements.\nLemma 4. Under Assumptions 2 and 5, and some small adjustment of the scale of φℓforℓ=\n0,⋯, L−1, given fixed θexcept for a measure-zero set in Rdimθ, and X=(x1,⋯, xN)except for a\nmeasure-zero set in RNdthat depends on θ, the gradient satisfies the following inequality\n∥∇θL(θ;X)∥2≥µlow,θ,X\nNL(θ;X)\nwhere µlow,θ,X>0is a strictly positive constant depending on θandX.\nMoreover, fix xi, and let Di,H={j∣hij=∥xi−xj∥≤H,∀j≠i}. Then there exists rθ,i,H>0and\nLθ,i,H>0, s.t., when√\n∑j∈Di,Hh2\nij≤rθ,i,H , we have\nµlow,θ,X≤Lθ,i,H√\n∑\nj∈Di,Hh2\nij.\nSpecifically, if H=hminandDi,H≠∅, there exists Lθ,i, rθ,i>0, s.t., when hmin≤rθ,i, we have\nµlow,θ,X≤Lθ,ihmin.\nProof. Consider\n∥∇θL(θ;X)∥2=∥1\nNN\n∑\ni=1∇fl(xi)∇θf(xi)∥2\n≥1\nN2µlow,θ,XN\n∑\ni=1∥∇fl(xi)∥2=µlow,θ,X\nNL(θ;X)\nwhere the first inequality follows from Lemma 6, and µlow,θ,X>0is a constant depending on θand\nX. The second statement follows from Lemma 6.\nBefore detailed proofs of lemmas, we first introduce the definition of a frame as follows.\nDefinition 7 (Frame) .The set of vectors {ek}in an inner-product vector space Vis a frame of Vif\nthere exists constants 0<µlow≤µup<∞, s.t.,\nµlow∥v∥2≤∑\nk∣⟨v, ek⟩∣2≤µhigh∥v∥2,∀v∈V.\nThe frame operator T∶V→Vis defined as\nTv=∑\nk⟨v, ek⟩ek=(∑\nkeke⊺\nk)v.\n33\n\nThen the gradient of the network fat all data points forms a frame in RNd:\nLemma 5. Under Assumptions 2 and 5 and some small adjustment of the scale of φℓforℓ=\n0,⋯, L−1, for any fixed θexcept for a measure-zero set,⎧⎪⎪⎨⎪⎪⎩⎛\n⎝∇θif(x1)\n⋮\n∇θif(xN))⎞\n⎠⎫⎪⎪⎬⎪⎪⎭dimθ\ni=1form a frame except\nfor a measure-zero set of (x1,⋯, xN)inRNd.\nProof. By Assumption 5, there exists U=(u1,⋯, uN)∈RNd, θ, s.t.\ndetr⎛\n⎜\n⎝∇θL−1˜fL−1(θ;u1)\n⋮\n∇θL−1˜fL−1(θ;uN)⎞\n⎟\n⎠≠0,\nwhere ˜fℓ(θ;uℓ)=f(θ;x).\nBy Assumption 5 and Lemma 10, the preimage of 0 is a measure zero set in RNdunder some small\nadjustment of the scale of φℓ, i.e.,\nLebNd⎛\n⎜\n⎝⎧⎪⎪⎪⎨⎪⎪⎪⎩U∈RNd∶⎛\n⎜\n⎝∇θL−1˜fL−1(θ;u1)\n⋮\n∇θL−1˜fL−1(θ;uN)⎞\n⎟\n⎠is not full rank⎫⎪⎪⎪⎬⎪⎪⎪⎭⎞\n⎟\n⎠=0\nNext consider the map U=U(X)=⎛\n⎝uL−1(x1)\n⋮\nuL−1(xN)⎞\n⎠∶RNd→RNd. By Lemma 9,\nLebNd⎛\n⎜\n⎝⎧⎪⎪⎪⎨⎪⎪⎪⎩(x1,⋯, xN)∈RNd∶⎛\n⎜\n⎝∇θL−1˜fL−1(θ;u(x1))\n⋮\n∇θL−1˜fL−1(θ;u(xN))⎞\n⎟\n⎠is not full rank⎫⎪⎪⎪⎬⎪⎪⎪⎭⎞\n⎟\n⎠=0\nTherefore, for any fixed θexcept for a measure-zero set,⎧⎪⎪⎨⎪⎪⎩⎛\n⎝∇θif(x1)\n⋮\n∇θif(xN))⎞\n⎠⎫⎪⎪⎬⎪⎪⎭dimθ\ni=1forms a frame except\nfor a measure-zero set of (x1,⋯, xN).\nLemma 6. Under Assumption 5 and some small adjustment of the scale of φℓforℓ=0,⋯, L−1,\nconsider fixed θexcept for a measure-zero set. The lower frame bound µlow,θ,Xis strictly positive\nexcept for a measure-zero set of (x1,⋯, xN)inRNd.\nMoreover, fix xi, and let Di,H={j∣hij=∥xi−xj∥≤H,∀j≠i}. Then there exists rθ,i,H>0and\nLθ,i,H>0, s.t., when√\n∑j∈Di,Hh2\nij≤rθ,i,H , we have\nµlow,θ,X≤Lθ,i,H√\n∑\nj∈Di,Hh2\nij.\nSpecifically, if H=hminandDi,H≠∅, there exists Lθ,i, rθ,i>0, s.t., when hmin≤rθ,i, we have\nµlow,θ,X≤Lθ,ihmin.\nProof. LetA(θ, X)be the frame operator, i.e.,\nA(θ, X)=dimθ\n∑\ni=1⎛\n⎝∇θif(x1)\n⋮\n∇θif(xN)⎞\n⎠⎛\n⎝∇θif(x1)\n⋮\n∇θif(xN)⎞\n⎠⊺\n=⎛\n⎝∇θf(x1)\n⋮\n∇θf(xN)⎞\n⎠⎛\n⎝∇θf(x1)\n⋮\n∇θf(xN)⎞\n⎠⊺\nWe would like to bound λmin(A). Consider any fixed θexcept for a measure-zero set.\n34\n\nLet\nB(θ, X)=dimθL−1\n∑\ni=1⎛\n⎜\n⎝∇θL−1,if(x1)\n⋮\n∇θL−1,if(xN)⎞\n⎟\n⎠⎛\n⎜\n⎝∇θL−1,if(x1)\n⋮\n∇θL−1,if(xN)⎞\n⎟\n⎠⊺\n=dimθL−1\n∑\ni=1⎛\n⎜\n⎝∇θL−1,i˜fL−1(θ;uL−1(x1))\n⋮\n∇θL−1,i˜fL−1(θ;uL−1(xN))⎞\n⎟\n⎠⎛\n⎜\n⎝∇θL−1,i˜fL−1(θ;uL−1(x1))\n⋮\n∇θL−1,i˜fL−1(θ;uL−1(xN))⎞\n⎟\n⎠⊺\nWe also denote\n˜B(θ, U)=˜B(θ,(uL−1,1,⋯, uL−1,N))∆=B(θ, X)\nThen obviously, we have\nλmin(A)≥λmin(B)=λmin(˜B).\nBy Lemma 5, λmin(B)>0except for a measure-zero set of (x1,⋯, xN)inRNd.\nIn the end, we would like to show the dependence on hmin. First, consider hmin=0, i.e., there exists\ni, js.t.xi=xj. Then obviously A(θ, X)is degenerate, and σmin(A(θ, X))=0.\nNext, let Di,H={j∣hij=∥xi−xj∥≤H,∀j≠i}. WLOG, assume that DN−k,H={N−k+1,⋯, N}\nfor some k, and consider X1=⎛\n⎝x1\n⋮\nxN−k⎞\n⎠, and X2=⎛\n⎝xN−k+1\n⋮\nxN⎞\n⎠.\nSince A(θ, X)is symmetric, its eigenvalues are real for any Xandθ. By Theorem 4.1 in Kurdyka\n& Paunescu (2008), we have that the map from Xto all the eigenvalues listed as a column is locally\nLipschitz. Therefore, there exists rθ,X,k>0andLθ,X,k>0, s.t., for any X2∈BX′\n2(rθ,X,k), where\nX′\n2=⎛\n⎝xN−k\n⋮\nxN−k⎞\n⎠, i.e.,√\n∑j∈DN−k,Hh2\nN−k,j≤rθ,X,k , we have\nλmin(θ, X 1, X2)≤Lθ,X,k∥X′\n2−X2∥=Lθ,X,k√\n∑\nj∈DN−k,Hh2\nN−k,j=Lθ,X,k⌟roo⟪⟪op\n⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪mo⟨⌟roo⟪⟨o⟪k−1\n∑\nj=0h2\nN−k,N−j.\nSpecifically, if H=hminandDN−k,H≠∅, we have\nλmin(θ, X 1, X2)≤Lθ,Xhmin,\nforhmin≤rθ,XandLθ,X, rθ,X>0.\nC.2 P OLYNOMIAL GENERALIZED SMOOTHNESS OF THE LOSS\nIn this section, we denote θmax,i=arg max {∥θ∥,∥θ′∥}, and umax=arg max {∥u∥,∥u′∥}. All the\nconstants C’s are≥0and independent of θ,¯φ, u; all the p’s in the subscripts are in N. We denote\nuℓ,i=uℓ,i(θ)to emphasize the dependency on θ, especially when comparing uℓ,iunder different\nθ’s.\nNote that S(⋅)has positive coefficients. Therefore S(⋅)is monotonically increasing on R≥0×⋯×R≥0,\nand\nS(a1,⋯, ai,⋯, ad)≤S(∣a1∣,⋯,∣ai∣,⋯,∣ad∣).\nTo derive each polynomial function precisely in this section, we provide a more detailed version of\nAssumption 4 as follows:\nAssumption 6. Assume for all ℓ=0,⋯, L,\n∥φℓ(θ;u)∥≤n0,ℓ\n∑\nt=1C0,ℓ,tnθ\n∏\ni=1∥θi∥p0,ℓ,t,i∥u∥p0,ℓ,t(7)\n35\n\n∥∇θℓφℓ(θ;u)∥≤n1,ℓ\n∑\nt=1C1,ℓ,tnθ\n∏\ni=1∥θi∥p1,ℓ,t,i∥u∥p1,ℓ,t(8)\n∥∇uφℓ(θ;u)∥≤n2,ℓ\n∑\nt=1C2,ℓ,tnθ\n∏\ni=1∥θi∥p2,ℓ,t,i∥u∥p2,ℓ,t(9)\n∥¯φℓ(θ;u)−¯φℓ(θ′;u′)∥≤(C1,1,ℓnθ\n∏\ni=1∥θmax,i∥p1,1,ℓ,i∥umax∥p1,1,ℓ+C1,1,ℓ,0)∥θ−θ′∥\n+(C1,2,ℓnθ\n∏\ni=1∥θmax,i∥p1,2,ℓ,i∥umax∥p1,2,ℓ+C1,2,ℓ,0)∥u−u′∥(10)\n∥∇θℓ¯φℓ(θ;u)−∇θℓ¯φℓ(θ′;u′)∥≤(C2,1,ℓnθ\n∏\ni=1∥θmax,i∥p2,1,ℓ,i∥umax∥p2,1,ℓ+C2,1,ℓ,0)∥θ−θ′∥\n+(C2,2,ℓnθ\n∏\ni=1∥θmax,i∥p2,2,ℓ,i∥umax∥p2,2,ℓ+C2,2,ℓ,0)∥u−u′∥\n(11)\n∥∇u¯φℓ(θ;u)−∇u¯φℓ(θ′;u′)∥≤(C3,1,ℓnθ\n∏\ni=1∥θmax,i∥p3,1,ℓ,i∥umax∥p3,1,ℓ+C3,1,ℓ,0)∥θ−θ′∥\n+(C3,2,ℓnθ\n∏\ni=1∥θmax,i∥p3,2,ℓ,i∥umax∥p3,2,ℓ+C3,2,ℓ,0)∥u−u′∥\n(12)\nLemma 7. Under the Assumption 6, we have that, for all ℓ=0,⋯, L,\n∥¯φℓ(θ;u)∥≤n0,ℓ\n∑\nt=1C0,ℓ,tnθ\n∏\ni=1∥θi∥p0,ℓ,t,i(13)\n∥∇θj¯φℓ(θ;u)∥≤n1,ℓ\n∑\nt=1C1,ℓ,tnθ\n∏\ni=1∥θi∥p1,ℓ,t,i(14)\n∥∇u¯φℓ(θ;u)∥≤n2,ℓ\n∑\nt=1C2,ℓ,tnθ\n∏\ni=1∥θi∥p2,ℓ,t,i(15)\n∥uℓ,i∥≤∥xi∥+ℓ−1\n∑\nj=0n0,j\n∑\nt=1C0,j,tnθ\n∏\ni=1∥θi∥p0,j,t,i(16)\n∥uℓ,i(θ)−uℓ,i(θ)∥≤g(ℓ)\n∑\nj=0Cu,j,inθ\n∏\nq=1∥θmax,q∥pu,j,q∥θ−θ′∥ (17)\nProof. By the definition of ϵ−normalization, we have\n∥τϵ(u)∥=⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪u√\n∥u∥2+ϵ2⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪⌟⎥rro⟪⟪⟩r⟪≤1.\nThen we have\n∥¯φℓ(θ;u)∥=∥φ(θ;τϵ(u))∥≤n0,ℓ\n∑\nt=1C0,ℓ,tnθ\n∏\ni=1∥θi∥p0,ℓ,t,i∥τϵ(u)∥p0,ℓ,t≤n0,ℓ\n∑\nt=1C0,ℓ,tnθ\n∏\ni=1∥θi∥p0,ℓ,t,i.\nThe inequality of ∇θj¯φℓ(θ;u)follows the same idea. For ∇u¯φℓ(θ;u), first consider\n∇τϵ(u)=1√\n∥u∥2+ϵ2I−1\n(√\n∥u∥2+ϵ2)3uu⊺\n36\n\nThen by Wely’s theorem,\n∥∇τϵ(u)∥≤1√\n∥u∥2+ϵ2+∥u∥2\n(√\n∥u∥2+ϵ2)3≤1\nϵ\nwhere the equality of the last inequality is achieved at ∥u∥=0. Then\n∥∇u¯φℓ(θ;u)∥=∥∇τϵ(u)φℓ(θ;τϵ(u))∇τϵ(u)∥\n≤∥∇τϵ(u)φℓ(θ;τϵ(u))∥∥∇τϵ(u)∥\n≤n2,ℓ\n∑\nt=1C2,ℓ,tnθ\n∏\ni=1∥θi∥p2,ℓ,t,i\nwhere C2,ℓ,tdepends on ϵand the last inequality follows the same idea as the previous two inequal-\nities.\nFor the upper bound of uℓ,i, note that\nuℓ+1,i=uℓ,i+¯φℓ(θ;uℓ,i)=⋯=u0,i+ℓ\n∑\nj=0¯φj(θ;uj,i).\nThen,\n∥uℓ,i∥≤∥xi∥+ℓ−1\n∑\nj=0∥¯φj(θ;uj,i)∥≤∥xi∥+ℓ−1\n∑\nj=0n0,j\n∑\nt=1C0,j,tnθ\n∏\ni=1∥θi∥p0,j,t,i\nwhere the second inequality follows from (13).\nAlso,\n∥uℓ,i(θ)−uℓ,i(θ′)∥=∥x0+ℓ−1\n∑\nj=0¯φj(θ;uj,i(θ))−x0−ℓ−1\n∑\nj=0¯φj(θ′;uj,i(θ′))∥\n≤ℓ−1\n∑\nj=0∥¯φj(θ;uj,i(θ))−¯φj(θ′;uj,i(θ′))∥\n≤ℓ−1\n∑\nj=0(C1,1,jnθ\n∏\nq=1∥θmax,q∥p1,1,j,q∥umax∥p1,1,j+C1,1,j,0)∥θ−θ′∥\n+ℓ−1\n∑\nj=0(C1,2,jnθ\n∏\nq=1∥θmax,q∥p1,2,j,q∥umax∥p1,2,j+C1,2,j,0)∥uj,i(θ)−uj,i(θ′)∥\nwhere the first inequality follows from (13).\nThen we denote the above inequality as follows\n∥uℓ,i(θ)−uℓ,i(θ′)∥\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\naℓ≤ℓ−1\n∑\nj=0(C1,1,jnθ\n∏\nq=1∥θmax,q∥p1,1,j,q∥umax∥p1,1,j+C1,1,j,0)∥θ−θ′∥\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\nbℓ\n+ℓ−1\n∑\nj=0(C1,2,jnθ\n∏\nq=1∥θmax,q∥p1,2,j,q∥umax∥p1,2,j+C1,2,j,0)\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\nλj∥uj,i(θ)−uj,i(θ′)∥\n⌟⟨⟨⟪rl⟫l⟩⟩⟪⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫m⟩⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟪⟨⟨⟪rl⟫mo⟨⌟⟨⟨⟪rl⟫r⟩⟩⟩⟪\naj.\nSince a0=∥xi−xi∥=0, define b0=0≥a0, and then by discrete Gronwall’s inequality (Proposition\n4.1 in Emmrich (1999) with θ=0, τj=1), we have\naℓ≤bℓ+ℓ−1\n∑\nj=0λjbjℓ−1\n∏\ns=j+1(1+λs),\n37\n\nnamely,\n∥uℓ,i(θ)−uℓ,i(θ′)∥\n≤(ℓ−1\n∑\nj=0(C1,1,jnθ\n∏\nq=1∥θmax,q∥p1,1,j,q∥umax,j,i∥p1,1,j+C1,1,j,0)\n+ℓ−1\n∑\nj=1(C1,2,jnθ\n∏\nq=1∥θmax,q∥p1,2,j,q∥umax,j,i∥p1,2,j+C1,2,j,0)\n×(j−1\n∑\nr=0C1,1,rnθ\n∏\nq=1∥θmax,q∥p1,1,r,q∥umax,r,i∥p1,1,r+C1,1,r,0)\n×ℓ−1\n∏\ns=j+1(1+C1,2,snθ\n∏\nq=1∥θmax,q∥p1,2,s,q∥umax,s,i∥p1,2,s+C1,2,s,0)) ∥θ−θ′∥\n≤(ℓ−1\n∑\nj=0(C1,1,jnθ\n∏\nq=1∥θmax,q∥p1,1,j,q(j−1\n∑\ns=0C0,snθ\n∏\nq=1∥θmax,q∥p0,s,q+∥xi∥)p1,1,s+C1,1,s,0)\n+ℓ−1\n∑\nj=1(C1,2,jnθ\n∏\nq=1∥θmax,q∥p1,2,j,q(j−1\n∑\ns=0C0,snθ\n∏\nq=1∥θmax,q∥p0,s,q+∥xi∥)p1,2,j+C1,2,j,0)\n×(j−1\n∑\nr=0C1,1,rnθ\n∏\nq=1∥θmax,q∥p1,1,r,q(r−1\n∑\ns=0C0,snθ\n∏\nq=1∥θmax,q∥p0,s,q+∥xi∥)p1,1,r+C1,1,r,0)\n×ℓ−1\n∏\ns=j+1(1+C1,2,snθ\n∏\nq=1∥θmax,q∥p1,2,s,q(s−1\n∑\nt=0C0,tnθ\n∏\nq=1∥θmax,q∥p0,t,q+∥xi∥)p1,2,s+C1,2,s,0)) ∥θ−θ′∥\n=g3(ℓ)\n∑\nj=0Cu,j,inθ\n∏\nq=1∥θmax,q∥pu,j,q∥θ−θ′∥\nwhere the second inequlaity follows from (16), g3(ℓ)is some polynomial of ℓ, and Cu,j,i is some\nconstant depending on ∥xi∥.\nCorollary 5. Under the same assumptions as Lemma 7, we have that, for all ℓ=0,⋯, L,\n∥¯φℓ(θ;uℓ,i(θ))−¯φℓ(θ′;uℓ,i(θ′))∥≤˜g3(ℓ)\n∑\nj=0C¯φ,ℓ,j,inθ\n∏\nq=1∥θmax,q∥p¯φ,ℓ,j,q∥θ−θ′∥ (18)\n∥∇θℓ¯φℓ(θ;uℓ,i(θ))−∇θℓ¯φℓ(θ′;uℓ,i(θ′))∥≤˜g3(ℓ)\n∑\nj=0C∇θ¯φ,ℓ,j,inθ\n∏\nq=1∥θmax,q∥p∇θ¯φ,ℓ,j,q∥θ−θ′∥(19)\n∥∇u¯φℓ(θ;uℓ,i(θ))−∇u¯φℓ(θ′;uℓ,i(θ′))∥≤˜g3(ℓ)\n∑\nj=0C∇u¯φ,ℓ,j,inθ\n∏\nq=1∥θmax,q∥p∇u¯φ,ℓ,j,q∥θ−θ′∥(20)\nwhere ˜g3(ℓ)is some polynomial of ℓ, and the C’s depends on ∥xi∥.\n38\n\nProof. For the first inequality,\n∥¯φℓ(θ;uℓ,i(θ))−¯φℓ(θ′;uℓ,i(θ′))∥\n≤(C1,1,ℓnθ\n∏\nq=1∥θmax,q∥p1,1,ℓ,q∥umax∥p1,1,ℓ+C1,1,ℓ,0)∥θ−θ′∥\n+(C1,2,ℓnθ\n∏\nq=1∥θmax,q∥p1,2,ℓ,q∥umax∥p1,2,ℓ+C1,2,ℓ,0)∥uℓ,i(θ)−uℓ,i(θ′)∥\n≤(C1,1,ℓnθ\n∏\nq=1∥θmax,q∥p1,1,ℓ,q(j−1\n∑\ns=0C0,snθ\n∏\nq=1∥θmax,q∥p0,s,q+∥xi∥)p1,1,ℓ+C1,1,ℓ,0)∥θ−θ′∥\n+(C1,2,ℓnθ\n∏\nq=1∥θmax,q∥p1,2,ℓ,q(j−1\n∑\ns=0C0,snθ\n∏\nq=1∥θmax,q∥p0,s,q+∥xi∥)p1,2,ℓ+C1,2,ℓ,0)\n×g3(ℓ)\n∑\nj=0Cu,j,inθ\n∏\nq=1∥θmax,q∥pu,j,q∥θ−θ′∥\n=˜g3(ℓ)\n∑\nj=0C¯φ,ℓ,j,inθ\n∏\nq=1∥θmax,q∥p¯φ,ℓ,j,q∥θ−θ′∥\nwhere the second inequality follows from (16) and (17), ˜g3(ℓ)is some polynomial of ℓ,C¯φ,ℓ,j,i\ndepends on ∥xi∥. The other two inequalities follow the same idea.\nLemma 8. Under Assumption 6, l(θ;xi)satisfies generalized smoothness\n∥∇θl(θ;xi)−∇θl(θ′;xi)∥≤nθ\n∑\nj=1∑\nℓ∈Jjg4(ℓ)\n∑\nj=0Cl,ℓ,j,inθ\n∏\nq=1∥θmax,q∥pl,ℓ,j,q∥θ−θ′∥ (21)\nwhere g4(ℓ)is some polynomial of ℓ. Consequently,\n∥∇θL(θ)−∇θL(θ′)∥≤S(∥θmax,1∥,⋯,∥θmax,nθ∥) ∥θ−θ′∥ (22)\nwhere S(∥θmax,1∥,⋯,∥θmax,nθ∥)=1\nN∑N\ni=1∑nθ\nj=1∑ℓ∈Jj∑g4(ℓ)\nj=0Cl,ℓ,j,i∏nθ\nq=1∥θmax,q∥pl,ℓ,j,q .\nProof. From the calculations at the beginning of this section, we know\n∇θjl(θ;xi)=∇fl(θ;xi)∇θℓf(xi)\n=∑\nℓ∈Jj∇fl(θ;xi)∇u¯φL(θ;uL,i)(Im+∇u¯φL−1(θ;uL−1,i))⋯(Im+∇u¯φℓ+1(θ;uℓ+1,i))∇θj¯φℓ(θ;uℓ,i)\n39\n\nThen\n∥∇θjl(θ;xi)−∇θjl(θ′;xi)∥\n≤∑\nℓ∈Jj∥∇fl(θ;xi)∇u¯φL(θ;uL,i)(Im+∇u¯φL−1(θ;uL−1,i))⋯(Im+∇u¯φℓ+1(θ;uℓ+1,i))∇θj¯φℓ(θ;uℓ,i)\n−∇fl(θ′;xi)∇u¯φL(θ′;uL,i)(Im+∇u¯φL−1(θ′;uL−1,i))⋯(Im+∇u¯φℓ+1(θ′;uℓ+1,i))∇θj¯φℓ(θ′;uℓ,i)∥\n=∑\nℓ∈Jj∥(¯φL(θ;uL,i)−yi)⊺∇u¯φL(θ;uL,i)(Im+∇u¯φL−1(θ;uL−1,i))⋯(Im+∇u¯φℓ+1(θ;uℓ+1,i))∇θj¯φℓ(θ;uℓ,i)\n−(¯φL(θ′;uL,i)−yi)⊺∇u¯φL(θ′;uL,i)(Im+∇u¯φL−1(θ′;uL−1,i))⋯(Im+∇u¯φℓ+1(θ′;uℓ+1,i))∇θj¯φℓ(θ′;uℓ,i)∥\n≤∑\nℓ∈Jj(∥¯φL(θ;uL,i)∥+∥yi∥)∥∇u¯φL(θ;uL,i)∥(1+∥∇u¯φL−1(θ;uL−1,i)∥)⋯\n×(1+∥∇u¯φℓ+1(θ;uℓ+1,i))∥)∥∇θj¯φℓ(θ;uℓ,i)−∇θj¯φℓ(θ′;uℓ,i)∥\n+⋯+(∥¯φL(θ;uL,i)∥+∥yi∥)∥∇u¯φL(θ;uL,i)∥(1+∥∇u¯φL−1(θ;uL−1,i)∥)⋯\n×∥∇u¯φs(θ;us,i)−∇u¯φs(θ′;us,i)∥⋯(1+∥∇u¯φℓ+1(θ′;uℓ+1,i))∥∇θj¯φℓ(θ′;uℓ,i)∥\n+⋯+∥¯φL(θ;uL,i)−¯φL(θ′;uL,i)∥∥∇u¯φL(θ′;uL,i)∥(1+∥∇u¯φL−1(θ′;uL−1,i)∥)⋯\n×(1+∥∇u¯φℓ+1(θ′;uℓ+1,i)∥)∥∇θj¯φℓ(θ′;uℓ,i)∥\n≤∑\nℓ∈Jjg4(ℓ)\n∑\nj=0Cl,ℓ,j,inθ\n∏\nq=1∥θmax,q∥pl,ℓ,j,q∥θ−θ′∥\nwhere the second inequality follows from triangular inequality, and the last inequality follows from\nLemma 7 and Corollary 5; Cl,ℓ,j,i is a constant that depends on ∥xi∥and∥yi∥, and g4(ℓ)is a poly-\nnomial of ℓ.\nThen\n∥∇θl(θ;xi)−∇θl(θ′;xi)∥≤nθ\n∑\nj=1∥∇θjl(θ;xi)−∇θjl(θ′;xi)∥\n≤nθ\n∑\nj=1∑\nℓ∈Jjg4(ℓ)\n∑\nj=0Cl,ℓ,j,inθ\n∏\nq=1∥θmax,q∥pl,ℓ,j,q∥θ−θ′∥\nand\n∥∇θL(θ)−∇θL(θ′)∥≤1\nNN\n∑\ni=1∥∇θl(θ;xi)−∇θl(θ′;xi)∥\n≤1\nNN\n∑\ni=1nθ\n∑\nj=1∑\nℓ∈Jjg4(ℓ)\n∑\nj=0Cl,ℓ,j,inθ\n∏\nq=1∥θmax,q∥pl,ℓ,j,q∥θ−θ′∥\nC.3 A DDITIONAL LEMMAS IN MEASURE THEORY AND DIFFERENTIAL TOPOLOGY\nWe first introduce some useful theorems in measure theory and topology.\nTheorem 6 (Parametric transversality theorem (Hirsch, 2012)) .LetV, M, N beCrmanifold with-\nout boundary and A⊂NaCrsubmanifold. Let F∶V→Cr(M, N)satisfy the following condi-\ntions:\n•Fev∶V×M→N,(v, x)↦Fv(x), isCr;\n•Fevis transverse to A;\n•r>max{0,dimN+dimA−dimM}.\nThen the set {v∈V∶Fvis transverse to A}is residual and therefore dense.\n40\n\nTheorem 7 (Mityagin (2015)) .Letf(x)be a real analytic function on (a connected open domain\nUof)Rd. Iffis not identically zero, then its zero set\nf−1(0)={x∈U∶f(x)=0}\nhas a zero measure, i.e., Leb(f−1(0))=0.\nTheorem 8 (Theorem G.3 in Wang et al. (2022)) .Letf∶Rd→Rdandf∈C1. If the set of critical\npoints of fis a null-set, i,e.,\nL({x∈Rd∶∇f(x)is not invertible })=0,\nthenL(f−1(B))=0for any null-set B.\nTheorem 9 (Lemma 3 in Rader (1973)) .Consider a function f∶U⊆Rd→Rdthat is pointwise\nLipschitz, i.e., for any x∈U, there exists a neighbourhood Uxofxand a constant Lx, s.t.\n∥f(x)−f(y)∥≤Lx∥x−y∥,∀y∈Ux.\nThen its image of a measure-zero set is still measure zero.\nTheorem 10 (Preimage Theorem) .Ifyis a regular value of a smooth map f∶X→Y, then the\npreimage f−1(y)is a submanifold of X, with dimf−1(y)=dimX−dimY.\nNext, we show the supplementary lemmas of the neural network (2) by applying the above theorems.\nLemma 9. Fix some θ. Under Assumption 3,\n1. Let uℓ=uℓ(x)∶Rd→Rd. The preimage of uℓfor any measure-zero set in Rdis a measure-zero\nset inRd, for all ℓ=0,⋯, L−1.\n2. Let Uℓ(x1,⋯, xn)=(uℓ(x1)\n⋮uℓ(xN))∶RNd→RNd. The preimage of Uℓfor any measure-zero set in\nRNdis a measure-zero set in RNd, for all ℓ=0,⋯, L−1.\nProof. Suppose the critical point set of uℓis measure zero, and thus by Theorem 8, the preimage of\nuℓfor any measure zero set is a null set. Consider\nuℓ+1=uℓ+1(x),\nwhere its Jacobian is\n∇uℓ+1(x)=(I+∇u¯φℓ(uℓ))⋯(I+∇u¯φ0(u0)).\nWe would like to prove that the critical point set of uℓ+1is measure zero, i.e.,\n{x∈Rd∶∇uℓ+1(x)is not full rank }.\nNote that\n∇uℓ+1(x)=(I+∇u¯φℓ(uℓ))∇uℓ(x),\nand thus\n{x∈Rd∶∇uℓ+1(x)is not full rank }\n={x∈Rd∶I+∇u¯φℓ(uℓ(x))is not full rank }∪{x∈Rd∶∇uℓ(x)is not full rank }.\nBy our assumption, the set\nLebd({x∈Rd∶∇uℓ(x)is not full rank })=0.\nAlso, by Lemma 10,\nLebd({u∈Rd∶I+∇u¯φℓ(u)is not full rank })=0,\nand then by Theorem 8, the preimage of the above measure zero set is still measure zero, i.e.,\nLebd({x∈Rd∶I+∇u¯φℓ(uℓ(x))is not full rank })=0.\n41\n\nThus the critical point set of uℓ+1is measure zero, i.e.\nLebd({x∈Rd∶∇uℓ+1(x)is not full rank })≤0+0=0.\nBy Theorem 8, the first conclusion holds for uℓ+1.\nFor the second statement, the Jacobian of Uis\n∇Uℓ+1(x1,⋯, xN)=⎛\n⎝∇uℓ+1(x1)\n⋱\n∇uℓ+1(xN)⎞\n⎠\n=⎛\n⎝I+∇u¯φℓ(uℓ(x1))\n⋱\nI+∇u¯φℓ(uℓ(xN))⎞\n⎠∇Uℓ(x1,⋯, xN)\n=⎛\n⎝INd+⎛\n⎝∇u¯φℓ(uℓ(x1))\n⋱\n∇u¯φℓ(uℓ(xN))⎞\n⎠⎞\n⎠∇Uℓ(x1,⋯, xN).\nThen the second statement follows a similar derivation as the first one.\nLemma 10. Letf(x, y)∶Rm×Rn2→Rmbe an analytic function. Consider g(x, y)=x+f(x, y)\nwhere α≥c>0for some constant c<1. Then the following statements hold for (1) fixed yand any\nxexcept for a measure-zero set, and (2) fixed xand any yexcept for a measure-zero set:\n1. The Jacobian w.r.t. xof either x+f(x, y)orx+(1+ϵ0)f(x, y), for some very small ϵ0, is full\nrank. Such ϵ0is dense in the neighbourhood of 0.\n2. The Jacobian of x+ηf(x, y)w.r.t. xhas non-degenerate Jacobian, where ηis in some dense set\nof(0,1\nc).\nProof. First, consider\nF(α, x, y)=det(αI+∇xf(x, y))=m\n∏\ni=1(α+λi(x, y)),\nwhere λi(x, y)is the eigenvalue of ∇xf(x, y). By Proposition 1, F(α, x, y)is a analytic function\nofα, x, y .\nFixx, y. Then F(α, x, y)is a polynomial of αand the critical points of F(α, x, y)are isolated.\nThus F′\nα(α, x, y)≠0for all alpha except for a measure 0 set of isolated points.\nNext we no longer fix x, y, letA={0}, and consider any open interval Iofαthat does not con-\ntain the critical points, which is a manifold. Then from the above discussion, ∇F(α, x, y)≠0,\nnamely Fis surjective and thus transverse to A. Therefore, by Theorem 6, the set Sα={α∈\nI∶Fαis transverse to A}is residual and dense, which also implies that Fα(x, y)=det(αI+\n∇xf(x, y))is transverse to Afor a dense set of αin(c,∞). Then, for any fixed αin this dense\nset,∇Fα(x, y)is surjective for any (x, y)∈F−1\nα(0), and hence 0is a regular point. Then by\nTheorem 10, F−1\nα(0)is a submanifold of codimension 1, and therefore\nLebmn2(F−1\nα(0))=0.\nThen we consider fixing xory, i.e., Fα,x(y)andFα,y(x). The above reasoning also holds true for\nthe two functions, and therefore\nLebm(F−1\nα,y(0))=0,for some fixed y,andLebn2(F−1\nα,x(0))=0,for some fixed x.\nThis concludes the second statement by choosing η=1\nα, which is also dense in (0,1\nc)since the\ncontinuous image of a dense set is also dense.\nFor the first statement, if -1 is not an eigenvalue of ∇xf(x, y), then I+∇xf(x, y)has full rank.\nOtherwise, since αis dense in any open set that does not contain 1, we can always find a small\nenough ϵ1, such that 1−ϵ1is inside this set Sα. Then I+1\n1−ϵ1∇xf(x, y)is has full rank. The first\nstatement follows from choosing ϵ0=1\n1−ϵ1−1.\n42\n\nLemma 11. For any (x1,⋯, xN)except for a measure-zero set in RNd, the Jacobian along the line\nsegment between any two points uℓ,i, uℓ,jhas full rank at every layer ℓ=0,⋯, L−1. Furthermore,\nfor any point uℓon the line segment between xiandxj, the Jacobian at any layer ℓ ℓ=0,⋯, L−1\nalso has full rank.\nProof. First, by Lemma 9, the critical point set of all uℓ, denoted by B, is a finite union of measure-\nzero sets in Rd, and thus is measure zero, i.e.,\nLebd(B)=Lebd(L−1\n⋃\nℓ=0{x∈Rd∶∇uuℓ(x)is not full rank })\n≤L−1\n∑\nℓ=0Lebd({x∈Rd∶∇uuℓ(x)is not full rank })=0.\nNext, we would like to show that the following set is measure zero in RNd\nD={(x1,⋯, xn)∶{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1];i, j=1,⋯, N}∩uℓ(B)≠∅for some ℓ=0,⋯, L−1}\nLet\n¯U(B)=L−1\n⋃\nℓ=0uℓ(B), and ¯B=L−1\n⋃\nℓ=0{x∈u−1\nℓ(¯U(B))}.\nSince uℓ(x)is analytic, and thus differentiable, then uℓ(x)is pointwise Lipschitz. By Theorem 9,\nwe have\nLebd(¯U(B))≤L−1\n∑\nℓ=0Lebd(uℓ(B))=0.\nThen by Theorem 8,\nLebd(¯B)≤L−1\n∑\nℓ=0Lebd(u−1\nℓ(¯U(B)))=0.\nWe consider\nD⊆{(x1,⋯, xn)∶{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1];i, j=1,⋯, N;ℓ=0,⋯, L−1}∩¯U(B)≠∅}\n⊆L−1\n⋃\nℓ=0⋃\ni,j⋃\nz∈¯U(B){(x1,⋯, xN)∶z∈{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1]}}\nWe denote Di,j,ℓ,z={(x1,⋯, xN)∶z∈{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1]}}and consider the\nmeasure of⋃z∈¯UDi,j,ℓ,z , i.e.,\nLebNd(⋃\nz∈¯UDi,j,ℓ,z)≤∫⋯∫ 1z∈{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1]}dx1⋯dxNdz\nBy Fubini-Tonelli’s Theorem,\nLebNd(⋃\nz∈¯UDi,j,ℓ,z)≤∫⋯∫ 1z∈{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1]}dxidzdx j⋯dxN\nFor any fixed xjandz, the set {uℓ(xi)∶z∈{tuℓ(xi)+(1−t)uℓ(xj)∶t∈[0,1]}}is a ray in\nRdand thus is measure zero. Therefore, its preimage is measure-zero in Rd. Also, from the above\nderivation, Lebd(¯U(B))=0. Then we have\nLebNd(⋃\nz∈¯UDi,j,ℓ,z)≤0.\nWe then consider ⋃L−1\nℓ=0⋃i,j⋃z∈¯U(B)Di,j,ℓ,z . Since there are (N\n2)pair of(i, j)andLsuchℓ, which\nare both finite, we have\nLebNd(D)≤LebNd(L−1\n⋃\nℓ=0⋃\ni,j⋃\nz∈¯U(B)Di,j,ℓ,z)≤L−1\n∑\nℓ=0∑\n(i,j)0=0\n43\n\nNext, we would like to show that the following set is measure zero in RNd\nD={(x1,⋯, xn)∶{uℓ(txi+(1−t)xj)∶t∈[0,1];i, j=1,⋯, N}∩uℓ(B)≠∅for some ℓ=0,⋯, L−1}.\nThe proof follows the same idea as the previous statement. The only difference is that for any fixed\nxjandz, the set {xi∶z∈{uℓ(txi+(1−t)xj)∶t∈[0,1]}}is a ray in Rdand thus is measure zero.\nLemma 12. Fixθand consider the network under some small adjustment of the scale of φℓfor\nℓ=0,⋯, L−1. For any (x1,⋯, xN)except for a measure-zero set in RNd, each layer satisfies the\nfollowing lower Lipschitz inequality\n∥uℓ,i−uℓ,j∥≥µu,ℓ,θ∥xj−xi∥,∀ℓ=0,⋯, L−1,∀i, j=1,⋯, N.\nwhere µu,ℓ,θ>0is some constant depending on θ, ℓ.\nProof. First, by Lemma 11, the Jacobians of uℓ(x)the line segments between each point uℓ,i, uℓ,j\nare full rank for all ℓ=0,⋯, L−1, except for a measure-zero set of X=(x1,⋯, xN)inRNd,\ndenoted by BNd.\nThen for fixed θandX=(x1,⋯, xN) /∈BNd, there exists some constant µu,ℓ,θ>0, s.t.\nµu,ℓ,θ= min\n{x∶uℓ(x)=tuℓ(xi)+(1−t)uℓ(xj),t∈[0,1]}λmin(∇xuℓ(x))\nwhere λminis the minimum singular value.\nThen\n∥uℓ(xi)−uℓ(xj)∥=∥∫1\n0∇uℓ(xi+t(xj−xi))(xj−xi)dt∥\n=∥∫1\n0∇uℓ(xi+t(xj−xi))dt⋅(xj−xi)∥\n≥µu,ℓ,θ∥xj−xi∥.\nD A PPROXIMATION :PROOF OF THEOREM 3\nProof of Theorem 3. Consider the ground truth function g∈Wr,p(Ω). Since Wm,p(Ω)is dense in\nWr,p(Ω)form≥r, we have that for any ϵ1>0, there exists ϕ∈Wm,p(Ω)such that\n∥g−ϕ∥r,p≤ϵ1.\nAlso, by Lemma 13 and Theorem 12, there exists a polynomial ψof order at most nd(N, d), s.t.,\n∥ϕ−ψ∥p\nr,p,Io≤(Chm\nmaxn+1h−r\nmin∥ϕ∥m,p,Io)p\nFor a family of neural network functions that can approximate polynomial of order nd, i.e., for any\nϵ2>0andψ∈Pnd, there exists some fin this class s.t.\n∥f−ψ∥p≤ϵ2\nLetϵ1=ϵ2=1\n2Chm\nmaxn+1h−r\nmin∥ϕ∥m,p,Io. Then combining all the results above, we have\n∥f−g∥p,Io≤∥f−ψ∥p+∥ψ−ϕ∥p+∥ϕ−g∥p\n≤∥f−ψ∥p+∥ψ−ϕ∥r,p+∥ϕ−g∥r,p\n≤2Chm\nmaxn+1h−r\nmin∥ϕ∥m,p,Io.\n44\n\nD.1 D EFINITIONS AND LEMMAS RELATED TO COMPUTATIONAL GEOMETRY\nThe definitions and concepts introduced in this section are mainly from De Berg (2000).\nDefinition 8 (Triangulation) .A triangulation of the polygon is defined as a decomposition of a\npolygon into triangles by a maximal set of non-intersecting diagonals, or equivalently, no vertex of\nany triangle lies in the interior of an edge of another triangle.\nDefinition 9 (Delaunay triangulation) .Given a set Pof points in the n-dimensional Euclidean\nspace, a Delaunay triangulation is a triangulation DT( P) such that if for every n-simplex (whose\nvertices belong to P) in the triangulation, there exists an open n-ball that passes through all the\nvertices of the simplex and that does not contain any other point of Pin its interior.\nDefinition 10 (n-simplex) .An−simplex in n-dimensional space is the convex hull of n+1affinely\nindependent points in Rn.\nProposition 3. Let{v0, v1, . . . , V d}⊂Rnbe the vertices of an n-simplex. Let ui=vi−v0,fori=\n1, . . . , n .\n•n+1vertices, (n+1\n2)edges,(n+1\nk+1)k-dimensional faces\n•volumn V=1\nn!∣det([u1u2⋯un])∣\nDefinition 11 (General position) .The set of points {xi}N\ni=1inRnis in general position if any n+1\npoints are affinely independent.\nTheorem 11 (Theorem 3.1, and Chapter 9, in De Berg (2000)) .Assume the set {xi}N\ni=1is in general\nposition. Then there exists a unique Delaunay triangulation, which consists of n-simplices.\nWe then prove the following lemma, showing that i.i.d. samples obeying absolutely continuous\ndensity are in general position with probability one.\nLemma 13. Suppose x1,⋯, xN∈Rnare sampled iid from a distribution Pthat is absolutely con-\ntinuous with respect to Lebesgue measure. Then with probability one over the joint distribution, the\nset{xi}N\ni=1is in general position, i.e., any n+1points are affinely independent.\nProof. First, a set of n+1points x0,⋯, xnis affinely independent if and only if the following vectors\nx1−x0,⋯, xn−x0are linearly independent. This is equivalent to\ndet([x1−x0,⋯, xn−x0])≠0.\nLetF(x0,⋯, xn)=det([x1−x0,⋯, xn−x0]). Then Fis a polynomial. Let\nN={(x0,⋯, xn)∣F(x0,⋯, xn)=0}.\nThen the zero set of the polynomial is a null set in R(n+1)d, i.e.,Nhas Lebesgue measure zero.\nAlso, Pis absolutely continuous with respect to Lebesgue measure L. Thus, P(N)=0, i.e.,\nP((x0,⋯, xn)∣F(x0,⋯, xn)=0)=0.\nThere are (N\nn+1)such set of n+1points, and the finite union of measure zero set is still measure\nzero. Therefore, the set {xi}N\ni=1is in general position with probability one.\nD.2 D ATA-DEPENDENT POLYNOMIAL APPROXIMATION IN SOBOLEV SPACE\nTheorem 12 (adaptive Bramble-Hilbert) .Letm>n/p,1≤p<∞. Given Ndata points\n{(xi, yi)}N\ni=1in general position, consider the convex hull of all the xi, i.e.,I=conv{x1,⋯, xN}⊆\nΩ. For any ϕ∈Wm,p(Io), there exists a polynomial interpolation ψof order p(N, n), s.t.,\n∥ϕ−ψ∥r,p≤Chm\nmaxn+1h−r\nmin∥ϕ∥m,p\nwhere hmin=minihi,minandhmaxn+1=maxIihi,maxis the maximum hijover all the n−simplex.\nThe proof is an adaptation of Watkins (1979).\n45\n\nProof. For any n−simplex Ii, define ∆vi=(vi1−vi0, vi2−vi0,⋯, vin−vi0)∈Rn×n, and hij=\n∥vij−vi0∥. Also, I=⋃iIi.\nForx∈Ii, define an affine map x=v0+∆viξ, where ξ∈D={ξ∈Rn∣ξ∈[0,1]n,∑n\ni=1ξi≤1}.\nThen this map is a one-to-one mapping of DontoIi. Let the vertices of Dbe¯vijforj=0,⋯, n.\nThen for any function ϕonI, define ¯ϕ(ξ)=ϕ(x)=ϕ(v0+∆viξ). Then\n∥Dαϕ(x)∥0,p,Io\ni=n\n∏\nj=1h−αi\nij∣Ji∣1/p∥Dα¯ϕ(ξ)∥0,p,Do\n=n\n∏\nj=1h−(αi−1/p)\nij∥Dα¯ϕ(ξ)∥0,p,Do\nwhere∣Ji∣=∏n\nj=1hijis the determinant of the Jacobian of the affine map. Then by hij≤1, we have\n∥¯ϕ(ξ)∥m,p,Do=⎛\n⎝∑\n∣α∣≤m∥Dα¯ϕ∥p\n0,p,Do⎞\n⎠1/p\n=⎛\n⎝∑\n∣α∣≤m⎛\n⎝n\n∏\nj=1hαj−1/p\nij⎞\n⎠p\n∥Dαϕ(x)∥p\n0,p,Io\ni⎞\n⎠1/p\n≤hm\ni,max∣Ji∣−n/p∥ϕ(x)∥m,p,Io\ni(23)\nand similarly,\n∥ϕ(x)∥m,p,Io\ni≤h−m\ni,min∣Ji∣n/p∥¯ϕ(ξ)∥m,p,Do (24)\nwhere hi,min=minjhij.\nFor any function ϕ∈C(I), we can define a unique polynomial interpolant ψ=Bϕ, where ψ∈C(I)\nis a polynomial with degree ≤p(N, n)for some p(N, n)≥m−1,pm−1=Bpm−1for any polynomial\npm−1of order at most m−1, and the operator Bis linear (see for example Saniee (2008)). The two\nfunctions ϕ, ψ are equal at the vertices of the n-simplex, i.e., ϕ(vij)=ψ(vij), for all j=0,⋯, n.\nLet¯ϕ(ξ)=ϕ(x)and¯ψ(ξ)=ψ(x). We can define the linear operator ¯B, s.t.,\n(ϕ−Bϕ)(x)=(¯ϕ−¯B¯ϕ)(ξ).\nThen, we can find a set of barycentric coordinate functions ¯ϕ0(ξ),⋯,¯ϕn(ξ), which is the basis\nfunction of a natural coordinate system for n−simplex, s.t.\n¯ϕs(¯vij)=δsjand¯ψ(ξ)=n\n∑\nj=0¯ϕj(ξ)¯ϕ(¯vij).\nwhere ¯vij=Then\n∥¯B¯ϕ∥r,p,Do=∥¯ψ∥r,p≤max\nj∥¯ϕ(¯vij)∥n\n∑\ns=0∥¯ϕs∥r,p,Do\n≤∥¯ϕ∥∞n\n∑\ns=0∥¯ϕs∥r,p,Do\n≤C′∥¯ϕ∥m,p,Don\n∑\ns=0∥¯ϕs∥r,p,Do=C′′∥¯ϕ∥m,p,Do\nwhere the last inequality follows from the Sobolev embedding theorem, and C′, C′′>0. This shows\nthat¯Bis a bounded operator from Wm,p(Do)toWr,p(Do), and so is I−¯B, i.e.,\n∥(I−¯B)¯ϕ∥r,p,Do≤C′′′∥¯ϕ∥m,p,Do (25)\nfor some universal constant C′′′>0.\nThen\n∥ϕ−ψ∥r,p,Io\ni=∥ϕ−Bϕ∥r,p,Io\ni≤h−r\ni,min∣Ji∣n/p∥¯ϕ−¯B¯ϕ∥r,p,Do\n=h−r\ni,min∣Ji∣n/p∥(I−¯B)¯ϕ∥r,p,Do\n≤C′′′h−r\ni,min∣Ji∣n/p∥¯ϕ∥m,p,Do\n≤C′′′hm\ni,maxh−r\ni,min∥ϕ∥m,p,Io\ni\n46\n\nwhere the first inequality follows from (24), the second inequality follows from (25), the last in-\nequality follows from (23).\nThen we have for any ϕ∈Wm,p(Io),\n∥ϕ−ψ∥r,p≤Chm\nmaxn+1h−r\nmin∥ϕ∥m,p\nwhere hmin=minihi,minandhmaxn+1=maxIihi,maxis the maximum hijover all the n−simplex,\nandC>0is some universal constant.\nE A DDITIONAL LEMMAS\nLemma 14. LetΩbe a bounded open set. Then the Sobolev space Wm,p(Ω)is dense in Lp(Ω).\nProof. By definition, Wm,p(Ω)⊆Lp(Ω). Also, we have C∞(Ω)is dense in Lp(Ω), and in\nWm,p(Ω). Then Wm,p(Ω)is dense in Lp(Ω).\nLemma 15. Under Assumption 3, the neural network f(x)∈Wr,p(Ω)for any 1≤r, p<∞.\nProof. By Assumption 3 and Proposition 1, the neural network function f(x)is analytic on some\nopen neighbourhood Ω1of¯Ω. Then f(x)∈C∞(Ω1), and therefore\nsup\nx∈¯Ω∥Dαf(x)∥ℓp<∞\nfor any non-negative α=(α1,⋯, αd), and any 1≤p<∞. By∣Ω∣<∞, we have the Lp(Ω)norm of\nany derivative\n∥Dαf(x)∥p\np≤∣Ω∣sup\nx∈¯Ω∥Dαf(x)∥p\nℓp<∞.\nTherefore, f(x)∈Wr,p(Ω)for any 1≤r, p<∞.\nF M ORE EXPERIMENTS USING ℓ2-SGD\nEffectiveness of Maximized-Distance Based Sampling on Smaller Scale Datasets. Figure 12\nillustrates the ℓ2SGD training loss curves for LLaMA-1-7B across different data selection strategies\non the TeaMs-RL dataset. We compare a 4.5k uniform subset , a 4.5k random subset , and the full\n9k dataset . Subfigure (a) shows the raw loss trajectories, while (b) applies smoothing to better visu-\nalize convergence patterns. The 4.5k uniform subset achieves convergence comparable to or faster\nthan the full dataset and consistently outperforms the random subset, particularly in the early stages\nof training. However, since this dataset size is relatively small, the advantage of data uniformity is\nnot very pronounced in the final stage of training. When the dataset size increases, uniform data\nselection becomes more effective in the later phase (see Figure 14).\nEffectiveness of Maximized-Distance Based Sampling on Larger Scale Datasets. Figures 13\nand 14 show the ℓ2-SGD training loss curves of llama-1-7b models trained on the WizardLM dataset\nunder three different data selection settings: the full 20k dataset, a 10k maximized-distance subset\nselected via maximization distance, and a 10k random subset. In both the raw (left) and smoothed\n(right) plots, the maximized-distance 10k subset outperforms the random 10k subset across training\nsteps, achieving faster convergence and more stable convergence. Notably, despite being half the\nsize, the maximized-distance subset often matches or even surpasses the performance of the full\ndataset in early and end training regarding the convergence. This result indicates the stability and\nefficiency of data uniformity sampling strategies, particularly in large-scale training tasks. It high-\nlights the finding that uniform data offers stronger learning signals than merely increasing sample\ncount without regard to data uniformity.\n47\n\n0 20 40 60 80 100 120\nStep24681012141618LossModel\nL2-sgd-llama-1-random-4.5k\nL2-sgd-llama-1-uniform-4.5k\nL2-sgd-llama-1-full-9k(a) without smoothness.\n0 20 40 60 80 100 120\nStep246810121416LossModel\nL2-sgd-llama-1-random-4.5k\nL2-sgd-llama-1-uniform-4.5k\nL2-sgd-llama-1-full-9k (b) with smoothness.\nFigure 12: ℓ2SGD Training Experiments—All Curves: Training loss comparison of TeaMs-RL\ndata point distributions for different dataset sizes: 4.5k uniform (maximized distance) dataset, 4.5k\nrandom dataset, 9k full dataset.\n0 5 10 15 20 25 30\nStep10111213Loss\nModel\nL2-sgd-llama-1-random-10k\nL2-sgd-llama-1-uniform-10k\nL2-sgd-llama-1-full-20k\n(a) without smoothness.\n0 5 10 15 20 25 30\nStep9.510.010.511.011.512.012.513.013.5Loss\nModel\nL2-sgd-llama-1-random-10k\nL2-sgd-llama-1-uniform-10k\nL2-sgd-llama-1-full-20k (b) with smoothness.\nFigure 13: ℓ2SGD Training Experiments: Training loss comparison of WizardLM data point distri-\nbutions for different dataset sizes: 10k uniform (maximized distance) dataset, 10k random dataset,\n20k full dataset.\n0 20 40 60 80\nStep2468101214Loss\nModel\nL2-sgd-llama-1-random-10k\nL2-sgd-llama-1-uniform-10k\nL2-sgd-llama-1-full-20k\n(a) without smoothness.\n0 20 40 60 80\nStep2468101214Loss\nModel\nL2-sgd-llama-1-random-10k\nL2-sgd-llama-1-uniform-10k\nL2-sgd-llama-1-full-20k (b) with smoothness.\nFigure 14: ℓ2SGD Training Experiments—All Curves: Training loss comparison of WizardLM\ndata point distributions for different dataset sizes: 10k uniform (maximized distance) dataset, 10k\nrandom dataset, 20k full dataset.\nG E XPERIMENT SETTINGS\nWe summarize the key configurations used in our experiments to ensure reproducibility and clarity.\nTable 1 details the training hyperparameters. Based on these and building on Llama-X5, we train\nLLaMA-1-7B (one A100 GPU) and 13B (two A100 GPUs) models on datasets of varying sizes:\n5https://github.com/AetherCortex/Llama-X\n48\n\nWizardLM6(10k uniform datasets, 10k random datasets, and 20k full datasets) and TeaMs-RL7\n(4.5k uniform datasets, 4.5k random datasets, and 9k full datasets). The uniform subsets from\nTeaMs-RL and WizardLM are constructed using Word2Vec embeddings and a greedy selection\nstrategy that maximizes uniformity. Specifically, one data point is randomly selected to initialize the\nset, and subsequent points are chosen to maximize the minimum cosine distance to all previously\nselected points. For TeaMs-RL, the random subset is composed of half randomly sampled data\npoints and half selected to minimize pairwise distances from the 9k full dataset. In the case of\nWizardLM, the random subset consists of the first 10k samples from the original 20k dataset.\nTable 2 shows the DeepSpeed configuration using cross-entropy loss with the Adam optimizer. Ta-\nble 3 presents the DeepSpeed configuration used for ℓ2-regularized SGD training with Zero Stage\n3 optimization and CPU offloading. Finally, we leverage lm-evaluation-harness (Gao et al., 2024)\nfor our evaluation. Table 4 outlines the evaluation setup, including tasks, few-shot settings, and\nbatch size settings used in the LM Evaluation Harness. These configurations collectively support\nour experiments across diverse datasets and model scales.\nTable 1: Key experimental settings used in our study, including training with ℓ2loss and cross-\nentropy loss using SGD and Adam optimizers.\nParameters value Parameters value\nGPUs [1, 2] model max length 512\nper device train batch size 64 per device eval batch size 1\ngradient accumulation steps 1 evaluation strategy no\nlearning rate 2e-5 warmup steps 2\nlogging steps 1 lr scheduler type constant\ngradient checkpointing True fp16 True\nTable 2: Key DeepSpeed configuration parameters used in our experiments (Zero Stage 3 with CPU\noffloading, FP16, AdamW optimizer).\nParameters Value Parameters Value\nzero optimization stage 3 overlap communication True\noffload optimizer device cpu pin memory (optimizer) True\noffload param device cpu pin memory (param) True\ncontiguous gradients True sub group size 0\nreduce bucket size auto prefetch bucket size auto\nparam persistence threshold auto max live parameters 0\nmax reuse distance 0 gather 16bit weights on save True\nfp16 enabled True auto cast False\nloss scale 0 initial scale power 32\nloss scale window 1000 hysteresis 2\noptimizer type AdamW learning rate 2e-5\nbeta1 0.9 beta2 0.999\nepsilon 1e-8 weight decay 0\ntrain batch size auto micro batch size per GPU auto\ngradient accumulation steps auto wall clock breakdown False\n6https://huggingface.co/datasets/WizardLMTeam/WizardLM_evol_instruct_\nV2_196k\n7https://github.com/SafeRL-Lab/TeaMs-RL\n49\n\nTable 3: Key DeepSpeed configuration parameters used in the experiments (Zero Stage 3 with CPU\noffloading, FP16, SGD optimizer).\nParameters Value Parameters Value\nzero optimization stage 3 overlap communication True\noffload optimizer device cpu pin memory (optimizer) True\noffload param device cpu pin memory (param) True\ncontiguous gradients True sub group size 0\nreduce bucket size auto prefetch bucket size auto\nparam persistence threshold auto max live parameters 0\nmax reuse distance 0 gather 16bit weights on save True\nallow untested optimizer True min loss scale 1\nfp16 enabled True auto cast False\nloss scale 0 initial scale power 32\nloss scale window 1000 hysteresis 2\noptimizer type SGD learning rate 2e-5\nmomentum 0.0 weight decay 0\ntrain batch size auto micro batch size per GPU auto\ngradient accumulation steps auto wall clock breakdown False\nTable 4: Evaluation settings for baseline models using the LM Evaluation Harness.\nParameter Value Parameter Value\nModel type hf-causal-experimental Batch size 2\nFew-shot for arcchallenge 25 Few-shot for truthfulqa mc 0\n50\n\n",
    "source": "http://arxiv.org/abs/2506.24120v1",
    "authors": [
      "Yuqing Wang",
      "Shangding Gu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "math.OC",
      "stat.ML"
    ],
    "type": "content"
  },
  {
    "id": "2506.24119v1_abstract",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "content": "Title: SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning\n\nAbstract: Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
    "source": "http://arxiv.org/abs/2506.24119v1",
    "authors": [
      "Bo Liu",
      "Leon Guertler",
      "Simon Yu",
      "Zichen Liu",
      "Penghui Qi",
      "Daniel Balcells",
      "Mickel Liu",
      "Cheston Tan",
      "Weiyan Shi",
      "Min Lin",
      "Wee Sun Lee",
      "Natasha Jaques"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24119v1_content",
    "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
    "content": "arXiv:2506.24119v1  [cs.AI]  30 Jun 2025\nSPIRALSPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning\nvia Multi-Agent Multi-T urn Reinforcement Learning\nBo Liu*1, Leon Guertler*2, Simon Yu*3, Zichen Liu*1,4\nPenghui Qi1,4, Daniel Balcells5, Mickel Liu6, Cheston Tan2, Weiyan Shi3, Min Lin4, Wee Sun Lee1,\nNatasha Jaques†6\n1National University of Singapore2Centre for Frontier AI Research (CFAR), A*STAR\n3Northeastern University4Sea AI Lab5Plastic Labs6University of Washington\nRecent advances in reinforcement learning have shown that language models can de-\nvelop sophisticated reasoning through training on tasks with verifiable rewards, but\nthese approaches depend on human-curated problem-answer pairs and domain-specific\nreward engineering. We introduce SPIRAL, a self-play framework where models learn\nby playing multi-turn, zero-sum games against continuously improving versions of\nthemselves , eliminating the need for human supervision. Through self-play, SPIRAL\ngenerates an infinite curriculum of progressively challenging problems as models must\nconstantly adapt to stronger opponents. To enable this self-play training at scale, We\nimplement a fully online, multi-turn, multi-agent reinforcement learning system for\nLLMs and propose role-conditioned advantage estimation (RAE) to stabilize multi-agent\ntraining. Using SPIRAL, self-play on zero-sum games produces reasoning capabilities\nthat transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6% im-\nprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000 expert\ngame trajectories. Analysis reveals that this transfer occurs through three cognitive\npatterns: systematic decomposition, expected value calculation, and case-by-case anal-\nysis. Multi-game training ( TicTacToe ,Kuhn Poker ,Simple Negotiation ) further enhances\nperformance as each game develops distinct reasoning strengths. Applying SPIRAL to\na strong reasoning model (DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% aver-\nage improvement. These results demonstrate that zero-sum games naturally develop\ntransferable reasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.\n Spiral-RL/SPIRAL\n Spiral-RL\nGame Avg\nBenchmark AvgMATH500AIME24 AIME25\nOlympiadBenchAMC-23\nMinerva MathGPQA\nMMLU-Pro01020304050607080Score (%)Qwen3-4B-Base\nQwen3-4B-SFT\nQwen3-4B-Mistral\nQwen3-4B-Gemini\nQwen3-4B (SPIRAL)\nFigure 1|SPIRAL achieves consistent improvements over base models across game performance\nand reasoning benchmarks. It also surpasses SFT on expert game trajectories and RL baselines\ntrained against fixed opponents (Mistral and Gemini).\n∗Equal contribution, order randomly decided by dice roll.\n†Corresponding author.\n\n1. Introduction\nRecent breakthroughs in language model reasoning, including OpenAI o1 (OpenAI, 2024)\nand DeepSeek-R1 (DeepSeek Team, 2024), reveal that reinforcement learning (RL) can unlock\ndramatic improvements in Chain-of-Thought reasoning (Wei et al., 2022). Through outcome-\nbased rewards, RL enables models to develop generalizable reasoning strategies and consistently\nsolve complex problems where supervised fine-tuning shows limited progress.\nHowever, current approaches to reasoning enhancement face a fundamental scalability\nbottleneck: their dependence on carefully engineered reward functions, domain-specific datasets,\nand expert supervision (Bai et al., 2022; DeepSeek Team, 2024; Ouyang et al., 2022). Each new\nreasoning domain requires experts to craft evaluation metrics, curate training problems, and\nvalidate reasoning traces. This manual process becomes increasingly unsustainable as we pursue\nmore general intelligence, limiting both the scale and diversity of reasoning challenges that\nmodels can learn from.\nSelf-play offers a solution to this scalability crisis by eliminating the need for human supervi-\nsion in training data creation (Silver et al., 2017; Tesauro, 1995). In self-play, models learn by\ncompeting against copies of themselves, where each match outcome provides automatic feed-\nback. As the model improves, its opponent improves equally, maintaining a consistent challenge\nlevel that drives continuous learning. This paradigm has already revolutionized AI across many\ndomains: from TD-Gammon’s backgammon supremacy (Tesauro, 1995) to AlphaGo’s conquest\nof Go (Silver et al., 2016, 2017) to OpenAI Five’s mastery of complex team coordination (Berner\net al., 2019). Yet despite these successes, applying self-play to enhance language model reasoning\nremains largely unexplored. Prior attempts have been limited to simple word games with offline\nupdates (Cheng et al., 2024), LoRA adaptations that constrain learning capacity (Dettmers et al.,\n2023; Park et al., 2025), or single-turn code generation tasks (Zhao et al., 2025), falling short of\nleveraging multi-turn competitive dynamics that enable extended strategic reasoning.\nWe introduce SPIRAL , which applies self-play to two-player zero-sum language games\nfor developing reasoning capabilities. Zero-sum language games provide ideal training en-\nvironments: they require strategic thinking and planning while remaining simple enough to\nenable stable learning, with clear rules that make outcomes verifiable. SPIRAL offers two\nkey advantages. First, unlike traditional RLVR approaches that depend on human-curated\nproblem-answer pairs, it generates infinite training data through game dynamics alone. Sec-\nond, compared to fixed-opponent training (see Fig. 2), self-play prevents overfitting to static\nstrategies by continuously evolving the challenge level. However, implementing this vision\nfor LLMs presents significant technical challenges. The computational demands of multi-turn,\nmulti-agent autoregressive generation require sophisticated distributed systems, while standard\nRL algorithms suffer from high variance in multi-agent settings. To address these challenges,\nwe implement a fully online, multi-turn, multi-agent reinforcement learning system with a\ndistributed actor-learner architecture. We also introduce role-conditioned advantage estimation\n(RAE), which stabilizes training by normalizing rewards relative to each player’s expected\nperformance. Without RAE, our ablation experiments show that models suffer from thinking\ncollapse, progressively abandoning reasoning traces that are critical for generalization.\nKey Findings. Training on zero-sum games produces reasoning capabilities that transfer\nbroadly. Starting from Qwen3-4B-Base (Yang et al., 2025), SPIRAL trained solely on Kuhn Poker\nachieves 8.6% improvement on mathematical reasoning and 8.4% on general reasoning bench-\nmarks, outperforming SFT on 25,000 expert trajectories. Since our training data contains only\ngame states with no mathematical content, we avoid any risk of benchmark data leakage. Anal-\n2\n\n𝐿(𝑀,𝑁,𝑀→)=𝑂1·strategy_diversity(𝑀,𝑁)+𝑂2·win(𝑀→)+...(15)\n33Figure 2|From human-designed rewards to self-discovered reasoning through SPIRAL. Left:\nTraditional RL requires human experts to design complex reward functions. Middle : Fixed\nopponent training leads to exploitation of static strategies. Right : SPIRAL enables continuous\nreasoning improvement through self-play, where both players develop increasingly sophisticated\nstrategies without human supervision.\nysis reveals that this transfer occurs through three cognitive patterns: systematic decomposition\n(breaking problems into steps), expected value calculation (probabilistic reasoning), and case-\nby-case analysis. Fixed-opponent training fails while self-play continuously improves. Different\ngames develop distinct reasoning strengths: TicTacToe trains spatial reasoning, Kuhn Poker\nbuilds probabilistic thinking, and Simple Negotiation fosters strategic optimization. Multi-game\ntraining combines these complementary skills synergistically. Applying SPIRAL to DeepSeek-\nR1-Distill-Qwen-7B (DeepSeek Team, 2024) achieves 2.0% average improvement, confirming\ngeneralizability across model families. Without RAE, models stop reasoning after 200 steps.\nThese results demonstrate that zero-sum games can serve as powerful reasoning gymnasi-\nums, developing cognitive skills that transfer far beyond their training domain. Building on\nthese findings, our work makes the following contributions:\n1.A Fully Online, Multi-T urn, Multi-Agent RL Framework for LLMs: We develop a dis-\ntributed actor-learner architecture that enables online self-play with full-parameter updates\nacross multiple two-player zero-sum language games. The multi-turn aspect trains models\nto reason through sequential decisions, directly preparing them for complex multi-step prob-\nlem solving. Unlike prior offline approaches, this provides a continuous curriculum as the\nmodel must adapt to an ever-improving opponent: itself. We release our implementation1to\nfacilitate further research.\n2.Role-conditioned Advantage Estimation (RAE): We introduce a variance-reduced advantage\nestimator specifically designed for multi-agent settings. By normalizing rewards relative\nto each player’s expected performance, RAE reduces variance in multi-agent learning. Our\nablation studies empirically validate that RAE stabilizes learning and prevents thinking\ncollapse. Without it, models progressively abandon reasoning traces, which is critical for\ngeneralization.\n3.Empirical Discovery of Transfer: We demonstrate that self-play on zero-sum games im-\nproves both out-of-distribution game performance and academic reasoning benchmarks\nwithout domain-specific training data. Our analysis identifies reasoning patterns (systematic\ndecomposition, expected value calculation, case-by-case analysis) that transfer from games\nto mathematics at measurable rates. Different games develop specialized skills that transfer\nto related domains, and multi-game training combines these abilities synergistically.\n1https://github.com/spiral-rl/spiral .\n3\n\n2. Related Work\nReinforcement Learning for LLM Reasoning. Reinforcement learning has evolved from align-\nment tasks using RLHF (Bai et al., 2022; Jaques et al., 2019; Ouyang et al., 2022) to directly\nimproving reasoning capabilities. Recent models like OpenAI o1 (OpenAI, 2024) and DeepSeek-\nR1 (DeepSeek Team, 2024) demonstrate that RL with verifiable rewards (RLVR) can unlock\nchain-of-thought reasoning using rule-based rewards (Lightman et al., 2023; Uesato et al., 2022).\nHowever, these approaches depend on human-curated problem sets and domain-specific reward\nengineering. SPIRAL eliminates this dependency by using self-play games to generate infinite\nreasoning challenges without human supervision.\nMulti-Agent RL for Language Models. Implementing MARL for full-scale LLMs presents\nsignificant technical challenges (Liu et al., 2025a; Wan et al., 2025). Prior work has made various\ncompromises: Sarkar et al. (2025) uses RNNs instead of transformers; Jacob et al. (2022) focuses\non simplified environments that don’t require full autoregressive generation; and Liao et al.\n(2024) shows the effectiveness of self-play with SFT on proprietary models. In contrast, SPIRAL\nimplements fully online, full-parameter MARL through a distributed actor-learner architecture,\nenabling continuous adaptation to evolving opponents across multiple games.\nSelf-Play for Autonomous Improvement. Self-play in LLMs began with alignment methods like\nSPIN (Chen et al., 2024) and Self-Rewarding Language Models (Yuan et al., 2024). Recent work\nexplores capability improvement: SPAG (Cheng et al., 2024) applies self-play to Adversarial\nTaboo but uses offline updates and remains confined to a single word game; SPC (Chen et al.,\n2025) and Genius (Xu et al., 2025) require human task distributions; Absolute Zero (Zhao\net al., 2025) generates coding tasks but needs deterministic verification. SPIRAL demonstrates\nthat self-play on strategic games improves academic benchmarks by 8.7% average without\ndomain-specific data.\nLLMs in Gaming. Games serve as testbeds for evaluating LLM capabilities (Duan et al., 2024;\nPaglieri et al., 2024; Ruoss et al., 2024; Zhang et al., 2024), but recent work explores games\nas training domains (Feng et al., 2024). LMRL-Gym (Abdulhai et al., 2023) benchmarks RL\nalgorithms for LLMs in game environments; RAGEN (Wang et al., 2025) uses single-agent\nmulti-turn RL; ViGaL (Xie et al., 2025) shows arcade games improve math reasoning; Divide-\nFuse-Conquer (Zhang et al., 2025) trains on grouped games using offline learning. SPIRAL\nuniquely combines: (1) multi-agent self-play where both players share parameters, (2) fully\nonline learning with continuous opponent evolution, and (3) transfer from zero-sum language\ngames to reasoning without seeing any benchmark-related problems during training.\n3. Preliminaries\nT urn-level Markov Decision Process (MDP). Language model training traditionally formulates\ngeneration as a token-level MDP (Bellman, 1957; Rafailov et al., 2024) where each action is a\nsingle token from vocabulary V. For multi-turn reasoning and game-playing, we instead adopt\na turn-level MDP formulation M=(S,A,𝑇,𝑟,𝛾). Here, statesSrepresent complete contexts\n(e.g., game configurations, problem states, or conversation histories), actions Aare complete\nresponses (containing many tokens), the transition function 𝑇:S×A→ Δ(S)determines state\ndynamics, 𝑟:S×A→ Rprovides immediate rewards, and 𝛾∈[0, 1]is the discount factor. The\nreturn is defined as the discounted sum of rewards: 𝑅(𝜏)=Í𝑇\n𝑡=0𝛾𝑡𝑟𝑡.\nThe key distinction: in token-level MDPs, each decision outputs one token; in turn-level\nMDPs, each decision produces a complete multi-token response before transitioning. At each\n4\n\nGPUK\nRole1AdvantageA₁(s,a)Role0AdvantageA₀(s,a)Role-conditionedAdvantageEstimation(RAE)RL Training!!\"#GameTrajectories\nWeightSyncZero-SumGamesTicTacToeKuhnPokerSimple Negotiation\nGPU2ParallelRollout…Learner\nVectorizedGameEnv!\"!($)\n…GPU1\nRole0:Win(Reward=+1)Role1:Lose(Reward=-1)Turn 1!!=[Bet50]PLAYER 0 (!#)Reward = 0!!=[Raise100]PLAYER 1 (!#)Reward = 0\nEnv:TerminatedMulti-Agent,Multi-Turn!\"#\"#!!!Trajectory$$\"$!##!##$!$…Turn 2!!=[Call]PLAYER 0 (!#)Reward = 0!\"=[Bet30]PLAYER 1 (!#)Reward = 0Turn T!$=[Bet50]PLAYER 0 (!#)Reward = 0!#=[Fold]PLAYER 1 (!#)Reward = 0Multi-Agent,Multi-Turn RolloutAcrossKGPUs!\"\"($)$\"$!Trajectory$!\"#\"###!!!#%!$……!$&!Self-PlaySPIRAL\n…ActorFigure 3|The SPIRAL Framework. SPIRAL employs an actor-learner architecture for scalable\nself-play training. Parallel actors sample trajectories from a diverse set of games using vector-\nized environments. A single policy 𝜋𝑖plays both roles, generating zero-sum, sparse reward\ngame trajectories. The centralized learner processes these trajectories using Role-conditioned\nAdvantage Estimation (RAE) to compute separate advantages, 𝐴0(𝑠,𝑎)and𝐴1(𝑠,𝑎), for each role.\nThese are then used for on-policy reinforcement learning updates.\nturn𝑡, the language model observes state 𝑠𝑡and generates:\n𝑦𝑡=⟨think⟩𝑐𝑡⟨/think⟩⟨answer⟩𝑎𝑡⟨/answer⟩, (1)\nwhere𝑐𝑡externalizes reasoning and 𝑎𝑡∈A is the executable action. (See App. A for how existing\nSFT and RLVR paradigms adapt to turn-level MDPs.)\nTwo-Player Zero-Sum Markov Games. We extend the single-agent MDP to competitive settings\nwith a two-player zero-sum Markov game (Littman, 1994) G=(S,A0,A1,𝑇,𝑟,𝛾), whereA0and\nA1are the action spaces for player 0 and player 1 respectively. The zero-sum property requires:\n𝑟0(𝑠,𝑎(0),𝑎(1))+𝑟1(𝑠,𝑎(0),𝑎(1))=0∀𝑠,𝑎(0),𝑎(1), (2)\nwhere𝑎(0)∈ A 0and𝑎(1)∈ A 1denote actions taken by each player. Given trajectory 𝜏=\n{(𝑠𝑡,𝑎(0)\n𝑡,𝑎(1)\n𝑡)}𝑇\n𝑡=0, the returns satisfy 𝑅1(𝜏)=−𝑅0(𝜏).\n4. The SPIRAL Framework\nWe present SPIRAL, a framework that enables language models to develop generalizable reason-\ning capabilities through multi-turn competitive self-play. The framework is illustrated in Fig. 3.\n4.1. SPIRAL as Self-Play on T urn-Based Zero-Sum Games\nSPIRAL implements self-play through turn-based zero-sum language games from a collection\nG={𝐺1,𝐺2, ...,𝐺𝑛}. Each game 𝐺𝑖is a two-player zero-sum Markov game, with the specific\n5\n\nstructure that players alternate turns rather than acting simultaneously. SPIRAL leverages three\nproperties of these games:\nFirst, the multi-turn structure mirrors sequential reasoning problems. Players alternate turns:\nat time𝑡, player𝑝=𝑡mod 2acts while the opponent waits, formally: 𝑎(𝑝)\n𝑡∈A𝑝and𝑎(1−𝑝)\n𝑡 =∅.\nUnlike single-turn bandit settings, this framework trains models to maintain context, plan ahead,\nand adapt strategies based on evolving game states.\nSecond, the zero-sum competitive dynamics create pressure for continuous improvement.\nEach game uses sparse terminal rewards: 𝑟𝑖(𝑠𝑡,𝑎(0)\n𝑡,𝑎(1)\n𝑡)=0for all non-terminal states. At\nterminal state 𝑠𝑇, players receive opposite rewards:\n𝑅0(𝜏)=𝜌𝑖(𝑠𝑇),𝑅1(𝜏)=−𝜌𝑖(𝑠𝑇), (3)\nwhere𝜌𝑖:Sterminal\n𝑖→{− 1, 0, 1}determines the outcome. This opposition forces models to\ndevelop robust strategies that cannot be easily exploited.\nFinally, both players share a single policy 𝜋𝜃with role conditioning. To enable this in the\nLLM setting, we prepend role-specific system prompts that specify whether the model is playing\nas Player 0 or Player 1 (see App. B.1 for example observations). At each turn, the active player\ngenerates:\n𝑦(𝑝)\n𝑡∼𝜋𝜃(·|𝑠𝑡,𝑝,𝐺𝑖), (4)\nwhere conditioning on 𝑝is achieved through the system prompt2. From𝑦(𝑝)\n𝑡, we extract the\naction𝑎(𝑝)\n𝑡to update the game state. This shared-parameter approach ensures that as the\nmodel improves at one role, it simultaneously faces a stronger opponent, creating an automatic\ncurriculum.\n4.2. Multi-T urn Self-Play Training Objective\nIn self-play, we could train two separate policies 𝜋𝜃0and𝜋𝜃1for each player. Each would\nmaximize their own expected return:\n𝐽0(𝜃0)=E𝐺∼GE𝜏∼𝜋𝜃0×𝜋𝜃1|𝐺[𝑅0(𝜏)] (5)\n𝐽1(𝜃1)=E𝐺∼GE𝜏∼𝜋𝜃0×𝜋𝜃1|𝐺[𝑅1(𝜏)] (6)\nHowever, SPIRAL uses a single shared policy 𝜋𝜃for both players, setting 𝜃0=𝜃1=𝜃. While\nthis creates opposing objectives since 𝑅1(𝜏)=−𝑅0(𝜏)in zero-sum games, the role conditioning\nthrough system prompts enables the model to learn distinct strategies for each position within a\nunified policy framework.\nTo optimize this shared policy, we apply policy gradient methods. Using REINFORCE (Williams,\n1992), the gradient becomes:\n∇𝜃𝐽(𝜃)=E𝐺∼GE𝜏∼𝜋𝜃×𝜋𝜃|𝐺∑︁\n𝑡∈𝑇0∇𝜃log𝜋𝜃(𝑦(0)\n𝑡|𝑠𝑡, 0,𝐺)·𝑅0(𝜏)\n|                                       {z                                       }\nPlayer 0 gradient+∑︁\n𝑡∈𝑇1∇𝜃log𝜋𝜃(𝑦(1)\n𝑡|𝑠𝑡, 1,𝐺)·𝑅1(𝜏)\n|                                       {z                                       }\nPlayer 1 gradient,\n(7)\n2We note that both 𝑝and𝐺𝑖can be subsumed into state 𝑠𝑡, but we choose to make them explicit here for clarity.\n6\n\nAlgorithm 1 SPIRAL: Role-Balanced Multi-Turn Self-Play\nRequire: Policy𝜋𝜃, GamesG={𝐺1, ...,𝐺𝑛}, decay rate 𝛼∈[0, 1]\n1:Initialize baselines 𝑏𝐺𝑖,𝑝=0 for all𝐺𝑖∈G,𝑝∈{0, 1}\n2:while not converged do\n3: // Self-Play Trajectory Collection\n4:B←∅\n5: for𝑘=1 to𝐾actors in parallel do\n6: Sample game 𝐺𝑖∼G, initialize 𝑠0∼𝐺𝑖\n7: forturn𝑡=0, 1, 2, ... until terminal do\n8: 𝑝←𝑡mod 2 ⊲Determine active player\n9: 𝑦(𝑝)\n𝑡∼𝜋𝜃(·|𝑠𝑡,𝑝,𝐺𝑖) ⊲Generate reasoning + action\n10: 𝑎(𝑝)\n𝑡←extract_action(𝑦(𝑝)\n𝑡)\n11: 𝑎(1−𝑝)\n𝑡←∅ ⊲Inactive player\n12: 𝑠𝑡+1←𝑇𝑖(𝑠𝑡,𝑎(0)\n𝑡,𝑎(1)\n𝑡)\n13: end for\n14: 𝑅0←𝜌𝑖(𝑠𝑇),𝑅1←−𝑅0\n15: Add(𝜏,𝐺𝑖)to batchB ⊲Store trajectory with its game\n16: end for\n17: // Role-Balanced Policy Optimization\n18: for(𝜏,𝐺𝑖)∈B do\n19: for𝑝∈{0, 1}do\n20: 𝑏𝐺𝑖,𝑝←𝛼𝑏𝐺𝑖,𝑝+(1−𝛼)𝑅𝑝(𝜏)\n21: 𝐴𝐺𝑖,𝑝(𝜏)←𝑅𝑝(𝜏)−𝑏𝐺𝑖,𝑝\n22: end for\n23: end for\n24: Update𝜃using policy gradient with advantages 𝐴𝐺𝑖,𝑝(Eq. 10)\n25:end while\nwhere𝑇𝑝={𝑡:𝑡mod 2=𝑝}denotes turns where player 𝑝acted. This formulation uses Monte\nCarlo returns which suffer from high variance, particularly problematic in self-play where the\nopponent’s strategy continuously evolves, making the environment highly non-stationary.\n4.3. Policy Optimization with Role-conditioned Advantage Estimation\nTo reduce the high variance inherent in multi-agent REINFORCE, we introduce Role-conditioned\nAdvantage Estimation (RAE). In two-player games, even with a shared policy, different roles\nmay have different expected returns due to game asymmetries (e.g., first-move advantage in\nTicTacToe, information asymmetry in Kuhn Poker).\nRAE maintains separate baselines 𝑏𝐺,𝑝for each game 𝐺∈Gand role𝑝∈{0, 1}, where each\nbaseline estimates the expected return E[𝑅𝑝(𝜏)]for that role in that game. We update these\nbaselines using exponential moving average (EMA) with a decay rate 𝛼∈[0, 1]:\n𝑏𝐺,𝑝←𝛼𝑏𝐺,𝑝+(1−𝛼)𝑅𝑝(𝜏) (update baseline) (8)\n𝐴𝐺,𝑝(𝜏)=𝑅𝑝(𝜏)−𝑏𝐺,𝑝 (compute advantage) (9)\nThis provides better variance reduction than a global baseline by accounting for role-specific\nasymmetries, such as the first-move advantage.\n7\n\nThe variance-reduced policy gradient becomes:\n∇𝜃𝐽SPIRAL(𝜃)=E𝐺∼GE𝜏∼𝜋𝜃×𝜋𝜃|𝐺∑︁\n𝑝∈{0,1}∑︁\n𝑡∈𝑇𝑝𝐴𝐺,𝑝(𝜏)·∇𝜃log𝜋𝜃(𝑦(𝑝)\n𝑡|𝑠𝑡,𝑝,𝐺)(10)\nBy centering returns around role-specific expectations, RAE ensures that gradient updates\nreflect genuine learning signal rather than inherent advantages of certain player positions. Note\nthat we do not normalize by response length to avoid length bias (Liu et al., 2025b). The complete\nprocedure is in Algorithm 1.\n4.4. Implementation\nTo implement SPIRAL, we develop a truly online multi-agent, multi-turn RL system for finetun-\ning LLMs. Our training framework builds on Oat (Liu et al., 2024), which provides interfaces\nof a distributed actor-learner architecture (Espeholt et al., 2018). We instantiate actors to ex-\necute the self-play loop, using vLLM (Kwon et al., 2023) for efficient model inference and\nTextArena (Guertler et al., 2025) to simulate the language games. The resulting multi-turn, multi-\ngame self-play experiences are used to update the LLM via policy gradient methods (Sutton\nand Barto, 2018), incorporating our proposed Role-conditioned Advantage Estimation in the\ncollocated learner.\n5. Experiment Setup\nWe design experiments to carefully test how training on different zero-sum games can improve\nreasoning capabilities that generalize to novel downstream math and reasoning tasks, as well as\nnovel zero-sum games. This section introduces the selection of game environments (Sec. 5.1)\nand the training (Sec. 5.2) and evaluation (Sec. 5.3) settings.\nWe investigate four key questions:\n1.RQ1: Can self-play on zero-sum games improve math and general reasoning capabilities?\nWe test whether competitive environment alone can develop transferable reasoning capabili-\nties without domain-specific training data, and analyze emergent patterns to understand\ntransfer mechanisms.\n2.RQ2: Does self-play’s automatic curriculum outperform fixed-opponent training? We\nevaluate whether the automatic curriculum with adaptive difficulty generated through\nself-play produces more robust reasoning than training against static opponents.\n3.RQ3: Do different games develop specialized reasoning skills? We test whether dis-\ntinct game mechanics cultivate complementary cognitive abilities and whether multi-game\ntraining creates synergistic benefits.\n4.RQ4: Is Role-Conditioned Advantage Estimation (RAE) essential for stable self-play\ntraining? We ablate our variance reduction technique to determine whether RAE is neces-\nsary to stabilize training and prevent thinking collapse in zero-sum self-play with shared\nparameters.\n5.1. Game Environments\nWe select three games from TextArena (Guertler et al., 2025) with distinct cognitive requirements:\n8\n\nTicTacToe (Spatial Reasoning) (Beck, 2008). A 3×3 grid game requiring pattern recognition and\nadversarial planning. Players must identify winning configurations, block opponent threats, and\nplan multi-step forcing sequences. We hypothesize these skills transfer to geometric problem-\nsolving and spatial visualization tasks. Tic-Tac-Toe is a deterministic, perfect-information game,\nwhich isolates pure strategic reasoning from uncertainty management.\nKuhn Poker (Probabilistic Reasoning) (Kuhn, 1950). A minimal poker variant with three\ncards (Jack, Queen, King) where players bet with hidden information. Success requires prob-\nability calculation, opponent modeling, and decision-making under uncertainty. We expect\nthese capabilities to transfer to problems involving probability, expected value, and strategic\nuncertainty.\nSimple Negotiation (Strategic Optimization) (Nash, 1950). A resource trading game where\ntwo players exchange Wood and Gold with opposing valuations to maximize portfolio value.\nSuccess requires multi-step planning, theory of mind to model opponent preferences, and\nstrategic communication through proposals and counteroffers. We hypothesize these skills\ntransfer to optimization problems, resource allocation tasks, and multi-constraint reasoning that\nrequires balancing competing objectives. Please see Fig. 7 in App. B for example environment\nobservations.\n5.2. Training Settings\nWe employ Qwen3-4B-Base (Yang et al., 2025) as the base model for all experiments. The training\nspans 400 steps with 128 samples per step, totaling 51,200 game transitions. We use a sampling\ntemperature 𝜏=1.0to collect game experiences and Adam (Kingma and Ba, 2014) with a\nconstant learning rate of 1×10−6to optimize the model. The training batch size is set as 128\nand the discount factor 𝛾=1.0(no discounting). We use 𝛼=0.95 for the exponential moving\naverage decay of role-specific baselines. Detailed hyperparameters are listed in Table 7 in App. B.\nEach experiment takes around 25 hours for full parameter tuning on 8 H100 GPUs with our\ndistributed actor-learner architecture.\n5.3. Evaluation Metrics\nWe track three key metrics throughout the training process: the frequency of invalid moves,\nthe game lengths and the win rates against Gemini-2.0-Flash-Lite3, to monitor the learning\ndynamics and determine the best models. To evaluate the trained models, we test their win\nrates against a fixed opponent on both training and unseen games and standard reasoning\nbenchmarks, as detailed below.\nOut-of-Distribution Generalization to Unseen Games. We evaluate transfer learning on seven\nunseen games that test whether learned skills generalize beyond training domains: Snake and\nConnect Four (spatial reasoning), Pig Dice and Liar’s Dice (probabilistic reasoning), Truth and\nDeception (strategic optimization). These games specifically probe whether spatial reasoning\nfrom TicTacToe, probabilistic reasoning from Kuhn Poker, and strategic optimization from\nSimple Negotiation transfer to novel game mechanics. We report the average win rate of\nour models against Gemini-2.0-Flash4across 512 independent games, with the starting player\nrandomized in each match to eliminate any first-move advantage.\nTransfer to Standard Reasoning Benchmarks. To investigate whether the reasoning abilities\n3Accessed via https://openrouter.ai/google/gemini-2.0-flash-lite-001 .\n4Accessed via https://openrouter.ai/google/gemini-2.0-flash-001 .\n9\n\ndeveloped through gameplay could transfer to non-game contexts, we evaluate our models on\na suite of established benchmarks. For mathematical reasoning, we use MATH500 (Hendrycks\net al., 2021), OlympiadBench (He et al., 2024), Minerva Math (Lewkowycz et al., 2022), AIME24,\nAIME25, and AMC23 datasets, which cover a wide range of topics including algebra, geometry,\nand competitive mathematics. For general reasoning, we utilize GPQA (Rein et al., 2024), which\nconsists of graduate-level science questions, and MMLU-Pro (Wang et al., 2024), a benchmark for\nmultidisciplinary knowledge. All evaluations on these benchmarks are conducted in a zero-shot\nsetting5to determine if game-induced reasoning could be successfully transferred to general\nproblem-solving. We use sampling temperature 0.6, top-p 0.95 and 4rollouts for all evaluations.\nReasoning Pattern Analysis. We analyze the cognitive patterns that emerge during training\nand their transfer to mathematical reasoning. Using GPT-4.1 ( gpt-4.1-2025-04-14 ) (OpenAI,\n2025) as LLM-as-Judge (Zheng et al., 2023), we classify reasoning traces from 290 game trajecto-\nries and 46,792 math problem solutions into three core patterns: Case-by-Case Analysis (sys-\ntematic enumeration of scenarios), Expected Value Calculation (probabilistic decision-making),\nand Pattern Recognition (identifying regularities and structures). The complete prompts used\nfor pattern discovery, classification, and transfer analysis are provided in App. D. We track the\nevolution of these patterns across training checkpoints (early: step 0, mid: step 128, late: step\n400) in both game and math domains to measure transfer rates. This analysis reveals which\ngame-induced reasoning strategies generalize to academic problem-solving and explains the\nmechanism behind cross-domain transfer.\n6. Experimental Results and Findings\n6.1. RQ1: Can self-play on zero-sum games improve math and general reasoning capabilities?\nTable 1 presents our main results. SPIRAL, trained exclusively on Kuhn Poker through self-\nplay, achieves notable improvements on both mathematical and general reasoning benchmarks.\nNotably, it even surpasses a model that was fine-tuned on 25,000 winning game trajectories\nfrom Qwen3-32B playing against itself.\nTable 1|Reasoning benchmark performance. SPIRAL improves reasoning without any domain-\nspecific training data.\nModel MATH500 AIME’24 AIME’25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average\nQwen3-4B-Base 65.8 10.0 3.3 33.3 50.0 24.3 30.6∗47.2∗32.9\nSFT (Qwen3-32B Distill) 76.0 16.7 13.3 38.4 50.0 31.2 33.0 48.8 39.7\nSPIRAL (Ours) 76.4 13.3 10.0 38.4 57.5 42.4 37.0 57.7 41.6\nImprovement +10.6 +3.3 +6.7 +5.1 +7.5 +18.1 +6.4 +10.5 +8.7\n∗Few-shot evaluation following Qwen3 technical report.\nThe results show consistent improvements across mathematical reasoning (+10.6% MATH500,\n+6.7% AIME’25, +7.5% AMC-23), and general reasoning (+6.4% GPQA, +10.5% MMLU-Pro).\nNotably, SPIRAL achieves these gains despite never seeing a single mathematical equation,\nacademic problem, or domain-specific training example during its entire training process. The\n18.1% improvement on Minerva Math and overall 8.7% gain across math benchmarks demon-\nstrate that competitive game environments successfully force discovery of reasoning patterns\nthat transfer broadly across domains.\n5Except for the base model, for which we provide few-shot examples that follow the Qwen3 Report settings.\n10\n\n6.1.1. Case Study: How Game Strategies Transfer to Mathematical Reasoning\nTo understand this surprising transfer, we use GPT-4.1 ( gpt-4.1-2025-04-14 ) to analyze 290\ngame trajectories and 46,792 math solutions across training checkpoints, identifying three core\nreasoning patterns that emerge during gameplay and transfer to math reasoning domains.\nPattern Discovery and Examples. We first illustrate what these patterns look like in both game\nand mathematical contexts:\nTable 2|Representative reasoning patterns and their manifestation in games and mathematics.\nWe use an LLM-as-a-judge framework to automatically evaluate the frequency of these patterns\nover training.\nPattern Game Context (Kuhn Poker) Math Transfer\nCase-by-Case\nAnalysis<think>\nCase 1 - Fold: You lose 1 chip. You have\n2 chips now.\nCase 2 - Call: You have 0% chance of\nwinning, so you will have 1 chip if you\nlose.\nSince losing 1 chip (fold) is better than\nlosing 2 chips (call and lose), the best\naction is to fold.\n</think><think> To solve𝑚2+2·3𝑛=𝑚(2𝑛+1−\n1):\nCase𝑛=0:𝑚2+2=𝑚, no real solutions\nCase𝑛=1:𝑚2+6=3𝑚, no real solu-\ntions\nCase𝑛=2:𝑚2+18=7𝑚, no real solu-\ntions\nCase𝑛=3:𝑚2+54=15𝑚gives𝑚=6, 9\n(valid)\nCase𝑛⩾4:No solutions (exponential\ndominance) </think>\nExpected Value\nCalculation<think> With card J against oppo-\nnent’s bet:\nEV(call) = P(win)×2 - P(lose)×2\n= 0×2 - 1×2 = -2\nEV(fold) = -1 (lose ante)\nSince EV(fold) > EV(call), fold is opti-\nmal. </think><think> For average of 𝐴+𝐵𝐶+𝐷𝐸𝐹\nover permutations:\nE[A] =Í𝑎𝑖\n6=−2\n6=−1\n3\nE[BC] =(𝐸[𝐵])(𝐸[𝐶])=(−1\n3)2=1\n9\nE[DEF] =(𝐸[𝐷])(𝐸[𝐸])(𝐸[𝐹])=(−1\n3)3=\n−1\n27\nTotal =−1\n3+1\n9−1\n27=−7\n27</think>\nPattern Recogni-\ntion<think> Previous rounds: Player 0\nhad K both times and won. Pattern\nidentified : Player 0 likely has strong\ncards or bluffs consistently. Given\nI have J (weak), betting might ex-\nploit their aggressive calling pattern.\n</think><think> Sum of three consecutive in-\ntegers(𝑛−1)+𝑛+(𝑛+1)=3𝑛.\nPattern recognized : sum is always di-\nvisible by 3.\nFor perfect cube: 3𝑛=𝑘3, so𝑘must be\ndivisible by 3.\nSmallest:𝑘=3⇒3𝑛=27</think>\nPattern Evolution and Transfer Analysis. Using GPT-4.1 to classify reasoning strategies, we\ntracked how these patterns develop during training and transfer to mathematics:\nFig. 4 shows how reasoning patterns evolve in both game and math domains across training\ncheckpoints. These patterns develop at different rates and transfer with varying effectiveness:\nCase-by-Case Analysis: We find that the meta-cognitive skill of systematic enumeration, or\nbreaking problems down into cases, transfers highly effectively from strategic games to math\nproblems (as shown in Fig. 4). We hypothesize this is because it represents a domain-agnostic\nway of structuring thinking that generally improves reasoning performance. Whether analyzing\nopponent possibilities in Poker or solution branches in mathematics, the core skill is the same.\nExpected Value Calculation: We find that training on games increases the use of expected\n11\n\nEarly Mid Late\nTraining Stage020406080Traces (%)\n81835\n154278\n255572Reasoning Pattern (Game)\nEarly Mid Late\nTraining Stage010203040506070\n313845\n121828486271Reasoning Pattern (Math)\nEarly Mid Late\nTraining Stage2830323436384042Score\n31.237.539.6Math BenchmarksPattern Recognition Expected Value Calculation Case-by-Case AnalysisFigure 4|Evolution of reasoning patterns during SPIRAL training and their transfer to\nmathematical reasoning. We track three core reasoning patterns (Pattern Recognition, Expected\nValue Calculation, and Case-by-Case Analysis) across 290 game trajectories and 46,792 math\nsolutions. Left: In the game domain, all patterns show substantial growth, with Expected Value\nCalculation reaching 78% by late training. Middle : These patterns transfer to mathematical\nreasoning with varying effectiveness: Case-by-Case Analysis maintains high transfer (72% to\n71%), Pattern Recognition shows amplification (35% to 45%), while Expected Value Calculation\ntransfers more selectively (78% to 28%). Right : Math benchmark scores improve from 31.2 to\n39.6 as these reasoning patterns develop, demonstrating that game-learned strategies enhance\nmathematical problem-solving capabilities.\nvalue calculation from 15% to 78% in the games themselves, and from 12% to 28% in the math\ndomain. While game-specific probabilistic reasoning is more frequent in games than in the\nmath benchmark (likely because most math problems lack explicit decision-theoretic structure),\nwe find that this form of reasoning benefits probability and optimization problems where EV\nconcepts directly apply.\nPattern Recognition: Interestingly, this form of reasoning shows an amplification effect,\nwhere the proportion evident in the math domain actually exceeds that of the game domain. We\nhypothesize that because mathematics inherently requires pattern recognition, game training\nenhances an already-present mathematical skill which is then deployed even more frequently\nwhen the model is prompted to answer a math problem. The combined training produces\nstronger pattern recognition (45%) than games alone require (35%).\nTransfer Mechanism. We believe three factors enable this cross-domain transfer:\n1. Competitive pressure strips away memorization: Self-play opponents continuously\nevolve, forcing models to develop genuine reasoning rather than pattern matching.\n2. Games isolate pure reasoning: Without domain complexity, games teach fundamental\ncognitive operations (enumeration, evaluation, synthesis) that generalize effectively.\n3. Structured output bridges domains: The<think> format learned in games provides a\nreasoning scaffold that models reuse for mathematical problems.\n12\n\n0 100 200 300 400\nPolicy iteration step0102030Average win rate (%)\nGame evaluation\n0 100 200 300 400\nPolicy iteration step10203040Average score (%)\nMath evaluation\n0 100 200 300 400\nPolicy iteration step02040Average score (%)\nGeneral evaluation\nRandom Opponent Mistral Opponent Gemini Opponent Self-Play (Ours)Figure 5|Performance comparison of self-play training and fixed-opponent baselines. All\nevaluations are averaged over multiple games/benchmarks (see Sec. 5.3). Mistral Opponent\nrefers to against Mistral-Small-3; Gemini Opponent refers to against Gemini-2.0-Flash-Lite.\nFinding 1: Zero-sum games teach transferable reasoning without domain data\nSelf-play on Kuhn Poker improves math and general reasoning benchmarks despite never\nseeing benchmark related problems. This works because gameplay forces discovery of\nthree reasoning patterns: Case-by-Case Analysis, Expected Value Calculation, and Pattern\nRecognition. These skills are developed during competing against an evolving opponent.\n6.2. RQ2: Does self-play’s automatic curriculum outperform fixed-opponent training?\nAlthough self-play promises potentially infinite curriculum generation, it remains unclear\nwhether the evolving challenge actually produces better learning than simply training against\nfixed opponents. We test whether adaptive difficulty through self-play leads to more robust\nreasoning development.\nExperimental Setup. We compare four training settings on the Kuhn Poker environment:\n(1)Self-Play : the model trains against continuously updating copies of itself; (2) Random\nOpponent : the opponent always provides valid actions but poses minimal strategic challenge; (3)\nMistral Opponent : using Mistral-Small-3 (Mistral, 2025) as a fixed intermediate-level opponent;\nand (4) Gemini Opponent ,: using Gemini-2.0-Flash-Lite (Gemini Team, Google, 2025) as a fixed\nstrong opponent. All variants use identical hyperparameters, infrastructure, and 400 training\nsteps.\nResults Analysis. Figure 5 reveals two distinct failure modes in fixed-opponent training:\n1. Curse of T urns in Format Learning: Training against random opponents leads to complete\ncollapse. Although these rule-based random agents offer no strategic challenge, they always\nproduce valid moves. Consequently, the model must generate correctly formatted, valid actions\nat every turn throughout the trajectory in order to receive any positive reward. However, the\nprobability of producing a fully valid trajectory decreases exponentially with the episode length,\nmaking it extremely difficult to explore and learn the correct action format over long horizons.\n2. Exploitation of Static Strategies: Fixed model-based opponents (Mistral, Gemini) facilitate\nformat learning more effectively than random opponents, as they occasionally produce invalid\nmoves that allow the agent to win and receive reward signals. However, we find that training\n13\n\nagainst these static opponents often leads to overfitting to their fixed strategies, resulting in poor\ngeneralization to math and general reasoning benchmarks (middle and right of Fig. 5).\nContinuous Adaptation Through Self-Play. Self-play uniquely maintains learning momentum\nthroughout training and generalizes better because self-play automatically adjusts difficulty.\nAs shown in Table 3, training against a fixed opponent (Gemini) initially gets zero win rate\n(thus no positive reward), leading to no effective learning before step 128 (see left of Fig. 5);\nat step 384 the agent learns to outperform the fixed opponent with 62.5% win rate, gradually\nexploiting its static strategies. On the other hand, against previous versions of itself, self-play\nmaintains 50-52% win rates throughout training, confirming the opponent evolves to match\ncurrent capabilities.\nTable 3|Win rates at different training stages of Gemini Opponent and Self-Play vs its opponent.\nTraining StageGemini Opponent Win Rate\nvs Gemini-2.0-Flash-LiteSelf-Play Win Rate\nvs Self (t-16)\nStep 16 0.0% 52.3%\nStep 128 37.5% 51.7%\nStep 384 62.5% 50.9%\nThe reasoning transfer results are even more striking. Self-play achieves 40% on math\nreasoning and 45% on general reasoning, outperforming the best fixed opponent (Gemini) by 5\nand 3 percentage points respectively. The relative improvement demonstrates that the diverse\nstrategies required by an evolving opponent create more generalizable reasoning patterns than\nexploiting static weaknesses.\nFinding 2: Self-play creates adaptive curriculum that outperforms fixed opponents\nSelf-play achieves better reasoning transfer than any fixed-opponent training. Rule-\nbased random opponents cause complete collapse due to the curse of turns in format\nlearning. Training against model-based opponents plateaus once the agent finds winning\ncounter-strategies. Self-play avoids both failures by continuously adjusting difficulty:\nthe opponent improves as the agent improves, forcing ongoing adaptation rather than\nexploitation of static weaknesses.\n6.3. RQ3: Do different games develop specialized reasoning skills?\nWe investigate whether distinct game mechanics cultivate different cognitive abilities. If games\ntruly serve as reasoning gymnasiums, then training on TicTacToe should develop different skills\nthan training on Kuhn Poker or Simple Negotiation. Furthermore, we test whether combining\nmultiple games produces synergistic benefits beyond the sum of individual training.\nExperimental Design. To isolate game-specific skill development, we train separate models\nexclusively on each game using identical hyperparameters and training duration. We then\nevaluate these specialists in three ways: (1) performance on their training game, (2) transfer to\nout-of-distribution games, and (3) generalization to math and general reasoning benchmarks.\nThis design reveals whether games develop narrow tactics or transferable cognitive capabilities.\nFor game performance evaluation, we use two complementary approaches: head-to-head\ncompetition between specialists to directly compare learned strategies, and performance against\n14\n\na fixed strong opponent (Gemini-2.0-Flash-Lite) to measure absolute skill levels. This dual\nevaluation reveals both relative strengths between specialists and their absolute capabilities.\nHead-to-Head Competition Reveals Specialized Strategies. We first examine how specialists\nperform when competing directly against each other. This head-to-head evaluation uses games\nthat isolate specific cognitive skills: we pair each training game with an unseen game requiring\nsimilar abilities (Snake for spatial reasoning like TicTacToe, Pig Dice for probabilistic reasoning\nlike Kuhn Poker, and Truth and Deception for strategic optimization like Simple Negotiation).\nTable 4|Game specialists excel at both their training games and unseen games requiring similar\ncognitive skills. Each cell shows the win rate in head-to-head competition between specialists\n(e.g., 57.5% means TicTacToe specialist wins 57.5% of games against the other two specialists on\nTicTacToe). Bold indicates best performance in each column.\nTraining Games OOD Games (Similar Skills)\nModel TicTacToe Kuhn Poker Simple Negotiation Snake Pig Dice Truth and Deception\n(Spatial) (Probabilistic) (Strategic)\nTicTacToe Specialist 57.5% 45.1% 30.4% 56.0% 56.7% 48.7%\nPoker Specialist 45.5% 64.2% 37.7% 42.5% 91.7% 45.4%\nNegotiation Specialist 40.5% 40.2% 62.7% 41.0% 1.1% 55.8%\nModels trained exclusively on TicTacToe excel at spatial pattern recognition, achieving 57.5%\nwin rate against other specialists on their training game. When tested on Snake, an unseen\nspatial reasoning game, they maintain 56.0% performance, demonstrating robust transfer. Kuhn\nPoker specialists develop probabilistic reasoning, dominating their training game (64.2%) and\nremarkably achieving 91.7% on Pig Dice. Simple Negotiation training produces models skilled\nin strategic optimization (62.7% on training game, 55.8% on Truth and Deception).\nMulti-Game Training Creates Synergistic Benefits. While head-to-head competition reveals\nspecialized strategies, we also evaluate absolute performance against a strong fixed opponent\n(Gemini-2.0-Flash-Lite) across both training games and randomly sampled out-of-distribution\ngames. The multi-game model demonstrates broader capabilities that often exceed specialist\nperformance:\nTable 5|Multi-game training achieves competitive performance across all training games while\nexcelling at novel composite challenges. All win rates shown are against Gemini-2.0-Flash-Lite as\na fixed opponent. The multi-game model outperforms all specialists on average, demonstrating\nthat diverse game training develops more flexible reasoning.\nTraining Games OOD Games\nModel TicTacToe KuhnPoker Simple Negotiation Liar’s Dice Connect Four Snake Average\nBase Model 17.5% 21.5% 15.6% 16.7% 20.5% 20.6% 18.7%\nSingle-Game Specialists\nTicTacToe Specialist 56.6% 24.4% 30.5% 31.4% 30.8% 30.7% 34.1%\nKuhn Poker Specialist 31.0% 48.5% 28.7% 24.9% 39.1% 33.9% 34.4%\nSimple Negotiation Specialist 27.7% 16.8% 39.1% 12.3% 32.0% 34.8% 27.1%\nMulti-Game Model 54.3% 53.9% 33.2% 51.4% 39.3% 37.2% 44.9%\nThe power of multi-game training becomes evident in performance on novel games. On\nLiar’s Dice, individual specialists struggle, achieving only 24.9% (Kuhn Poker) and 12.3%\n(Simple Negotiation) against Gemini-2.0-Flash-Lite. However, the multi-game model achieves\n15\n\n51.4%, demonstrating emergent capabilities beyond any single training game. This pattern of\nmulti-game superiority holds across the out-of-distribution games, with the multi-game model\nachieving the highest average performance (44.9%) compared to all specialists.\nMulti-Game Training Enhances Even Strong Reasoning Models. To test whether game-\nbased skill development is limited to weaker models, we apply multi-game SPIRAL training\nto DeepSeek-R1-Distill-Qwen-7B, a model that already achieves 59.7% average on reasoning\nbenchmarks (see App. C.2 for comprehensive results across all training configurations):\nTable 6|Multi-game SPIRAL training improves reasoning capabilities across model scales. Even\nstrong reasoning models benefit from game-based skill development.\nModel MATH500 AIME’24 AIME’25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average\nQwen3-4B-Base 65.8 10.0 3.3 33.3 50.0 24.3 30.6 47.2 33.1\n+ SPIRAL (Multi-Game) 74.2 20.0 16.7 39.4 55.0 38.6 36.9 57.7 42.3\nDeepSeek-Distill-Qwen-7B 90.8 46.7 36.7 56.9 92.5 48.2 48.6 57.1 59.7\n+ SPIRAL (Multi-Game) 93.0 43.3 46.7 57.9 92.5 51.1 49.6 58.9 61.7\nMulti-game SPIRAL improves Qwen3-4B by 9.2 percentage points (33.1% to 42.3%) and\nenhances the already-strong DeepSeek-Distill-Qwen-7B model by 2.0 points (59.7% to 61.7%).\nThis demonstrates that game-based reasoning skills provide value even for models that already\nexcel at mathematical reasoning, suggesting that games teach complementary cognitive abilities\nnot captured by traditional training.\nFinding 3: Different games develop specialized skills that work together\nEach game builds distinct abilities that transfer to similar out-of-distribution games.\nMulti-game training combines these specialized skills, achieving superior performance on\nnovel games that individual specialists struggle with. This synergistic training improves\nreasoning benchmarks even for already-strong models, demonstrating that games teach\ncomplementary cognitive skills.\n6.4. RQ4: Is Role-Conditioned Advantage Estimation (RAE) essential for stable self-play\ntraining?\nWe ablate our variance reduction technique to determine whether RAE is necessary to stabilize\ntraining and prevent thinking collapse in zero-sum self-play with shared parameters. Figure 6\nreveals that without proper variance reduction, self-play training catastrophically fails:\n1. Thinking Collapse: The most striking failure occurs in response length (bottom right).\nWithout RAE, reasoning traces plummet from 3,500 to near-zero characters after 200 steps.\nModels quite literally stop thinking, generating minimal outputs like <think></think><an\nswer>\\boxed{bet}</answer> . This collapse coincides with gradient norms approaching\nzero, indicating the model has converged to a degenerate policy.\n2. Performance Degradation: Without RAE, the model learns solely to play the game while\nabandoning CoTs, leading to generalization failure. As a result, the math reasoning performance\ncrashes from 35% to 12% at around step 150 (a 66% relative decrease), and the general reasoning\ndrops from 44% to 40%.\n3. Training Instability: Policy gradient norms without RAE exhibit higher initial values with\nspikes, then collapses to near-zero after step 200. In contrast, RAE maintains stable gradient\n16\n\n0 100 200 300 4001520253035Average win rate (%)\nGame evaluation\n0 100 200 300 400203040Average score (%)\nMath evaluation\n0 100 200 300 400\nPolicy iteration step4042444648Average score (%)\nGeneral evaluation\n0 100 200 300 400\nPolicy iteration step0.000.050.100.150.20Policy gradient norm\n0 100 200 300 400\nPolicy iteration step01000200030004000Response length (char)\nREINFORCE with RAE\nVanilla REINFORCEFigure 6|Training dynamics comparing REINFORCE with RAE (orange) versus vanilla REIN-\nFORCE (gray). RAE maintains stable performance across all metrics while vanilla REINFORCE\nsuffers catastrophic thinking collapse. Top row : Performance on game playing, math reasoning,\nand general reasoning benchmarks. Bottom row : Policy gradient norm shows training instability\nwithout RAE; response length reveals thinking collapse where models stop generating reasoning\ntraces.\nnorms of around 0.1 throughout training, enabling continuous learning.\nThis variance reduction is crucial for maintaining stable reasoning. High-variance gradients\ncause models to oscillate between extreme strategies, eventually converging to degenerate short\nresponses that minimize variance but abandon reasoning entirely.\nFinding 4: RAE prevents models from abandoning reasoning during self-play\nWithout RAE, models began to truncate their reasoning processes after 200 steps, gen-\nerating empty reasoning traces like <think></think>. This thinking collapse causes\nmath performance to crash while gradients become unstable then drop to zero. RAE\nmaintains stable gradients throughout training, allowing models to keep generating sub-\nstantive reasoning. Self-play alone is not enough; proper variance reduction is essential\nfor maintaining reasoning in competitive games.\n7. Conclusion\nSummary of Contributions. We introduced SPIRAL, enabling language models to develop\nreasoning capabilities through competitive self-play without human-curated data. Our technical\ncontributions include a fully online multi-turn MARL system for LLMs and Role-conditioned\nAdvantage Estimation (RAE), which prevents thinking collapse in zero-sum games. Our empiri-\ncal results show that training on Kuhn Poker alone improves mathematical reasoning by 8.7%\naverage and Minerva Math by 18.1%, surpassing models trained on 25,000 expert demonstra-\n17\n\ntions. Different games develop distinct transferable skills, and multi-game training achieves\nsuperior performance on both reasoning benchmarks and novel out-of-distribution games,\ndemonstrating synergistic benefits from diverse game training.\nLimitations. While eliminating human-curated problems, SPIRAL still requires designed game\nenvironments. Our experiments use simple games (TicTacToe, Kuhn Poker, Simple Negotiation);\nscaling to complex environments remains unexplored. The computational requirements are\nsubstantial (8 H100 GPUs for 25 hours per experiment). Performance plateaus after extended\ntraining, and our evaluation focuses on academic benchmarks rather than real-world reasoning\ntasks requiring common sense or ethical judgment.\nFuture Directions. This work opens several avenues: expanding to cooperative games, incor-\nporating partial observability, and designing games targeting specific reasoning weaknesses.\nUnderstanding why certain games develop particular skills could enable principled environment\ndesign. We envision ecosystems of self-improving agents generating increasingly sophisticated\nchallenges, creating autonomous reasoning development beyond human supervision.\nFinal Thoughts. SPIRAL demonstrates that simple games can unlock complex reasoning without\ndomain-specific data. By harnessing competitive pressure, we create systems that discover\ntheir own curricula and continuously improve. The transfer from gameplay to mathematics\ncould suggest that intelligence emerges not from sophisticated supervision but could from\nenvironmental challenges that force models to think. This paradigm shift points toward AI\nsystems that autonomously push reasoning boundaries and continuously evolve through self-\nplay.\nReferences\nM. Abdulhai, I. White, C. Snell, C. Sun, J. Hong, Y. Zhai, K. Xu, and S. Levine. Lmrl gym:\nBenchmarks for multi-turn reinforcement learning with language models, 2023. URL https:\n//arxiv.org/abs/2311.18232 .\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\nC. McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073, 2022.\nJ. Beck. Combinatorial games: Tic-tac-toe theory. 01 2008. doi: 10.1017/CBO9780511735202.\nR. Bellman. A markovian decision process. Journal ofmathematics and mechanics, 1957.\nC. Berner, G. Brockman, B. Chan, V . Cheung, P . D˛ ebiak, C. Dennison, D. Farhi, Q. Fischer,\nS. Hashme, C. Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv\npreprint arXiv:1912.06680, 2019.\nJ. Chen, B. Zhang, R. Ma, P . Wang, X. Liang, Z. Tu, X. Li, and K.-Y. K. Wong. Spc: Evolving\nself-play critic via adversarial games for llm reasoning. arXiv preprint arXiv:2504.19162 , 2025.\nZ. Chen, Y. Deng, H. Yuan, K. Ji, and Q. Gu. Self-play fine-tuning converts weak language\nmodels to strong language models. In ICML, 2024.\nP . Cheng, T. Hu, H. Xu, Z. Zhang, Y. Dai, L. Han, X. Li, et al. Self-playing adversarial language\ngame enhances llm reasoning. Advances inNeural Information Processing Systems , 37:\n126515–126543, 2024.\n18\n\nDeepSeek Team. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement\nlearning. arXiv preprint arXiv:2401.00000, 2024.\nT. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer. Qlora: Efficient finetuning of\nquantized llms. Advances inneural information processing systems, 36:10088–10115, 2023.\nJ. Duan, R. Zhang, J. Diffenderfer, B. Kailkhura, L. Sun, E. Stengel-Eskin, M. Bansal, T. Chen, and\nK. Xu. Gtbench: Uncovering the strategic reasoning limitations of llms via game-theoretic\nevaluations. arXiv preprint arXiv:2402.12348, 2024.\nL. Espeholt, H. Soyer, R. Munos, K. Simonyan, V . Mnih, T. Ward, Y. Doron, V . Firoiu, T. Harley,\nI. Dunning, et al. Impala: Scalable distributed deep-rl with importance weighted actor-learner\narchitectures. In International conference onmachine learning , pages 1407–1416. PMLR, 2018.\nX. Feng, B. Liu, Y. Song, H. Fu, Z. Wan, G. A. Koushik, Z. Hu, M. Yang, Y. Wen, and J. Wang.\nNatural language reinforcement learning. arXiv preprint arXiv:2411.14251, 2024.\nGemini Team, Google. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality,\nlong context, and next generation agentic capabilities. Technical report, Google, June 2025.\nURL https://storage.googleapis.com/deepmind-media/gemini/gemini_v2_5_\nreport.pdf . Technical Report.\nL. Guertler, B. Cheng, S. Yu, B. Liu, L. Choshen, and C. Tan. Textarena, 2025. URL https:\n//arxiv.org/abs/2504.11442 .\nC. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu,\nL. Qi, Z. Liu, and M. Sun. Olympiadbench: A challenging benchmark for promoting agi with\nolympiad-level bilingual multimodal scientific problems, 2024.\nD. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.\nMeasuring mathematical problem solving with the math dataset. NeurIPS, 2021.\nA. P . Jacob, A. Gupta, and J. Andreas. Emergent linguistic phenomena in multi-agent communi-\ncation games. arXiv preprint arXiv:2205.05984, 2022.\nN. Jaques, A. Ghandeharioun, J. H. Shen, C. Ferguson, A. Lapedriza, N. Jones, S. Gu, and\nR. Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in\ndialog. arXiv preprint arXiv:1907.00456, 2019.\nD. P . Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nH. W. Kuhn. A simplified two-person poker. Contributions totheTheory ofGames , 1:97–103,\n1950.\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu, J. E. Gonzalez, H. Zhang, and I. Stoica.\nEfficient memory management for large language model serving with pagedattention. In\nProceedings oftheACM SIGOPS 29th Symposium onOperating Systems Principles, 2023.\nJ. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side\ninformation. Advances inneural information processing systems, 20, 2007.\nA. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V . Ramasesh, A. Slone,\nC. Anil, I. Schlag, T. Gutman-Solo, et al. Solving quantitative reasoning problems with\nlanguage models. Advances inNeural Information Processing Systems, 35:3843–3857, 2022.\n19\n\nA. Liao, N. Tomlin, and D. Klein. Efficacy of language model self-play in non-zero-sum games,\n2024. URL https://arxiv.org/abs/2406.18872 .\nH. Lightman, V . Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\nI. Sutskever, and K. Cobbe. Let’s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nM. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In\nMachine learning proceedings 1994, pages 157–163. Elsevier, 1994.\nM. Liu, L. Jiang, Y. Liang, S. S. Du, Y. Choi, T. Althoff, and N. Jaques. Chasing moving\ntargets with online self-play reinforcement learning for safer language models, 2025a. URL\nhttps://arxiv.org/abs/2506.07468 .\nZ. Liu, C. Chen, X. Wan, C. Du, W. S. Lee, and M. Lin. Oat: A research-friendly framework for\nllm online alignment. https://github.com/sail-sg/oat , 2024.\nZ. Liu, C. Chen, W. Li, P . Qi, T. Pang, C. Du, W. S. Lee, and M. Lin. Understanding r1-zero-like\ntraining: A critical perspective. arXiv preprint arXiv:2503.20783, 2025b.\nMistral. Mistral-small-3.1-24b-instruct-2503. https://huggingface.co/mistralai/Mistr\nal-Small-3.1-24B-Instruct-2503 , 2025.\nJ. Nash. The bargaining problem. Econometrica, 18(2):155–162, 1950.\nOpenAI. Learning to reason with llms. OpenAI Blog, 2024. URL https://openai.com/o1 .\nOpenAI. GPT-4.1. OpenAI API, 2025. URL https://openai.com/index/gpt-4-1/ .\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P . Mishkin, C. Zhang, S. Agarwal,\nK. Slama, A. Ray, et al. Training language models to follow instructions with human feedback.\nAdvances inNeural Information Processing Systems, 35:27730–27744, 2022.\nD. Paglieri, B. Cupiał, S. Coward, U. Piterbarg, M. Wolczyk, A. Khan, E. Pignatelli, Ł. Kuci ´ nski,\nL. Pinto, R. Fergus, et al. Balrog: Benchmarking agentic llm and vlm reasoning on games.\narXiv preprint arXiv:2411.13543, 2024.\nC. Park, S. Han, X. Guo, A. Ozdaglar, K. Zhang, and J.-K. Kim. Maporl: Multi-agent post-co-\ntraining for collaborative large language models with reinforcement learning. arXiv preprint\narXiv:2502.18439, 2025.\nR. Rafailov, J. Hejna, R. Park, and C. Finn. From r to q*: Your language model is secretly a\nq-function. In Conference onLanguage Modeling, 2024.\nD. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman.\nGpqa: A graduate-level google-proof q&a benchmark. In First Conference onLanguage\nModeling, 2024.\nA. Ruoss, F. Pardo, H. Chan, B. Li, V . Mnih, and T. Genewein. Lmact: A benchmark for in-context\nimitation learning with long multimodal demonstrations. arXiv preprint arXiv:2412.01441 ,\n2024.\nB. Sarkar, W. Xia, C. K. Liu, and D. Sadigh. Training language models for social deduction with\nmulti-agent reinforcement learning. arXiv preprint arXiv:2502.06060, 2025.\n20\n\nZ. Shao, P . Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al.\nDeepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv\npreprint arXiv:2402.03300, 2024.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser,\nI. Antonoglou, V . Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep\nneural networks and tree search. nature, 529(7587):484–489, 2016.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Ku-\nmaran, T. Graepel, et al. Mastering chess and shogi by self-play with a general reinforcement\nlearning algorithm. arXiv preprint arXiv:1712.01815, 2017.\nR. S. Sutton and A. G. Barto. Reinforcement Learning: AnIntroduction . The MIT Press, second\nedition, 2018.\nG. Tesauro. Temporal difference learning and td-gammon. Communications oftheACM , 38(3):\n58–68, 1995.\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\nI. Higgins. Solving math word problems with process-and outcome-based feedback. arXiv\npreprint arXiv:2211.14275, 2022.\nZ. Wan, Y. Li, X. Wen, Y. Song, H. Wang, L. Yang, M. Schmidt, J. Wang, W. Zhang, S. Hu,\net al. Rema: Learning to meta-think for llms with multi-agent reinforcement learning. arXiv\npreprint arXiv:2503.09501, 2025.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang,\net al. Mmlu-pro: A more robust and challenging multi-task language understanding bench-\nmark. In The Thirty-eight Conference onNeural Information Processing Systems Datasets\nand Benchmarks Track, 2024.\nZ. Wang, K. Wang, Q. Wang, P . Zhang, L. Li, Z. Yang, X. Jin, K. Yu, M. N. Nguyen, L. Liu, et al.\nRagen: Understanding self-evolution in llm agents via multi-turn reinforcement learning.\narXiv preprint arXiv:2504.20073, 2025.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-\nof-thought prompting elicits reasoning in large language models. In Advances inNeural\nInformation Processing Systems, volume 35, pages 24824–24837, 2022.\nR. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning. Machine learning, 8:229–256, 1992.\nY. Xie, Y. Ma, S. Lan, A. Yuille, J. Xiao, and C. Wei. Play to generalize: Learning to reason through\ngame play. arXiv preprint arXiv:2506.08011, 2025.\nH. Xin, Z. Ren, J. Song, Z. Shao, W. Zhao, H. Wang, B. Liu, L. Zhang, X. Lu, Q. Du, et al.\nDeepseek-prover-v1. 5: Harnessing proof assistant feedback for reinforcement learning and\nmonte-carlo tree search. arXiv preprint arXiv:2408.08152, 2024.\nF. Xu et al. Genius: A generalizable and purely unsupervised self-training framework for\nadvanced reasoning, 2025.\nA. Yang, A. Li, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Gao, C. Huang, C. Lv, et al. Qwen3\ntechnical report. arXiv preprint arXiv:2505.09388, 2025.\n21\n\nW. Yuan, R. Y. Pang, K. Cho, X. Li, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language\nmodels. arXiv preprint arXiv:2401.10020, 2024.\nX. Zhang, H. Zheng, A. Lv, Y. Liu, Z. Song, F. Sung, X. Chen, and R. Yan. Divide-fuse-conquer:\nEliciting\" aha moments\" in multi-scenario games. arXiv preprint arXiv:2505.16401, 2025.\nY. Zhang, S. Mao, T. Ge, X. Wang, A. de Wynter, Y. Xia, W. Wu, T. Song, M. Lan, and F. Wei. Llm\nas a mastermind: A survey of strategic reasoning with large language models. arXiv preprint\narXiv:2404.01230, 2024.\nA. Zhao, Y. Wu, Y. Yue, T. Wu, Q. Xu, M. Lin, S. Wang, Q. Wu, Z. Zheng, and G. Huang. Absolute\nzero: Reinforced self-play reasoning with zero data. arXiv preprint arXiv:2505.03335, 2025.\nL. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P . Xing,\nH. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot\narena, 2023. URL https://arxiv.org/abs/2306.05685 .\nQ. Zhu, D. Guo, Z. Shao, D. Yang, P . Wang, R. Xu, Y. Wu, Y. Li, H. Gao, S. Ma, et al. Deepseek-\ncoder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint\narXiv:2406.11931, 2024.\n22\n\nAppendix\nThis appendix provides comprehensive details supporting our main findings. App. A shows\nhow existing SFT and RLVR paradigms adapt to our turn-level MDP formulation. App. B\ncontains implementation specifics including hyperparameters, infrastructure details, and game\nenvironment observations referenced in Section 5.2. App. C presents extended results tables and\ndetailed pattern evolution analysis that support our case study in Section 6.1. App. D describes\nthe complete methodology for our reasoning pattern analysis, detailing how we discovered and\nquantified transfer mechanisms. Finally, App. E provides comprehensive descriptions of for all\ngame environments used in our experiments, including both training and evaluation games.\nA. Training Paradigms in T urn-Level MDPs\nThis section shows how existing training paradigms adapt to our turn-level MDP formulation\nintroduced in Section .\nA.1. Supervised Fine-T uning (SFT) in T urn-Level MDPs\nIn the turn-level setting, SFT requires a dataset DSFT={(𝑠𝑖,𝑐∗\n𝑖,𝑎∗\n𝑖)}𝑁\n𝑖=1of states with expert\nreasoning traces 𝑐∗\n𝑖and actions 𝑎∗\n𝑖. The model learns to imitate complete turn-level responses:\nLSFT(𝜃)=−E(𝑠,𝑐∗,𝑎∗)∼D SFT\u0002\nlog𝜋𝜃(𝑐∗,𝑎∗|𝑠)\u0003\n. (11)\nNote that in single-turn settings where each state 𝑠appears only once, SFT reduces to stan-\ndard behavior cloning. The key limitation remains: SFT requires expensive human annotation\nof both reasoning traces and final answers.\nA.2. Reinforcement Learning with Verifiable Rewards (RLVR) in T urn-Level MDPs\nRLVR (DeepSeek Team, 2024) eliminates the need for reasoning supervision, requiring only\nstate-answer pairs DRLVR={(𝑠𝑖,𝑎∗\n𝑖)}𝑁\n𝑖=1. In the turn-level formulation:\n𝐽RLVR(𝜃)=E𝑠∼D RLVR ,𝑦∼𝜋𝜃(·|𝑠)[𝑟(𝑠,𝑎)], (12)\nwhere𝑟(𝑠,𝑎)=I[𝑎=𝑎∗]indicates answer correctness and 𝑦contains both reasoning and action.\nIn single-turn settings without subsequent interactions, RLVR reduces to a contextual bandit\nproblem (Langford and Zhang, 2007). Recent works on mathematical (DeepSeek Team, 2024;\nShao et al., 2024) and code reasoning (Xin et al., 2024; Zhu et al., 2024) show that even this\nsimplified bandit-style RLVR can unlock sophisticated reasoning. However, these approaches\nstill require human-curated problem sets DRLVR , which SPIRAL eliminates through self-play.\nB. Experimental Setup Details\nThis section provides complete implementation details for reproducing our experiments. We\nbegin with visual examples of game environments, followed by our hyperparameter configura-\ntions.\n23\n\nB.1. Game Environment Observations\nThe language models receive structured text observations from each game environment. Fig. 7\nshows example observations from our three training games: TicTacToe, Kuhn Poker, and Simple\nNegotiation. These observations serve as the input prompts 𝑠𝑡at each turn, providing complete\ngame state information in natural language format.\nTicTacToeKuhn PokerSimple NegotiationYou are Player 0 in TicTacToe. Your stones appear as 'O' and your opponent's stones appear as 'X'.On your turn, you choose one empty cell by its numbered index and place your stone there.For example, '[4]' places your stone in the center cell of the board.Your objective is to form a continuous line of three of your stones in any row, column, or diagonal.Current Board:0 | 1 | 2---+---+---3 | 4 | 5---+---+---6 | 7 | 8Available Moves: [0], [1], [2], [3], [4], [5], [6], [7], [8][GAME] You are Player 1 in a 5 roundgame of Kuhn Poker.Game Rules:-Kuhn Poker uses a 3-card deck with J, Q, K (J lowest, K highest)-Each player antes 1 chip and receives 1 card each round-Game continues for 5 rounds-The player with the most chips after all rounds winsAction Rules:-'[check]': Pass without betting (only if no bet is on the table)-'[bet]': Add 1 chip to the pot (only if no bet is on the table)-'[call]': Match an opponent's bet by adding 1 chip to the pot-'[fold]': Surrender your hand and let your opponent win the pot[GAME] Starting round 1 out of 5 rounds.Your card is: KYour available actions are:[check], [bet]You are Player 0 in the Negotiation Game.You have some resources, and your task is to trade such that the total value of your resources increases.The resources and associated values you currently have are:+ [Wheat]    Qty: 12   Value: 6+ [Wood]     Qty: 18   Value: 8+ [Sheep]    Qty: 8    Value: 17+ [Brick]    Qty: 10   Value: 23+ [Ore]      Qty: 7    Value: 35At each turn, you can talk to your opponent or make a trade offer.Use the following special tokens for actions:-[Offer]: To make a trade offer.Format: [Offer: Offered Resources -> Requested Resources]Example: [Offer: 3 Sheep, 2 Ore -> 5 Brick, 2 Sheep]-[Accept]: To accept an incoming offer.-[Deny]: To deny an incoming offer (default).You can include additional text before and/or after these tokens.The game lasts for 10 turns in total.\nFigure 7|Example observations of three training game environments.\nFor games with partial observability such as Kuhn Poker and Simple Negotiation, we\nmaintain Markovian state representations by concatenating historical actions into the current\nstate𝑠𝑡. This ensures the model has sufficient information for decision-making despite hidden\ninformation.\nSimilarly, Fig. 8 presents observations from five evaluation environments used to test out-of-\ndistribution generalization. These games were never seen during training, allowing us to assess\nwhether learned skills transfer to novel game mechanics.\nB.2. Hyperparameter Configuration\nTable 7 presents the complete hyperparameter settings used across all experiments. These\nconfigurations were selected through preliminary experiments to balance training stability and\ncomputational efficiency. Please see our open-source codebase for a complete and reproducible\nexperiment example.\nThese hyperparameters remain fixed across all game environments and model scales to\nensure fair comparison. The distributed training infrastructure utilizes 8 H100 GPUs, with\nparallel actors generating game trajectories while a centralized learner performs synchronous\npolicy updates.\nC. Additional Results and Analysis\nC.1. Detailed Evolution of Case-by-Case Analysis\nTo understand how reasoning patterns develop during training, we tracked the evolution of\ncase-by-case analysis across checkpoints. Table 8 shows a concrete example from Minerva Math\nProblem 135, illustrating how models progressively develop structured reasoning.\nThis progression demonstrates how competitive self-play forces models to develop in-\ncreasingly structured approaches. Early attempts show unorganized reasoning, while later\n24\n\nSnakePigDiceTruthAndDeception[GAME] 2-Player Snake on a 5×5 grid.You control snake 0. Valid moves: '[up]'/'[down]'/'[left]'/'[right]' (or w/s/a/d).Objective: survive longest or be the longest and get the highest score (turn limit 40 turns).[GAME] Current Board:+-----------+| . . . . . || . 0 . A . || . . . . . || . . . 1 . || . . . . A |+-----------+[GAME] You are Player 0 playing a game of Pig Dice.Rules:- On your turn, you can either '[roll]' or '[hold]'- Roll a 2-6: Add to your turn total- Roll a 1: Lose turn total and end turn- Hold: Add turn total to your score and end turn- First to 100 points winsWhen it's your turn, you'll see the current scores and turn total.Respond with '[roll]' to roll the die or '[hold]' to bank your points.[GAME] You current turn total is 0. This is the first roll of your turn.Available actions: '[roll]' or '[hold]'[GAME] You are Player 0, and you are the Deceiver in the 'Truth and Deception' game.You know which of the following two facts is correct:Fact 1: The blue-ringed octopus is harmless to humans. (wrong)Fact 2: The blue-ringed octopus is one of the most venomous marine animals. (correct)Your goal is to convince the other player to choose the incorrect fact.You will have a conversation for 6 turns.After that, the other player will make their guess.On your turn, simply type your message.\nLiarsDiceConnectFour[GAME] You are Player 0 in an 2-player Liar's Dice game.Rules:- On your turn, you may either:  1) Make a new bid with a higher quantity or higher face (or both) than the current bid; i.e. '[Bid: 3, 4]',  2) Call the last bid by typing '[Call]'.If you call:  - If the actual count of that face value among all dice is less than the bid, the last bidder loses one die.  - Otherwise, the caller loses one die.A player who reaches 0 dice is eliminated. The last remaining player wins.[GAME] New round - Remaining dice:     Player 0: 5;    Player 1: 5Your current Dice arre: 1, 6, 5, 1, 1[GAME] You are Player 0 in Connect Four.Your disc symbol: X.The game board has 6 rows and 7 columns.Players take turns dropping their disc into one of the columns (0 to 6).The first to connect (their own) four discs vertically, horizontally, or diagonally wins.On your turn, enter the column number in squared brackets to make your move.For example: '[col 4]' or '[col 1]'.[GAME] Board state:0 1 2 3 4 5 6-------------. . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . .. . . . . . .Figure 8|Example observations of five evaluation game environments.\ncheckpoints exhibit clear case separation and systematic analysis, a pattern that emerges from\ngame playing and transfers to mathematical problem solving.\nC.2. Comprehensive Benchmark Results\nTable 9 presents extended results showing SPIRAL’s performance across different training\nconfigurations and base models.\nThese results reveal several important insights. First, single-game SPIRAL training (40.0-\n41.4% average) outperforms supervised fine-tuning on 25,000 expert examples (38.4% average),\nvalidating that self-play can discover more effective reasoning strategies than imitating expert\ndemonstrations. Second, multi-game training (42.3-42.7% average) consistently outperforms\nsingle-game variants, suggesting that diverse cognitive challenges create more robust reasoning\ncapabilities. Third, SPIRAL improves even strong models like DeepSeek-Distill-Qwen-7B (from\n59.7% to 61.7%), demonstrating that competitive game self-play training can enhance models\nthat already excel at reasoning tasks.\nD. Case Study Methodology\nThis section details our systematic approach to discovering and analyzing reasoning pattern\ntransfer from games to mathematics. Rather than searching for predetermined patterns, we\nemployed a bottom-up discovery process to identify what reasoning strategies naturally emerge\nand transfer between domains.\nD.1. Data Collection Framework\nOur analysis examined reasoning traces from two sources across three training checkpoints:\n25\n\nParameter Value\nAC T O R\nMaximum response length 4096 tokens\nSampling temperature 1.0\n(top P , top k) (1.0, -1)\nLE A R N E R\nOptimizer AdamW\nAdam parameters ( 𝛽1,𝛽2) (0.9, 0.95)\nWeight decay 0.0\nGradient norm clipping 1.0\nBatch size 128\nDiscount factor 1.0\nEMA decay rate 0.95\nLearning rate scheduler Constant\nLearning rate 1 ×10−6\nInner proximal update epoch 2\nKL loss coefficient 0.0\nKL penalty coefficient 0.0\nPolicy clipping parameter 0.2\nTable 7|Hyperparameter configurations used in all experiments.\nGame Trajectories: We collected 290 complete Kuhn Poker games, focusing on winning trajecto-\nries to identify successful reasoning strategies. Each trajectory includes the complete thought\nprocess from initial card observation through final decision.\nMathematical Solutions: We analyzed 46,792 solution attempts across MATH500, AIME,\nOlympiadBench, and Minerva Math benchmarks. Solutions were categorized by success\n(score=1) or failure (score=0) to understand which reasoning approaches prove effective.\nTemporal Analysis: Checkpoints at steps 0 (initial), 128 (intermediate), and 400 (final) capture\nthe evolution of reasoning complexity throughout training.\nD.2. Bottom-Up Pattern Discovery Process\nRather than searching for predefined patterns, we employed GPT-4.1 to discover patterns that\nnaturally emerge in the data. This bottom-up approach ensures we capture the actual reasoning\nstrategies used rather than imposing our expectations.\n26\n\nTable 8|Evolution of Case-by-Case Analysis in Minerva Problem 135\nStage Problem: Round cosmic ages to nearest power of 10\nEarly (Ckpt 16) Approach: Lumped reasoning without explicit case separation\n<output> The age when helium formed is approximately 10 minutes... The age\nwhen hydrogen formed is 377,000 years... The current age is 13.8 Gyr. Rounding to\nnearest power of 10, we get: 14 Gyr</output>\nCase Structure: No explicit cases - all subproblems handled in continuous text\nResult: Subproblem 2: 14 Gyr ×(should be 10)\nMid (Ckpt 128) Approach: Explicit enumeration of cases with structured analysis\n<output>\nCase 1 - Subproblem 0:\n1. Understanding the Context: BBN at ∼20 minutes\n2. Nearest Power of 10: 101=10\n3. Final Answer: 1 minute\nCase 2 - Subproblem 1:\n1. Understanding the Context: Recombination at ∼377,000 years\n2. Nearest Power of 10: 105=100, 000\n3. Final Answer: 400000 years\nCase 3 - Subproblem 2:\n1. Understanding the Context: Current age ∼13.8 Gyr\n2. Nearest Power of 10: Identifies 1010correctly\n3. Final Answer: 13.8 Gyr</output>\nCase Structure: Clear case separation with consistent 3-step process\nResult: Still incorrect final rounding ×\nLate (Ckpt 400) Approach: Systematic case-by-case with correct execution\n<think>\nCase 1 - Subproblem 0:\n1. Understand Context: He formation in early universe\n2. Estimate Time: ∼20 minutes after Big Bang\n3. Round to Power of 10: 20 minutes\nCase 2 - Subproblem 1:\n1. Understand Context: H atom formation (recombination)\n2. Estimate Time: ∼380,000 years\n3. Round to Power of 10: 400000 years\nCase 3 - Subproblem 2:\n1. Understand Context: Current universe age\n2. Convert Units: 13.8 billion years = 13.8 Gyr\n3. Round to Power of 10: 13.8 →101= 10 Gyr</think>\nCase Structure: Complete systematic enumeration with correct logic\nResult: All cases solved correctly ✓\n27\n\nTable 9|SPIRAL training improves reasoning benchmarks for different base models\nModel MATH500 AIME’24 AIME’25 OlympiadBench AMC-23 Minerva Math GPQA MMLU-Pro Average\nQwen3-4B-Base Family\nQwen3-4B-Base 65.8 10.0 3.3 33.3 50.0 24.3 30.6 47.2 33.1\n+ SFT 76.0 16.7 13.3 38.4 50.0 31.2 33.0 48.8 38.4\n+ Mistral Opponent (KuhnPoker) 64.0 6.7 6.7 29.8 50.0 26.1 35.6 43.6 32.8\n+ Gemini Opponent (KuhnPoker) 69.2 10.0 13.3 33.8 50.0 33.8 35.3 55.5 37.6\n+ SPIRAL (TicTacToe) 75.6 10.0 13.3 38.5 55.0 42.6 37.6 57.7 41.3\n+ SPIRAL (KuhnPoker) 76.4 13.3 10.0 38.4 57.5 42.4 37.0 56.5 41.4\n+ SPIRAL (Negotiation) 76.4 10.0 10.0 41.2 57.5 36.0 34.7 54.2 40.0\n+ SPIRAL (TicTacToe+KuhnPoker) 76.2 13.3 16.7 40.7 60.0 41.5 35.7 57.2 42.7\n+ SPIRAL (TicTacToe+KuhnPoker+Negotiation) 74.2 20.0 16.7 39.4 55.0 38.6 36.9 57.7 42.3\nDeepSeek-Distill-Qwen-7B Family\nDeepSeek-Distill-Qwen-7B 90.8 46.7 36.7 56.9 92.5 48.2 48.6 57.1 59.7\n+ SPIRAL (Multi-Game) 93.0 43.3 46.7 57.9 92.5 51.1 49.6 58.9 61.7\nPattern Discovery Prompt\nAnalyze these {domain} reasoning traces using a BOTTOM-UP approach.\nDon’t look for predefined patterns. Instead, discover what reasoning\npatterns actually exist.\nREASONING TRACES ({len(sample)} samples from {len(traces)} total):\n{json.dumps([t[’reasoning’] for t in sample[:40]], indent=2)}\nYour task:\n1.Read through ALL the reasoning traces carefully\n2.Identify RECURRING patterns or structures that appear multiple times\n3.Group similar reasoning approaches together\n4.Name each discovered pattern based on what it actually does\n5.Count how many traces use each pattern\n6.Provide example quotes for each pattern\nFormat your response as:\nPATTERN 1: [Descriptive Name]\n- Description: [What this pattern does]\n- Count: X/{len(sample)} traces\n- Example quotes: [2-3 actual quotes showing this pattern]\nBe specific and grounded in the actual data. If you see a pattern only\nonce, don’t include it.\nThis discovery process revealed three dominant patterns that emerged independently in\nboth domains:\n1.Case-by-Case Analysis: Systematic enumeration of scenarios\n2.Expected Value Calculation: Probabilistic decision-making\n3.Pattern Recognition: Identifying regularities and structures\nD.3. Cross-Domain Transfer Quantification\nAfter discovering patterns in each domain, we compared them to identify which strategies\ntransfer between games and mathematics:\n28\n\nPattern Comparison Prompt\nCompare the reasoning patterns discovered in games vs mathematics:\nGAME PATTERNS:\n{game_patterns}\nMATH PATTERNS:\n{math_patterns}\nAnalyze:\n1.Which patterns appear in BOTH domains? (These show transfer)\n2.Which patterns are unique to each domain?\n3.Calculate transfer rates for shared patterns\n4.Identify the most successfully transferred reasoning strategies\n5.Explain WHY certain patterns transfer well\nFocus on concrete evidence of transfer, not speculation.\nThe transfer analysis revealed:\nCase-by-Case Analysis shows near-perfect transfer (72% in games to 71% in math) because\nsystematic enumeration represents domain-agnostic structured thinking. Whether analyzing\nopponent possibilities in Poker or solution branches in mathematics, the core cognitive skill\nremains identical.\nExpected Value Calculation exhibits limited transfer (78% in games to 28% in math) because\nexplicit probabilistic decision-making appears primarily in probability and optimization prob-\nlems. Most mathematical domains lack the decision-theoretic structure that makes this pattern\nuniversally applicable in games.\nPattern Recognition demonstrates amplification during transfer (35% in games to 45% in\nmath). Mathematics inherently requires pattern identification, so game training enhances an\nalready-essential mathematical skill, producing stronger pattern recognition than games alone\ndevelop.\nD.4. Pattern Evolution Analysis\nTo understand how reasoning develops during training, we tracked pattern emergence across\ncheckpoints:\nEvolution Analysis Prompt\nAnalyze how reasoning patterns evolve across training checkpoints:\n{json.dumps(evolution_analysis, indent=2)}\nFor each checkpoint:\n1.Describe the complexity of reasoning\n2.Identify new patterns that emerge\n3.Track how patterns become more sophisticated\n4.Show concrete examples of improvement\nFocus on the actual evolution you can see in the data.\nD.5. Concrete Transfer Example Identification\nTo validate transfer claims, we identified parallel reasoning structures across domains:\n29\n\nTransfer Example Prompt\nFind the clearest examples of reasoning transfer from games to\nmathematics:\n{json.dumps(examples, indent=2)}\nFor each complexity level (short/medium/long):\n1.Identify parallel reasoning structures\n2.Quote specific passages that show transfer\n3.Explain what cognitive skill is being transferred\n4.Rate the clarity of transfer (1-10)\nFocus on examples where the same reasoning approach clearly appears in\nboth domains.\nD.6. Pattern Classification at Scale\nAfter discovering patterns through bottom-up analysis, we classified all traces to measure\ntransfer rates:\nPattern Classification Prompt\nAnalyze these {domain} reasoning traces and find examples of each\npattern.\nPATTERNS TO FIND:\n1.Case-by-Case Analysis: Systematic enumeration of different\nscenarios/cases\n2.Expected Value Calculation: Explicit probability calculations,\ncomputing expected outcomes\n3.Pattern Recognition: Identifying recurring structures, noticing\ntrends\nREASONING TRACES:\n{json.dumps(batch, indent=2)}\nFor EACH pattern, identify ALL examples that clearly demonstrate it.\nReturn in this EXACT format:\nCASE_BY_CASE_INDICES: [list of indices]\nEXPECTED_VALUE_INDICES: [list of indices]\nPATTERN_RECOGNITION_INDICES: [list of indices]\nBe strict - only include clear examples of each pattern.\nD.7. Validation Methodology\nTo ensure robust findings, we implemented multiple validation steps:\nSampling Strategy: We analyzed 50 random trajectory samples per checkpoint to avoid\nselection bias while maintaining computational feasibility.\nSuccess Stratification: Separate analysis of successful and failed attempts revealed which\nreasoning strategies genuinely contribute to problem-solving rather than merely appearing\nfrequently.\nManual Verification: Spot-checking GPT-4.1’s pattern classifications against raw traces\nconfirmed the accuracy of automated analysis.\n30\n\nScale Validation: After discovering patterns through focused analysis, we classified all\n46,792 mathematical traces to verify that observed transfer rates hold at scale.\nThis methodology ensures our findings reflect genuine cognitive transfer rather than super-\nficial pattern matching, providing quantitative evidence that competitive gameplay develops\nreasoning skills applicable far beyond the training domain.\nE. Game Environment Specifications\nThis section provides detailed specifications for all game environments used in our experiments,\nincluding both training and evaluation games.\nE.1. Training Game Environments\nTicTacToe tests spatial pattern recognition through perfect information gameplay. Players\nalternate placing marks on a 3 ×3 grid, aiming to create lines of three. The deterministic nature\nisolates pure strategic reasoning from uncertainty management. Success requires recognizing\nwinning patterns, blocking opponent threats, and creating fork positions that guarantee victory.\nWe hypothesize these skills transfer to geometric reasoning and spatial visualization tasks.\nKuhn Poker introduces probabilistic reasoning through minimal hidden information. With\nonly three cards (Jack, Queen, King), one per player plus one undealt, the game distills poker\nto essential elements of bluffing and value betting. Players can check, bet, call, or fold, with\noutcomes determined by card strength. Success requires calculating expected values, modeling\nopponent behavior, and making decisions under uncertainty. These capabilities should transfer\nto probability problems and strategic decision-making.\nSimple Negotiation develops multi-constraint optimization through resource trading. Two\nplayers exchange Wood and Gold tokens, with opposing utility functions creating natural\ntension. Each player aims to maximize portfolio value through proposals and counteroffers.\nSuccess requires understanding opponent preferences, planning multi-step trades, and strategic\ncommunication. We expect these skills to enhance optimization problems and multi-constraint\nreasoning tasks.\nE.2. Out-of-Distribution Evaluation Games\nOur evaluation suite tests whether learned skills generalize to novel mechanics:\nSnake extends spatial reasoning to dynamic environments. Players control snakes navigating\ngrids to collect apples while avoiding collisions with walls, themselves, or opponents. This tests\nwhether static pattern recognition from TicTacToe transfers to trajectory planning and dynamic\nobstacle avoidance.\nConnect Four adds directional constraints to spatial reasoning. Players drop discs into columns\nwith gravity, aiming to connect four vertically, horizontally, or diagonally. The vertical placement\nmechanic tests whether TicTacToe strategies generalize to games with restricted movement\noptions.\nPig Dice isolates risk-reward decision making. Players repeatedly roll dice to accumulate points\nbut lose all turn points when rolling 1. This tests whether probabilistic reasoning from Kuhn\nPoker extends to sequential risk assessment and expected value calculation in different contexts.\nLiar’s Dice combines probability with deception detection. Players make escalating claims\n31\n\nabout hidden dice totals and challenge suspected bluffs. Success requires probability calculation,\nopponent modeling, and strategic deception, testing whether Kuhn Poker skills scale to multi-\nround bluffing games.\nTruth and Deception focuses on asymmetric information and persuasion. One player knows the\ntrue fact among options and misleads through conversation while the other must identify truth\nthrough questioning. This evaluates whether negotiation skills transfer to pure communication\nstrategy.\nThese diverse evaluation games probe different aspects of transfer learning, revealing which\ncognitive skills generalize beyond their training context and confirming that SPIRAL develops\nfundamental reasoning capabilities rather than game-specific tactics.\n32\n\n",
    "source": "http://arxiv.org/abs/2506.24119v1",
    "authors": [
      "Bo Liu",
      "Leon Guertler",
      "Simon Yu",
      "Zichen Liu",
      "Penghui Qi",
      "Daniel Balcells",
      "Mickel Liu",
      "Cheston Tan",
      "Weiyan Shi",
      "Min Lin",
      "Wee Sun Lee",
      "Natasha Jaques"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "type": "content"
  },
  {
    "id": "2506.24117v1_abstract",
    "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models",
    "content": "Title: Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models\n\nAbstract: Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies.",
    "source": "http://arxiv.org/abs/2506.24117v1",
    "authors": [
      "David M. Smiley"
    ],
    "categories": [
      "cs.CL"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24117v1_content",
    "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models",
    "content": "arXiv:2506.24117v1  [cs.CL]  30 Jun 2025Computational Detection of\nIntertextual Parallels in Biblical Hebrew\nA Benchmark Study Using\nTransformer-Based Language Models\nDavid M. Smiley\nUniversity of Notre Dame\ndsmiley@nd.edu\nAbstract\nIdentifying parallel passages in biblical Hebrew is foundational in biblical schol-\narship for uncovering intertextual relationships. Traditional methods rely on manual\ncomparison, which is labor-intensive and prone to human error. This study evaluates\nthe potential of pre-trained transformer-based language models, including E5, Ale-\nphBERT, MPNet, and LaBSE, for detecting textual parallels in the Hebrew Bible.\nFocusing on known parallels between the books of Samuel/Kings and Chronicles, I\nassessed each model’s capability to generate word embeddings that delineate parallel\nfrom non-parallel passages. Utilizing cosine similarity and Wasserstein Distance mea-\nsures, I found that E5 and AlephBERT show significant promise, with E5 excelling in\nparallel detection and AlephBERT demonstrating stronger non-parallel differentiation.\nThese findings indicate that pre-trained models can enhance the efficiency and accu-\nracy of detecting intertextual parallels in ancient texts, suggesting broader applications\nfor ancient language studies.\n1 Introduction\n1The identification of parallel passages in biblical Hebrew (BH) texts has long been a\ncornerstone of biblical scholarship, serving as a critical tool for understanding intertextual\nrelationships and theological development (Fewell, 1992). Scholars have traditionally relied\non manual comparisons to trace these connections (Harvey, 2004; Miller, 2011; Schnittjer,\n2021). These parallels are not merely textual repetitions but often involve reinterpreta-\ntions and recontextualizations that provide profound insights into the evolution of biblical\nnarratives (Fishbane, 1985; Kalimi, 2004).\nTraditionalmethodsfordetectingtheseparallels, however, arelabor-intensiveandsuscep-\ntible to human error, particularly when the connections are implicit, nuanced, or even subtle.\n1All code and data can be found in the following Github repository: https://github.com/dmsmiley/\ndetect-bh\n1\n\nThe linguistic complexity of BH, with its rich morphology and intricate verbal structures, ex-\nacerbates these challenges (Singh et al., 2012). These difficulties are further magnified when\nscholars manually attempt to analyze texts spanning extensive chronological, thematic, and\ntheological corpora, which push the limits of one’s attention and lead to an inconsistency in\nintertextual findings.\nRecent advances in natural language processing (NLP) present new possibilities for ad-\ndressing these limitations. Large language models (LLMs), particularly transformer-based\narchitectures, excel at capturing semantic relationships across vast datasets (Devlin et al.,\n2019). These models process large amounts of text with high accuracy, generating rich word\nembeddings that reflect nuances of meaning and context in a way that traditional methods\ncannot.\nIn this study, I evaluate the potential of pre-trained transformer models, such as E5,\nAlephBERT, MPNet, and LaBSE to aid detection of parallel passages in the Hebrew Bible\n(HB). By focusing on known parallels that the book of Chronicles (Chr) reuses from the\nearlier narratives of Samuel (Sam) and Kings (Kgs), I assess each models’ ability to generate\nword embeddings that accurately delineate known parallel and non-parallel verses in the HB.\nThis research not only seeks to provide benchmarks of model accuracy for tasks in textual\nsimilarity, but it also lays the groundwork for future NLP applications in biblical studies and\ncan be applied to other ancient literature more broadly.\n2 Previous Computational Attempts at Biblical He-\nbrew Parallels\nVarious computational methods have been applied to the challenge of identifying parallels\nin BH, though they have often been constrained by the tools and techniques available at the\ntime. Notably, van Peursen and Talstra (2007) sought to systematically compare parallel\ntexts from 2 Kgs, Isaiah, and Chr using a computer-assisted synopsis. Their approach\nhighlights the difficulty in establishing a rule-based method for identifying parallel texts.\nCriteria of what counts for a parallel are often subjective and proper parameters are opaque.\nIn recent works, Shmidman, Koppel, and Porat (2018) have significantly refined rule-\nbased approaches for detecting parallels in large Hebrew and Aramaic corpora. Their al-\ngorithms are designed to detect approximate matches, which account for rephrasing, or-\nthographic differences, and interpolations. This method has proven effective in identifying\ntextual reuse in complex corpora like the Babylonian Talmud, where near-identical passages\noften exhibit minor discrepancies.\nShmidman (2022) further refined his work by creating a hashing algorithm that breaks\neach lexeme into the smallest unit for word representation. However, as Shmidman acknowl-\nedges, this method is susceptible to numerous false positive matches. The inherent limitation\nof rule-based methods lies in their inability to capture deeper semantic relationships, often\nmissing the broader contextual nuances in rewritten texts like those in Chr. This under-\nscores the challenges faced by rule-based approaches in understanding the complex semantic\nlandscape of ancient texts.\nThese frequency- and rule-based approaches rely too heavily on surface-level lexical\n2\n\nmatching rather than more sophisticated linguistic representations (Mars, 2022). Further-\nmore, parallel passages are often not exact copies of previous texts, meaning that passages\nwith significant omissions, additions, or rewrites, such as those in Chr, are not accurately\ncaptured by these methods. This highlights the need for a method(s) that can effectively\ncapture semantics rather than solely being dependent on superficial matching.\nType of\nTextual\nDiffer-\nenceSam/Kgs\nRefer-\nenceSam/Kgs Text Chr\nRefer-\nenceChr Text\nAddition\nand Name\nChanges2 Sam 5:6 The king and his men\nwent to Jerusalem\nagainst the Jebusites, the\ninhabitants of the land.1 Chr\n11:4David and all of Israel\nwent to Jerusalem, which\nwas Jebus. There the\nJebusites were the\ninhabitants of the land.\nOmission 2 Sam\n6:15David and all of the\nhouse of Israel brought\nup the ark of the\ncovenant...1 Chr\n15:28[ ] All of Israel brought\nup the ark of the\ncovenant...\nAddition\nand\nOmission2 Kgs\n11:4Jehoiada...took the\nofficers of one hundred\nfrom the Carites and the\nguard, and brought them\nto him in the house of\nYHWH, and he cut a\ncovenant with them.2 Chr\n23:1Jehoiada...took the\nofficers of one hundred,\nAzariah son of Jeroham,\nIshmael son of\nJehohanan, Azariahu son\nof Obed, Maasieah son of\nAdaiahu, and Elishaphat\nson of Zichri into a\ncovenant agreement with\nhim.\nOmission 2 Sam\n7:14I will be like a father to\nhim, and he will be like a\nson to me. Whenever he\ngoes astray, I will punish\nhim with the staff of men\nand with the beatings of\nthe songs of man.1 Chr\n17:13I will be like a father to\nhim, and he will be like a\nson to me. [ ]\nTable 1: Examples from Kalimi’s work (2004) on the rewriting patterns employed by the\nauthor of Chr to reshape earlier parallel passages.\nRecent advances, particularly in transformer-based models, offer a solution to problem\nparallels, like those shown above. Unlike frequency-based methods, transformers can gen-\nerate embeddings at the verse level based on the relationship of a word in a given context,\nthus capturing the semantic variation (Vaswani et al., 2017). This ability to contextualize\nwords semantically makes them particularly suited to uncovering parallel passages in BH.\n3\n\nDespite their potential, no systematic evaluation has been conducted to benchmark these\nmodern language models on BH texts. While models like AlephBERT (Seker et al., 2021)\nhave been trained on vast amounts of modern Hebrew (MH), their effectiveness on ancient\nHebrew remains untested. The linguistic differences between MH and BH, combined with\nthe absence of BH-specific language models, create a significant gap in our understanding of\nhow well these pre-trained models can handle ancient language tasks.\nThisstudyaddressesthisgapbyprovidingthefirstcomprehensivebenchmarkoftransformer-\nbasedmodelsfordetectingbiblicalparallels. ByleveragingknownparallelsbetweenSam/Kgs\nand Chr, I systematically evaluate how different pre-trained LLMs perform when creating\nembeddings for BH verses. This benchmark study not only advances NLP techniques for\nancient texts but also provides crucial empirical data for scholars seeking to apply these\npowerful tools to biblical scholarship without needing to develop models from scratch.\n3 Objectives\nThe primary objective of this study is to assess the effectiveness of transformer-based lan-\nguage models in identifying parallel passages within BH texts. Specifically, I will evaluate\nhow well pre-trained models such as E5, AlephBERT, MPNet, and LaBSE can capture the\nrelationships between parallel texts, particularly those found between the books of Sam/Kgs\nand Chr.\nBy comparing how well these models create word embeddings that can distinguish true\nparallel passages from non-parallels, I can estimate their suitability for finding unknown\nconnections which have traditionally relied on manual methods in biblical scholarship. In\ndoing so, I can demonstrate how transformer-based models can enhance the accuracy and\nefficiency of detecting intertextual parallels in ancient texts. In addition to assessing the\nstrengths and limitations of each model, this study contributes to the broader effort to\nintegrate advanced NLP techniques into the field of digital humanities. Our findings offer\nan empirical method for detecting previously unknown parallel and intertextual connections\non historical texts.\n4 Data and Models\n4.1 Dataset\nThe Hebrew text for this study is drawn from the Biblia Hebraica Stuttgartensia Amstelo-\ndamensis (BHSA) corpus, as compiled by the Eep Talstra Centre for Bible and Computer at\nVrije Universiteit Amsterdam (Peursen et al., 2015). Additionally, there are 558 verses from\nChr with recognized, known parallels in Sam/Kgs (Endres et al., 1998), ensuring a consistent\nand empirical set of passages for evaluating transformer-based models’ performance.\n4.2 Pre-Trained Models\nFour pre-trained transformer models were used, each selected for their strengths in text clas-\nsification and documentary similarity. Some are optimized for their scale and multilingual\n4\n\ncapabilities like Multilingual E5 (Wang et al., 2024). Another has its strength in representing\nlarger sentences and paragraphs such as MPNet (Song et al., 2020). LaBSE is a language\nagnostic approach to embeddings (Feng et al., 2022). In contrast the final model, Aleph-\nBERT, is specifically pre-trained on MH (Seker et al., 2021). By selecting robust, diverse\nmodels I am able to observe which approaches generalize embeddings best for the task of\ntext similarity in BH.\n5 Model Evaluation\n5.1 Cosine Similarity\nCosine similarity is the preferred measure for evaluating vector proximity in text similarity\ntasks by calculating the cosine of the angle between two vectors in high-dimensional space.\nThis score is particularly useful for analyzing embeddings generated by LLMs, as it captures\nboth syntactic and semantic relationships between words (Gomez et al., 2022).\nIn the context of BH, parallel passages often display variations in word choice, spelling,\nand theological interpretation, as shown above (Kalimi, 2004). So a later account is not\nalways a simple “copy and paste” version of its earlier parallel text, which is why frequency-\nbased methods are insufficient to account for finding parallels. By representing each passage\nas a vector, cosine similarity quantifies the semantics of a text, regardless of superficial\nadditions, omissions, or spelling differences.\nFor this study, cosine similarity was computed in two ways. First, each passage in Chr\nwas compared to its known parallel in the books of Sam/Kgs, generating a cosine similarity\nscore:\nCosine(vChr\ni,vSam/Kgs\nj )\nSecond, each passage from Chr was compared to every other verse in Sam/Kgs that is not a\nknown parallel to compute the mean non-parallel cosine similarity:\nµNonParallel (vChr\ni) =1\nNN/summationdisplay\nj=1Cosine (vChr\ni,vSam/Kgs\nj ),wherevj/∈Parallels (vi)\nThis approach evaluates how well the models detect true parallels and their ability to avoid\nfalse positives by assigning high similarity to unrelated passages.\n5.2 Parallel and Non-Parallel Verse Cosine Similarities\nThe table below summarizes the performance of the pre-trained models, including the mean\ncosine similarity for parallel and non-parallel passages, and the statistical significance (p-\nvalue) between the differences of the mean cosine for parallel and non-parallel texts. The\nAppendix contains the histogram of the parallel cosine similarity scores for each model.\nE5 consistently outperforms the other models, achieving a high mean cosine similarity of\n0.966 for parallel passages, with 75.0% of passages exceeding a 95% similarity. However, its\nhigh similarity score for non-parallel passages (0.882) suggests an inability to properly delin-\neate between the parallels and non-parallels. AlephBERT, while designed for Hebrew, un-\nderperforms relative to E5, with a mean cosine similarity of 0.914 for parallel passages. Only\n5\n\nModel Mean\nCosine\n(Parallel)Mean\nCosine\n(Non-\nParallel)P-value % Cosine ≥\n95%% Cosine≥\n98%\nE5 0.966 0.882 1.06×10−28475.0% 40.11%\nAlephBERT 0.914 0.638 9.98×10−31444.78% 17.27%\nMPNet 0.903 0.649 1.61×10−23146.76% 25.72%\nLaBSE 0.828 0.375 2.48×10−27026.07% 11.33%\nTable 2: Cosine similarity, including p-value from t-test of the difference between the mean\ncosine similarity scores.\n17.27% of parallels exceeded 98% similarity. However, AlephBERT demonstrates stronger\nseparation between parallel and non-parallel passages (mean cosine for non-parallels: 0.638),\nsuggesting that it might be properly fitting better than E5.\nMPNet and LaBSE exhibit the weakest performance with lower mean cosine similarities\nfor parallel passages (0.903 and 0.828, respectively) and less distinct separation between par-\nallel and non-parallel passages. This likely stems from their lack of optimization for Hebrew,\nwhich affects their ability to capture the linguistic nuances of the text. A t-test confirms\nthat the observed differences between mean parallel and non-parallel cosine similarities are\nsignificant across all models, with p-values below 1e-100. For instance, E5’s p-value of 1.06e-\n284 indicates a highly significant distinction between the mean cosine score for parallel and\nnon-parallel texts. So the models are at least able to consistently achieve similarity scores\nthat distinguish parallel and non-parallel passages, despite not being optimized on BH.\n5.3 Distribution-BasedTestingforParallelandNon-ParallelVerses\nTo further assess model performance a distribution-based metric called the Wasserstein Dis-\ntance was employed, which measures the separation between the cosine similarity distribu-\ntions for parallel and non-parallel passages. A higher Wasserstein Distance indicates greater\nseparation and a stronger proclivity to distinguish between parallel and non-parallel verses\n(Leo et al., 2023).\nModel Wasserstein Distance\nE5 0.0812\nAlephBERT 0.2764\nMPNet 0.2540\nLaBSE 0.4532\nTable 3: Wasserstein Distance scores for each model.\nThe results reveal important insights about model characteristics. E5 exhibits the lowest\nWasserstein Distance (0.0812), indicating significant overlap between the cosine similarity\n6\n\ndistributions for parallel and non-parallel passages. This confirms E5’s tendency to assign\nhighsimilarityscoresfortrueparallelsbroadly; however, itdoessoattheexpenseofassigning\nhigh cosine similarity scores to false positives quite consistently, as reflected in this limited\ndistribution separation.\nIn contrast, AlephBERT shows a larger Wasserstein Distance (0.2764), indicating better\nseparation between parallel and non-parallel passages. While AlephBERT’s overall cosine\nsimilarity for parallel passages is lower than E5’s, its greater separation suggests that it is\nless prone to false positives.\nMPNet and LaBSE also exhibit larger Wasserstein Distances (0.25 and 0.45, respec-\ntively), reflecting better distinction between parallel and non-parallel passages. However,\ntheir lower overall performance in mean cosine similarity limits their effectiveness for de-\ntecting true parallels. MPNet has a slightly worse, but similar performance to AlephBERT\non both measures. While LaBSE has the largest Wasserstein Distance, as the next section\nwill confirm, it may be too conservative in identifying parallels, potentially missing subtle\nintertextual connections.\n5.4 Evaluating False Positives with Classification Metrics\nCosine similarity was also used to find the closest match for each passage, helping to identify\nwhich specific text was most similar to our parallel text in Chr. This nearest-neighbor\napproach evaluates whether the models are finding true parallels or capturing incorrect\nmatches. To further assess model performance, classification reports were created, focusing\non precision, recall, and F1-score. The micro average metrics (Rainio et al., 2024) gave\na clear picture of how well each model distinguished between true parallels and unrelated\npassages based on finding the closest cosine similarity match for each verse in Chr.\nModel Recall Precision F1-Score\nE5 0.85 0.92 0.88\nAlephBERT 0.82 0.92 0.87\nMPNet 0.68 0.90 0.78\nLaBSE 0.72 0.86 0.78\nTable 4: Classification report of the closest parallel passage according to cosine similarity.\nE5achievedamicroaverageprecisionof0.92andarecallof0.85, indicatingthatwhilethe\nmajority of detected parallels were correct, around 15% of true parallels were not identified\nas the closest parallel text based on cosine similarity. This resulted in a strong F1-score of\n0.88, demonstrating effective performance overall.\nAlephBERT demonstrated similar performance, with a micro average precision of 0.92\nand a recall of 0.82, resulting in an F1-score of 0.87. The metrics for AlephBERT are close\nto those of E5, suggesting that both models perform similarly in identifying true parallels\nand avoiding false positives for the closest parallel detected.\nMPNet and LaBSE showed lower effectiveness overall. They performed well with F1-\nscores of 0.78, but still fell short of the performance levels seen with E5 and AlephBERT.\n7\n\nThese classification metrics complement cosine similarity and Wasserstein Distance anal-\nyses by providing insights into the balance between precision and recall for each model.\nE5 and AlephBERT both effectively detect parallels, with similar precision and recall met-\nrics, indicating comparable performance. MPNet and LaBSE demonstrate limited reliability,\nsuggesting a combined approach using E5 and AlephBERT may yield the best results for\nidentifying intertextual parallels in BH texts.\n5.5 Model Conclusions\nBased on the evaluation of pre-trained models, E5 and AlephBERT stand out as the most\neffective models for detecting true parallel passages in BH texts. E5 excels at detecting\nexplicit parallels, achieving high mean cosine similarity for known passages. However, its\nperformance also reveals a tendency to catch false positives, with high similarity scores for\nnon-parallel texts, as reflected in its relatively low Wasserstein Distance. This makes E5\nhighly capable of identifying parallels but a bit less effective in distinguishing true from false\npositives.\nAlephBERT, while slightly behind E5 in mean cosine similarity for parallel passages,\nshows a slight edge in avoiding false positives. Its lower similarity for non-parallel texts\nand higher Wasserstein Distance indicate better separation between related and unrelated\npassages. This makes AlephBERT particularly suitable when minimizing incorrect matches\nis a priority, providing a complementary strength to E5.\nMPNet, and LaBSE showed weaker performance in identifying true parallels, largely due\nto their lower optimization for BH. These models exhibited lower cosine similarities and\ngreater difficulty in distinguishing parallel from non-parallel texts. Despite their strengths,\nthey should be avoided in their current forms as it relates to creating word embeddings for\nBH.\nThese findings suggest that using both the E5 and AlephBERT models as a check and\nbalance of one another offers a promising solution for enhancing parallel detection accuracy.\nFuture work should focus on fine-tuning these pre-trained models for BH to maximize their\ncomplementary strengths, potentially revolutionizing the identification of intertextual paral-\nlels, not only in BH, but also in other ancient texts. This approach enables scholars to utilize\nadvanced NLP tools without the need to develop new models from scratch, instead focusing\non optimizing and adapting existing models for specific historical and linguistic contexts.\n6 Implications for Future Studies\nThis study demonstrates the capability of pre-trained transformer-based models to iden-\ntify parallel verses in BH using document similarity measures such as cosine similarity and\nWasserstein Distance. E5 and AlephBERT show significant promise in detecting these pas-\nsages despite differences in lexical choice or sentence structure. Although not perfect, by\nadapting these models and fine-tuning them later I can bypass the issues of attempting to\nbuild our own BH language model from scratch.\nExpanding this research to other ancient languages, such as Syriac, Greek, or Latin,\ncould provide further insights into intertextuality in other areas of ancient language study.\n8\n\nIf these models generalize well with BH, then in theory they could also work well in other\nlanguages on which they are not explicitly trained. Developing embeddings from pre-trained\nmodels for ancient texts could transform the study of literary connections in the ancient\nworld, offering new tools for scholars in the digital humanities.\n7 Limitations\nWhile this study demonstrates the promise of pre-trained transformer-based models for de-\ntecting biblical parallels, several limitations should be acknowledged. First, this evaluation\nfocuses exclusively on narrative texts, specifically the known parallels between Sam/Kgs\nand Chr. The HB encompasses numerous, variegated genres, like poetry, wisdom litera-\nture, prophetic oracles, and legal codes, each with their own ways of displaying intertextual\nconnections. The performance of these models on something like poetic parallelism in the\nPsalter, the covenant code in Exodus, or prophetic sayings of Jeremiah remains untested,\nthus limiting the generalizability of our findings across the full spectrum of biblical literature.\nSecond, the embeddings generated by these pre-trained models, while effective for simi-\nlarity detection, remain largely opaque and uninterpretable. This \"black box\" nature of all\nlanguage models built on the transformer architecture prevents scholars from understand-\ning why certain passages are deemed similar, potentially obscuring important linguistic or\ntheological nuances that traditional philological methods would capture. This limitation is\nparticularly significant for biblical studies, where understanding the specific mechanisms of\ntextual reuse is often as important as identifying the parallels themselves.\nFinally, this study evaluates existing pre-trained models rather than developing a BH-\nspecific language model. While our approach demonstrates that adaptation of existing mod-\nels is viable and practical, a purpose-built model trained on BH corpora might better capture\nthe unique morphological, syntactic, and semantic features of ancient Hebrew texts, poten-\ntially improving both accuracy and interpretability. Unfortunately, the HB is a small corpus\nand building a language model from the ground up is probably not feasible with the data\nneeds of transformers, although fine tuning a model like AlephBERT or E5 for textual sim-\nilarity on BH should not be ruled out and could actually point toward productive avenues\nfor future research.\n9\n\nAppendix\nEach graph contains the distribution of cosine similarity scores for all known parallels (in\nblue) alongside the distribution of non-parallel cosine similarity scores (in red) according to\nthe embeddings created by the different models.\n(a) E5\n (b) AlephBERT\n(c) LaBSE\n (d) MPNet\nReferences\n•Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019) “BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding”, Proceedings of the\n2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pp. 4171–4186. https://doi.org/10.\n18653/v1/N19-1423\n•Enders, J., Millar, W., & Burns, J. (1998). Chronicles and Its Synoptic Parallels in\nSamuel, Kings, and Related Biblical Texts . The Liturgical Press.\n•Feng, F., Yang, Y., Cer, D., et al., (2022). Language-agnostic BERT Sentence Embed-\nding. https://doi.org/10.48550/arXiv.2007.01852\n10\n\n•Fewell,D.(1992). Reading between Texts: Intertextuality and the Hebrew Bible . Louisville:\nWestminster John Knox Press.\n•Fishbane, M. (1985). Biblical Interpretation in Ancient Israel . New York: Oxford\nPress.\n•Gomez, J., andVázquez, P.(2022). AnEmpiricalEvaluationofDocumentEmbeddings\nand Similarity Metrics for Scientific Articles. Applied Sciences, 12 (11), 5664. https:\n//doi.org/10.3390/app12115664\n•Harvey, J. (2004). Retelling the Torah: The Deuteronomistic Historian’s Use of Tetra-\nteuchal Narratives . New York: Bloomsbury.\n•Kalimi, I. (2004). The Reshaping of Ancient Israelite History in Chronicles . Eisen-\nbrauns.\n•Leo, J., Ge, E., and Li, S., (2023) Wasserstein Distance in Deep Learning. http:\n//dx.doi.org/10.2139/ssrn.4368733\n•Mars, M. (2022). From Word Embeddings to Pre-Trained Language Models: A State-\nof-the-ArtWalkthrough. Applied Sciences, 12 (17), 8805. https://doi.org/10.3390/\napp12178805\n•Miller, G. (2011). Intertextuality in Old Testament Research. Currents in Biblical\nResearch, 9 , pp. 283–309.\n•Rainio, O., Tueho, J., and Klén, R. (2024). Evaluation Metrics and Statistical Tests\nfor Machine Learning. Scientific Reports, 14 , 6086. https://doi.org/10.1038/\ns41598-024-56706-x\n•Schnittjer, G. (2021). Old Testament Use of the Old Testament . Grand Rapids: Zon-\ndervan.\n•Seker, M., Bandel, E., Barekat, D., et al., (2021). AlephBERT: A Hebrew Large\nPre-Trained Language Model to Start-off your Hebrew NLP Application With. In\nProceedings of the 2022 Conference on Computational Natural Language Processing ,\npp. 321–330. https://doi.org/10.48550/arXiv.2104.04052\n•Shmidman, A., Koppel, M., and Porat, E. (2018). Identification of Parallel Passages\nAcross a Large Hebrew/Aramaic Corpus. Journal of Data Mining and Digital Human-\nities.https://doi.org/10.46298/jdmdh.1388\n•Shmidman, A. (2022). Automatic Identification of Biblical Citations and Allusions in\nHebrew Texts. In Jewish Studies in the Digital Age , pp. 335-348. https://doi.org/\n10.1515/9783110744828-015\n•Singh, N., and Habash, N. (2012). Hebrew Morphological Preprocessing for Statistical\nMachine Translation. In Proceedings of the 16th EAMT Conference , pp. 43-50.\n11\n\n•Song, K., Tan, X., Qin, T., et al., (2020). MPNet: Masked and Permuted Pre-training\nfor Language Understanding. In Proceedings of the 34th International Conference on\nNeural Information Processing Systems , pp. 16857–16867. https://doi.org/10.\n48550/arXiv.2004.09297\n•van Peursen, W., and Talstra, E. (2007). Computer-Assisted Analysis of Parallel Texts\nin the Bible. Vetus Testamentum, 57 (1), pg. 45–72.\n•vanPeursen, W., Sikkel, C., andRoorda, D.(2015). HebrewTextDatabaseETCBC4b.\nhttps://doi.org/10.17026/dans-z6y-skyh\n•Vaswani, A., Shazeer, N., Parmar, N., et al., (2017). Attention is All You Need. In\nAdvances in Neural Information Processing Systems 30 (NIPS 2017), pp. 5998–6008.\nhttps://doi.org/10.48550/arXiv.1706.03762\n•Wang, L., Yang, N., Huang, X., et al., (2024). Multilingual E5 Text Embeddings: A\nTechnical Report. https://doi.org/10.48550/arXiv.2402.05672\n12\n\n",
    "source": "http://arxiv.org/abs/2506.24117v1",
    "authors": [
      "David M. Smiley"
    ],
    "categories": [
      "cs.CL"
    ],
    "type": "content"
  },
  {
    "id": "2506.24115v1_abstract",
    "title": "Nonlinear Symmetry-Fragmentation of Nonabelian Anyons In Symmetry-Enriched Topological Phases: A String-Net Model Realization",
    "content": "Title: Nonlinear Symmetry-Fragmentation of Nonabelian Anyons In Symmetry-Enriched Topological Phases: A String-Net Model Realization\n\nAbstract: Symmetry-enriched topological (SET) phases combine intrinsic topological\norder with global symmetries, giving rise to novel symmetry phenomena. While\nSET phases with Abelian anyons are relatively well understood, those involving\nnon-Abelian anyons remain elusive. This obscurity stems from the\nmulti-dimensional internal gauge spaces intrinsic to non-Abelian anyons -- a\nfeature first made explicit in [1,2] and further explored and formalized in our\nrecent works [3-8]. These internal spaces can transform in highly nontrivial\nways under global symmetries. In this work, we employ an exactly solvable model\n-- the multifusion Hu-Geer-Wu string-net model introduced in a companion paper\n[9] -- to reveal how the internal gauge spaces of non-Abelian anyons transform\nunder symmetries. We uncover a universal mechanism, global symmetry\nfragmentation (GSF), whereby symmetry-invariant anyons exhibit internal Hilbert\nspace decompositions into eigensubspaces labeled by generally fractional\nsymmetry charges. Meanwhile, symmetry-permuted anyons hybridize and fragment\ntheir internal spaces in accordance with their symmetry behavior. These\nfragmented structures realize genuinely nonlinear symmetry representations --\nto be termed coherent representations -- that transcend conventional linear and\nprojective classifications, reflecting the categorical nature of symmetries in\ntopological phases. Our results identify nonlinear fragmentation as a hallmark\nof non-Abelian SETs and suggest new routes for symmetry-enabled control in\ntopological quantum computation.",
    "source": "http://arxiv.org/abs/2506.24115v1",
    "authors": [
      "Nianrui Fu",
      "Siyuan Wang",
      "Yu Zhao",
      "Yidun Wan"
    ],
    "categories": [
      "cond-mat.str-el",
      "cond-mat.stat-mech",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24115v1_content",
    "title": "Nonlinear Symmetry-Fragmentation of Nonabelian Anyons In Symmetry-Enriched Topological Phases: A String-Net Model Realization",
    "content": "arXiv:2506.24115v1  [cond-mat.str-el]  30 Jun 2025Prepared for submission to JHEP\nNonlinear Symmetry-Fragmentation of Nonabelian\nAnyons In Symmetry-Enriched Topological Phases: A\nString-Net Model Realization\nNianrui FuaSiyuan WangaYu Zhao1aYidun Wan2a,b\naState Key Laboratory of Surface Physics, Department of Physics, Center for Field Theory and\nParticle Physics, and Institute for Nanoelectronic devices and Quantum computing, Fudan Uni-\nversity, Shanghai 200433, China\nbHefei National Laboratory, Hefei 230088, China\nE-mail: nrfu21@m.fudan.edu.cn, siyuanwang18@fudan.edu.cn,\nyuzhao20@fudan.edu.cn, ydwan@fudan.edu.cn\nAbstract: Symmetry-enriched topological (SET) phases combine intrinsic topological or-\nder with global symmetries, giving rise to novel symmetry phenomena. While SET phases\nwith Abelian anyons are relatively well understood, those involving non-Abelian anyons re-\nmain elusive. This obscurity stems from the multi-dimensional internal gauge spaces intrin-\nsic to non-Abelian anyons—a feature first made explicit in [1, 2] and further explored and\nformalized in our recent works [3–8]. These internal spaces can transform in highly nontriv-\nial ways under global symmetries. In this work, we employ an exactly solvable model—the\nmultifusion Hu-Geer-Wu string-net model introduced in a companion paper [9]—to reveal\nhow the internal gauge spaces of non-Abelian anyons transform under symmetries. We un-\ncover a universal mechanism, global symmetry fragmentation (GSF), whereby symmetry-\ninvariant anyons exhibit internal Hilbert space decompositions into eigensubspaces labeled\nby generally fractional symmetry charges. Meanwhile, symmetry-permuted anyons hy-\nbridize and fragment their internal spaces in accordance with their symmetry behavior.\nThese fragmented structures realize genuinely nonlinear symmetry representations—to be\ntermed coherent representations—that transcend conventional linear and projective classi-\nfications, reflecting the categorical nature of symmetries in topological phases. Our results\nidentifynonlinearfragmentationasahallmarkofnon-AbelianSETsandsuggestnewroutes\nfor symmetry-enabled control in topological quantum computation.\n1Corresponding author\n2Corresponding author\n\nContents\n1 Introduction 2\n2S3Quantum Double Phase in the S3Enlarged HGW String-Net Model 3\n3 Enriching the S3Quantum Double with EM-Exchange Symmetry 5\n4 SymmetryTransformationandFragmentationofthe S3QuantumDouble\nAnyons 6\n4.1 Symmetry Transformation Through Domain Wall 6\n4.2 Nonlinear Representation of the EM-Exchange Symmetry 9\n5 Discussion 12\nA String-net Model 13\nA.1 Topological Features 15\nA.2 Excited States 16\nA.3 The Output UMTC is the Center of the Input UFC 18\nA.4 The String-Net Model with Non-Commutative Input UFCs 19\nB The Anyon Species of S3Quantum Double Phase 21\nC Multi-fusion Category as the Input of SET Model 21\nC.1 Input data: A Multifusion Category with Associated Isomorphisms 22\nC.2 Hamiltonian 23\nD TheCategoricalDataof Z2SymmetryEnriched S3Multi-fusionCategory 24\nD.1 The Fusion Rules 24\nD.2 The 6j-Symbols 24\nE The anyon Spectrum of EM-Exchange Symmetry Enriched S3String-net\nModel 25\nF Anyon Fragmentation Pattern 29\nG Proof that ρGandρHare not projective representations 29\n– 1 –\n\n1 Introduction\nSymmetry-enriched topological (SET) phases arise when topological phases are endowed\nwith global symmetries[10–23], yielding structures believed to extend beyond both the Lan-\ndau–Ginzburg paradigm1[24–26] and ordinary topological order. When topological phases\nand symmetries interplay in SET phases, more structures emerge[12, 16–18, 27], greatly\nenriching the landscape of possible quantum phases and having important implications for\nboth theoretical classification schemes[17, 18, 28, 29] and practical applications.\nSET phases may be classified by symmetry fractionalizations[18, 21, 30–35]; how-\never, symmetry fractionalizations in general do not directly correspond to how anyons\nare transformed under global symmetries[18], especially for non-Abelian anyons. Symme-\ntry transformations of non-Abelian anyons in SET phases remain elusive. The key point\nlies in the internal gauge spaces of non-Abelian anyons: Unlike Abelian anyons, which\nare 1-dimensional, non-Abelian anyons may carry multiple flux types and gauge charge\nsectors[2, 7, 36, 37]—and thus bearing multi-dimensional nontrivial internal Hilbert spaces\nand transforming under symmetry in sophisticated ways—not merely acquiring a phase\nfactor—under a global symmetry action.\nTherefore, it is essential to explicitly account for the internal spaces of non-Abelian\nanyonsinordertodescribetheirtransformationsunderglobalsymmetryactions. Neverthe-\nless, conventional TQFT approaches treat anyons as abstract simple objects in a modular\ntensor category[1, 38], hiding their internal gauge structures. Our recent construction of\nthe enlarged Hu-Geer-Wu (HGW) string-net model of topological phases[8] made it pos-\nsible to explicitly formulate and investigate non-Abelian anyons’ internal spaces and is\nfurther extended to a model of SET phases[8, 9]. By means of these internal gauge spaces\nof anyons, we show rich structures of symmetry actions emerge on non-Abelian topological\norders.\nIn our work, we employ this string-net model of SET phases (see Sections II and III)\nto probe the global-symmetry actions on non-Abelian anyons. Although our approach is\ngeneric, to be concrete, we consider a specific example of the S3quantum-double phase\nenriched by its electromagnetic-exchange ( Z2) symmetry. We find that under EM-exchange\nsymmetry, the two non-Abelian types, CandF, are permuted, while the other non-Abelian\nanyontypesremaininvariantunderthesymmetry. Nevertheless, thesenon-Abeliananyons’\nmulti-dimensional internal Hilbert spaces undergo nontrivial global symmetry transforma-\ntions: The internal Hilbert spaces of certain types of anyons may fragment into eigensub-\nspaces labeled by distinct (and generally fractional) global symmetry charges. Crucially,\nthesetransformationscannotbecapturedbyordinarylinearorprojectiverepresentations—\ninstead, they realize genuinely nonlinear representations arising from the categorical sym-\nmetry nature of the EM–exchange symmetry of the S3quantum-double topological phase.\nAs for the pair of anyon types permuted under the EM-exchange symmetry, their inter-\nnal spaces are hybridized and then fragmented according to global symmetry rather than\nanyon types. We call this ubiquitous mechanism global symmetry fragmentation (GSF).\n1A recent paper by two of us[8] shows that topological phases and their phase transitions can be encom-\npassed by a generalized Landau-Ginzburg Paradigm.\n– 2 –\n\nFigure1 : Partofthestring-netmodellattice. Atail(wavyline)isattachedtoanarbitrary\nedge of each plaquette.\nOur findings establish nonlinear symmetry fragmentation as a universal feature of\nnon-Abelian SET phases. Understanding how global symmetries enrich non-Abelian topo-\nlogical phases not only deepens our theoretical understanding of SET phases but also may\noffer new insights and means of realizing universal topological quantum computation—\nparticularly given recent evidence that S3quantum double model can support universal\nquantum computation[39]—and suggests routes for engineering novel symmetry-enriched\nquantum materials.\n2S3Quantum Double Phase in the S3Enlarged HGW String-Net Model\nIn this section we briefly introduce the D(S3)topological phase realized in the enlarged\nHGW string-net model[8] with input UFC Vec(S3). This model is a lattice gauge theory\ncoupled with anyonic matter defined on a 2-dimensional trivalent spatial lattice. Each\nplaquette has a tail anchored to one of its edges2. Each edge/tail is oriented and labeled\nby a group element in\nS3={1,r,r2,s,sr,rs :r3=s2= (sr)2= 1},\nwhich is the configuration space of the gauge field. Reversing the orientation of an edge/tail\nwhile inversing the group element on it does not change the gauge field configuration. The\ngauge-field Hilbert space is spanned by all possible gauge-field configurations, obeying the\ngroup multiplication rules of S3at any vertex: Let i,j, andkare respectively the group\nelements on the three edges incident counterclockwise at a vertex (assuming each edge is\noriented towards the vertex; if not, reversing both its orientation and group element), the\nmultiplication ijk= 1∈S3must hold.\nIn the enlarged HGW string-net model, the S3lattice gauge field couples to the eight\ntypes ofD(S3)anyons, denoted as A,B,C,D,E,F,G,H , which are the matter fields re-\nsiding in plaquettes. Because S3is non-Abelian, several types of anyons are non-Abelian ,\n2The original string-net model in Ref. [24], which has no such tails, cannot fully describe charge excita-\ntions. These added tails carry anyons’ charges, thus enlarging the Hilbert space to encompass the complete\nanyon spectrum of the model.\n– 3 –\n\neach occupying a multi-dimensional internal gauge space, analogous to the colour spaces\nof quarks[40–45]. Nevertheless, standard TQFTs overlook the internal gauge dofs because\nthey treat each anyon as a structureless simple object in a modular tensor category. For a\ngenericJ-type anyon in a plaquette, its internal basis states are labeled by a pair (p,α).\nHere,p∈S3is theflux type that sources the gauge curvature in the plaquette via\nthe magnetic Gauss law, which dictates a commensurate gauge-field configuration on the\ntail in the plaquette. A flux type pmay have an associated charge space, spanned by\nthe label 1≤α≤dim(Charge space). In the current example, a charge space is a in\nfact representation space of S3. Abelian anyons only have a unique flux type pand one-\ndimensional charge space, while a non-Abelian anyon may admit several fluxes and/or a\nmulti-dimensional charge space. Such internal spaces transform covariantly under local\ngauge transformations [8].\nTable 1 records the 8 D(S3)anyon types, their fluxes, and internal-space basis.\nAnyon Species ABCDEFGH\nFluxes (Valued in S3) 11 1s,rs,srs,rs,srr,r2r,r2r,r2\nCharge Space Dimension 11 2 1 1 1 1 1\nGauge Space Dimension 11 2 3 3 2 2 2\nInternal-Space Basis 1111,12s,rs,srs,rs,srr,r2r,r2r,r2\nTable 1: Anyon species of D(S3)phase\nIn a pure topological phase, an anyon’s internal states are not observable, as they\nwould transform as the anyon hops on the lattice and inevitably interacts with the gauge\nfield. The only gauge-invariant quantity or the physical observable of an anyon is the anyon\ntype.\nTheD(S3)anyon braiding and self statistics are captured by the corresponding mod-\nularSandTmatrices:\nS=1\n6\n1 1 2 3 3 2 2 2\n1 1 2−3−3 2 2 2\n2 2 4 0 0−2−2−2\n3−3 0 3−3 0 0 0\n3−3 0−3 3 0 0 0\n2 2−2 0 0 4−2−2\n2 2−2 0 0−2−2 4\n2 2−2 0 0−2 4−2\n, T =\n1 0 0 0 0 0 0 0\n0 1 0 0 0 0 0 0\n0 0 1 0 0 0 0 0\n0 0 0 1 0 0 0 0\n0 0 0 0−1 0 0 0\n0 0 0 0 0 1 0 0\n0 0 0 0 0 0 e2πi\n3 0\n0 0 0 0 0 0 0 e−2πi\n3\n.(2.1)\n– 4 –\n\nNote that any global symmetry permissible on a topological phase must preserve the Sand\nTmatrices of the phase. Seen from the modular SandTmatrices above, we can see that\nthere theD(S3)phase admits a Z2symmetry that exchanges its anyon types CandF,\nwhile preserving all other anyon types. Since the C-type anyon has only a trivial flux but\na 2-dimensional charge space, while the F-type anyon has two flux type but trivial charge\nspace, this Z2symmetry is an EM-exchange symmetry.\n3 Enriching the S3Quantum Double with EM-Exchange Symmetry\nIn this section, we describe how to extend the enlarged HGW model describing the D(S3)\nphase to a model describing the EM-exchange symmetry enriched D(S3)phase. We focus\non this particular example, while the general construction is reported in a companian\npaper[9].\nItisknownthattheoriginalLevin-Wenstring-netmodelcandescribeSETphasesupon\nreplacing its input UFCs by multifusion categories[16–18]. Nevertheless, such constructions\nhave two limitations. First, they cannot manifest anyons’ internal spaces, thereby unable\nto investigate symmetry actions on non-Abelian anyons properly. Second, they require a\nmanual input of the global symmetries to be endowed on the topological phases rather than\nusing only the input UFCs of the string-net model. Our SET model overcomes these two\nshortcomings because (1) it is based on the enlarged HGW model that manifests anyons’\ninternal spaces by construction, and (2) it extracts the input multifusion categories from\nthe original input UFCs of the enlarged HGW model without ad hocknowledge of the\nglobal symmetries to be endowed.\nIn our companion work[9], we can first construct multifusion categories, each equipped\nwith an appropriate isomorphism (necessary for defining symmetry sectors), from the input\nUFCFof the enlarged HGW model. Then, by replacing Fwith one of these derived\nmultifusion category together with a chosen isomorphism as the input data of the enlarged\nHGW model, we obtain a lattice exactly solvable model of an SET phase. This model is\nreviewed in Appendix C.\nSuch a multifusion category is constructed from the irreducible bimodules over the\nFrobenius algebra objects of F. The technical details can be found in Ref. [9]. Here, we\nfocus on the case where F=Vec(S3), as described in the previous section. For the model\ndescribing the EM-exchange symmetry enriched D(S3)phase, we directly adopt from Ref.\n[9] the corresponding input multifusion category equipped with the necessary isomorphism\nderived from Vec(S3). This multifusion category can be arranged in a 2×2matrix:\nM=\n{1++,r++,r2\n++,s++,rs++,sr++} { α+−, β+−}\n{α−+, β−+} { 1−−,r−−,r2\n−−,s−−,rs−−,sr−−}\n.(3.1)\nThe fusion rules and 6j-symbols of this multifusion category can be found in Appendix\nD. Asexplained inAppendix C,thetwodiagonal setsof elements ofthe multi-fusion matrix\nM, i.e.,{1++,r++,r2\n++,s++,rs++,sr++}and{1−−,r−−,r2\n−−,s−−,rs−−,sr−−}, are two\nUFCs isomorphic to each other and are both isomorphic to Vec(S3). They are respectively\n– 5 –\n\nα+−+−=\nα−++−\nFigure 2 : Domain wall\nthe sets of the basic degrees of freedom of the two different symmetry sectors, labled by +\nand−, related by the EM-exchange symmetry. The off-diagonal sets in Mare the domain-\nwall degrees of freedom defined as follows. A domain wall between symmetry sectors +and\n−is oriented (see Fig. 2): With respect to the orientation, on the domain wall’s left and\nright are always respectively the +and−sectors, so the corresponding domain-wall degree\nof freedom take value in the set {α+−,β+−}. If we reverse the orientation of the domain\nwall, its degree of freedom should instead take value in {α−+,β−+}. The EM-exchange\nsymmetry transformation operator swaps the 2sectors:\nG:aij→a¯i¯j,∀a∈{1,r,r2,s,rs,sr}, i,j∈{+,−}, (3.2)\nwhere, ¯+ =−,¯−= +.\nThe SET model still has anyon excitations, which can be extracted from the half braid-\ningz-tensor of the multifusion category (3.1), as shown in Appendix E. More importantly,\nwe can read off directly from the z-tensors how the D(S3)anyons are transformed under\nthe symmetry. For example, when an anyon Cin the +sector hops into the −sector, it\nbecomes anyon F. Let’s look into the details.\n4 Symmetry Transformation and Fragmentation of the S3Quantum Dou-\nble Anyons\nNow are ready to study how the EM-exchange symmetry acts on the D(S3)phase and\ntransform the anyons therein in detail. The phenomenon global symmetry fragmentation\nwill surface naturally.\n4.1 Symmetry Transformation Through Domain Wall\nAs different symmetry sectors differ by a symmetry transformation, we can say that when\nananyoninonesymmetrysectorcrossesadomainwallandentersanothersymmetrysector,\nit undergoes a symmetry transformation, conducted by the domain wall, as depicted in\nFigure 3. For example, a C(anF) anyon in sector +upon crossing a domain wall would be\ntransformed into an F(aC) anyon in sector−. The conventional methods of studying SET\nphases would have to stop at this level of understanding the symmetry transformations of\nnon-Abelian anyons. Nonetheless, by means of our domain wall constructed in the enlarged\nHGW model, explicit transformations on anyons’ internal spaces can be revealed.\n– 6 –\n\naG(a)\nDW\nFigure 3 : Anyonacrossing a domain wall (blue thick line) undergoes a symmetry trans-\nformation and becomes anyon G(a). Red horizontal line represents the anyon’s hopping\npath.\nAs aforementioned, an anyon’s internal states are changing when the anyon hops on\nthe lattice. So, to focus only on global symmetry transformations on an anyon’s internal\nspace, we should consider the scenario where the anyon resides right in a plaquette right\nbeside a domain wall, then crosses the domain wall into the plaquette on the other side,\nand stops there.\nBack to the EM-exchange symmetry enriched D(S3)phase. Although according to\nEq. (3.1), apparently there are two types of domain walls labeled by αandβ, they in\nfact differ by a Z2gauge generated by the gauge degree of freedom s∈S3withs2= 1.[9].\nAs such, if we know the symmetry transformations on the anyons conducted by crossing\ndomain wall α, we can easily derive those conducted by crossing domain wall β. Hence,\nhereafter, we would consider domain wall αonly unless otherwise mentioned. The EM-\nexchange symmetry transformations on the D(S3)anyons can be directly read off from\nthez-tensors (Appendix E) of the SET model. For instance, from z(F,C);α+−\nr++,(11)−−,α+−= 1,\nwe can see that under symmetry transformation, Frtransforms into C11. We record all\nthese transformations in Table 2, which gives the representations ρJ(Gem)of the symmetry\ntransformation Gemover anyon J(while for any anyon Jthe representation of trivial\naction 1is trivial:ρJ(1) = 1). Note that here, Jmay not be a single anyon type of D(S3)\nbecause as seen, the EM-exchange symmetry can mix anyon types CandF. As such, in\nthe following, we would also consider J=C⊕F, which is a nonsimple object in D(S3).\nFor example\nρD(Gem) =1√\n3\n1 1 1\n1ei2π\n3e−i2π\n3\n1e−i2π\n3ei2π\n3\n. (4.1)\nThis result leads to three observations:\n1. Abelian anyons AandBare the symmetry singlets. So, they remain well-defined\nanyons in the SET phase, as indicated in the SET’s z-tensors (Appendix E).\n2. Certainnon-Abeliananyons’internalspacesarefragmentedintosubspacesaseigenspaces\nofthesymmetryaction. Forexample,theinternalspaceofanyon Hisfragmentedinto\n– 7 –\n\nD(S3)Anyon\nInternal Basis\nStatesEM-exchange symmetry\ntransformation\nA A\nB B\nC11 Fr\nC12 Fr2\nFr C11\nFr2 C12\nGr ei2π\n3Gr\nGr2 ei2π\n3Gr2\nHr e−i2π\n3Hr2\nHr2 e−i2π\n3Hr\nDs1√\n3(Ds+Drs+Dsr)\nDrs1√\n3(Ds+ei2π\n3Drs+e−i2π\n3Dsr)\nDsr1√\n3(Ds+e−i2π\n3Drs+ei2π\n3Dsr)\nEs1√\n3(Es+Ers+Esr)\nErs1√\n3(Es+ei2π\n3Ers+e−i2π\n3Esr)\nEsr1√\n3(Es+e−i2π\n3Ers+ei2π\n3Esr)\nTable 2: EM-exchange symmetry transformations on the internal states of D(S3)anyons,\nas conducted by hopping the anyons across an α-type domain wall. Here, Jx, whereJis\nan anyon type, and xis a internal basis label, is a shorthand notation of an internal state\nof aJ-type anyon.\ntwo symmetry eigenstates: Hr+Hr2andHr−Hr2, with definite symmetry charges\n2\n3and1\n6. (We will explain the nature of these charges in the next subsection.) We\ndub this phenomenon on non-Abelian anyons global symmetry fragmentation , which\nis common in any symmetry-enriched non-Abelian topological phases. An exception\nis theGtype, whose internal space is not fragmented; rather, a G-type anyon as a\nwhole acquires a global symmetry charge of 1/3. The fragmentation of type- Dand\nEanyons are somewhat more involved but can be easily derived from the z-tensors\nin Appendix E and is shown in Appendix F. It’s worth of note that the symmetry\nfragmentation pattern in an SET phase is gauge dependent. As we mentioned ear-\n– 8 –\n\nlier, the EM-exchange symmetry transformations can be defined as conducted by the\nβ-type domain wall (called the β-gauge), which is related to those in Table 2 by the\nα-type domain wall (the α-gauge). If we take the β-gauge, it would be the G-type\nanyons acquire fragmentation, while the H-type not fragmented.\n3. AnyonsCandFare exchanged. More concretely, their internal spaces are mixed up:\nC11±FrandC12±Fr2. So, the mixed internal spaces of CandFare fragmented\ninto two 2-dimensional eigenspaces of the symmetry:\nspan{C11+Fr, C12+Fr2},\nwith charge 0, and\nspan{C11−Fr, C12−Fr2},\nwith charge 1/2.\n4.2 Nonlinear Representation of the EM-Exchange Symmetry\nWe have attributed to the fragmented internal spaces of anyons fractional symmetry\ncharges; however, what is the nature of these charges? Do these charges label certain linear\nor projective representations of the symmetry group? We now address these questions. In\nthe current example, clearly these charges do not correspond to any linear or projective\nrepresentations of the EM-exchange symmetry because the symmetry group is Z2, which\nhas linear representations labeled by integers only and has no projective representations\nat all. In fact, the fractional charges we have found label nonlinear representations of Z2.\nThis can be shown by composing two consecutive symmetry transformations on the D(S3)\nanyons as follows.\nSince the symmetry transformations are conducted by hopping the anyons cross a\ndomain wall, composing two such transformations on an anyon should be hopping the\nanyon cross 2domain walls, as seen in Figure 4. In Figure 4(a), as pointed out in the\nfigure caption, Pachner moves3are invoked in composing the two hoppings; accordingly,\ncomposing the corresponding two symmetry formations must involve the effects of the\nPachner moves and thus cannot be directly multiplying the two representation matrices4\nof the two transformations.\nIn our EM-exchange symmetry enriched S3quantum double phase, composing two\nnontrivial symmetry transformations reads\nGJ\nem◦GJ\nem:=/summationdisplay\nbωabρJ\nab(Gem)ρJ\nbc(Gem), (4.2)\nwhich gives that/summationdisplay\nbωabρJ\nab(Gem)ρJ\nbc(Gem) =ρJ\nac(1) = 1, (4.3)\n3Pachner moves are topological moves. See Appendix A.1.\n4Note that nonlinear representations are not matrices; however, here we assume a section in the repre-\nsentation bundle is chosen for us to express the representation as matrices. The nonlinearity is manifest in\nthe composition of two symmetry transformations.\n– 9 –\n\n(a)1\nα+− α−+1\nα+− α−+1\nα+− α−+1\nα+− α−+\n(b)1\nα+− α−+1\nα+− α−+?\nFigure 4 : The composition of symmetry transformations, which is equivalent as the anyon\ncrosses two domain wall (blue thick line). The orientation of the edges are all set to be\ngoing upward.\nwhere matrices ρJ(Gem), e.g., (4.1), can be extracted from Table 2. Here, ωis a rank- 2\ntensor, which is due to the Pachner moves and is independent of anyon type J. Non-zero\nelements of ωare\nω1,r=ω1,r2=ω1,s=ω1,rs=ω1,sr= 1,\nωs,r=ωs,r2=ωs,rs=ωs,sr= 1,\nωr,r=ωr2,r2=ωrs,rs=ωsr,sr=ei2π\n3,\nωr,r2=ωrs,sr=e−i2π\n3.(4.4)\nAs opposed to the case of projective representations, ωis not a phase factor associated\nwith an entire matrix ρJ(Gem)but has different components for different matrix elements\nofρJ(Gem). Therefore, the representations ρJare neither linear nor projective, but non-\nlinear representations of the EM-exchange symmetry. The fractional charges carried by\nthe fragmented internal spaces of D(S3)anyons precisely label the irreducible nonlinear\nrepresentations. A basis transformation must be applied to an anyon’s internal space to\nmanifest the fragmentation and the corresponding irreducible nonlinear representations.\nTo be specific, we consider a the following instances.\nHere we provide 3examples:\n1. AccordingtoTable2, for J=H, thebasisstates HrandHr2areexchangedunderthe\nsymmetry transformation. Hence, the representation ρHcan be decomposed into two\n1-dimensional representations ρH+andρH−, whereH+=hr+Hr2,H−=Hr−Hr2.\nWe haveρH+(Gem) =e−i2π\n3, ρH−(Gem) =eiπ\n3, such that (4.3) in this case reads\nρH+(Gem)ρH+(Gem) =ei2π\n3ρH+(1);\nρH−(Gem)ρH−(Gem) =ei2π\n3ρH−(1).(4.5)\n– 10 –\n\nC11\nC12Fr\nFr2Gem\nGems s\nFigure 5 : Symmetry transformation Gemand gauge transformation sin the fragmented\nC⊕Fspace.\nOne might then mistake the irreducible representations ρH+andρH−as projective\nrepresentations of Z2and further regard them as linear representations because any\napparent projective representation of Z2is equivalent to a linear representation be-\ncauseH2(Z2,U(1)) ={0}. Nevertheless, as in Appendix G, we can show that ρH+\nandρH−can never be made equivalent to linear representations of Z2. Thus,His\nindeed fragmented into two nontrivial nonlinear representations of Z2characterized\nby charges 2/3and1/6. Now we have seen the true meaning of these charges, which\nwere first inferred in Table 2.\n2. IfJ=G, the two basis states GrandG2\nrboth acquire a phase factor ei2π\n3under the\nsymmetry action. So, (4.3) now reads\nρGr(Gem)ρGr(Gem) =e−i2π\n3ρGr(1);\nρGr2(Gem)ρGr2(Gem) =e−i2π\n3ρGr2(1).(4.6)\nLikewise in Appendix G, we can show that the composition of symmetry transfor-\nmations above indeed signifies a nonlinear representation of Z2. One might however\nmisconceive that G’s internal space is also fragmented to two copies of the same\n1-dimensional nonlinear representation. In fact, as the basis states GrandGr2are\nalways interchangeable under G’s internal gauge, they are not physically distinguish-\nable. Therefore, one should regard the 2-dimensional internal space of Gas an inde-\ncomposable 2-dimensional nonlinear representation space of Z2characterized by the\nphase factor ei2π\n3or charge 1/3.\n3. More interestingly is the case where J=C⊕F. The 4-dimensional representa-\ntionρC⊕Fcould be decomposed into two 2-dimensional representations ρ(C⊕F)+and\nρ(C⊕F)−, where\nV(C⊕F)+=span{C11+Fr, C12+Fr2},\nV(C⊕F)−=span{C11−Fr, C12−Fr2},(4.7)\nsuch thatρ(C⊕F)+(Gem) =1andρ(C⊕F)−(Gem) =−1. These two 2-dimensional\nrepresentations are irreducible for the following reason. Since the internal gauges of\n– 11 –\n\nC(F) can mixC11(Fr) andC12(Fr2), we cannot differentiate C11+Fr(C11−Fr)\nfromC12+Fr2(C12−Fr2). As such, all states in the 2-dimensional space V(C⊕F)+\n(V(C⊕F)−) carry the same symmetry charge 0(1/2), rendering the space a degenerate\neigenspace of the symmetry action. The composition rule (4.3) now takes the form\nρ(C⊕F)+(Gem)ρ(C⊕F)+(Gem) =ρ(C⊕F)+(1);\nρ(C⊕F)+(Gem)ρ(C⊕F)+(Gem) =ρ(C⊕F)+(1).(4.8)\nTherefore, it is clear that when the D(S3)phase is endowed with the EM-exchange\nsymmetry, certain types of its non-Abelian anyons do transform and fragment into nonlin-\near representations under the symmetry. This justifies the terminology nonlinear symmetry\nFragmentation of nonabelian anyons. The EM-exchange symmetry is essentially Z2, which\nappears to be an innocent group symmetry; however, it is in fact a categorical symmetry,\nwhich is reflected in its nonlinear representations that amalgamate the Z2group nature and\ntheVec(S3)categorical nature via the 6jsymbols. By the systematic construction of lattice\nmodels of SET phases done in our companion paper[9], nonlinear symmetry fragmentation\nis a general feature of symmetry-enriched non-Abelian topological phases.\n5 Discussion\nIn this paper, for the first time in the literature to date, we have explicitly studied the\nglobal symmetry action on non-Abelian anyons in an SET phase—the S3quantum double\nphase enriched by the EM-exchange symmetry. We discovered the phenomenon of non-\nlinear symmetry fragmentation by exemplifying how D(S3)non-Abelian anyons’s internal\nspaces are rearranged and decomposed into nonlinear irreducible representations of the\nEM-exchange (a Z2) symmetry. This phenomenon is not unique to the S3quantum double\nphaseenrichedbytheEM-exchangesymmetrybutubiquitousinSETphasesinvolvingnon-\nAbelian topological phases. A recent work[39] shows that the D(S3)non-Abelian anyons\ncan support universal quantum computation. Since now we have a finer understanding of\nhow such anyons behave in a richer phase—the D(S3)enriched by the EM-exchange sym-\nmetry, it would be interesting to explore the utility of nonlinear symmetry fragmentation\nfor topological quantum computation, such as improving gate and/or algorithm efficiency\nby harnessing the power of global symmetries.\nFurthermore, we have not seen a non-Abelian topological phase enriched by a non-\nAbelian global symmetry. A possible example is the D4quantum double phase enriched\nby the permutation symmetry S3. It would be illuminating to study the interplay between\nthe non-Abelian anyons and the non-Abelian global symmetry in such examples. Even\nmore exotically, if a non-Abelian topological phase is endowed with not a group but an\nan algebraic global symmetry, how would the non-Abelian anyons therein transform under\nthe global symmetry? How would the concept of nonlinear symmetry fragmentation be\ngeneralised in such cases?\n– 12 –\n\nAcknowledgments\nThe authors thank Hongguang Liu, Yuting Hu, and Yifei Wang for inspiring and helpful\ndiscussions. YW is supported by NSFC Grant No. KRH1512711, the Shanghai Municipal\nScience and Technology Major Project (Grant No. 2019SHZDZX01), Science and Technol-\nogy Commission of Shanghai Municipality (Grant No. 24LZ1400100), and the Innovation\nProgram for Quantum Science and Technology (No. 2024ZD0300101). The authors are\ngrateful for the hospitality of the Perimeter Institute during his visit, where the main part\nof this work is done. This research was supported in part by the Perimeter Institute for\nTheoretical Physics. Research at Perimeter Institute is supported by the Government of\nCanada through the Department of Innovation, Science and Economic Development and\nby the Province of Ontario through the Ministry of Research, Innovation and Science.\nA String-net Model\nIn this section, we briefly review the string-net model defined in Ref. [5], which was\nadapted from that in [2]. The string-net model is an exactly solvable model defined on a\n2-dimensional lattice. An example lattice is depicted in Fig. 1. All vertices are trivalent.\nWithin each plaquette of the lattice, a tail is attached to an arbitrary edge of the plaquette,\npointing inward. We will demonstrate that different choices of the edge to which the tail\nis attached are equivalent in Appendix A.1. Each edge and tail is oriented, but we’ll show\nthat different choices of directions are equivalent.\nThe input data of the string-net model is a unitary fusion category F, described by\na finite set LF, whose elements are called simple objects , equipped with three functions\nN:L3\nF→N,d:LF→R+, andG:L6\nF→C. The function Nsets the fusion rules of the\nsimple objects, satisfying\n/summationdisplay\ne∈LFNe\nabNd\nec=/summationdisplay\nf∈LFNd\nafNf\nbc, Nc\nab=Nb∗\nc∗a.\nThere exists a special simple object 1∈LF, called the trivial object , such that for any\na,b∈LF,\nNb\n1a=Na\n1b=δab,\nwhereδis the Kronecker symbol. For each a∈LF, there exists a unique simple object\na∗∈LF, called the opposite object ofa, such that\nN1\nab=N1\nba=δba∗.\nWe only consider the case where for any a,b,c∈LF,Nc\nab= 0or1. In this case, we define\nδabc=Nc∗\nab∈{0,1}.\nThe basic configuration of the string-net model is established by labeling each edge and\ntail with a simple object in LF, subject to the constraint on all vertices that δijk= 1for\n– 13 –\n\nthe three incident edges or tails meeting at this vertex, all pointing toward the vertex and\nrespectively counterclockwise labeled by i,j,k∈LF. We can reverse the direction of any\nedge or tail and simultaneously conjugate its label as j→j∗, which keeps the configuration\ninvariant. The Hilbert space Hof the model is spanned by all possible configurations of\nthese labels on the edges and tails.\nThe function d:LF→R+returns the quantum dimensions of the simple objects\ninLF. It is the largest eigenvalues of the fusion matrix and forms the 1-dimensional\nrepresentation of the fusion rule.\ndadb=/summationdisplay\nc∈LFNc\nabdc.\nIn particular, d1= 1, and for any a∈LF,da=da∗≥1.\nThe function G:L6\nF→Cdefines the 6j-symbolsof the fusion algebra. It satisfies\n/summationdisplay\nndnGpqn\nv∗u∗aGuvn\nj∗i∗bGijn\nq∗p∗c=Gabc\ni∗pu∗Gc∗b∗a∗\nvq∗j,/summationdisplay\nndnGijp\nklnGj∗i∗q\nl∗k∗n=δpq∗\ndpδijpδklq,\nGijm\nkln=Gklm∗\nijn∗=Gjim\nlkn∗=Gmij\nnk∗l∗=αmαnGj∗i∗m∗\nl∗k∗n∗,/vextendsingle/vextendsingle/vextendsingleGabc\n1bc/vextendsingle/vextendsingle/vextendsingle=1√dbdcδabc,(A.1)\nwhereαm=G1mm∗\n1m∗m∈{± 1}is the Frobenius-Schur indicator of simple object m.\nThe Hamiltonian of the string-net model reads\nH:=−/summationdisplay\nPlaquettes PQP, Q P:=1\nD/summationdisplay\ns∈LFdsQs\nP, (A.2)\nwhere operator Qs\nPacts on edges surrounding plaquette Pand has the following matrix\nelements on a hexagonal plaquette:\nQs\nPe1e2\ne3\ne4\ne5e6i7i1i2i3\ni4\ni5i6p :=δp,1δj1,j7/summationdisplay\njk∈LF6/productdisplay\nk=1/parenleftigg/radicalig\ndikdjkGekiki∗\nk+1\nsj∗\nk+1jk/parenrightigge1e2\ne3\ne4\ne5e6j7j1j2j3\nj4\nj5j61,\nand\nD:=/summationdisplay\na∈LFd2\na\nis the total quantum dimension of UFC F. Here, we only show the actions of the QP\noperator on a hexagonal plaquette. The matrix elements of QPoperators on other types\nof plaquettes are defined similarly. We also omit the “ |·⟩” labels surrounding all diagram\nfor simplicity, unless they are specifically required.\nIt turns out that\n(Qs\nP)†=Qs∗\nP, Qr\nPQs\nP=/summationdisplay\nt∈LFNt\nrsQt\nP, Q2\nP=QP, QP1QP2=QP2QP1.\n– 14 –\n\nThe summands QPin Hamiltonian Hare commuting projectors, so the Hamiltonian is\nexactly solvable. The ground-state subspace H0of the system is the projection\nH0=/bracketleftigg/productdisplay\nPlaquettes PQP/bracketrightigg\nH. (A.3)\nIf the lattice has the sphere topology, the model has a unique ground state |Φ⟩up to scalar\nfactors.\nA.1 Topological Features\nWe briefly review the topological nature of the ground-state subspace of the string-net\nmodel defined in Ref. [2]. Topologically, any two lattices with the same topology can be\ntransformed into each other by the Pachner moves . There are unitary linear maps between\nthe Hilbert spaces of two string-net models with the same input UFC on different lattices\nrelated by the Pachner moves[46], formally denoted as operators T. The ground states are\ninvariant under such linear transformations. There are three kinds of elementary Pachner\nmoves, whose corresponding linear transformations are:\nTma\nbd\nc=/summationdisplay\nn∈LF/radicalbig\ndmdnGabm\ncdnna\nbd\nc,\nT\na×xyb\n=/radicaligg\ndxdy\ndaδabδxya∗a,\nTa=1\nD/summationdisplay\nxy∈LF/radicaligg\ndxdy\ndaδxya∗\naxya\n.(A.4)\nHere, “×” marks a plaquette to be contracted. These three elementary Pachner moves\nand their corresponding unitary transformations can be composed. Given initial and final\nlattices, there are multiple ways to compose these elementary Pachner moves, but different\nways result in the same transformation matrices on the ground-state Hilbert space.\nWe have also noted that for a plaquette, different choices of edge to which the tail\nis attached are equivalent. These variations lead to distinct lattice configurations and,\nconsequently, different Hilbert spaces for the lattice model. The equivalence between such\ntwo Hilbert spaces is established by the following linear transformation T′:\nT′e1\ni0i1i2\np=/summationdisplay\nj∈LF/radicalig\ndi1djGi∗\n2e1i1\ni0p∗j\nene1\ni0ji2\np\nin.(A.5)\n– 15 –\n\nThe states where tails attach to other edges can be obtained recursively in this manner.\nFor convenience, in certain cases, we will temporarily incorporate auxiliary states with\nmultiple tails within a single plaquette. These states, despite having multiple tails in one\nplaquette, are all equivalent to states within the Hilbert space (with only one tail in each\nplaquette):\ni0i1r\nsj =/summationdisplay\nu∈LF/radicalig\ndjdpGr∗i∗\n1j\ni0s∗pi0i1p. (A.6)\nA.2 Excited States\nAnexcited state|φ⟩of the string-net model is an eigenstate such that QP|φ⟩= 0at some\nplaquettesP. In such a state, we say there are anyonsin these plaquettes P. We also refer\nto the ground states as trivial excited states, in which there are only trivial anyons in all\nplaquettes. We assume the sphere topology, in which the model has a unique ground state;\nnevertheless, the results in this section apply to other topologies.\nWe start with the simplest excited states with a pair of anyons in two adjacent pla-\nquettes with a common edge E. This state can be generated by ribbon operator WJ;pq\nE,\nwhich is a composition of an auxiliary operator WJ;pq\nE\nWJ;pq\nEj:=/summationdisplay\nk∈LF/radicaligg\ndk\ndjEzJ;k\npqjE\njkjq\np∗(A.7)\nand Pachner moves (A.5) and (A.6), where jEis the dof on edge E. For any state in the\nHilbert space of the model, each plaquette contains exactly one tail attached to a specific\nedge (could be edge E) of the plaquette. Operator WJ;pq\nE(A.7) introduces two new tails\non edgeE, and these new tails must be moved and fused with the original tails of the\nplaquettes by the subsequent Pachner moves in the creation opeartor.\nThe coefficients in Eq. (A.7), zJ;k\npqj, are called the half-braiding tensor of anyon type J,\ndefined by the following equation:\nδjtNt\nrs\ndtzJ;w\npqt=/summationdisplay\nu,l,v∈LFzJ;v\nlqrzJ;u\nplsdudvGr∗s∗t\np∗wu∗Gsrj∗\nqw∗vGs∗ul∗\nrv∗w. (A.8)\nWe will discuss the physical significance of this equation in Appendix A.3. The label J,\ncalled the anyon type , labels different minimal solutions zJof Eq. (A.8) that cannot be\nthe sum of any other nonzero solutions. Categorically, anyon type Jare labeled by simple\nobjectsinthe centerofUFC F, amodulartensorcategorywhosecategoricaldatarecordall\ntopological properties of the topological order that the string-net model describes, denoted\nasZ(F). In particular, the trivial anyon I∈LZ(F)satisfies\nzI;k\npqj=δp1δq1δjk.\n– 16 –\n\nThe statistics of anyon Jare recorded by the diagonal element of modular Tmatrix\nof UMTC Z(F), where\nTJK=1\ndtδJK/summationdisplay\np∈LFdpzJ;p\nttt.\nHere,tis an arbitrary charge of anyon J. The braiding of two anyons JandKis recorded\nby the modular Smatrix, whose matrix elements are\nSJK=1\nD/summationdisplay\np,q,k∈LFdk¯zJ;k\nppq¯zK;k\nqqp.\nAn anyonJhas trivial self-statistics if θJ= 1; two anyons JandKbraid trivially if and\nonly ifSJK=dJdK, wheredJis the quantum dimension of anyon J, defined as\ndJ=/summationdisplay\nJ’Chargespdp.\nStates with two quasiparticles in two non-adjacent plaquettes are generated by ribbon\noperators along longer paths. These longer ribbon operators result from concatenating\nshorter ribbon operators. For example, to create two quasiparticles J∗andJwith charges\np∗\n0andpnin two non-adjacent plaquettes P0andPn, we can choose a sequence of plaquettes\n(P0,P1,···,Pn), wherePiandPi+1are adjacent plaquettes with their common edge Ei.\nThe ribbon operator WJ;p0pn\nP0Pnis\nWJ;p0pn\nP0Pn:=/bracketleftigg/summationdisplay\np1p2···pn−1∈LFn−1/productdisplay\nk=1/parenleftig\ndpkBPkWJ;pkpk+1\nEk/parenrightig/bracketrightigg\nWJ;p0p1\nE0.\nDifferentchoicesofplaquettepaths (P0,P1,···,Pn)givethesameoperator WJ;p0pn\nP0Pnifthese\nsequences can deform continuously from one to another. Following the same procedure, we\ncan also define the creation operator of three or more anyons.\nAt the end of this section, we define the measurement operator MJ\nPmeasuring whether\nthere is an anyon Jexcited in plaquette P:\nMJ\nPe1e2\ne3\ne4\ne5e6i7i1i2i3\ni4\ni5i6p :=/summationdisplay\ns,t∈LFdsdtzJ;t\npps\ndpe1e2\ne3\ne4\ne5e6i7i1i2i3\ni4\ni5i6p\ntps\n×. (A.9)\nThe set of measurement operators are orthonormal and complete:\nMJ\nPMK\nP=δJKMJ\nP,/summationdisplay\nJ∈LZ(F)MJ\nP=1.\n– 17 –\n\nA.3 The Output UMTC is the Center of the Input UFC\nAs mentioned earlier, the string-net model’s output UMTC Z(F)is the center of its input\nUFCF, and the model is a physical representation of this center relationship. In this\nappendix, we explicitly demonstrate how this representation is understood.\nCategorically, an object Jin the center Z(F)is denoted as a pair J= (XJ,cXJ,·),\nwhereXJis an object in UFC F, andcxJ,·, called a half-braiding , is a set of morphisms\n{cXJ,y:XJ⊗y→y⊗XJ|y∈F}.\nA morphism cXJ,ybraids object XJwith object yinFand can be depicted as\nXJyyXJ\ncXJ,y =\nXJXJ\nyy\n.\nIn a UFC, all morphisms can be decomposed as direct sums of fusion of simple objects,\nand so can a half-braiding:\nXJXJ\nyy\n=/circleplusdisplay\np,q∈LJ/circleplusdisplay\nk∈LF/radicaligg\ndk\ndyzJ;k\npqy\npq\nyy\nk. (A.10)\nHere,LJ⊆LF, such that the direct sum of simple objects in LJisXJ:\nXJ=/circleplusdisplay\np∈LJp.\nThe expansion coefficients zJ;y\npqkare known as the half-braiding tensor ofJ. A half-braiding\nshould commute with any fusion in F:\nyu v\npq\n=\nyu v\npq\n. (A.11)\nExpanding Eqs. (A.11) using Eq. (A.10) leads to Eq. (A.8).\nFor a string-net model with input UFC F, an anyon type Jis a simple object in the\noutput UMTC Z(F), andJ’s charges take value in LJ. The action of creation operator\nWJ;pq\nEdirectly represents the half-braiding morphism cXJ,jEof objectXJwithjE∈LF,\nthe dof on edge E:\npq\njEjE\n=/summationdisplay\nk∈LF/radicaligg\ndk\ndjEzJ;k\npqjE\npq\njEjE\nk =WJ;pq\nEjE.\n– 18 –\n\nA.4 The String-Net Model with Non-Commutative Input UFCs\nIn this appendix, we briefly introduce the string-net model, whose input UFC Fhas non-\ncommutative fusion rules δ, i.e., there exists a,b,c∈LF, such that δabc̸=δacb. Note\nthat the states in the Hilbert space of such a string-net model must satisfy the constraint\nδabc̸= 0for any three incident edges or tails meeting at a vertex, counterclockwise carrying\ndegrees of freedom a,b,c, and all pointing toward the vertex.\nFor these non-commutative string-net models, Eq. (A.8) for half-braiding tensors z\nmay admit matrix solutions. In other words, a minimal solution zJ;k\npqjof the half-braiding\ntensor may not be a complex number but instead a unitary matrix, where Jis the anyon\ntypes labeling distinct minimal solutions, and p,q,j,k∈LF. ThezJtensor then becomes:\n[zJ;k\npqj]αβ∈C, 1≤α≤n(J,p), 1≤β≤n(J,q),\nwheren(J,p)∈N. For those anyons Jwithn(J,p)>1for chargep, a dyon type is not only\nlabeled by the anyon species Jbut also by the multiplicity index 1≤α≤n(J,p):\n(J,p,α ), J∈LZ(F), pisJ’s charge, 1≤α≤n(J,p).\nFor our convenience, we refer to the pair (J,p)of anyon type Jand charge type pas adyon\nmultiplet , which contains n(J,p)distinct dyon types (J,p,α ), whereαis called the multiplet\nindex. The number n(J,p)is called the degeneracy of the dyon multiplet (J,p). The pair\n(p,α), consisting of the charge type pand the multiplet index α, is referred to as the local\ndofof the anyon J.\nIn the Hilbert space of a non-commutative string-net model, two excited states, each\nwith a dyon in the same plaquette having the same anyon types Jand charge types p\nbut different multiplet indices α, should be orthogonal excited states. Nevertheless, the\noriginal Hilbert space, whose local Hilbert space on each tail is spanned only by the charges\np∈LF, is not capable of accommodating such a large number of distinct excited states.\nTherefore, to capture the full dyon spectrum, we have to enlargethe local Hilbert space\nof each tail, as in our previous work [ArXiv:2408.02664]. Dofs on edges do not need to be\nextended, as the edges pertain to the ground states because any path along an edge forms\na closed loop, where each vertex only respects the fusion rules, disregarding the multiplet\nindices.\nFor convenience, the local subspace on each tail is spanned by local dofs of dyons,\ndenoted as pJ,α, whereJis an anyon species having charge p, and 1≤α≤n(J,p). Different\nbasis states on a tail are orthogonal local states:\n/angbracketleftigg\npJ/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleqK,β/angbracketrightigg\n=δJKδpqδαβ.\nHere, we enlarged the tails’ local Hilbert spaces even for commutative string-net models.\nThis does not affect our discussions, as two excited states with different anyon types in the\nsame plaquettes must be orthogonal.\n– 19 –\n\nThe simplest creation operator WJ;(p,α)(q,β)\nEcreating a pair of dyons (J∗,p∗,α)and\n(J,q,β )in two adjacent plaquettes separated by edge Eis now defined as\nWJ;(p,α)(q,β)\nE\nenini2 e1\njjj1I,1\n1I,1:=/summationdisplay\nk∈LF/radicaligg\ndk\ndj[zJ;k\npqj]αβ\nenini2 e1\njkjqJ,β\np∗\nJ∗,α,(A.12)\nHere, I∈LZ(F)is the trivial anyon with unique charge 1∈LFand has no degeneracy\nnI= 1. The creation operator multiplications depend on the CG coefficients of the tensor\nproducts of the half-braiding z-tensors. We will not detail them here.\nA typical example of such a non-commutative input UFC is Vec(G), whereGis a\nnonabelian group. The simple objects of UFC Vec(G)are labeled by group elements in G,\nand for any a,b,c,d,e,f∈G,\nδabc= 1ifc∗=abelse 0, d a= 1, Gabe\ncdf=δabeδbcf∗δcde∗δdaf,\nwherec∗is the inversed element of c∈G. Equation (A.8) for z-tensors turns out to be\nzJ;(pa)\np(a∗pa)azJ;(a∗pab)\n(a∗pa)(b∗a∗pab)b=zJ;(pab)\np(b∗a∗pab)(ab).\nConsequently, the anyon types Jare labeled by a pair\nJ= (¯p,ρ),\nwhere ¯p={g∗pg|g∈G}forp∈Gis a conjugate class of Gcontaining element p, andρis\nan irreducible representation of the centralizer of conjugate class ¯p:\nZ(¯p) ={g∈G|g∗pg=p}.\nThe multiplet degeneracy of each dyon multiplet (J,p)of anyon type Jis the dimension\nof irrepρof centralizer Z(¯p):\nn((¯p,ρ),p)=dimρ,\nand the quantum dimension of anyon Jisn((¯p,ρ),p)|¯p|, the number of all J’s local dofs.\nIn particular, the fluxons in the string-net model with input UFC VecGare those dyons\nwith anyon type (¯1,ρ), whose trivial conjugate class is ¯1 ={1}⊂GandZ(¯1) =G. The\ncorrespoinding half-braiding tensor is:\nz(¯1,ρ);r\n1,1,r=Dρ\nαβ(g), 1≤α,β≤dimρ,∀r∈G.\nwhereDρis the representation matrix of irrep ρof groupG.\n– 20 –\n\nB The Anyon Species of S3Quantum Double Phase\nThere are 8types of anyons in D(S3)phase, which is showed in Table 1.\nTheir corresponding z-tensors are:\nzA;1\n1,1,1=zA;r\n1,1,r=zA;r2\n1,1,r2=zA;s\n1,1,s=zA;rs\n1,1,rs=zA;sr\n1,1,sr= 1; (B.1)\nzB;1\n1,1,1=zB;r\n1,1,r=zB;r2\n1,1,r2= 1, zB;s\n1,1,s=zB;rs\n1,1,rs=zB;sr\n1,1,sr=−1; (B.2)\nzC;1\n11,11,1= 1, zC;r\n11,11,r=ei2π\n3, zC;r2\n11,11,r2=e−i2π\n3,\nzC;1\n12,12,1= 1, zC;r\n12,12,r=e−i2π\n3, zC;r2\n12,12,r2=ei2π\n3,\nzC;s\n11,12,s= 1, zC;rs\n11,12,rs=ei2π\n3, zC;sr\n11,12,sr=e−i2π\n3,\nzC;s\n12,11,s= 1, zC;rs\n12,11,rs=e−i2π\n3, zC;sr\n12,11,sr=ei2π\n3;(B.3)\nzD;s\ns,s,1=zD;sr\ns,rs,r =zD;rs\ns,sr,r2=zD;rs\nrs,rs, 1=zD;s\nrs,sr,r =zD;sr\nrs,s,r2= 1,\nzD;sr\nsr,sr, 1=zD;rs\nsr,s,r =zD;s\nsr,rs,r2=zD;1\ns,s,s=zD;r\ns,rs,sr =zD;r2\ns,sr,rs = 1,\nzD;1\nrs,rs,rs =zD;r\nrs,sr,s =zD;r2\nrs,s,sr =zD;1\nsr,sr,sr =zD;r\nsr,s,sr =zD;r2\nsr,rs,s = 1;(B.4)\nzE;s\ns,s,1=zE;sr\ns,rs,r =zE;rs\ns,sr,r2=zE;rs\nrs,rs, 1=zE;s\nrs,sr,r =zE;sr\nrs,s,r2= 1,\nzE;sr\nsr,sr, 1=zE;rs\nsr,s,r =zE;s\nsr,rs,r2= 1, zE;1\ns,s,s=zE;r\ns,rs,sr =zE;r2\ns,sr,rs =−1,\nzE;1\nrs,rs,rs =zE;r\nrs,sr,s =zE;r2\nrs,s,sr =zE;1\nsr,sr,sr =zE;r\nsr,s,sr =zE;r2\nsr,rs,s =−1;(B.5)\nzF;r\nr,r,1=zF;r2\nr,r,r=zF;1\nr,r,r2=zF;r2\nr2,r2,1=zF;1\nr2,r2,r=zF;r\nr2,r2,r2= 1,\nzF;rs\nr,r2,s=zF;sr\nr,r2,rs=zF;rs\nr,r2,sr=zF;sr\nr2,r,s=zF;s\nr2,r,rs=zF;rs\nr2,r,sr= 1;(B.6)\nzG;r\nr,r,1= 1, zG;r2\nr,r,r=ei2π\n3, zG;1\nr,r,r2=e−i2π\n3,\nzG;r2\nr2,r2,1= 1, zG;1\nr2,r2,r=e−i2π\n3, zG;r\nr2,r2,r2=ei2π\n3,\nzG;rs\nr,r2,s= 1, zG;sr\nr,r2,rs=ei2π\n3, zG;rs\nr,r2,sr=e−i2π\n3,\nzG;sr\nr2,r,s= 1, zG;s\nr2,r,rs=e−i2π\n3, zG;rs\nr2,r,sr=ei2π\n3;(B.7)\nzH;r\nr,r,1= 1, zH;r2\nr,r,r=e−i2π\n3, zH;1\nr,r,r2=ei2π\n3,\nzH;r2\nr2,r2,1= 1, zH;1\nr2,r2,r=ei2π\n3, zH;r\nr2,r2,r2=e−i2π\n3,\nzH;rs\nr,r2,s= 1, zH;sr\nr,r2,rs=e−i2π\n3, zH;rs\nr,r2,sr=ei2π\n3,\nzH;sr\nr2,r,s= 1, zH;s\nr2,r,rs=ei2π\n3, zH;rs\nr2,r,sr=e−i2π\n3.(B.8)\nC Multi-fusion Category as the Input of SET Model\nIn this section, we introduce how the multifusion category can be used as the input of SET\nstring-net model.\n– 21 –\n\nFigure 6 : The lattice of HGW model.xy\ngigjgigj\nzgj\ngjugk\ngjvgjgl\nFigure 7 : The fattened lattice of\none plaquette.\nC.1 Input data: A Multifusion Category with Associated Isomorphisms\nWhen a topological phase Cis paired with a global symmetry G, it becomes an SET\nphase, denoted as CG. The SET phase CGcomprises multiple G-graded sectors, each of\nwhich is topologically equivalent to Cbut distinguished by a unique element from the\nsymmetry group. The action of Gfacilitates transitions between these G-graded sectors.\nThis observation leads to the development of a lattice model for CGby iterating the HGW\nstring-net model of Ca corresponding number of times. These iterations are interconnected\nthrough the elements of the group G, ensuring that in any basis configuration of the string-\nnet model, each plaquette is associated with a specific G-graded sector. Given that the\ninput data for the HGW model of phase Cis a UFC F, it follows that the input data\nfor the lattice model associated with the CGphase should similarly involve a proportional\nnumber of Fcopies, which are interconnected via the symmetry G. This configuration is\nmathematically formalized as a multifusion category Mendowed with a G-action.\nTo grasp the concept of a multifusion category, consider a straightforward example\nwhere a multifusion category M, derived from an F, is generally expressed in matrix form:\nM=\nFg1g1Fg1g2···Fg1gn\nFg2g1Fg2g2···Fg2gn\n............\nFgng1Fgng2···Fgngn\n, (C.1)\nEachFgigjcan be considered as an instance of Fwhen viewed as a UFC. Generally, the\nsizencorresponds to|G|, considering Gis finite. A simple object within Mis symbolized\nbyxgigj, which also represents a simple object within Fgigj, wherexis a simple object\nwithinF. These simple objects adhere to the subsequent fusion rules:\nxgigj⊗ygkgl=\n\n(x⊗y)gigl(gj=gk),\nO (gj̸=gk), (C.2)\n– 22 –\n\nwhere Ois the zero object of M, andx⊗yis a fusion in F.\nAccording to the fusion rules, the lattice illustrated in Figure 6 can be expanded by\ntransforming each edge or tail into a double line, whereby the indices of the group elements\nreside on the double lines [16], as shown in Figure 7. It is evident that each plaquette can\nnow be uniquely identified by a group element g, withgserving as the label for the loop\nof the inner-side line within the respective plaquette. We refer to the plaquette as being in\nag-sector. This method is equivalent to introducing spins into the plaquettes [17].\nThe simple objects corresponding to the diagonal elements Fggof the multifusion\nmatrix (C.1) characterize the degrees of freedom (dofs) in sector- g. In contrast, the simple\nobjects associated with the off-diagonal elements characterize the domain wall degrees of\nfreedom. Specifically, the simple objects within Fgigjdescribe the domain wall between\nsector-giand sector-gj.\nConsidering the structure of the input data, the lattice configuration of our SET model\nfor phase CGmirrors precisely that of the HGW string-net model for phase C. This lattice\nis a honeycomb arrangement, where each plaquette is represented with a tail (a wiggly\ndangling edge), as depicted in Figure 6. Each edge and tail of the lattice embodies a\ndegree of freedom that assumes values from the simple objects of M, thereby ensuring that\nthe fusion rules of Mare satisfied at every vertex within the lattice.\nC.2 Hamiltonian\nThe Hamiltonian takes the form\nH=−/summationdisplay\npQp=−/summationdisplay\np/summationdisplay\nsQs\np,(C.3)\nwhereprepresents a summation over all plaquettes and sencompasses all simple objects\nin the multifusion category M. The Hamiltonian here is identical to the one described in\nRef. [7], which provides a reformulation of the Hamiltonian for the HGW string-net model,\nensuring uniform energy levels for all elementary anyon excitations. The operators Qs\npare\ndiscussed in detail in Appendix A.\nWhile Hamiltonian (C.3) theoretically aligns with the HGW string-net model repre-\nsenting a pure topological phase, the physical phenomena it represents diverges notably\nfrom that of the HGW model. Specifically, the system incorporates both anyon excitations,\nknown as anyons, and local excitations, which the HGW model lacks. In the Symmetry-\nEnriched Topological (SET) phase characterized by Hamiltonian (C.3), the anyon excita-\ntions are defined in respect to the local excitations, which should be considered as part of\nthe vacuum. This phenomenon, referred to as symmetry fractionalization, arises due to\nthe nontrivial action of global symmetry on the topological vacuum, which includes local\nexcitations. Consequently, this interaction results in distinct sectors graded by the sym-\nmetry group G. Through the fusion rules of anyons, specific types of anyons are permitted\nto reside within a G-graded sector where they carry fractionalized symmetry charges. It\nis important to note that the ground-state degeneracy of a symmetry-enriched topological\n(SET) phase, denoted as CG, remains equivalent to that of the purely topological phase C\nfor any given topology, notwithstanding the presence of G-graded sectors.\n– 23 –\n\nD The Categorical Data of Z2Symmetry Enriched S3Multi-fusion Cat-\negory\nThe input multifusion category of Z2symmetry enriched S3string-net model can be rep-\nresented by the multifusion matrix\nM=\n{1++,r++,r2\n++,s++,rs++,sr++} { α+−, β+−}\n{α−+, β−+} { 1−−,r−−,r2\n−−,s−−,rs−−,sr−−}\n.(D.1)\nIn this section, we will list the categorical data of this multifusion category.\nD.1 The Fusion Rules\nThe fusion rules are\nδMKMLMN=δkln,∀MK,ML,MN∈{1−−,r++,r2\n++,s++,rs++,sr++},\nδα−+1++α+−=δα−+r++α+−=δα−+r2\n++α+−=δβ−+1++β+−=δβ−+r++β+−=δβ−+r2\n++β+−= 1,\nδα−+s++β+−=δα−+rs++β+−=δα−+sr++β+−=δβ−+s++α+−=δβ−+rs++α+−=δβ−+sr++α+−= 1,\nδα−+α+−1−−=δα−+α+−r−−=δα−+α+−r2\n−−=δβ−+β+−1−−=δβ−+β+−r−−=δβ−+β+−r2\n−−= 1,\nδα−+β+−s−−=δα−+β+−rs−−=δα−+β+−sr−−=δβ−+α+−s−−=δβ−+α+−rs−−=δβ−+α+−sr−−= 1.\n(D.2)\nD.2 The 6j-Symbols\nThe non-zero 6j-symbols are\nGα+−α−+1++\nα+−α−+1−−=Gα+−α−+1++α+−α−+r−−=Gα+−α−+1++\nα+−α−+r2\n−−=1√\n3,\nGα+−α−+r++\nα+−α−+1−−=1√\n3, Gα+−α−+r++α+−α−+r−−=e−i2π\n3√\n3, Gα+−α−+r++\nα+−α−+r2\n−−=ei2π\n3√\n3,\nGα+−α−+r2\n++\nα+−α−+1−−=1√\n3, Gα+−α−+r2\n++α+−α−+r−−=ei2π\n3√\n3, Gα+−α−+r2\n++\nα+−α−+r2\n−−=e−i2π\n3√\n3,\nGβ+−β−+1++\nβ+−β−+1−−=Gβ+−β−+1++\nβ+−β−+r−−=Gβ+−β−+1++\nβ+−β−+r2\n−−=1√\n3,\nGβ+−β−+r++\nβ+−β−+1−−=1√\n3, Gβ+−β−+r++\nβ+−β−+r−−=ei2π\n3√\n3, Gβ+−β−+r++\nβ+−β−+r2\n−−=e−i2π\n3√\n3,\nGβ+−β−+r2\n++\nβ+−β−+1−−=1√\n3, Gβ+−β−+r2\n++\nβ+−β−+r−−=e−i2π\n3√\n3, Gβ+−β−+r2\n++\nβ+−β−+r2\n−−=ei2π\n3√\n3,\nGα+−β−+s++\nβ+−α−+1−−=Gα+−β−+s++\nβ+−α−+r−−=Gα+−β−+s++\nβ+−α−+r2\n−−=1√\n3,\nGα+−β−+rs++\nβ+−α−+1−−=1√\n3, Gα+−β−+rs++\nβ+−α−+r−−=ei2π\n3√\n3, Gα+−β−+rs++\nβ+−α−+r2\n−−=e−i2π\n3√\n3,\n– 24 –\n\nGα+−β−+sr++\nβ+−α−+1−−=1√\n3, Gα+−β−+sr++\nβ+−α−+r−−=e−i2π\n3√\n3, Gα+−β−+sr++\nβ+−α−+r2\n−−=ei2π\n3√\n3,\nGα+−α−+1++\nβ+−β−+s−−=Gα+−α−+1++\nβ+−β−+rs−−=Gα+−α−+1++\nβ+−β−+sr−−=1√\n3,\nGα+−α−+r++\nβ+−β−+s−−=1√\n3, Gα+−α−+r++\nβ+−β−+rs−−=e−i2π\n3√\n3, Gα+−α−+r++\nβ+−β−+sr−−=ei2π\n3√\n3,\nGα+−α−+r2\n++\nβ+−β−+s−−=1√\n3, Gα+−α−+r2\n++\nβ+−β−+rs−−=ei2π\n3√\n3, Gα+−α−+r2\n++\nβ+−β−+sr−−=e−i2π\n3√\n3,\nGα+−β−+s++\nα+−β−+s−−=Gα+−β−+s++\nα+−β−+rs−−=Gα+−β−+s++\nα+−β−+sr−−=1√\n3,\nGα+−β−+rs++\nα+−β−+s−−=1√\n3, Gα+−β−+rs++\nα+−β−+rs−−=ei2π\n3√\n3, Gα+−β−+rs++\nα+−β−+sr−−=e−i2π\n3√\n3,\nGα+−β−+sr++\nα+−β−+s−−=1√\n3, Gα+−β−+sr++\nα+−β−+rs−−=e−i2π\n3√\n3, Gα+−β−+sr++\nα+−β−+sr−−=ei2π\n3√\n3,\nGabc\nµν∗γ∗=1\n4√\n3δabcδν∗cµδγ∗aνδµ∗bγ,\n∀a,b,c∈{1++,r++,r2\n++,s++,rs++,sr++}, µ,ν,γ∈{α+−,β+−},\nGabc\nµ∗νγ=1\n4√\n3δabcδνcµ∗δγaν∗δµbγ∗,\n∀a,b,c∈{1−−,r−−,r2\n−−,s−−,rs−−,sr−−}, µ,ν,γ∈{α+−,β+−}.\nE The anyon Spectrum of EM-Exchange Symmetry Enriched S3String-\nnet Model\nIn this section, we provide the topological excitaion spectrum of EM-exchange symmetry\nenrichedS3string-net model. There are 8kinds of anyon excitations, each one can be\ndenoted by a pair (X,Y ), which means that it appears as the original D(S3)X-type (Y-\ntype) anyon in the +(−) sector. The z-tensors of these excitations are:\nz(A,A);1++\n1++,1++,1++=z(A,A);r++\n1++,1++,r++=z(A,A);r2\n++\n1++,1++,r2\n++=z(A,A);s++\n1++,1++,s++= 1,\nz(A,A);rs++\n1++,1++,rs++=z(A,A);sr++\n1++,1++,sr++=z(A,A);α+−\n1++,1−−,α+−=z(A,A);β+−\n1++,1−−,β+−= 1,\nz(A,A);1−−\n1−−,1−−,1−−=z(A,A);r−−\n1−−,1−−,r−−=z(A,A);r2\n−−\n1−−,1−−,r2\n−−=z(A,A);s−−\n1−−,1−−,s−−= 1,\nz(A,A);rs−−\n1−−,1−−,rs−−=z(A,A);sr−−\n1−−,1−−,sr−−=z(A,A);α−+\n1−−,1++,α−+=z(A,A);β−+\n1−−,1++,β−+= 1;\nz(B,B);1++\n1++,1++,1++=z(B,B);r++\n1++,1++,r++=z(B,B);r2\n++\n1++,1++,r2\n++= 1,\nz(B,B);s++\n1++,1++,s++=z(B,B);rs++\n1++,1++,rs++=z(B,B);sr++\n1++,1++,sr++=−1,\n– 25 –\n\nz(B,B);1−−\n1−−,1−−,1−−=z(B,B);r++\n1−−,1−−,r++=z(B,B);r2\n++\n1−−,1−−,r2\n++= 1,\nz(B,B);s−−\n1−−,1−−,s−−=z(B,B);rs−−\n1−−,1−−,rs−−=z(B,B);sr−−\n1−−,1−−,sr−−=−1,\nz(B,B);α+−\n1−−,1−−,α+−=z(B,B);β+−\n1−−,1−−,β+−=z(B,B);α−+\n1−−,1++,α−+=z(B,B);β−+\n1−−,1++,β−+= 1;\nz(C,F);1++\n(11)++,(11)++,1++= 1, z(C,F);r++\n(11)++,(11)++,r++=ei2π\n3, z(C,F);r2\n++\n(11)++,(11)++,r2\n++=e−i2π\n3,\nz(C,F);1++\n(12)++,(12)++,1++= 1, z(C,F);r++\n(12)++,(12)++,r++=e−i2π\n3, z(C,F);r2\n++\n(12)++,(12)++,r2\n++=ei2π\n3,\nz(C,F);s++\n(11)++,(12)++,s++= 1, z(C,F);rs++\n(11)++,(12)++,rs++=ei2π\n3, z(C,F);sr++\n(11)++,(12)++,sr++=e−i2π\n3,\nz(C,F);s++\n(12)++,(11)++,s++= 1, z(C,F);rs++\n(12)++,(11)++,rs++=e−i2π\n3, z(C,F);sr++\n(12)++,(11)++,sr++=ei2π\n3,\nz(C,F);r−−\nr−−,r−−,1−−=z(C,F);r2\n−−r−−,r−−,r−−=z(C,F);1−−\nr−−,r−−,r2\n−−= 1,\nz(C,F);r2\n−−\nr2\n−−,r2\n−−,1−−=z(C,F);1−−\nr2\n−−,r2\n−−,r−−=z(C,F);r−−\nr2\n−−,r2\n−−,r2\n−−= 1,\nz(C,F);rs−−\nr−−,r2\n−−,s−−=z(C,F);sr−−\nr−−,r2\n−−,rs−−=z(C,F);rs−−\nr−−,r2\n−−,sr−−= 1,\nz(C,F);sr−−\nr2\n−−,r−−,s−−=z(C,F);s−−\nr2\n−−,r−−,rs−−=z(C,F);rs−−\nr2\n−−,r−−,sr−−= 1,\nz(C,F);α+−\n(11)++,r−−,α+−=z(C,F);α+−\n(12)++,r2\n−−,α+−=z(C,F);α−+\nr−−,(11)++,α−+=z(C,F);α−+\nr2\n−−,(12)++,α−+= 1,\nz(C,F);β+−\n(11)++,r2\n−−,β+−=z(C,F);β+−\n(12)++,r−−,β+−=z(C,F);β−+\nr2\n−−,(11)++,β−+=z(C,F);β−+\nr−−,(12)++,β−+= 1;\nz(F,C);1−−\n(11)−−,(11)−−,1−−= 1, z(F,C);r−−\n(11)−−,(11)−−,r−−=ei2π\n3, z(F,C);r2\n−−\n(11)−−,(11)−−,r2\n−−=e−i2π\n3,\nz(F,C);1−−\n(12)−−,(12)−−,1−−= 1, z(F,C);r−−\n(12)−−,(12)−−,r−−=e−i2π\n3, z(F,C);r2\n−−\n(12)−−,(12)−−,r2\n−−=ei2π\n3,\nz(F,C);s−−\n(11)−−,(12)−−,s−−= 1, z(F,C);rs−−\n(11)−−,(12)−−,rs−−=ei2π\n3, z(F,C);sr−−\n(11)−−,(12)−−,sr−−=e−i2π\n3,\nz(F,C);s−−\n(12)−−,(11)−−,s−−= 1, z(F,C);rs−−\n(12)−−,(11)−−,rs−−=e−i2π\n3, z(F,C);sr−−\n(12)−−,(11)−−,sr−−=ei2π\n3,\nz(F,C);r++\nr++,r++,1++=z(F,C);r2\n++r++,r++,r++=z(F,C);1++\nr++,r++,r2\n++= 1,\nz(F,C);r2\n++\nr2\n++,r2\n++,1++=z(F,C);1++\nr2\n++,r2\n++,r++=z(F,C);r++\nr2\n++,r2\n++,r2\n++= 1,\nz(F,C);rs++\nr++,r2\n++,s++=z(F,C);sr++\nr++,r2\n++,rs++=z(F,C);rs++\nr++,r2\n++,sr++= 1,\nz(F,C);sr++\nr2\n++,r++,s++=z(F,C);s++\nr2\n++,r++,rs++=z(F,C);rs++\nr2\n++,r++,sr++= 1,\nz(F,C);α−+\n(11)−−,r++,α−+=z(F,C);α−+\n(12)−−,r2\n++,α−+=z(F,C);α+−\nr++,(11)−−,α+−=z(F,C);α+−\nr2\n++,(12)−−,α+−= 1,\nz(F,C);β−+\n(11)−−,r2\n++,β−+=z(F,C);β−+\n(12)−−,r++,β−+=z(F,C);β+−\nr2\n++,(11)−−,β+−=z(F,C);β+−\nr++,(12)−−,β+−= 1;\nz(G,G);r++\nr++,r++,1++= 1, z(G,G);r2\n++r++,r++,r++=ei2π\n3, z(G,G);1++\nr++,r++,r2\n++=e−i2π\n3,\n– 26 –\n\nz(G,G);r2\n++\nr2\n++,r2\n++,1++= 1, z(G,G);1++\nr2\n++,r2\n++,r++=e−i2π\n3, z(G,G);r++\nr2\n++,r2\n++,r2\n++=ei2π\n3,\nz(G,G);rs++\nr++,r2\n++,s++= 1, z(G,G);sr++\nr++,r2\n++,rs++=ei2π\n3, z(G,G);rs++\nr++,r2\n++,sr++=e−i2π\n3,\nz(G,G);sr++\nr2\n++,r++,s++= 1, z(G,G);s++\nr2\n++,r++,rs++=e−i2π\n3, z(G,G);rs++\nr2\n++,r++,sr++=ei2π\n3,\nz(G,G);r−−\nr−−,r−−,1−−= 1, z(G,G);r2\n−−r−−,r−−,r−−=ei2π\n3, z(G,G);1−−\nr−−,r−−,r2\n−−=e−i2π\n3,\nz(G,G);r2\n−−\nr2\n−−,r2\n−−,1−−= 1, z(G,G);1−−\nr2\n−−,r2\n−−,r−−=e−i2π\n3, z(G,G);r−−\nr2\n−−,r2\n−−,r2\n−−=ei2π\n3,\nz(G,G);rs−−\nr−−,r2\n−−,s−−= 1, z(G,G);sr−−\nr−−,r2\n−−,rs−−=ei2π\n3, z(G,G);rs−−\nr−−,r2\n−−,sr−−=e−i2π\n3,\nz(G,G);sr−−\nr2\n−−,r−−,s−−= 1, z(G,G);s−−\nr2\n−−,r−−,rs−−=e−i2π\n3, z(G,G);rs−−\nr2\n−−,r−−,sr−−=ei2π\n3,\nz(G,G);α+−r++,r−−,α+−=z(G,G);α+−\nr2\n++,r2\n−−,α+−=z(G,G);β+−\nr++,r2\n−−,β+−=z(G,G);β+−\nr2\n++,r−−,β+−=ei2π\n3,\nz(G,G);α−+r−−,r++,α−+=z(G,G);α−+\nr2\n−−,r2\n++,α−+=z(G,G);β−+\nr−−,r2\n++,β−+=z(G,G);β−+\nr2\n−−,r++,β−+=ei2π\n3;\nz(H,H);r++\nr++,r++,1++= 1, z(H,H);r2\n++r++,r++,r++=e−i2π\n3, z(H,H);1++\nr++,r++,r2\n++=ei2π\n3,\nz(H,H);r2\n++\nr2\n++,r2\n++,1++= 1, z(H,H);1++\nr2\n++,r2\n++,r++=ei2π\n3, z(H,H);r++\nr2\n++,r2\n++,r2\n++=e−i2π\n3,\nz(H,H);rs++\nr++,r2\n++,s++= 1, z(H,H);sr++\nr++,r2\n++,rs++=e−i2π\n3, z(H,H);rs++\nr++,r2\n++,sr++=ei2π\n3,\nz(H,H);sr++\nr2\n++,r++,s++= 1, z(H,H);s++\nr2\n++,r++,rs++=ei2π\n3, z(H,H);rs++\nr2\n++,r++,sr++=e−i2π\n3,\nz(H,H);r−−\nr−−,r−−,1−−= 1, z(H,H);r2\n−−r−−,r−−,r−−=e−i2π\n3, z(H,H);1−−\nr−−,r−−,r2\n−−=ei2π\n3,\nz(H,H);r2\n−−\nr2\n−−,r2\n−−,1−−= 1, z(H,H);1−−\nr2\n−−,r2\n−−,r−−=ei2π\n3, z(H,H);r−−\nr2\n−−,r2\n−−,r2\n−−=e−i2π\n3,\nz(H,H);rs−−\nr−−,r2\n−−,s−−= 1, z(H,H);sr−−\nr−−,r2\n−−,rs−−=e−i2π\n3, z(H,H);rs−−\nr−−,r2\n−−,sr−−=ei2π\n3,\nz(H,H);sr−−\nr2\n−−,r−−,s−−= 1, z(H,H);s−−\nr2\n−−,r−−,rs−−=ei2π\n3, z(H,H);rs−−\nr2\n−−,r−−,sr−−=e−i2π\n3,\nz(H,H);α+−\nr++,r2\n−−,α+−=z(H,H);α+−\nr2\n++,r−−,α+−=z(H,H);β+−\nr++,r−−,β+−=z(H,H);β+−\nr++,r−−,β+−=e−i2π\n3,\nz(H,H);α−+\nr−−,r2\n++,α−+=z(H,H);α−+\nr2\n−−,r++,α−+=z(H,H);β−+\nr−−,r++,β−+=z(H,H);β−+\nr2\n−−,r2\n++,β−+=e−i2π\n3;\nz(D,D);s++\ns++,s++,1++=z(D,D);sr++s++,rs++,r++=z(D,D);rs++\ns++,sr++,r2\n++= 1,\nz(D,D);rs++\nrs++,rs++,1++=z(D,D);s++rs++,sr++,r++=z(D,D);sr++\nrs++,s++,r2\n++= 1,\nz(D,D);sr++\nsr++,sr++,1++=z(D,D);rs++sr++,s++,r++=z(D,D);s++\nsr++,rs++,r2\n++= 1,\nz(D,D);1++s++,s++,s++=z(D,D);r++s++,rs++,sr++=z(D,D);r2\n++s++,sr++,rs++= 1,\nz(D,D);1++rs++,rs++,rs++=z(D,D);r++rs++,sr++,s++=z(D,D);r2\n++rs++,s++,sr++= 1,\nz(D,D);1++sr++,sr++,sr++=z(D,D);r\nsr++,s++,sr++=z(D,D);r2\n++sr++,rs++,s++= 1,\n– 27 –\n\nz(D,D);s−−\ns−−,s−−,1−−=z(D,D);sr−−s−−,rs−−,r−−=z(D,D);rs−−\ns−−,sr−−,r2\n−−= 1,\nz(D,D);rs−−\nrs−−,rs−−,1−−=z(D,D);s−−rs−−,sr−−,r−−=z(D,D);sr−−\nrs−−,s−−,r2\n−−= 1,\nz(D,D);sr−−\nsr−−,sr−−,1−−=z(D,D);rs−−sr−−,s−−,r−−=z(D,D);s−−\nsr−−,rs−−,r2\n−−= 1,\nz(D,D);1−−s−−,s−−,s−−=z(D,D);r−−s−−,rs−−,sr−−=z(D,D);r2\n−−s−−,sr−−,rs−−= 1,\nz(D,D);1−−rs−−,rs−−,rs−−=z(D,D);r−−rs−−,sr−−,s−−=z(D,D);r2\n−−rs−−,s−−,sr−−= 1,\nz(D,D);1−−sr−−,sr−−,sr−−=z(D,D);R\nsr−−,s−−,sr−−=z(D,D);r2\n−−sr−−,rs−−,s−−= 1,\nz(D,D);β+−s++,s−−,α+−=z(D,D);β+−s++,rs−−,α+−=z(D,D);β+−rs++,s−−,α+−=z(D,D);β+−s++,sr−−,α+−=z(D,D);β+−sr++,s−−,α+−=1√\n3,\nz(D,D);β+−rs++,rs−−,α+−=z(D,D);β+−sr++,sr−−,α+−=ei2π\n3√\n3, z(D,D);β+−rs++,sr−−,α+−=z(D,D);β+−sr++,rs−−,α+−=e−i2π\n3√\n3,\nz(D,D);α+−\ns++,s−−,β+−=z(D,D);α+−\ns++,rs−−,β+−=z(D,D);α+−\nrs++,s−−,β+−=z(D,D);α+−\ns++,sr−−,β+−=z(D,D);α+−\nsr++,s−−,β+−=1√\n3,\nz(D,D);α+−\nrs++,rs−−,β+−=z(D,D);α+−\nsr++,sr−−,β+−=e−i2π\n3√\n3, z(D,D);α+−\nrs++,sr−−,β+−=z(D,D);α+−\nsr++,rs−−,β+−=ei2π\n3√\n3,\nz(D,D);β−+s−−,s++,α−+=z(D,D);β−+s−−,rs++,α−+=z(D,D);β−+rs−−,s++,α−+=z(D,D);β−+s−−,sr++,α−+=z(D,D);β−+sr−−,s++,α−+=1√\n3,\nz(D,D);β−+rs−−,rs++,α−+=z(D,D);β−+sr−−,sr++,α−+=ei2π\n3√\n3, z(D,D);β−+rs−−,sr++,α−+=z(D,D);β−+sr−−,rs++,α−+=e−i2π\n3√\n3,\nz(D,D);α−+\ns−−,s++,β−+=z(D,D);α−+\ns−−,rs++,β−+=z(D,D);α−+\nrs−−,s++,β−+=z(D,D);α−+\ns−−,sr++,β−+=z(D,D);α−+\nsr−−,s++,β−+=1√\n3,\nz(D,D);α−+\nrs−−,rs++,β−+=z(D,D);α−+\nsr−−,sr++,β−+=e−i2π\n3√\n3, z(D,D);α−+\nrs−−,sr++,β−+=z(D,D);α−+\nsr−−,rs++,β−+=ei2π\n3√\n3;\nz(E,E);s++\ns++,s++,1++=z(E,E);sr++s++,rs++,r++=z(E,E);rs++\ns++,sr++,r2\n++= 1,\nz(E,E);rs++\nrs++,rs++,1++=z(E,E);s++rs++,sr++,r++=z(E,E);sr++\nrs++,s++,r2\n++= 1,\nz(E,E);sr++\nsr++,sr++,1++=z(E,E);rs++sr++,s++,r++=z(E,E);s++\nsr++,rs++,r2\n++= 1,\nz(E,E);1++s++,s++,s++=z(E,E);r++s++,rs++,sr++=z(E,E);r2\n++s++,sr++,rs++=−1,\nz(E,E);1++rs++,rs++,rs++=z(E,E);r++rs++,sr++,s++=z(E,E);r2\n++rs++,s++,sr++=−1,\nz(E,E);1++sr++,sr++,sr++=z(E,E);r\nsr++,s++,sr++=z(E,E);r2\n++sr++,rs++,s++=−1,\nz(E,E);s−−\ns−−,s−−,1−−=z(E,E);sr−−s−−,rs−−,r−−=z(E,E);rs−−\ns−−,sr−−,r2\n−−= 1,\nz(E,E);rs−−\nrs−−,rs−−,1−−=z(E,E);s−−rs−−,sr−−,r−−=z(E,E);sr−−\nrs−−,s−−,r2\n−−= 1,\nz(E,E);sr−−\nsr−−,sr−−,1−−=z(E,E);rs−−sr−−,s−−,r−−=z(E,E);s−−\nsr−−,rs−−,r2\n−−= 1,\nz(E,E);1−−s−−,s−−,s−−=z(E,E);r−−s−−,rs−−,sr−−=z(E,E);r2\n−−s−−,sr−−,rs−−=−1,\n– 28 –\n\nz(E,E);1−−rs−−,rs−−,rs−−=z(E,E);r−−rs−−,sr−−,s−−=z(E,E);r2\n−−rs−−,s−−,sr−−=−1,\nz(E,E);1−−sr−−,sr−−,sr−−=z(E,E);R\nsr−−,s−−,sr−−=z(E,E);r2\n−−sr−−,rs−−,s−−=−1,\nz(E,E);β+−s++,s−−,α+−=z(E,E);β+−s++,rs−−,α+−=z(E,E);β+−rs++,s−−,α+−=z(E,E);β+−s++,sr−−,α+−=z(E,E);β+−sr++,s−−,α+−=1√\n3,\nz(E,E);β+−rs++,rs−−,α+−=z(E,E);β+−sr++,sr−−,α+−=ei2π\n3√\n3, z(E,E);β+−rs++,sr−−,α+−=z(E,E);β+−sr++,rs−−,α+−=e−i2π\n3√\n3,\nz(E,E);α+−\ns++,s−−,β+−=z(E,E);α+−\ns++,rs−−,β+−=z(E,E);α+−\nrs++,s−−,β+−=z(E,E);α+−\ns++,sr−−,β+−=z(E,E);α+−\nsr++,s−−,β+−=1√\n3,\nz(E,E);α+−\nrs++,rs−−,β+−=z(E,E);α+−\nsr++,sr−−,β+−=e−i2π\n3√\n3, z(E,E);α+−\nrs++,sr−−,β+−=z(E,E);α+−\nsr++,rs−−,β+−=ei2π\n3√\n3,\nz(E,E);β−+s−−,s++,α−+=z(E,E);β−+s−−,rs++,α−+=z(E,E);β−+rs−−,s++,α−+=z(E,E);β−+s−−,sr++,α−+=z(E,E);β−+sr−−,s++,α−+=1√\n3,\nz(E,E);β−+rs−−,rs++,α−+=z(E,E);β−+sr−−,sr++,α−+=ei2π\n3√\n3, z(E,E);β−+rs−−,sr++,α−+=z(E,E);β−+sr−−,rs++,α−+=e−i2π\n3√\n3,\nz(E,E);α−+\ns−−,s++,β−+=z(E,E);α−+\ns−−,rs++,β−+=z(E,E);α−+\nrs−−,s++,β−+=z(E,E);α−+\ns−−,sr++,β−+=z(E,E);α−+\nsr−−,s++,β−+=1√\n3,\nz(E,E);α−+\nrs−−,rs++,β−+=z(E,E);α−+\nsr−−,sr++,β−+=e−i2π\n3√\n3, z(E,E);α−+\nrs−−,sr++,β−+=z(E,E);α−+\nsr−−,rs++,β−+=ei2π\n3√\n3.\nF Anyon Fragmentation Pattern\nIn this section, we provide the non-Abelian anyon fragmentation pattern of D(S3)phase\nunder EM-exchange symmetry (in α-gauge), as shown in Table 3.\nG Proof that ρGandρHare not projective representations\nHere, we prove that the representations ρG(4.5) andρH(4.6) are not projective represen-\ntations, which are equivalent to linear representations of Z2, although they appear to be.\nOur proof goes by showing that the phase factors in (4.5) and (4.6), which are due to the\nnonlinearity indicator ω, cannot be transformed away consistently:\nSince the nonlinearity indicator ωonly depends on the Pachner moves, we can express\nthe components of ωsolely using 6j-symbols5as\nωr,r= 3Gr2,α,α\nr,α,αG1,r,r2\nα,α,αG1,r,r2\nα,α,α,\nωr2,r2= 3Gr,α,α\nr2,α,αG1,r,r2\nα,α,αG1,r,r2\nα,α,α,\nωr,r2= 3Gr2,α,α\nr2,α,αG1,r,r2\nα,α,αG1,r,r2\nα,α,α,\nωr2,r= 3Gr,α,α\nr,α,αG1,r,r2\nα,α,αG1,r,r2\nα,α,α.(G.1)\n5AsGaijbjkeik\nckldlifjl=Ga¯i¯jb¯j¯ke¯i¯k\nc¯k¯ld¯l¯if¯j¯lalways holds for any i, j, k, l∈{+,−}due to the symmetry, we use Gabe\ncdfto\ndenote them for convenience:\nGabe\ncdf:=Gaijbjkeik\nckldlifjl=Ga¯i¯jb¯j¯ke¯i¯k\nc¯k¯ld¯l¯if¯j¯l.\n– 29 –\n\nNon-Abelian anyons Definite symmetry charge states (spaces) Symmetry charge\nC⊕Fspan{C11+Fr, C12+Fr2} 0\nspan{C11−Fr, C12−Fr2}1\n2\nG span{Gr, Gr2}1\n3\nHHr+Hr22\n3\nHr−Hr21\n6\nDDrs−Dsr1\n4\n(1 +√\n3)Ds+Drs+Dsr 0\n(1−√\n3)Ds+Drs+Dsr1\n2\nEErs−Esr1\n4\n(1 +√\n3)Es+Ers+Esr 0\n(1−√\n3)Es+Ers+Esr1\n2\nTable 3 : Non-Abelian anyon fragmentation pattern of D(S3)phase under EM-exchange\nsymmetry (in α-gauge)\nIn order to transform them away, we can only resort to the gauge transformation on the\n6j-symbols[9, 47]\nGabe\ncdf→uabeucde∗\nudaf∗ubcfGabe\ncdf. (G.2)\nBut this is impossible without violating the gauge constraint condition\nG0,α,α\ni,α,α = 1,∀i∈{1,r,r2}, (G.3)\nwhich must be satisfied by the string-net model with any input UFC.\nThus, the phase factors in (4.5) (4.6) cannot be transformed away consistently in the\nstring-net model, rendering ρGandρHtruly nonlinear representations.\nReferences\n[1] T. Lan and X. G. Wen, Physical Review B - Condensed Matter and Materials Physics 90,\n115119 (2014), arXiv:1311.1784 .\n[2] Y. Hu, N. Geer, and Y.-S. Wu, Physical Review B 97, 195154 (2018), arXiv:1502.03433 .\n[3] H. Wang, Y. Li, Y. Hu, and Y. Wan, Journal of High Energy Physics 2020, 30 (2020).\n[4] Y. Zhao, S. Huang, H. Wang, Y. Hu, and Y. Wan, SciPost Physics Core 6, 076 (2023).\n[5] Y. Zhao, H. Wang, Y. Hu, and Y. Wan, Journal of High Energy Physics 2024, 1 (2024).\n[6] Y. Zhao and Y. Wan, arXiv preprint arXiv:2408.02664 (2024).\n[7] Y. Zhao and Y. Wan, arXiv preprint arXiv:2409.05852 (2024).\n– 30 –\n\n[8] Y. Zhao and Y. Wan, arXiv preprint arXiv:2506.05319 (2025).\n[9] N. Fu, Y. Zhao, and Y. Wan, “Symmetry-enriched topological orders and their gauging: A\nstring-net model realization,” (2025), companion paper, to appear in arXiv soon.\n[10] A. Mesaros and Y. Ran, Physical Review B 87, 155115 (2013), arXiv:1212.0835 .\n[11] L.-Y. Hung and Y. Wan, International Journal of Modern Physics B 28, 1450172 (2014).\n[12] L.-Y. Hung and Y. Wan, Physical Review B 87, 195103 (2013).\n[13] Y.-M. Lu and A. Vishwanath, Physical Review B 93, 155121 (2016).\n[14] Y. Gu, L.-Y. Hung, and Y. Wan, Phys. Rev. B 90, 245125 (2014), arXiv:1402.3356 .\n[15] M. Barkeshli, P. Bonderson, M. Cheng, and Z. Wang, Physical Review B 100, 115147\n(2019), arXiv:1410.4540 .\n[16] L. Chang, M. Cheng, S. X. Cui, Y. Hu, W. Jin, R. Movassagh, P. Naaijkens, Z. Wang, and\nA. Young, Journal of Physics A: Mathematical and Theoretical 48, 12FT01 (2015).\n[17] C. Heinrich, F. Burnell, L. Fidkowski, and M. Levin, Physical Review B 94, 235136 (2016).\n[18] M. Cheng, Z.-C. Gu, S. Jiang, and Y. Qi, Physical Review B 96, 115107 (2017).\n[19] D. J. Williamson and Z. Wang, Annals of Physics 377, 311 (2017).\n[20] D. J. Williamson, N. Bultinck, and F. Verstraete, arXiv preprint arXiv:1711.07982 (2017).\n[21] S.-Q. Ning, Z.-X. Liu, and P. Ye, (2018), arXiv:1801.01638 .\n[22] Y. Hu, Z. Huang, L.-Y. Hung, and Y. Wan, Journal of High Energy Physics 2022, 26 (2022).\n[23] Q.-R. Wang and M. Cheng, Physical Review B 106, 115104 (2022).\n[24] M. A. Levin and X.-G. Wen, Physical Review B—Condensed Matter and Materials Physics\n71, 045110 (2005).\n[25] X. Chen, F. Wang, Y.-M. Lu, and D.-H. Lee, Nuclear Physics B 873, 248 (2013).\n[26] J. C. Wang, Z.-C. Gu, and X.-G. Wen, Physical review letters 114, 031601 (2015).\n[27] D. J. Williamson, N. Bultinck, and F. Verstraete, arXiv:1711.07982v1 .\n[28] L.-Y. Hung and X.-g. Wen, Physical Review B 87, 165107 (2013), arXiv:1212.1827v1 .\n[29] J. Y. Lee, A. M. Turner, and A. Vishwanath, Physical Review B 98, 214416 (2018).\n[30] H. Yao, L. Fu, and X.-L. Qi, arXiv preprint arXiv:1012.4470 (2010).\n[31] A. Essin and M. Hermele, Phys. Rev. B 87, 104406 (2013) (2012), arXiv:1212.0593v2 .\n[32] M. Barkeshli, E. Berg, and S. Kivelson, Science 346, 722 (2014).\n[33] M. Barkeshli, P. Bonderson, M. Cheng, and Z. Wang, Physical Review B 100, 115147\n(2019).\n[34] H. Song and M. Hermele, Physical Review B 91, 014405 (2015).\n[35] Y. Li and M. Litvinov, arXiv preprint arXiv:2405.15951 (2024).\n[36] S. D. Sarma, M. Freedman, and C. Nayak, npj Quantum Information 2015 1:1 1, 1 (2015),\narXiv:1501.02813 .\n[37] A. Stern, “Anyons and the quantum Hall effect-A pedagogical review,” (2008),\narXiv:0711.4697 .\n– 31 –\n\n[38] L. Kong and Z.-H. Zhang, “An invitation to topological orders and category theory,” (2022),\narXiv:2205.05565 [cond-mat.str-el] .\n[39] C. F. B. Lo, A. Lyons, R. Verresen, A. Vishwanath, and N. Tantivasadakarn, arXiv preprint\narXiv:2502.14974 (2025).\n[40] K. Iguchi, Physical Review Letters 78, 3233 (1997).\n[41] S. Guruswamy and K. Schoutens, Nuclear Physics B 556, 530 (1999), arXiv:9903045\n[cond-mat] .\n[42] M. Barkeshli, C.-M. Jian, and X.-L. Qi, Physical Review B 87, 045130 (2013).\n[43] Y. Hu, S. D. Stirling, and Y. S. Wu, Physical Review B 89, 115133 (2014), arXiv:1303.1586 .\n[44] Y. Li, H. Wang, Y. Hu, and Y. Wan, Journal of High Energy Physics 2019, 1 (2019).\n[45] Y. Li, H. Wang, Y. Hu, and Y. Wan, Journal of High Energy Physics 2019, 78 (2019).\n[46] Y. Hu, S. D. Stirling, and Y.-S. Wu, Physical Review B—Condensed Matter and Materials\nPhysics85, 075107 (2012).\n[47] L.-Y. Hung and Y. Wan, Physical Review B 86, 235132 (2012), arXiv:1207.6169 .\n– 32 –\n\n",
    "source": "http://arxiv.org/abs/2506.24115v1",
    "authors": [
      "Nianrui Fu",
      "Siyuan Wang",
      "Yu Zhao",
      "Yidun Wan"
    ],
    "categories": [
      "cond-mat.str-el",
      "cond-mat.stat-mech",
      "hep-th",
      "math-ph",
      "math.MP"
    ],
    "type": "content"
  },
  {
    "id": "2506.24113v1_abstract",
    "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
    "content": "Title: Epona: Autoregressive Diffusion World Model for Autonomous Driving\n\nAbstract: Diffusion models have demonstrated exceptional visual quality in video\ngeneration, making them promising for autonomous driving world modeling.\nHowever, existing video diffusion-based world models struggle with\nflexible-length, long-horizon predictions and integrating trajectory planning.\nThis is because conventional video diffusion models rely on global joint\ndistribution modeling of fixed-length frame sequences rather than sequentially\nconstructing localized distributions at each timestep. In this work, we propose\nEpona, an autoregressive diffusion world model that enables localized\nspatiotemporal distribution modeling through two key innovations: 1) Decoupled\nspatiotemporal factorization that separates temporal dynamics modeling from\nfine-grained future world generation, and 2) Modular trajectory and video\nprediction that seamlessly integrate motion planning with visual modeling in an\nend-to-end framework. Our architecture enables high-resolution, long-duration\ngeneration while introducing a novel chain-of-forward training strategy to\naddress error accumulation in autoregressive loops. Experimental results\ndemonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes\nlonger prediction duration compared to prior works. The learned world model\nfurther serves as a real-time motion planner, outperforming strong end-to-end\nplanners on NAVSIM benchmarks. Code will be publicly available at\n\\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.",
    "source": "http://arxiv.org/abs/2506.24113v1",
    "authors": [
      "Kaiwen Zhang",
      "Zhenyu Tang",
      "Xiaotao Hu",
      "Xingang Pan",
      "Xiaoyang Guo",
      "Yuan Liu",
      "Jingwei Huang",
      "Li Yuan",
      "Qian Zhang",
      "Xiao-Xiao Long",
      "Xun Cao",
      "Wei Yin"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24113v1_content",
    "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving",
    "content": "arXiv:2506.24113v1  [cs.CV]  30 Jun 2025Epona : Autoregressive Diffusion World Model for Autonomous Driving\nKaiwen Zhang1,2*Zhenyu Tang1,3*Xiaotao Hu1,5Xingang Pan6\nXiaoyang Guo1Yuan Liu5Jingwei Huang7Li Yuan3Qian Zhang1\nXiao-Xiao Long4 †Xun Cao4Wei Yin1 §\n1Horizon Robotics2Tsinghua University3Peking University\n4Nanjing University5The Hong Kong University of Science and Technology\n6Nanyang Technological University7Tencent Hunyuan\n0s 5s 10s 11sPredict stopping at the red light Start driving at the green  light\n      (B) T rajectory-controlled V ideo Generation\nCondition Frames\nThe same\ncondition framesInput action control\nat each frameDiverse trajectory-controlled future video generation\n       (C)T raffic W orld Knowledge Understanding\n0s 3s 6s 10s\n30s 60s 90s 2min\n(D) End-to-End T rajectory Planning\n      (A) Consistent High-resolution Long V ideo Generation\nFigure 1. Versatile capabilities of Epona .Given historical driving context, our Epona can generate consistent minutes-long future driving\nscenes at high resolution (A). It can be controlled by diverse trajectories (B), and understand real-world traffic knowledge (C). In addition,\nour world model can predict future trajectories and serve as an end-to-end real-time motion planner (D).\nAbstract\nDiffusion models have demonstrated exceptional visual\nquality in video generation, making them promising for au-\ntonomous driving world modeling. However, existing video\ndiffusion-based world models struggle with flexible-length,\nlong-horizon predictions and integrating trajectory plan-\nning. This is because conventional video diffusion mod-\nels rely on global joint distribution modeling of fixed-length\n*These authors contributed equally to this work.†Project advisor.\n§Project lead, Corresponding author.frame sequences rather than sequentially constructing lo-\ncalized distributions at each timestep. In this work, we\npropose Epona, an autoregressive diffusion world model\nthat enables localized spatiotemporal distribution modeling\nthrough two key innovations: 1) Decoupled spatiotempo-\nral factorization that separates temporal dynamics model-\ning from fine-grained future world generation, and 2) Mod-\nular trajectory and video prediction that seamlessly inte-\ngrate motion planning with visual modeling in an end-to-\nend framework. Our architecture enables high-resolution,\nlong-duration generation while introducing a novel chain-\nof-forward training strategy to address error accumulation\n1\n\nin autoregressive loops. Experimental results demonstrate\nstate-of-the-art performance with 7.4% FVD improvement\nand minutes longer prediction duration compared to prior\nworks. The learned world model further serves as a real-\ntime motion planner, outperforming strong end-to-end plan-\nners on NAVSIM benchmarks. Code will be publicly avail-\nable at https://github.com/Kevin-thu/Epona/.\n1. Introduction\nRecently, with the rapid development of video generation\nmodels, world models have attracted significant attention\nand emerged as a powerful paradigm for physical world\nsimulations and autonomous decision-making [1, 13, 20,\n32]. These foundation models enable agents to under-\nstand inherent world knowledge and predict future dynam-\nics, making them particularly promising for autonomous\ndriving. Unlike traditional separate perception-planning\npipelines, which require extensive annotations and explicit\nsupervision, generative driving world models [11, 15, 17,\n23, 25, 56, 57, 66] integrate visual scene understanding\nwith future prediction in a self-supervised manner, offering\na new solution toward human-like, end-to-end autonomous\ndriving.\nGenerative world models primarily fall into two cat-\negories: diffusion-based approaches and GPT-style au-\ntoregressive methods. The diffusion-based paradigm\n(e.g., Vista [17]), while achieving impressive visual fi-\ndelity through joint distribution modeling of fixed-length\nvideos [3, 22, 45, 48], fundamentally suffers from its in-\nability to model per-timestep local distributions. This lim-\nitation manifests in critical deficiencies: failure to support\nvariable-length long-range prediction crucial for dynamic\nworld simulation, and infeasible trajectory planning due to\nthe lack of mutlimodal prediction mechanism.\nConversely, GPT-style approaches [5, 42, 43, 52] address\ntemporal flexibility through autoregressive next-token pre-\ndiction (as seen in GAIA-1 [23]). However, the quantiza-\ntion and tokenization process significantly degrades visual\nquality and planning precision. Moreover, the causal nature\nof autoregressive transformers constrains them to predicting\nonly the next action rather than planning long-horizon tra-\njectories [11], limiting their potential to serve as end-to-end\ndriving planners. Both paradigms reveal complementary\nshortcomings - diffusion models lack temporal decomposi-\ntion while autoregressive transformers sacrifice continuous\nvisual precision - highlighting the need for a unified frame-\nwork that reconciles these divergent advantages for compe-\ntent driving world modeling.\nWe introduce Epona , an autoregressive diffusion world\nmodel that achieves high-resolution long-horizon video\ngeneration and accurate trajectory planning. Our core in-\nnovation comes from three key designs: 1) Decoupledspatiotemporal factorization. While existing video dif-\nfusion methods model joint spatial-temporal distributions\nof past and future frames, we assume their temporal la-\ntent modeling lacks explicit causality constraints, leading\nto error accumulation in long sequences. Epona addresses\nthis through spacetime-disentangled processing: A GPT-\nstyle transformer with causal attention handles temporal dy-\nnamics in compressed latent space, while twin diffusion\ntransformers separately handle spatial rendering and trajec-\ntory generation. 2) Asynchronous multi-modal genera-\ntion. Building upon this foundation, we decouple trajec-\ntory planning from visual generation through parallel de-\nnoising processes. Two specialized DiTs [31, 39] asyn-\nchronously generate 3-second vehicle trajectories and the\nsingle next future frame. Both streams share flow-matching\nobjectives [2, 34, 35] conditioned on the same temporal la-\ntent, ensuring alignment while preserving modality-specific\noptimizations. 3) A chain-of-forward training strategy\naddresses error accumulation and content drift in autore-\ngressive loops.\nEpona offers several additional advantages. 1) Long-\nhorizon generation. Our autoregressive diffusion model\ncan achieve long-time generation over up to 2 minutes, sig-\nnificantly outperforming existing world models. 2) Real-\ntime trajectory planning. The separate multi-modal gen-\neration architecture enables to perform trajectory planning\nsolely while video prediction is deactivated, significantly re-\nducing the inference FLOPS. This enables high-quality and\neven real-time trajectory planning, achieving rates of up to\n20 Hz. 3) Visual detail preservation. Our autoregressive\nformulation adopts continuous visual tokenizer instead of\ndiscrete ones, thus preserving rich scene details.\nExtensive experiments demonstrate the effectiveness\nand superiority of our world model. For video genera-\ntion, our model achieves state-of-the-art FVD [50] on the\nNuScenes [6] benchmark, surpassing the best-performing\nVista [17] by 7.4%, while extending generation length from\n15 seconds to over 2 minutes (600 frames). Thanks to joint\nsupervision with trajectory prediction, Epona allows action\ncontrol to simulate diverse driving scenarios, as shown in\nFig. 1 (B). For motion planning, our method outperforms\nstrong end-to-end planners on NA VSIM [12] without per-\nception inputs ( e.g., 3D boxes/lanes). Notably, we observe\nthat Epona learns essential traffic world knowledge ( e.g.,\nstop driving at red light) purely from self-supervised future\nprediction tasks, as shown in Fig. 1 (C). This suggests that\nour world model can implicitly learn real-world driving dy-\nnamics, making it a promising direction for next-generation\nautonomous driving systems.\n2\n\nFutur e trajectory plan \nDCAE \nEncoder\nCondition frames Latents \nxy\nHistorical t rajectories Compact\nlatentTemporal-aware\nDCAE Dncoder\nPredicted next frame \nChain-of-Forward TrainingAutor egressive\nprediction\nMultimodal\nSpatiotemporal\nTransformer\nTrajectory \nPlanning  DiTNext-frame \nPrediction DiTFigure 2. Overview of Epona .Our world model utilizes a multimodal spatiotemporal transformer to process the historical context of the\nfirstTframes and employs a next-frame prediction DiT to generate the frame at T+ 1and a trajectory planning DiT to forecast the future\nN-frame pose trajectory. By adopting a chain-of-forward strategy, our approach enables high-quality and long-horizon video generation\nwith an autoregressive manner.\n2. Related Work\n2.1. World Models for Autonomous Driving\nConstructing real-world driving world models have drawn\nconsiderable attention in recent years, among which vision-\ncentric approaches gain prominence due to their superior\nsensor flexibility, data accessibility, and more human-like\nrepresentation forms. Early efforts primarily focused on\nadapting pre-trained diffusion models ( e.g., Stable Diffu-\nsion [3, 45]) to driving scenarios through fine-tuning. How-\never, these methods either lacked critical planning mod-\nules [15, 17, 18] or were limited to low-resolution, short-\nterm generation [16, 36, 56, 57, 60, 61], making them un-\nsuitable for consistent long-range prediction and real-time\nplanning. Recent work [11, 23, 25, 66] explored harness-\ning GPT-like architecture to unify visual and action model-\ning and achieved long-range autoregressive generation. Yet,\nthese methods require encoding images and trajectories into\ndiscrete tokens, which significantly degrades visual fidelity\nand trajectory precision. Similarly, while the newly re-\nleased Cosmos [1] foundation model can serve as a driving\nworld model, it does not introduce a new framework, facing\nthe same limitations as the previous methods. In addition,\nits large parameter count and high computational demands\nlimit its practicality. In contrast, we propose a novel autore-\ngressive diffusion world model framework for autonomous\ndriving, enabling long-range autoregressive generation in\ncontinuous visual and trajectory representations.\n2.2. Long Video Generation\nLong-term prediction is not only a key challenge for cur-\nrent video generation models but also a crucial capability\nfor robust world models, as it reflects the model’s abil-\nity to learn consistent environment dynamics and accu-\nrately simulate real-world temporal progression [13]. Since\noriginal video diffusion models ( e.g. SVD [3]) are limited\nto fixed-length short clips generation, prior methods haveexplored extrapolating video length by noise reschedul-\ning [41], overlapped generation [54, 55] or hierarchical gen-\neration [62]. However, these techniques fail to resolve in-\nherent model constraints, often resulting in inconsistencies\nand abrupt visual changes in long videos. Autoregressive\napproaches [11, 53, 58] naturally support variable-length\ngeneration but suffer from quality degradation due to do-\nmain shift between teacher-forcing training and error ac-\ncumulation in sampling. GameNGen [51] and Driving-\nWorld [25] introduce noise augmentation and random token\ndropout during training to alleviate the problem by simulat-\ning the error in sampling, but still limited to specific model\narchitectures. We propose a general chain-of-forward strat-\negy allowing the model to directly learn inference errors\nduring training, effectively reducing autoregressive drift.\nMeanwhile, recent works such as Diffusion Forcing [8, 47]\nand FIFO-Diffusion [29] explore integrating autoregressive\ngeneration in video diffusion by adjusting frame-wise noise\nlevels and leveraging causal network designs. Our model\nadopts a similar causal temporal modeling strategy but re-\ndefines the architecture into a two-stage end-to-end frame-\nwork, allowing joint generation of motion plans and next-\nframe images.\n3. Method\nIn this section, we formally present the model framework\nand training techniques of Epona . We begin with prelimi-\nnaries on diffusion models in Sec. 3.1 and discuss our world\nmodel formulation design insights in Sec. 3.2. Then we in-\ntroduce our proposed autoregressive diffusion world model\nframework in Sec. 3.3, including three dedicated modules:\na multimodal spatiotemporal transformer to capture histor-\nical context, a trajectory planning DiT to generate future\n3-seconds trajectories, and a next-frame prediction DiT to\ngenerate the next-frame images. To mitigate autoregressive\ndrift and enable long-horizon video generation, we propose\na simple yet effective chain-of-forward training strategy, de-\n3\n\n········· ···\n········· ···\n········· ···\n+\n··· ··· ··· ···Video-Dif fusion\nMethodOurs\nDenoiseDenoiseDiscrete tokenize\nNoise\nContinuous tokenizeConditionsAR MethodFigure 3. Comparison of Different World Modeling Formula-\ntion. Up: Conventional autoregressive pipeline quantizes conti-\nnous images into discrete tokens and perform next-token predic-\ntion iteratively. Middle: The video-diffusion-based methods gen-\nerate future nframes simultaneously. Down: Our method autore-\ngressively predicts fine-grained future frames in continuous space.\ntailed in Sec.3.4. Additionally, to enhance video quality, we\nintroduce a temporal-aware DCAE decoder in Sec.3.5. An\noverview of our method is illustrated in Fig. 2.\n3.1. Preliminary\nDiffusion models [22, 48] are a family of powerful genera-\ntive models that transform noise samples x(1)drawn from a\nprior distribution p1=N(0,I)into data samples x(0)from\nthe target distribution in terms of a differentiable equation:\ndx(t)=vΘ(x(t), t)dt, t∈[0,1], (1)\nwhere velocity vis parametrized by a neural network Θ.\nRectified flow [2, 34, 35] proposes to define a straight prob-\nability path between p0andp1to improve training and sam-\npling efficiency and optimize the network Θusing a velocity\nprediction loss:\nx(t)= (1−t)x(0)+tϵ, ϵ∼ N(0,I), (2)\nLRF=Ex(0),ϵ,t\u0002\n∥vΘ(x(t), t)−(x(0)−ϵ)∥2\u0003\n,(3)\nwhich has proven to be an effective and scalable approach\nin state-of-the-art image and video generation models [14,\n26, 31]. In Epona , we adopt the diffusion model and recti-\nfied flow objective for both next-frame image and trajectory\ngeneration. Particularly, to enhance efficiency, we encode\nimages into compact latents using a pre-trained deep com-\npression encoder [9] and adopt a latent diffusion model [45]\nfor image synthesis.\n3.2. Reformulation of World Model Designs\nIn this section, we discuss different world model formu-\nlation design choices, as illustrated in Fig. 3. Given\na sequence of previous front-view camera observa-\ntions{Ot}T\nt=1and the corresponding driving trajec-\ntory{at−1→t}T\nt=1, the goal of driving world moelsis to predict future driving dynamics based on histor-\nical context. Here, each driving action at1→t2:=\n(∆θt1→t2,∆xt1→t2,∆yt1→t2)∈R3represents the vehi-\ncle’s motion from t1tot2, where ∆θdenotes the orien-\ntation change, and (∆x,∆y)specify the relative displace-\nment in the ego-coordinate frame. For consistency, we de-\nfinea0→1= (0,0,0). Existing methods tackle this problem\nby formulating world modeling in the following two ways:\nVideo diffusion-based world models. Current leading\ndriving world models, like Vista [17], formulates world\nmodeling in the form of video diffusion models [3], which\njointly capture the global spatiotemporal distribution of\nboth past and a fixed-length future,\np\u0000\n{OT+i}n\ni=1,{Ot,at}T\nt=1\u0001\n.\nThis formulation disrupts the causal temporal structure be-\ntween historical observations and future predictions, limit-\ning its ability to model real-world progressive dynamics and\ngenerate flexible-length long-term videos.\nGPT-based world models. Alternatively, autoregres-\nsive transformer-based world models [11, 23, 25] dis-\ncretize image observations into token sequences Ot=\n[t1,t2,···,tL]and model the conditional image distribu-\ntion as a token-by-token prediction,\nLY\ni=1p(ti|t<i,{Ot,at}T\nt=1).\nHowever, this independent token modeling weakens spa-\ntial correlations and the quantization process distortes high-\nfrequency details, leading to degraded generation quality.\nOur approach. In contrast, we formulate world modeling\nasa sequential future prediction process in the temporal do-\nmain . Specifically, given past driving observations {Ot}T\nt=1\nand the driving trajectory {at−1→t}T\nt=1, our model predicts\nboth a policy for future trajectory planning,\nπ\u0000\n{aT→T+i}n\ni=1| {Ot,at−1→t}T\nt=1\u0001\n,\nand a conditional distribution over the next-frame camera\nobservation as a whole,\np\u0000\nOT+1| {Ot,at−1→t}T\nt=1,aT→T+1\u0001\n.\nThe next-frame prediction is conditioned on either a model-\npredicted action or an externally provided action aT→T+1.\nBy decoupling causal temporal modeling from fine-grained\nfuture prediction, our model can generate flexible-length\nlong videos autoregressively in continuous representations.\nMoreover, by factorizing trajectory planning from visual\ngeneration, our model can seamlessly serve as a real-time\nmotion planner, bridging a critical gap between current driv-\ning world models and end-to-end motion planners.\n4\n\n3.3. Epona: Autoregressive Diffusion World Model\nBased on the reformulated world modeling design, we pro-\npose Epona , an autoregressive diffusion world model for\nautonomous driving. Our framework consists of three key\ncomponents. First, a Multimodal Spatiotemporal Trans-\nformer (MST) encodes historical context {Ot,at}T\nt=1into\na compact latent representation, effectively capturing envi-\nronmental context and driving dynamics. Then, based on\nthe historical latents, we employ two specialized diffusion\ntransformers to predict fine-grained future details, including\na tiny Trajectory Planning Transformer (TrajDiT) that mod-\nels the policy πfor trajectory planning, and a Next-frame\nPrediction Transformer (VisDiT) that models the visual dis-\ntribution pfor future image generation. This modular de-\nsign enables a range of autonomous driving applications.\nFor instance, MST and VisDiT can be used independently\nfor controllable driving simulations, while MST and Tra-\njDiT facilitate real-time motion planning.\nMultimodal Spatiotemporal Transformer (MST). Given\nthe encoded past driving scenes {Zt}T\nt=1and trajectory\n{at−1→t}T\nt=1, we introduce a multimodal spatiotemporal\ntransformer to effectively integrate temporal dynamics and\nmultimodal information from historical context for future\nprediction. Inspired by prior work in video generation and\nworld modeling [4, 25, 37], our approach employs inter-\nleaved multimodal spatial attention layers and causal tem-\nporal attention layers. This design progressively incorpo-\nrates historical information into a compact latent repre-\nsentation while significantly reducing memory consump-\ntion compared to full-sequence attention. Additionally,\nthis design naturally supports historical contexts of variable\nlength.\nSpecifically, we first project the flattened visual latent\npatches Z∈RB×T×L×Cand action sequences a∈\nRB×T×3into an embedding space. Then we concatenate\nthem along the spatial dimension and add temporal posi-\ntional embeddings to obtain the latent embedding sequence\nE∈RB×T×(L+3)×D. This sequence is processed through\ninterleaved multimodal spatiotemporal layers as follows\n(using einops [44] notation):\nE←rearrange (E,(b t) l c →(b l) t c) )\nE←CausalTemporalLayer (E,CausalMask )\nE←rearrange (E,(b l) t c →(b t) l c) )\nE←MultimodalSpatialLayer (E),\nwhere Bis the batch size, Tis the number of condition-\ning frames, L=H×Wis the number of flattened latents\nin an image, Cis the image latent channel dimension, D\nis the embedding dimension, and CausalMask is the tri-\nangular causal attention mask. Finally, we use the latent\nembedding of the last frame, F∈RB×(L+3)×D, as the\ncompact latent representation for the next-stage prediction.\nRectified Flow LossChain-of-Forwar d\nEpona Figure 4. Concept illustration of our training process. Here x\ncan be either image latents or trajectories.\nAfter training, this embedding encapsulates the historical\ncontext {Ot,at−1→t}T\nt=1.\nTrajectory Planning Diffusion Transformer (TrajDiT).\nTrajDiT predicts future trajectories using a tiny diffusion\ntransformer. Following the DiT frameworks in most ad-\nvanced open-source text-to-image and video generation\nmodels [26, 31], we adopt a Dual-Single-Stream architec-\nture. In the dual-stream phase, the historical latent repre-\nsentation Fand trajectory data are processed independently\nthrough transformer blocks, with only attention operations\nlinking them. In the single-stream phase, they are concate-\nnated to pass through subsequent transformer blocks for\neffective information fusion. Detailed architecture can be\nfound in the supplementary material.\nDuring training, we add noise to the target trajectories\na∈RB×N×3using Eq. 2. The model then predicts velocity\nvtrajconditioned on F, where Nis the planning horizon.\nWe optimize using the rectified flow loss:\nLtraj=Ea,ϵ,t\u0002\n∥vtraj(a(t), t)−(a−ϵ)∥2\u0003\n. (4)\nFor inference, random Gaussian noise is iteratively de-\nnoised conditioned on Fto generate future trajectory plans.\nNext-frame Prediction Diffusion Transformer (VisDiT).\nVisDiT has a similar architecture as TrajDiT, with an addi-\ntional modulation [39] branch for action control aT→T+1.\nWe also use the flow loss for visual supervision:\nLvis=EZT+1,ϵ,t\u0002\n∥vvis(ZT+1(t), t)−(ZT+1−ϵ)∥2\u0003\n,\n(5)\nTogether, the total loss jointly optimizes the entire world\nmodel:\nL=Ltraj+Lvis. (6)\nDuring inference, VisDiT denoises ˆZT+1, conditioned on\nFand the action either predicted by TrajDiT or provided by\nuser. The latents are then decoded using the DCAE decoder\nto generate the next-frame image ˆOT+1.\n3.4. Chain-of-Forward Training\nWith our proposed autoregressive diffusion world model,\nwe can autoregressively generate future videos frame by\nframe. However, long-term generation suffers from a long-\nstanding autoregressive drift problem [51]: during training,\n5\n\nTable 1. Comparisons of generated videos on the NuScenes [6] validation set. Our model achieves state-of-the-art FVD score compared\nto existing driving world models, while extending the video length to over two minutes. *The max duration number indicates the horizon\nthat produces plausible results, following existing methods.\nMetric DriveGAN [30] DriveDreamer [56] WoV oGen [36] Drive-WM [57] GenAD (OpenDV) [61] Vista [17] DrivingWorld [25] Ours\nFID↓ 73.4 52.6 27.6 15.8 15.4 6.9 7.4 7.5\nFVD↓ 502.3 452.0 417.7 122.7 184.0 89.4 90.9 82.8\nMax Duration / Frames* N/A 4s / 48 2.5s / 5 8s / 16 4s / 8 15s / 150 40s / 400 120s / 600\nVista\nOurs\n0s 10s 20s 30s 1min 2min+!\nFigure 5. Qualitative Comparison between Vista [17] and Epona .Zoom in for better views.\nthe model predicts the next frame using ground-truth histor-\nical context, whereas during inference, it relies on its own\npast predictions. This domain gap between teacher-forcing\ntraining and autoregressive sampling leads to error accumu-\nlation and rapid quality degradation.\nTo mitigate this, we introduce a chain-of-forward train-\ning strategy. Periodically, we perform multiple forward\npasses using self-predicted frames to enhance the model’s\nrobustness to inference noise (see Fig. 4). Notably, to en-\nsure training efficiency, instead of sampling next-frame la-\ntents from pure noise, we leverage the model-predicted ve-\nlocity vΘto estimate the denoised latents in one step:\nˆx(0)=x(t)+tvΘ(x(t), t) (7)\nThe estimated ˆx(0), along with previous conditioned\nframes, is then used in the next forward pass to autoregres-\nsively generate subsequent frames. This process simulates\nprediction noise, helping the model adapt to deviations and\nimproving long-term video generation quality.\n3.5. Temporal-aware DCAE Decoder\nUnlike conventional autoencoders that downsample images\nby a factor of 8, DCAE [9] progressively increases this to\n32, reducing latent tokens by 16×. In our world model, we\nadopt DCAE for image encoding to improve training effi-\nciency and reduce memory usage, enabling conditioning on\nlonger historical contexts.\nHowever, as an image autoencoder, DCAE lacks tem-\nporal interactions, causing flickering when decoding video\nframe by frame, which degrades visual quality. To address\nthis, we propose a temporal-aware DCAE to enhance inter-\nframe consistency. Specifically, to maximize pretrained pa-\nrameters while minimizing architectural changes, we intro-\nduce spatiotemporal self-attention layers before the DCAEdecoder while keeping the encoder fixed during fine-tuning.\nThis facilitates multi-frame interactions, greatly improving\ntemporal consistency in generated videos.\n4. Experiment\n4.1. Implementation Details\nWorld Model. Our Epona consists of 2.5 Bparame-\nters, including a 12-layer multimodal spatiotemporal trans-\nformer with 1.3 Bparameters, a 12-layer next-frame pre-\ndiction diffusion transformer with 1.2 Bparameters, and a\n2-layer trajectory planning diffusion transformer with 50 M\nparameters. It is trained on publicly available videos from\nthe NuPlan dataset [7] and 700 scenes from the NuScenes\ndataset [6] from scratch, all images are resized to 512×1024.\nWe utilize the rectified flow [35] objective for both video\ngeneration and trajectory planning tasks, training the entire\nmodel in an end-to-end manner. The training was conducted\non 48 NVIDIA A100 GPUs for nearly two weeks, with a\ntotal of 600k iterations and a batch size of 96. During train-\ning, we apply Chain-of-Forward every 10 steps, each time\nperforming three forward passes. We use the AdamW op-\ntimizer with a learning rate of 1×10−4and set the weight\ndecay to 5×10−2. For inference, we report our speed for\neach module on a single NVIDIA 4090 GPU in Table 2. In\nall our experiments, we set our DiT sampling step to 100.\nNotice that with MST and TrajDiT, our world model can\nseamlessly serve as a real-time motion planner.\nEvaluations on video generation. We employ 1628 video\nclips from the NuPlan test set [7] and 1646 video clips from\nthe NuScenes validation dataset [6] for performance eval-\nuation, respectively. During the test, our world model is\nconditioned on 10 consecutive past frames to generate the\nsubsequent frame and repeat the process autoregressively to\n6\n\nCondition frame2s 5s 10s 16s\nCondition frame\n2s 5s 10s 16sTrajectory Condition\n0s 2s \n5s 10s \n16s \nTurn Left\n Turn Right\n Turn Left\n Straight\nx\ny\nFigure 6. Trajectory-controlled video generation. Our world model can generate controllable videos based on predefined trajectories.\nTable 2. Inference speed. We evaluate our inference speed for\ngenerating a 3-second trajectory and a 512×1024 image per mod-\nule on a single NVIDIA 4090 GPU.\nDiT sampling steps MST TrajDiT VisDiT\n10 ∼0.02s ∼0.03s ∼0.3s\n100 ∼0.02s ∼0.3s ∼2s\nsynthesize future video frames. We use the Frechet Video\nDistance (FVD) [50] and the Frechet Inception Distance\n(FID) [21] to evaluate the quality of the generated videos.\nEvaluations on trajectory planning. We evaluate trajec-\ntory planning using the NuScenes benchmark [6] and the\nNA VSIM benchmark [12]. For the NuScenes, we use L2\nerror and collision rate as the evaluation metrics following\nthe existing works [24, 46, 66] to evaluate the planning per-\nformance. L2 error measures the L2 distance between the\npredicted and ground truth trajectories, while the collision\nrate measures the frequency of predicted trajectory intersec-\ntions with objects. The NA VSIM benchmark assesses per-\nformance using the predictive driver model score (PDMS),\nderived from five factors, as shown in Table 4.\n4.2. Evaluation of Video Generation\nQuantitative Comparison of Generated Videos. We\npresent a quantitative comparison with existing methods on\nthe NuScenes dataset [6] in Table 1. Since most methods are\nnot publicly available, we compare with the reported results\nfrom their respective papers. Notably, the existing meth-\nods (e.g., Vista [17]) fine-tune video diffusion models pre-\ntrained on large-scale datasets, while our world model, in-\ncluding next-frame DiT, is trained from scratch. As shown\nin Table 1, our generated videos achieve state-of-the-art\nFVD scores, indicating smoother and more realistic videogeneration quality. Moreover, our world model can gener-\nate significantly longer video frames compared to existing\napproaches as shown in Table 1.\nQualitative Comparison of Generated Videos. We pro-\nvide a qualitative comparison with the state-of-the-art open-\nsource driving world model, Vista [17]. Since Vista is a\n25-frame fixed-length video diffusion model, we perform\nrollout to generate longer videos as illustrated in their pa-\nper. As shown in Fig. 5, our Epona generates consistent\nlong-horizon driving scenes with high-fidelity visuals and\ndetailed structures and vehicles.\nTrajectory-controlled Video Generation. Fig. 6 illus-\ntrates the pose controllability of our model. Given the pre-\ndefined pose trajectory, the different condition frames can\ngenerate future frames that conform to the corresponding\nmotion path, which is crucial for obtaining autonomous\ndriving videos under extreme scenarios.\nExtra Long-range Video Generation. Epona combines\nthe strengths of autoregressive and diffusion models, facil-\nitating the generation of high-quality, long-duration videos\nconditioned on input frames. As shown in Fig.5 and Fig.7,\nour model can autoregressively generate minute-long driv-\ning videos with high fidelity and consistency, without no-\nticeable drift. More long-term generation videos are pro-\nvided in the supplementary materials.\n4.3. Evaluation of Trajectory Planning\nFor the NuScenes benchmark [6], we compare our world\nmodel with several existing methods, as shown in Table 3.\nAlthough our model does not achieve the best results, it at-\ntains competitive performance without any additional su-\npervision. It is worth noting that incorporating more super-\nvision typically leads to better performance but cost expen-\nsive annotations. Additionally, similar to Doe-1 [66], our\n7\n\n0s 5s 10s\n20s 30s 1min\nw/o Chain-of-Forward\n0s 10s\n30s5s\n20s 1minw/ Chain-of-ForwardFigure 7. Qualitative Comparison between long videos generated by models w/ and w/o Chain-of-Forward training. Left: Visual\nquality deteriorates rapidly after 10–20 seconds. Right: The same driving scenes with Chain-of-Forward training maintain high visual\nquality, generating minute-long videos without significant degradation. Zoom in for better views.\nTable 3. End-to-end motion planning performance on the NuScenes [6] dataset. Note that our model achieves a low collision rate,\ndemonstrating its understanding of basic traffic rules via simple next-frame prediction.∗represents only using the front camera as input.\nMethod Input Auxiliary SupervisionL2 (m) ↓ Collision Rate (%) ↓\n1s 2s 3s Avg. 1s 2s 3s Avg.\nST-P3 [24] Camera Map & Box & Depth 1.33 2.11 2.90 2.11 0.23 0.62 1.27 0.71\nUniAD [46] Camera Map & Box & Motion & Tracklets & Occ 0.48 0.96 1.65 1.03 0.05 0.17 0.71 0.31\nOccNet [49] Camera 3D-Occ & Map & Box 1.29 2.13 2.99 2.14 0.21 0.59 1.37 0.72\nOccWorld [64] Camera 3D-Occ 0.52 1.27 2.41 1.40 0.12 0.40 2.08 0.87\nV AD-Tiny [28] Camera Map & Box & Motion 0.60 1.23 2.06 1.30 0.31 0.53 1.33 0.72\nV AD-Base [28] Camera Map & Box & Motion 0.54 1.15 1.98 1.22 0.04 0.39 1.17 0.53\nGenAD [65] Camera Map & Box & Motion 0.36 0.83 1.55 0.91 0.06 0.23 1.00 0.43\nDoe-1 [66] Camera∗QA 0.50 1.18 2.11 1.26 0.04 0.37 1.19 0.53\nOurs Camera∗None 0.61 1.17 1.98 1.25 0.01 0.22 0.85 0.36\nmodel only utilizes the front camera, whereas other meth-\nods rely on multi-view inputs for planning. As shown in\nTable 3, our approach can generate reasonable trajectories\nwhile achieving the lowest collision rate for a 1-second hori-\nzon, which is crucial for long-term realistic video predic-\ntions. For the more challenging NA VSIM benchmark [12],\nas shown in Table 4, our method achieves state-of-the-art\nresults in overall PDMS when conditioned on the past 2 sec-\nonds of observations to predict 4-second future trajectories,\nshowing the strong motion planning capability.\n4.4. Ablation Study\nEffect of Shared Latent for Multi-modal Joint Predic-\ntion. To assess the benefit of jointly modeling scene and\ntrajectory via a shared latent representation, we conduct\nan ablation by disabling video prediction and training the\nmodel solely for trajectory prediction. This variant is eval-\nuated on the NA VISIM test set. As shown in Tab. 5, re-\nmoving video prediction leads to a noticeable drop in plan-\nning performance. This result highlights the advantage of\nthe shared latent F, which encourages the world model to\nbetter capture complex driving dynamics by leveraging vi-\nsual signals. This ablation confirms that coupling video and\ntrajectory prediction within a unified latent space in world\nmodels significantly benefits downstream planning tasks.\nEffect of Chain-of-Forward Training. To evaluate the\nimpact of our chain-of-forward strategy on model perfor-\n0 5 10 15 20 25 30 35 40\nAutoregressive Frames1012141618202224FID Score\n9.059.8110.6611.97 12.1314.15\n13.5516.17\n14.7318.12\n16.1020.37\n17.3122.60\n18.5425.16\nw/ Chain-of-Forward\nw/o Chain-of-ForwardFigure 8. Effect of Chain-of-Forward training. FID comparison\nin NuPlan test set between models w/ and w/o Chain-of-Forward\ntraining strategy.\nmance, we conduct an ablation study comparing results\nwith and without this strategy. Given that our model iter-\natively generates the next frame, the chain-of-forward ap-\nproach simulates potential inference errors during training,\nthereby enhancing the model’s robustness. As shown in\nFig. 7 and Fig. 8, as the model autoregressively generates\nlonger sequences, the gap in visual quality and FID score\nbetween models with and without the chain-of-forward\nstrategy becomes increasingly significant, validating its ef-\nfectiveness in long-term video generation.\nEffect of Temporal-aware DCAE Decoder. Consider-\n8\n\nTable 4. End-to-end motion planning performance on the NA VSIM [12] test set. NC: no at-fault collision. DAC: drivable area\ncompliance. TTC: time-to-collision. Comf.: comfort. EP: ego progress. PDMS: the predictive driver model score. LAW[33] is in the\nperception-free setting. Our world model outperforms strong end-to-end planners in the overall PDMS score.\nMethod Input NC↑DAC↑TTC↑Comf. ↑EP↑PDMS ↑\nHuman / 100 100 100 99.9 87.5 94.8\nUniAD[46] Camera 97.8 91.9 92.9 100 78.8 83.4\nPARA-Drive[59] Camera 97.9 92.4 93.0 99.8 79.3 84.6\nLAW[33] Camera 96.4 95.4 88.7 99.9 81.7 84.6\nTransFuser[40] Camera & Lidar 97.7 92.8 92.8 100 79.2 84.0\nDRAMA[63] Camera & Lidar 98.0 93.1 94.8 100 80.1 85.5\nV ADv2[10] Camera & Lidar 97.2 89.1 91.9 100 76.0 80.9\nOurs Camera 97.9 95.1 93.8 99.9 80.4 86.2\nTable 5. Comparison of planning results on the NA VSIM test\nset.Jointly predicting the next scene using a shared latent signifi-\ncantly improves planning performance.\nMethod NC↑DAC↑TTC↑Comf. ↑EP↑PDMS ↑\nOurs w/o Joint Training 94.5 89.7 88.1 99.9 74.7 78.1\nOurs 97.9 95.1 93.8 99.9 80.4 86.2\ning that the original DCAE is an image-based autoencoder\nwithout temporal modeling capability, we incorporate a\ntemporal interaction module before the DCAE decoder. As\nshown in Table 6, our world model achieves improved per-\nformance with the temporal module, effectively reducing\nflickering and enhancing the smoothness of the generated\nvideos.\nTable 6. Comparison of the Generated Videos w/and w/o\nTemporal-aware DCAE Decoder Module on NuPlan [7] test\nset.Temporal-aware DCAE Decoder can mitigate flickering arti-\nfacts and improve smoothness in generated videos.\nMethods FVD 10↓FVD 25↓FVD 40↓\nw/oTemporal Module 52.95 76.46 100.11\nOurs 50.77 61.46 74.88\nEffect of Different Context Length. We gradually in-\ncrease the length of conditioned frames to investigate its\nimpact on model performance. As shown in Table 7, as the\nnumber of conditioned frames increases, our world model\nimproves in FVD performance due to longer historical in-\nformation. However, longer conditioned frames require\nhandling extended sequences, which poses computational\nchallenges. Given our model setting, 10 frames represent\nthe upper limit for conditioning. Therefore, we ultimately\nselect 10 frames as the conditioning length in our approach.Table 7. Comparison of different condition frames on NuPlan\n[7] test set. Epona generates better videos when conditioning more\nframes.\nFrame number FVD 10↓FVD 25↓FVD 40↓\n2 59.85 81.58 103.70\n5 55.46 71.28 86.76\n10 50.77 61.46 74.88\n5. Conclusion\nWe have presented Epona , an autoregressive diffusion\nworld model for autonomous driving that jointly predicts\nhigh-fidelity future trajectories and driving scenes based\non historical driving context. Thanks to our proposed de-\ncoupled spatiotemporal modeling and asynchronous multi-\nmodal generation strategies, our model achieves high-\nquality and long-term prediction. In addition, our model\ncould serves as a real-time motion planner via performing\ntrajectory planning. We have demonstrated that our ap-\nproach significantly advances the state of the art in driving\nworld models, uncovering the large potential for building\nnext-generation autonomous driving systems.\n9\n\nReferences\n[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji,\nErik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin\nChen, Yin Cui, Yifan Ding, et al. Cosmos world foun-\ndation model platform for physical ai. arXiv preprint\narXiv:2501.03575 , 2025. 2, 3\n[2] Michael S. Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants, 2023. 2, 4\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram V oleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127 , 2023. 2, 3, 4\n[4] A. Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn,\nSeung Wook Kim, Sanja Fidler, and Karsten Kreis. Align\nyour latents: High-resolution video synthesis with latent dif-\nfusion models. CVPR , pages 22563–22575, 2023. 5\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Lan-\nguage models are few-shot learners. Advances in Neural\nInformation Processing Systems (NeurIPS) , 33:1877–1901,\n2020. 2\n[6] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\nmodal dataset for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition , pages 11621–11631, 2020. 2, 6, 7, 8\n[7] Holger Caesar, Juraj Kabzan, Kok Seang Tan, Whye Kit\nFong, Eric Wolff, Alex Lang, Luke Fletcher, Oscar Beijbom,\nand Sammy Omari. nuplan: A closed-loop ml-based plan-\nning benchmark for autonomous vehicles. arXiv preprint\narXiv:2106.11810 , 2021. 6, 9\n[8] Boyuan Chen, Diego Marti Monso, Yilun Du, Max Sim-\nchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion\nforcing: Next-token prediction meets full-sequence diffu-\nsion, 2024. 3\n[9] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang,\nHaotian Tang, Muyang Li, Yao Lu, and Song Han. Deep\ncompression autoencoder for efficient high-resolution diffu-\nsion models. arXiv preprint arXiv:2410.10733 , 2024. 4, 6\n[10] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing\nXu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang\nWang. Vadv2: End-to-end vectorized autonomous driving\nvia probabilistic planning. arXiv preprint arXiv:2402.13243 ,\n2024. 9\n[11] Yuntao Chen, Yuqi Wang, and Zhaoxiang Zhang. Driving-\ngpt: Unifying driving world modeling and planning with\nmulti-modal autoregressive transformers. arXiv preprint\narXiv:2412.18607 , 2024. 2, 3, 4, 1\n[12] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo\nWeng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor\nGilitschenski, Boris Ivanovic, Marco Pavone, Andreas\nGeiger, and Kashyap Chitta. Navsim: Data-driven non-\nreactive autonomous vehicle simulation and benchmarking.\nInNeurIPS , 2024. 2, 7, 8, 9[13] Jingtao Ding, Yunke Zhang, Yu Shang, Yuheng Zhang, Ze-\nfang Zong, Jie Feng, Yuan Yuan, Hongyuan Su, Nian Li,\nNicholas Sukiennik, et al. Understanding world or predict-\ning future? a comprehensive survey of world models. arXiv\npreprint arXiv:2411.14499 , 2024. 2, 3\n[14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim\nEntezari, Jonas M ¨uller, Harry Saini, Yam Levi, Dominik\nLorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim\nDockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yan-\nnik Marek, and Robin Rombach. Scaling rectified flow trans-\nformers for high-resolution image synthesis, 2024. 4\n[15] Ruiyuan Gao, Kai Chen, Bo Xiao, Lanqing Hong, Zhen-\nguo Li, and Qiang Xu. Magicdrivedit: High-resolution long\nvideo generation for autonomous driving with adaptive con-\ntrol. arXiv preprint arXiv:2411.13807 , 2024. 2, 3, 1\n[16] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo\nLi, Dit-Yan Yeung, and Qiang Xu. MagicDrive: Street view\ngeneration with diverse 3d geometry control. In ICLR , 2024.\n3\n[17] Shenyuan Gao, Jiazhi Yang, Li Chen, Kashyap Chitta,\nYihang Qiu, Andreas Geiger, Jun Zhang, and Hongyang\nLi. Vista: A generalizable driving world model with\nhigh fidelity and versatile controllability. arXiv preprint\narXiv:2405.17398 , 2024. 2, 3, 4, 6, 7\n[18] Songen Gu, Wei Yin, Bu Jin, Xiaoyang Guo, Junming Wang,\nHaodong Li, Qian Zhang, and Xiaoxiao Long. Dome: Tam-\ning diffusion model into high-fidelity controllable occupancy\nworld model. ArXiv , abs/2410.10429, 2024. 3\n[19] Xi Guo, Chenjing Ding, Haoxuan Dou, Xin Zhang, Weixuan\nTang, and Wei Wu. Infinitydrive: Breaking time limits in\ndriving world models, 2024. 1\n[20] David Ha and J ¨urgen Schmidhuber. World models. arXiv\npreprint arXiv:1803.10122 , 2018. 2\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems ,\n30, 2017. 7\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems , pages 6840–6851. Curran Asso-\nciates, Inc., 2020. 2, 4\n[23] Anthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez,\nGeorge Fedoseev, Alex Kendall, Jamie Shotton, and Gian-\nluca Corrado. Gaia-1: A generative world model for au-\ntonomous driving. arXiv preprint arXiv:2309.17080 , 2023.\n2, 3, 4, 1\n[24] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi\nYan, and Dacheng Tao. St-p3: End-to-end vision-based au-\ntonomous driving via spatial-temporal feature learning. In\nEuropean Conference on Computer Vision , pages 533–549.\nSpringer, 2022. 7, 8\n[25] Xiaotao Hu, Wei Yin, Mingkai Jia, Junyuan Deng, Xiaoyang\nGuo, Qian Zhang, Xiaoxiao Long, and Ping Tan. Driving-\nworld: Constructingworld model for autonomous driving via\nvideo gpt. arXiv preprint arXiv:2412.19505 , 2024. 2, 3, 4,\n5, 6\n10\n\n[26] Tencent Hunyuan. Hunyuanvideo: A systematic framework\nfor large video generative models, 2024. 4, 5, 1, 2\n[27] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing\nWen, Chi Zhang, Xiangyu Zhang, and Tiancai Wang.\nAdriver-i: A general world model for autonomous driving,\n2023. 1\n[28] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie\nChen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang,\nand Xinggang Wang. Vad: Vectorized scene representa-\ntion for efficient autonomous driving. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\npages 8340–8350, 2023. 8\n[29] Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han.\nFifo-diffusion: Generating infinite videos from text without\ntraining. In NeurIPS , 2024. 3\n[30] Seung Wook Kim, Jonah Philion, Antonio Torralba, and\nSanja Fidler. Drivegan: Towards a controllable high-quality\nneural simulation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition , pages\n5820–5829, 2021. 6\n[31] Black Forest Labs. Flux. https://github.com/\nblack-forest-labs/flux , 2024. 2, 4, 5, 1\n[32] Yann LeCun. A path towards autonomous machine intelli-\ngence version 0.9. 2, 2022-06-27. Open Review , 62(1):1–62,\n2022. 2\n[33] Yingyan Li, Lue Fan, Jiawei He, Yuqi Wang, Yuntao Chen,\nZhaoxiang Zhang, and Tieniu Tan. Enhancing end-to-end\nautonomous driving with latent world model. 2024. 9\n[34] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxi-\nmilian Nickel, and Matt Le. Flow matching for generative\nmodeling, 2023. 2, 4\n[35] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003 , 2022. 2, 4,\n6\n[36] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li\nZhang. WoV oGen: World V olume-Aware Diffusion for Con-\ntrollable Multi-Camera Driving Scene Generation. arXiv\npreprint arXiv:2312.02934 , 2023. 3, 6\n[37] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei\nLiu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: La-\ntent diffusion transformer for video generation, 2024. 5\n[38] Yiyang Ma, Xingchao Liu, Xi aokang Chen, Wen Liu,\nChengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie, Haowei\nZhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu,\nand Chong Ruan. Janusflow: Harmonizing autoregression\nand rectified flow for unified multimodal understanding and\ngeneration. ArXiv , abs/2411.07975, 2024. 1\n[39] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision , pages 4195–4205,\n2023. 2, 5\n[40] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-\nmodal fusion transformer for end-to-end autonomous driv-\ning. In CVPR , 2021. 9\n[41] Haonan Qiu, Menghan Xia, Yong Zhang, Yingqing He, Xin-\ntao Wang, Ying Shan, and Ziwei Liu. Freenoise: Tuning-free\nlonger video diffusion via noise rescheduling, 2023. 3[42] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. OpenAI blog , 2018. 2\n[43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI blog , 1(8):9, 2019. 2\n[44] Alex Rogozhnikov. Einops: Clear and reliable tensor manip-\nulations with einstein-like notation. In International Confer-\nence on Learning Representations , 2022. 5\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj ¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2022. 2, 3, 4\n[46] Xiaogang Shi, Bin Cui, Gillian Dobbie, and Beng Chin Ooi.\nUniad: A unified ad hoc data processing system. ACM Trans-\nactions on Database Systems (TODS) , 42(1):1–42, 2016. 7,\n8, 9\n[47] Kiwhan Song, Boyuan Chen, Max Simchowitz, Yilun Du,\nRuss Tedrake, and Vincent Sitzmann. History-guided video\ndiffusion, 2025. 3\n[48] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. In ICLR , 2021. 2, 4\n[49] Wenwen Tong, Chonghao Sima, Tai Wang, Li Chen, Silei\nWu, Hanming Deng, Yi Gu, Lewei Lu, Ping Luo, Dahua Lin,\net al. Scene as occupancy. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 8406–\n8415, 2023. 8\n[50] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717 , 2018. 2, 7\n[51] Dani Valevski, Yaniv Leviathan, Moab Arar, and Shlomi\nFruchter. Diffusion models are real-time game engines,\n2024. 3, 5\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems , 30, 2017. 2\n[53] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description, 2022. 3\n[54] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye,\nYu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long\nvideo generation via temporal co-denoising. arXiv preprint\narXiv:2305.18264 , 2023. 3\n[55] Fu-Yun Wang, Zhaoyang Huang, Qiang Ma, Guanglu Song,\nXudong Lu, Weikang Bian, Yijin Li, Yu Liu, and Hongsheng\nLi. Zola: Zero-shot creative long animation generation with\nshort video model. In ECCV , 2024. 3\n[56] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, Jia-\ngang Zhu, and Jiwen Lu. Drivedreamer: Towards real-world-\ndriven world models for autonomous driving. arXiv preprint\narXiv:2309.09777 , 2023. 2, 3, 6\n11\n\n[57] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen,\nand Zhaoxiang Zhang. Driving into the Future: Multiview\nVisual Forecasting and Planning with World Model for Au-\ntonomous Driving. In CVPR , 2024. 2, 3, 6\n[58] Yuqing Wang, Tianwei Xiong, Daquan Zhou, Zhijie Lin,\nYang Zhao, Bingyi Kang, Jiashi Feng, and Xihui Liu. Loong:\nGenerating minute-level long videos with autoregressive lan-\nguage models, 2024. 3\n[59] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and\nMarco Pavone. Para-drive: Parallelized architecture for real-\ntime autonomous driving. In CVPR , 2024. 9\n[60] Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He,\nQian Zhang, Xiaoxiao Long, and Wei Yin. Goalflow: Goal-\ndriven flow matching for multimodal trajectories generation\nin end-to-end autonomous driving. ArXiv , abs/2503.05689,\n2025. 3\n[61] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu\nLi, Bo Dai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping\nLuo, Jun Zhang, Andreas Geiger, Yu Qiao, and Hongyang\nLi. Generalized Predictive Model for Autonomous Driving.\nInCVPR , 2024. 3, 6\n[62] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie Li,\nShuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan\nWang, Zicheng Liu, Houqiang Li, and Nan Duan. Nuwa-xl:\nDiffusion over diffusion for extremely long video generation,\n2023. 3\n[63] Chengran Yuan, Zhanqi Zhang, Jiawei Sun, Shuo Sun, Zefan\nHuang, Christina Dao Wen Lee, Dongen Li, Yuhang Han,\nAnthony Wong, Keng Peng Tee, and Marcelo H. Ang Jr\nau2. Drama: An efficient end-to-end motion planner for au-\ntonomous driving with mamba, 2024. 9\n[64] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui\nZhang, Yueqi Duan, and Jiwen Lu. Occworld: Learning a 3d\noccupancy world model for autonomous driving. In Euro-\npean conference on computer vision , pages 55–72. Springer,\n2024. 8\n[65] Wenzhao Zheng, Ruiqi Song, Xianda Guo, Chenming\nZhang, and Long Chen. Genad: Generative end-to-end au-\ntonomous driving. In European Conference on Computer\nVision , pages 87–104. Springer, 2024. 8\n[66] Wenzhao Zheng, Zetian Xia, Yuanhui Huang, Sicheng\nZuo, Jie Zhou, and Jiwen Lu. Doe-1: Closed-loop au-\ntonomous driving with large world model. arXiv preprint\narXiv:2412.09627 , 2024. 2, 3, 7, 8\n[67] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala,\nMichihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe\nMa, Luke S. Zettlemoyer, and Omer Levy. Transfusion: Pre-\ndict the next token and diffuse images with one multi-modal\nmodel. ArXiv , abs/2408.11039, 2024. 1\n12\n\nEpona : Autoregressive Diffusion World Model for Autonomous Driving\nSupplementary Material\nA. Detailed Architecture of Dual-Single-\nStream DiT\nWe are inspired by recent state-of-the-art image and video\ngeneration architectures [26, 31] and integrate dual-stream\nDiT blocks and single-stream DiT blocks to construct our\nTrajDiT and VisDiT. In the dual-stream DiT, condition in-\nformation and noise are processed separately and interact\nonly within the attention mechanism. In contrast, the single-\nstream DiT concatenates condition information and noise\nfrom the beginning for unified processing. Additionally, ac-\ntion control is mapped as an auxiliary control to obtain scale\nand shift parameters for adaptive modulation. The detailed\narchitecture is illustrated in Fig. 9.\nB. More Discussions with Related Works\nComparison with GAIA-1 [23], DrivingGPT [11], and\nADriver-I [27]. Compared to these multi-modal driving\nworld models, our method adopts a fundamentally different\narchitecture by directly integrating trajectory prediction into\nthe video generation process via diffusion models. To the\nbest of our knowledge, we are the firstdriving world model\nto use diffusion models for generating continuous, multi-\nstep action trajectories, which brings two key advantages:\n1.Multi-step vs. single-step prediction. Unlike prior ap-\nproaches that interleave single-step image and action\ngeneration using transformers, our model predicts an en-\ntireN-step future trajectory in one shot. This is par-\nticularly beneficial for real-time motion planning in au-\ntonomous driving.\n2.Continuous vs. discrete action representation. While ex-\nisting methods discretize continuous action spaces into\ntokens, our diffusion model generates high-resolution\ncontinuous trajectories directly, enabling more precise\nplanning and control.\nAmong these methods, only DrivingGPT reports NA V-\nISIM planning metrics. As shown in Tab. 8, our approach\nachieves significantly stronger results on this benchmark.\nDue to the absence of released code or full evaluation pro-\ntocols for GAIA-1 and ADriver-I, we additionally com-\npare against state-of-the-art end-to-end motion planners,\nwhere our model demonstrates competitive or superior per-\nformance.\nComparison with MagicDriveDiT [15] and Infinity-\nDrive [19]. MagicDriveDiT and InfinityDrive are concur-\nrent works focusing on video generation for autonomous\ndriving. Based on their reported FVD scores on nuScenesTable 8. Comparison of planning results with DrivingGPT on the\nNA VSIM test set.\nMethod NC↑DAC↑TTC↑Comf. ↑EP↑PDMS ↑\nDrivingGPT 98.9 90.7 94.9 95.6 79.7 82.4\nOurs 97.9 95.1 93.8 99.9 80.4 86.2\n(MagicDriveDiT: 94.84, InfinityDrive: 70.06), our method\n(82.83) exhibits competitive visual generation performance.\nWe acknowledge that MagicDriveDiT achieves slightly\nbetter visual quality, which we attribute to differences in\nvideo encoders: they utilize a specialized 3D-V AE, while\nwe adopt a deep-compression autoencoder for better com-\npression and training efficiency. This trade-off may intro-\nduce additional visual artifacts, and we plan to improve the\nDCAE component in future work.\nMore importantly, as elaborated in Sec. 3.2, these video\ndiffusion-based methods are designed for scene synthesis\nwithout modeling causal dynamics or agent interactions. As\na result, they lack support for flexible-length sequence gen-\neration and real-time planning, which are crucial in world\nmodel settings for decision making and policy learning.\nComparison with Transfusion [67] and JanusFlow [38].\nWhile these multimodal generative models also combine\ndiffusion and autoregression, their design principles differ\nsignificantly from ours. Transfusion and JanusFlow com-\nbine token-wise text autoregression and diffusion for image\nunderstanding and generation. In contrast, our model com-\nbine frame-wise latent autoregression and diffusion with\nnovel decoupled architecture design to tackle the unique\nproblem of temporal dynamics and coherence with video\ninputs and outputs, which is more challenging.\nC. More Long-term Video Generation Results\nAs shown in Fig. 10, we present the generation of minute-\nlong ultra-long videos while maintaining high-quality visu-\nals and preserving the integrity and details of surrounding\nbuildings and vehicles. Additionally, our world model con-\ntinuously generates the next frames with new contents with-\nout experiencing context drift.\n1\n\nCondition-stream last output\nNorm Norm\nScale & ShiftData-stream last output\nScale & Shift\nQK-Norm QK-Norm\nAttention (with 2D R oPE)\nGate Gate\n+ +\nNorm Norm\nScale & Shift Scale & Shift\nMLP MLP\nGate Gate\n+ +Dual-stream DiT block\nCondition-stream last output\nNorm\nScale & ShiftData-stream last output\nQK-Norm\nMLP\nAttention (with 2D R oPE)\n·Single-stream DiT block\n·\nScale & Shift\n+timestepMLPaction\ncontrol\nMLP\n+Figure 9. Detailed architecture of dual-stram DiT and single stream DiT blocks. We use nearly identical architecures for both TrajDiT\nand VisDiT, modified from text-image and video DiT architecture from [26, 31]. Action control is only for VisDiT.\n2\n\nCondition Frame T= 20s T= 40s T= 60s\nT= 80s T= 100s T= 120s T= 140s\nCondition Frame T= 20s T= 40s T= 60s\nT= 80s T= 100s T= 120s T= 140s\nCondition Frame T= 20s T= 40s T= 60s\nT= 80s T= 100s T= 120s T= 140sFigure 10. Visualization of Longer Videos. Our world model is capable of generating extended videos (140 seconds) while maintaining\nhigh visual quality and detailed vehicles and buildings.\n3\n\n",
    "source": "http://arxiv.org/abs/2506.24113v1",
    "authors": [
      "Kaiwen Zhang",
      "Zhenyu Tang",
      "Xiaotao Hu",
      "Xingang Pan",
      "Xiaoyang Guo",
      "Yuan Liu",
      "Jingwei Huang",
      "Li Yuan",
      "Qian Zhang",
      "Xiao-Xiao Long",
      "Xun Cao",
      "Wei Yin"
    ],
    "categories": [
      "cs.CV"
    ],
    "type": "content"
  },
  {
    "id": "2506.24112v1_abstract",
    "title": "Singular value transformation for unknown quantum channels",
    "content": "Title: Singular value transformation for unknown quantum channels\n\nAbstract: Given the ability to apply an unknown quantum channel acting on a\n$d$-dimensional system, we develop a quantum algorithm for transforming its\nsingular values. The spectrum of a quantum channel as a superoperator is\nnaturally tied to its Liouville representation, which is in general\nnon-Hermitian. Our key contribution is an approximate block-encoding scheme for\nthis representation in a Hermitized form, given only black-box access to the\nchannel; this immediately allows us to apply polynomial transformations to the\nchannel's singular values by quantum singular value transformation (QSVT). We\nthen demonstrate an $O(d^2/\\delta)$ upper bound and an $\\Omega(d/\\delta)$ lower\nbound for the query complexity of constructing a quantum channel that is\n$\\delta$-close in diamond norm to a block-encoding of the Hermitized Liouville\nrepresentation. We show our method applies practically to the problem of\nlearning the $q$-th singular value moments of unknown quantum channels for\narbitrary $q>2, q\\in \\mathbb{R}$, which has implications for testing if a\nquantum channel is entanglement breaking.",
    "source": "http://arxiv.org/abs/2506.24112v1",
    "authors": [
      "Ryotaro Niwa",
      "Zane Marius Rossi",
      "Philip Taranto",
      "Mio Murao"
    ],
    "categories": [
      "quant-ph"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24112v1_content",
    "title": "Singular value transformation for unknown quantum channels",
    "content": "arXiv:2506.24112v1  [quant-ph]  30 Jun 2025Singular value transformation for unknown quantum channels\nRyotaro Niwa,1Zane Marius Rossi,1Philip Taranto,2and Mio Murao1\n1Department of Physics, The University of Tokyo,\n7-3-1 Hongo, Bunkyo-ku, Tokyo 113-0033, Japan\n2Department of Physics & Astronomy, University of Manchester, Manchester M13 9PL, United Kingdom\n(Dated: July 1, 2025)\nGiven the ability to apply an unknown quantum channel acting on a d-dimensional system, we\ndevelop a quantum algorithm for transforming its singular values. The spectrum of a quantum\nchannel as a superoperator is naturally tied to its Liouville representation, which is in general non-\nHermitian. Our key contribution is an approximate block-encoding scheme for this representation in\na Hermitized form, given only black-box access to the channel; this immediately allows us to apply\npolynomial transformations to the channel’s singular values by quantum singular value transforma-\ntion (QSVT). We then demonstrate an O(d2/δ) upper bound and an Ω( d/δ) lower bound for the\nquery complexity of constructing a quantum channel that is δ-close in diamond norm to a block-\nencoding of the Hermitized Liouville representation. We show our method applies practically to the\nproblem of learning the q-th singular value moments of unknown quantum channels for arbitrary\nq >2, q∈R, which has implications for testing if a quantum channel is entanglement breaking.\nIntroduction.— To date, various quantum algorithms\nhave demonstrated that the laws of quantum mechanics\ncan be leveraged to efficiently process information. A uni-\nfying framework encompassing many important quantum\nalgorithms such as search, matrix inversion, and Hamilto-\nnian simulation, is the quantum singular value transfor-\nmation (QSVT) [1]. Generalizing the results of quantum\nsignal processing (QSP) [2], it allows one to directly ap-\nply polynomial functions to the singular values of linear\noperators encoded into sub-blocks of a larger unitary.\nWhile QSVT is a powerful framework for designing\nquantum algorithms, the cost of block-encoding a linear\noperator (Def. 1) can sometimes dominate the overall al-\ngorithmic cost. To efficiently construct block-encodings,\nmany works have restricted to specific cases, e.g., where\nthe input matrix is sparse [3] or has special structure [4].\nWhen the matrix of interest has physical meaning, such\nas a density matrix or a quantum channel, it is often\nassumed that one has access to a purification of the den-\nsity matrix [1] or the Kraus operators of the channel, re-\nspectively [5]. However, these access models may be too\nstrong from the viewpoint of physical implementability; a\npurification of a density matrix or the Kraus operators of\na channel cannot always be obtained given the quantum\nstates or channels themselves.\nIn this Letter, we consider the fundamental task of\ntransforming the singular values of quantum channels in\nthe most general and physical access model: one where\nthe quantum channel can only be applied as a black\nbox. As we aim to investigate the singular values of\nthe quantum channel—a superoperator acting on den-\nsity matrices—we first have to make sense of density\nmatrices as vectors. Under such vectorization , the ac-\ntion of a quantum channel is given by a matrix called\ntheLiouville representation [6], which is in general non-\nHermitian. This representation translates channel appli-\ncation into matrix multiplication, making it particularly\nsuited for discussing spectral properties of a channel, for\ninstance, the mixing time of Lindblad dynamics [7] dic-\nFIG. 1. (a) Approximate block-encoding scheme for the Li-\nouville representation EA. By applying an unknown channel\nEmultiple times we construct a quantum channel eUthat ap-\nproximates a unitary block-encoding UEAof the Hermitized\nLiouville representation. (b) Circuit for quantum singular\nvalue transformation (QSVT). Here, R(n)\nz=e−iϕnσzare sin-\ngle qubit rotations while grey boxes are projection-controlled-\nNOT operations locating EAas a sub-block of UEA.\ntated by its spectral gap. This unique feature is absent in\nother matrix representations of the channel such as the\nChoi representation [8, 9]. Although the non-Hermiticity\nof the Liouville representation seemingly complicates its\nphysical implementation, we show we can construct a\nquantum channel arbitrarily close to a unitary encod-\ning this matrix as a sub-block. We emphasize that our\nscheme is achieved under the black-box channel access\nmodel, avoiding assumptions such as having access to the\nchannel’s Kraus operators. With this block-encoded Li-\nouville representation at hand, we can freely apply stan-\ndard quantum algorithmic tools such as QSVT, which\nwill allow us to transform the singular values of block-\nencoded matrices without having to know their singular\nvalues orsingular vectors (see FIG. 1).\nAs a concrete application of our scheme, we consider\nthe task of learning the q-th singular value moments of\n\n2\nunknown quantum channels for arbitrary q >2, q∈R,\nbased on a polynomial approximation to f(x) =1\n2xq−2.\nThis extends previous works on entanglement detec-\ntion [10, 11], which discuss the special case of even integer\nq, and builds upon recent work for estimating the trace\nof quantum state powers Tr ρqforq∈R[12]. We also\nshow that our exponentiation scheme is useful for testing\nif a quantum channel is entanglement breaking based on\nthe reshuffling criterion of entanglement [13, 14].\nOur work not only extends recent work [15] discussing\nexponentiation of Hermiticity-preserving maps to the\nrealm of non-Hermiticity preserving maps, but also pro-\nvides a foundation for universally manipulating black-\nbox quantum channels with standard techniques such as\nQSVT, allowing us to directly evaluate functions of un-\nknown quantum channels without reducing to tomogra-\nphy and/or classical postprocessing.\nPreliminaries.— LetS(H),L(H) respectively denote\nthe set of density matrices and linear operators on the\nHilbert space H, and let C(H) denote the set of com-\npletely positive trace-preserving (CPTP) maps acting on\nL(H). A quantum channel E ∈C(H) has various rep-\nresentations; two particularly important ones relevant to\nour manuscript are the Liouville representation and the\nChoi representation [16], as we describe below.\n(A)Liouville representation: When ρ∈S(H) is rep-\nresented by its vectorized form |ρ⟩⟩ ∈ H ⊗ H as\nρ=X\nijρij|i⟩⟨j| → |ρ⟩⟩:=X\nijρij|i⟩ ⊗ |j⟩, (1)\nthe action of a quantum channel E ∈C(H) becomes the\nmatrix multiplication\n|E(ρ)⟩⟩=EA|ρ⟩⟩. (2)\nThe matrix EA∈L(H ⊗ H ) is called the Liouville repre-\nsentation [6] of the channel E, which is in general non-\nHermitian . This suggests that EAitself cannot be ob-\ntained as a quantum state. On the other hand, an im-\nportant consequence of Eq. (2) is that EAis naturally\nassociated with the spectrum of the channel Eas a su-\nperoperator. Indeed, if\nE(ρn) =λnρn(λn∈C) (3)\nfor some ρn∈L(H) (not necessarily ρn∈S(H)), then\nEA|ρn⟩⟩=|E(ρn)⟩⟩=λn|ρn⟩⟩. (4)\nThus the eigenvalues of the non-Hermitian matrix EAco-\nincide with the eigenvalues of the quantum channel (see\nAppendix A). Analogously, the singular values of the\nquantum channel Ecan be defined as the square root of\nthe eigenvalues of the map E†◦E. Since map composition\nbecomes matrix multiplication in the Liouville represen-\ntation due to (2), the singular values of the quantum\nchannel coincide with those of EA:\nEA=UΣV†,Σ = diag[ σ1, σ2,···, σd2], (5)where U, V∈L(H ⊗ H ) are unitary matrices. Note that\nthe eigenvalues and singular values of EAare not equiv-\nalent in general because of the non-Hermiticity of EA.\n(B) Choi representation: Alternatively, under the\nChoi–Jamio lkowski isomorphism [8, 9], a quantum chan-\nnelEcan be identified with its (normalized) Choi state\nEB∈L(H ⊗ H ) defined by\nEB:= (E ⊗I)|Φ+\nd⟩⟨Φ+\nd|, (6)\nwhere |Φ+\nd⟩:=1√\ndP\ni|i⟩ ⊗ |i⟩denotes the maximally en-\ntangled state (MES), and d= dim H[17]. It reflects\nsome foundational qualities of the map, for instance,\nany completely positive map corresponds to a positive\nsemidefinite EB, with trace preservation further imply-\ning an affine constraint. Therefore, for any E ∈C(H),\nthe Choi state EBcan be prepared as a quantum state,\ni.e,EB∈S(H ⊗ H ), in contradistinction to the Liouville\nrepresentation EA. On the other hand, map composi-\ntion under the Choi representation translates to the link\nproduct [18] rather than matrix multiplication, and the\nspectrum of EBis completely different from that of the\nquantum channel E. Thus, the Choi representation is\nnot necessarily suited for discussing the channel’s spec-\ntral properties, which are naturally encapsulated in the\nLiouville representation EA.\nIt is known that the Liouville representation EAand\nthe Choi state EBof a channel Eare related to each\nother via the reshuffling operation R:|i⟩⟨j| ⊗ |k⟩⟨l| →\n|i⟩⟨k| ⊗ |j⟩⟨l|(see, e.g., Ref. [19]):\ndR(EB) =EA. (7)\nSince Ris linear but not completely-positive, it cannot\nbe applied to the Choi state EBas a quantum operation\nto produce1\ndEA; this makes it challenging at first glance\nto construct a desired block-encoding of EA.\nBlock-encoding the Liouville representation.— To ad-\ndress the issue of non-Hermiticity, we construct an ap-\nproximate block-encoding for the Liouville representation\nin aHermitized form.\nDefinition 1. IfAis an n-qubit operator and Uis an\n(n+a) qubit unitary operator that satisfies\n∥A−α(⟨0|⊗a⊗I)U(|0⟩⊗a⊗I)∥ ≤ϵ, (8)\nwe say that Uis an ( α, a, ϵ )-block encoding ofA.\nTheorem 1. Given the ability to apply an unknown\nquantum channel Eacting on a d-dimensional system,\none can construct a quantum channel eUthat is δ-close\nin diamond norm to a (1,3,0)-block encoding unitary of\n2\nπHwithO(d2k\nδlog21\nδ)queries to E, where His the Her-\nmitized Liouville representation\nH:=1\n2d1−k\u0014OEA\nE†\nAO\u0015\n. (9)\nk∈[0,1\n2]ensures for arbitrary channels that ∥H∥∞≤1\n2.\nRestricted to unital channels, this condition relaxes to\nk∈[0,1].\n\n3\nProof. Consider the states ρ±∈S(HX⊗ H Y⊗ H Z) de-\npicted below, where HX=C2,HY=CdandHZ=Cd:\n|±⟩\nρ±\n|Φ+\nd⟩E (10)\nThe top line represents the control qubit system HX,\nwhile the second and third lines represent d-dimensional\nsystems HYandHZ, respectively. They are obtained by\napplying the controlled- SWAP operation to the Choi state\nEBwith the control qubit initialized as |±⟩:= (|0⟩ ±\n|1⟩)/√\n2. The states ρ±have the form\nρ±=1\n2|0⟩⟨0| ⊗(E ⊗I)|Φ+\nd⟩⟨Φ+\nd| ±1\n2d|0⟩⟨1| ⊗(EAFY Z)TZ\n±1\n2d|1⟩⟨0| ⊗(FY ZE†\nA)TZ+1\n2|1⟩⟨1| ⊗(I⊗ E)|Φ+\nd⟩⟨Φ+\nd|,\n(11)\nwhere FY Zdenotes the SWAP operator swapping HYand\nHZ, and ( ·)TZdenotes the partial transposition with re-\nspect to HZ.\nUtilizing a variant of density matrix exponentia-\ntion [15, 20], the unitary e−iρTZtfor a state ρ∈S(HX⊗\nHY⊗ H Z) can be approximately applied on a state\nσ∈S(H′\nX⊗ H′Y⊗ H′\nZ) by interspersing e−ieH∆twith\neH:=FX∪Y⊗d|Φ+\nd⟩⟨Φ+\nd|Zand tracing out the Hilbert\nspace of ρas\nTrρ[e−ieH∆t(ρ⊗σ)eieH∆t] =σ−i∆t[ρTZ, σ] +O(∆t2),\n=e−iρTZ∆tσeiρTZ∆t+O(∆t2).\n(12)\nHere, the SWAP operator FX∪Yswaps HX⊗HYofρ±and\nH′\nX⊗H′\nYofσ, while the unnormalized MES d|Φ+\nd⟩⟨Φ+\nd|Z\nacts on HZofρandH′\nZofσ. Choosing ρ=ρ±, we can\napproximate the unitary\nU′\nEA:=eiρTZ\n−∆te−iρTZ\n+∆t\n= exp\u0014\n−i\nd\u0012OEAFY Z\nFY ZE†\nA O\u0013\n∆t\u0015\n+O(∆t2).(13)\nFurther choosing ∆ t=2δ\ndkand applying the above pro-\ncedure O(d2k\n4δ) times, we have O(∆t2)×O(d2k\n4δ) =O(δ).\nThus, we arrive at a δ-close approximation of the unitary\neUEA:= exp\u0014\n−i\n2d1−k\u0012OEAFY Z\nFY ZE†\nA O\u0013\u0015\n. (14)\nBy conjugating eUEAwith controlled- SWAPs, we obtain\nUEA=\u0012\nI O\nOFY Z\u0013\nU′\nEA\u0012\nI O\nOFY Z\u0013\n= exp\u0014\n−i\n2d1−k\u0012OEA\nE†\nAO\u0013\u0015\n:=e−iH. (15)Here, Hsatisfies ∥H∥∞≤1\n2as long as k≤1\n2for gen-\neral channels and k≤1 for unital channels, respectively\n(see Appendix A, Lemma 3). Note that U†\nEAis also ob-\ntained by replacing eiρTZ\n−∆te−iρTZ\n+∆t→e−iρTZ\n−∆teiρTZ\n+∆t\nin Eq. (13). Furthermore, control- UEAand its inverse\ncan be similarly obtained by replacing ρ±with|1⟩⟨1| ⊗\nρ±[15, 21].\nNow, suppose U=e−iH, where ∥H∥∞≤1\n2. Given\naccess to control- Uand its inverse, we can obtain a block\nencoding of sin Hsince (c- Uindicating controlled- U)\nsinH= (⟨+| ⊗I)(c-U)(Y⊗I)(c-U†)(|+⟩ ⊗I).(16)\nForϵ′∈(0,1\n2], there exists an efficiently computable odd\npolynomial P(x)∈R[x] of degree O(log1\nϵ′) such that\n∥P(x)∥[−1,1]≤1 and ∥P(x)−2\nπarcsin x∥[−1\n2,1\n2]≤ϵ′(Ref.\n[1], Lemma 70). Thus, we can apply the quantum sin-\ngular value transformation (QSVT) [1] (see Appendix B)\nbased on the polynomial approximation of2\nπarcsin( x)\non the domain [ −1\n2,1\n2] to obtain a block-encoding of2\nπH\nwith O(log1\nϵ′) queries to U.\nPutting this all together, we can construct a quantum\nchannel eUthat is δ-close in diamond norm to a (1 ,2, ϵ′)-\nblock encoding of2\nπHusing O\u0010\nd2k\nδlog21\nϵ′\u0011\nqueries to the\nblack-box channel E(see FIG. 1 (a)). The substitution\nϵ′→O(δ), δ→δ/2 in the aforementioned statement fur-\nther implies that we have a quantum channel eUthat is\nδ-close in diamond norm to a (1 ,3,0)-block-encoding of\n2\nπH, using O\u0010\nd2k\nδlog21\nδ\u0011\nqueries to E.\nWe note that it is also possible to first implement the\nreshuffling operation at the state level in a Hermitized\nform, using an auxiliary MES. This allows us to estimate\nthe full singular value spectrum of unital quantum chan-\nnels. However, when similarly used for constructing an\napproximate block-encoding of the Hermitized Liouville\nrepresentation, it has a sub-optimal normalization factor\n(see Appendix C).\nLower bound for query complexity.— To evaluate the\nefficiency of our proposed protocol, we now prove a cor-\nresponding lower bound for the query complexity of con-\nstructing an approximate block-encoding for the chan-\nnel’s Hermitized Liouville representation, taking d= 2n.\nTheorem 2. Given the ability to apply an unknown\nquantum channel Eacting on a d= 2ndimensional sys-\ntem, a universal scheme that converts Einto a channel\neUthat is δ-close in diamond norm to the unitary channel\nU(t)(·) =UEA(t)(·)UEA(t)†\nUEA(t) = exp\u0012\n−i\u0014OEA\nE†\nAO\u0015\nt\u0013\n(17)\nrequires Ω(dt2\nδ)applications of the unknown channel E.\nOur proof strategy is to consider a specific channel dis-\ncrimination task (see Appendix D). Since Hamiltonian\n\n4\nsimulation algorithms via QSVT [22] have dimension-\nindependent query complexity that is linear in t, the\nabove theorem implies a Ω(dt2\nδ) query complexity lower\nbound for approximating a block-encoding unitary of the\nHermitized Liouville representation. We can compare\nthis result with Thm. 1, which implies an eO(d2t2\nδ) up-\nper bound for approximating U(t) up to precision δ.\nQSVT for unknown quantum channels.— With this\napproximate block-encoding of the Liouville representa-\ntion in Thm. 1 at hand, we can freely apply QSVT to\nimplement polynomial transformations to the singular\nvalues of unknown quantum channels (see FIG. 1 (b)).\nTheorem 3. LetPϵ′′(x)be an ϵ′′-approximating poly-\nnomial for the function f(x)on[−1,1], with degP=\nQ(ϵ′′). A quantum channel δ-close in diamond norm to\na(1,6,0)-block-encoding unitary of f(H), where\nH=1\nd1−k\u0014OEA\nE†\nAO\u0015\ncan be constructed using O\u0010\n(dkQ)2\nδlog2Q\nδ\u0011\nqueries to the\nunknown channel Ewith k∈[0,1\n2]for general channels.\nRestricted to unital channels, this condition relaxes to\nk∈[0,1].\nThis result is a direct consequence of Thm. 1 and the\nnotion of a samplizer [12], which establishes the cost in-\nherent in transforming a circuit querying unitaries block-\nencoding a density matrix ρinto a quantum circuit given\naccess to copies of ρitself (see Appendix B). Note that\nthe normalization factor inside the matrix function fvan-\nishes for unital channels by setting k= 1.\nLearning singular value moments.— As a concrete ex-\nample, we consider the problem of learning the q-thsin-\ngular value moment of an unknown quantum channel E:\nSq:=1\ndqd2X\ni=1σq\ni= Tr\u0002\nR(EB)R(EB)†\u0003q\n2. (18)\nThe first moment S1serves as a criterion for testing if\nthe channel is entanglement breaking. This is because\nthe reshuffling criterion of entanglement [13, 14] states\nthat\nρ∈SEP = ⇒ ∥R (ρ)∥1≤1, (19)\nwhere SEP is the set of separable quantum states, and\na quantum channel is entanglement breaking if and only\nif its Choi state is separable [23]. In general, Sqserves\nas a lower bound for S1in the regime q > 1. It was\npreviously considered in the context of entanglement de-\ntection for even integer q, based on approaches such as\nSWAP circuits [10], and classical shadow tomography [11].\nHere we extend the range to arbitrary real moments\nq >2, q∈R. Our overall scheme is to perform QSVT\non the block-encoded Hermitized Liouville representa-\ntion based on an ϵ′′-approximating even polynomial for1\n2xq−2, which uniformly converges in the range [ −1,1]\nand is of degree O(1/ϵ′′1\nq−2). Then, we apply the circuit\ndepicted below:\n|±⟩ H\n|0a⟩\nU|1⟩\nI\nd E\n|Φ+\nd⟩\nE\nHere, Uis the (approximate) unitary block-encoding of\nM≃f(H) obtained via QSVT, where His the (normal-\nized) Hermitized Liouville representation. Repeating this\ncircuit produces an estimate for1\nd2Trh\nE†\nAEA⟨1|M|1⟩i\n.\nThe overall query complexity for estimating Sqto ad-\nditive precision ϵwith probability larger than 1 −δis\n\n\neO\u0012log1\nδ\nd3\n2(q−2)ϵ3+2\nq−2\u0013\n(general channels)\neO\u0012log1\nδ\nd3(q−2)ϵ3+2\nq−2\u0013\n(unital channels)(20)\nwhere eOignores logarithmic factors in d, ϵ(see Ap-\npendix E for details). This shows exponential advantage\ncompared to performing SWAP circuits or classical shadow\ntomography on Choi states [10, 11] in the large dlimit\n(see Appendix F). Notice how our black-box channel ac-\ncess model allows us to utilize the time degree of freedom\nin the above circuit, which was absent in states.\nTo directly estimate S1, we can also approximate the\nDQC1 circuit [24] (see Appendix B) performed on the\nunitary UEA(t), leveraging Thm. 1. Combined with sam-\npling methods based on the Fourier-cosine expansion of\nthe absolute function [15] (see Appendix G), we can ob-\ntain an ϵ-close estimate for the quantity ∥R(EB)∥1with\nprobability over Psuc.≥1−δusing\neO\u0012d4∥R(EB)∥1log1\nδ\nϵ3\u0013\n(21)\nqueries to the black-box channel. In the worst case, this\nscaling becomes eO\u0010d5log1\nδ\nϵ3\u0011\n.\nLet us compare the above scheme with Choi state to-\nmography. Since the reshuffling operation can increase\nthe 1-norm distance ||ρ1−ρ2||1by a factor of d, i.e;\n∥R(ρ1)− R(ρ2)∥1≤d∥ρ1−ρ2∥1, estimating the 1st\nmoment ∥R(EB)∥1up to precision ϵvia full-tomography\nof Choi states can require O\u0010d6log1\nδ\nϵ2\u0011\nsamples of EBin\nthe worst case (see Appendix H). Thus, our approach in\nEq. (21) shows improved d-dependence. Our scheme also\nhas the advantage that ∥R(EB)∥1is directly measured.\n\n5\nThis can be contrasted with the tomographic approach,\nwhere the learner is required to store the measurement\ndata ˆ ρin an exponentially large classical memory, re-\nalign the indices, and then perform diagonalization with\na classical computer.\nConclusion.— We have developed a block-encoding\nscheme for the Liouville representation of a quantum\nchannel under the most general and physical assumption:\nthe black-box channel access model. Our scheme achieves\naδ-approximate block-encoding of the Hermitized Liou-\nville representation with O(d2/δ) queries. We have also\nprovided a corresponding Ω( d/δ) lower bound based on\nchannel discrimination arguments. As a concrete exam-\nple of our method, we considered the problem of calcu-\nlating arbitrary singular value moments for q >2, q∈R\nby choosing an appropriate polynomial approximation of\nthe function f(x) =1\n2xq−2. This shows an advantage in\nquery complexity compared to existing methods that can\nbe performed on Choi states. We finally showed that our\nmethod can be applied to directly estimate the first mo-\nment||R(EB)||1, which can be used to test if a channel\nis entanglement-breaking.\nAlthough we expect in general that our protocol for\ntransforming singular values of quantum channels re-\nquires a dimension-dependent query complexity, our\nscheme should be advantageous compared to obtaining\na classical description of the channel via tomography.\nThis is because the block-encoding unitary can be effi-\nciently processed fully on a quantum computer with stan-dard tools such as QSVT, allowing one to transform the\nsingular values of arbitrary black-box channels and effi-\nciently measure useful quantities without requiring expo-\nnentially large classical memory and computation cost.\nThere are many directions for future research. First,\nwe note that the upper and lower bounds proven in this\nLetter do not match—improving these bounds is of fun-\ndamental interest. From a more practical viewpoint, in-\nvestigating the role of locality or structure of the black-\nbox channels could permit more efficient protocols for\nreasonable and common experimental settings. We also\nanticipate that the recently proposed framework of quan-\ntum eigenvalue transformation [25] could be combined\nwith the block-encoding of the Liouville representation to\nefficiently transform the eigenvalues of quantum channels\nwith complex spectra. By providing a block-encoding cir-\ncuit for the Liouville representation, our work opens new\navenues towards expressively manipulating the spectra of\nblack-box quantum channels with established techniques\nsuch as QSVT.\nAcknowledgments.— We are grateful to Pawel Woc-\njan, Anirban Chowdhury, and Toshinari Itoko for fruit-\nful discussions and comments. This work was supported\nby MEXT Quantum Leap Flagship Program (MEXT\nQLEAP) JPMXS0118069605 and JPMXS0120351339,\nJapan Society for the Promotion of Science (JSPS) KAK-\nENHI Grants No. 23K21643, and IBM Quantum. ZMR\nacknowledges funding from the Japan Society for the Pro-\nmotion of Science (JSPS) Postdoctoral Fellowship for Re-\nsearch in Japan (KAKENHI 24KF0136).\n[1] A. Gily´ en, Y. Su, G. H. Low, and N. Wiebe, Quan-\ntum singular value transformation and beyond: expo-\nnential improvements for quantum matrix arithmetics, in\nProc. 51st Annu. ACM SIGACT Symp. Theory Comput.\n(2019) p. 193, arXiv:1806.01838 [quant-ph].\n[2] G. H. Low, T. J. Yoder, and I. L. Chuang, Methodology of\nResonant Equiangular Composite Quantum Gates, Phys.\nRev. X 6, 041067 (2016), arXiv:1603.03996 [quant-ph].\n[3] D. Camps, L. Lin, R. V. Beeumen, and C. Yang, Explicit\nQuantum Circuits for Block Encodings of Certain Sparse\nMatrices (2023), arXiv:2203.10236 [quant-ph].\n[4] C. S¨ underhauf, E. Campbell, and J. Camps, Block-\nencoding structured matrices for data input in quantum\ncomputing, Quantum 8, 1226 (2024), arXiv:2302.10949\n[quant-ph].\n[5] C.-F. Chen, M. J. Kastoryano, F. G. S. L. Brand˜ ao, and\nA. Gily´ en, Quantum Thermal State Preparation (2023),\narXiv:2303.18224 [quant-ph].\n[6] J. J. Wallman and S. T. Flammia, Randomized bench-\nmarking with confidence, New J. Phys. 16, 103032\n(2014), arXiv:2303.182241404.6025 [quant-ph].\n[7] G. Lindblad, On the generators of quantum dynamical\nsemigroups, Commun. Math. Phys. 48, 119 (1976).\n[8] A. Jamio lkowski, Linear transformations which preserve\ntrace and positive semidefiniteness of operators, Rep.\nMath. Phys. 3, 275 (1972).\n[9] M.-D. Choi, Completely positive linear maps on complexmatrices, Linear Algebra Appl. 10, 285 (1975).\n[10] J. Cai and W. Song, Novel Schemes for Directly Mea-\nsuring Entanglement of General States, Phys. Rev. Lett.\n101, 190503 (2008), arXiv:0804.2246 [quant-ph].\n[11] Z. Liu, Y. Tang, H. Dai, P. Liu, S. Chen, and X. Ma, De-\ntecting Entanglement in Quantum Many-Body Systems\nvia Permutation Moments, Phys. Rev. Lett. 129, 260501\n(2022).\n[12] Y. Liu and Q. Wang, On Estimating the Trace\nof Quantum State Powers, in Proc. 2025 Annu.\nACM-SIAM Symp. Discrete Algorithms (2025) p. 947,\narXiv:2410.13559 [quant-ph].\n[13] O. Rudolph, Further Results on the Cross Norm Criterion\nfor Separability, Quantum Inf. Process. 4, 219 (2005),\narXiv:quant-ph/0202121 [quant-ph].\n[14] K. Chen and L.-A. Wu, A matrix realignment method\nfor recognizing entanglement, Quantum Inf. Comput. 3,\n193 (2003), arXiv:quant-ph/0205017 [quant-ph].\n[15] F. Wei, Z. Liu, G. Liu, Z. Han, D.-L. Deng, and Z. Liu,\nSimulating non-completely positive actions via exponen-\ntiation of Hermitian-preserving maps, npj Quantum Inf.\n10, 134 (2024), arXiv:2308.07956 [quant-ph].\n[16] The Liouville representation EAand the Choi state EBare\nsometimes called the A-form and the B-form matrices,\nrespectively (as they were originally derived in Ref. [26]).\n[17] We note that the (unnormalized) Choi matrix dEBis also\noften considered in the literature, which we distinguish\n\n6\nin this manuscript from the Choi state EB.\n[18] G. Chiribella, G. M. D’Ariano, and P. Perinotti, Theo-\nretical framework for quantum networks, Phys. Rev. A\n80, 022339 (2009), arXiv:0904.4483 [quant-ph].\n[19] S. Milz, F. A. Pollock, and K. Modi, An Introduction to\nOperational Quantum Dynamics, Open Syst. Inf. Dyn.\n24, 1740016 (2017), arXiv:1708.00769 [quant-ph].\n[20] S. Lloyd, M. Mohseni, and P. Rebentrost, Quantum prin-\ncipal component analysis, Nat. Phys. 10, 631 (2014),\narXiv:1307.0401 [quant-ph].\n[21] S. Kimmel, C. Y.-Y. Lin, G. H. Low, M. Ozols, and T. J.\nYoder, Hamiltonian simulation with optimal sample com-\nplexity, npj Quantum Inf. 3, 1 (2017), arXiv:1608.00281\n[quant-ph].\n[22] G. H. Low and I. L. Chuang, Hamiltonian Simulation by\nQubitization, Quantum 3, 163 (2019), arXiv:1610.06546\n[quant-ph].\n[23] M. Horodecki, P. W. Shor, and M. B. Ruskai, Entan-\nglement Breaking Channels, Rev. Math. Phys. 15, 629\n(2003), arXiv:quant-ph/0302031 [quant-ph].\n[24] E. Knill and R. Laflamme, Power of One Bit of Quan-\ntum Information, Phys. Rev. Lett. 81, 5672 (1998),\narXiv:quant-ph/9802037 [quant-ph].\n[25] G. H. Low and Y. Su, Quantum Eigenvalue Processing,\nin2024 IEEE 65th Annu. Symp. Found. Comput. Sci.\n(2024) p. 1051, arXiv:2401.06240 [quant-ph].\n[26] E. C. G. Sudarshan, P. M. Mathews, and J. Rau, Stochas-\ntic Dynamics of Quantum-Mechanical Systems, Phys.\nRev.121, 920 (1961).\n[27] J. Watrous, The Theory of Quantum Information (Cam-\nbridge Univ. Press, Cambridge, 2018).[28] J. M. Martyn, Z. M. Rossi, A. K. Tan, and I. L. Chuang,\nGrand Unification of Quantum Algorithms, PRX Quan-\ntum2, 040203 (2021), arXiv:2105.02859 [quant-ph].\n[29] G. H. Low and I. L. Chuang, Optimal Hamiltonian Sim-\nulation by Quantum Signal Processing, Phys. Rev. Lett.\n118, 010501 (2017), arXiv:1606.02685 [quant-ph].\n[30] E. Tang and K. Tian, A CS guide to the quantum singular\nvalue transformation (2023), arXiv:2302.14324 [quant-\nph].\n[31] In truth we have to know milder things, like a bound on\nthe range of possible singular values, and the span of the\nleft and right singular vectors respectively, but these are\noften trivially known.\n[32] M. Keyl and R. F. Werner, Estimating the spectrum\nof a density operator, Phys. Rev. A 64, 052311 (2001),\narXiv:quant-ph/0102027 [quant-ph].\n[33] R. O’Donnell and J. Wright, Efficient quantum tomogra-\nphy II (2016), arXiv:1612.00034 [quant-ph].\n[34] G. Chiribella, G. M. D’Ariano, and P. Perinotti, Trans-\nforming quantum operations: Quantum supermaps, EPL\n(Europhys. Lett.) 83, 30004 (2008), arXiv:0804.0180\n[quant-ph].\n[35] G. Gour, Comparison of Quantum Channels by Super-\nchannels, IEEE Trans. Inf. Theory 65, 5880 (2019),\narXiv:1808.02607 [quant-ph].\n[36] S. Pirandola and C. Lupo, Ultimate precision of adaptive\nnoise estimation, Phys. Rev. Lett. 118, 100502 (2017),\narXiv:1609.02160 [quant-ph].\n[37]Udconsists of Heisenberg-Weyl operators, which is equiv-\nalent to Pauli operators for nqubit systems.\n\n7\nSUPPLEMENTAL MATERIAL\nDIAGRAM NOTATIONS\nHere we introduce diagram notations that facilitate calculations. We assign an index to each bra and ket degrees\nof freedom, and represent the Kronecker delta δabas a line connecting the indices aandb. This leads to us to denote\nthe MES |Φ+\nd⟩⟨Φ+\nd|and the SWAP operator FABin the following way;\n\n\n|Φ+\nd⟩⟨Φ+\nd|=1\ndX\ni,j|i⟩⟨j| ⊗ |i⟩⟨j|=1\nd\nFAB=X\ni,j|i⟩⟨j| ⊗ |j⟩⟨i|=\n .(22)\nIt is then easier to see relations such as\nd(|Φ+\nd⟩⟨Φ+\nd|)TA=FAB, (23)\nwhere ( ·)TAdenotes the partial transposition over system A. We also denote the Kraus operators as below;\n(24)\nThe transposition of the Kraus operator is represented with the right pointing triangle, and the complex conjugate is\nrepresented with the black color. We always omit summations such asP\nm,i,j in the diagrammatic equation. When\nthe channels are applied multiple times, we assign a number to the Kraus operators to distinguish KmandKm′.\nWhen there is no confusion, we omit the numbers assigned to the Kraus operators.\nUsing the above notations, it is now easy to see that the Liouville representation (also called the A-form Choi\nmatrix) and the Choi state (also called the B-form Choi matrix) of the channel are related to each other via the\nreshuffling operation.\n\n\nEB=1\ndX\nm,i,jKm|i⟩⟨j|K†\nm⊗ |i⟩⟨j|=\nEA=X\nmKm⊗K∗\nm=dR(EB) =\n(25)\nOur key observation in (11) can be easily seen from the diagram notation;\nρ±=1\nd\n\n±\n±\n\n=1\nd\n\n±\u0012\n \u0013TZ\n±\u0012\n \u0013TZ\n\n. (26)\n\n8\nAppendix A: Basic properties of a quantum channel\nHere we briefly review some basic properties of a quantum channel. As mentioned in the main text, the eigenvalue\nof a quantum channel E:L(H)→L(H) is defined as\nE(ρR\nn) =λnρR\nn,(ρR\nn∈L(H)) (A1)\nwhere λn∈C. This also implies\nE†(ρL\nn) =λ∗\nnρL\nn,(ρL\nn∈L(H)), (A2)\nwhere ρR\nn, ρL\nnare sometimes referred to as the right-eigenvector and left-eigenvector, respectively. They satisfy\nTr\u0002\n(ρL\nm)†ρR\nn\u0003\n= 0 ( m̸=n), (A3)\nwhich allows one to decompose the action of the channel as\nE(ρ) =X\nncnλnρR\nn. (A4)\nThis is essentially a spectrum problem of a non-Hermitian matrix , and some notable differences arise from the\nHermitian case. For example, in the Hermitian matrix case, eigenvectors corresponding to different eigenvalues are\nalways orthogonal to each other, but this is not the case for non-Hermitian matrices. Below are some basic properties\nof the eigenvalues of quantum channels.\nLemma 1. The eigenvalue and eigenoperator of quantum channels have the following properties;\n1.Ehas eigenvalue 1.\n2. If λnis an eigenvalue, λ∗\nnis also an eigenvalue. Also, right eigenoperator is traceless except for the fixed point.\n3.|λn| ≤1.\nProof. 1; The following proof is given in [27]. Define a CPTP map Φ as\nΦn(ρ) =1\n2n2n−1X\nk=0E(k)(ρ). (A5)\nWe define the following set Dn≡ {Φn(ρ)|ρ∈ L(H)}. It holds that\nΦn+1(ρ) =1\n2n+12n+1−1X\nk=0E(k)(ρ) =1\n2Φn(ρ) +1\n2Φn(E(n)(ρ)) = Φ n\u00121\n2ρ+1\n2E(n)(ρ)\u0013\nand thus Dn+1⊆Dn, so there exist ρ0∈D0TD1T···. Then, for arbitrary positive integer n,ρ0= Φ n(σ) holds for\nsome σ∈ L(H). It follows that\nE(ρ0)−ρ0=E(Φn(σ))−Φn(σ) =E(2n)(σ)−σ\n2n.\nThus,\n∥E(ρ0)−ρ0∥1≤1\n2n−1(A6)\nfor arbitrary n, meaning that E(ρ0) =ρ0.\nProof. 2;Eis Hermiticity preserving, so\nE((ρR\nn)†) = (E(ρR\nn))†=λ∗\nn(ρR\nn)†(A7)\nThe right eigenoperator is traceless Tr ρR\nn= 0 except for the fixed point, since Tr E(ρ) = Tr ρ.\n\n9\nProof. 3; Trace norm is monotonically decreasing under trace-preserving positive maps (see [27], 3.40)\n|λn|=∥λnρR\nn∥1\n∥ρRn∥1=∥E(ρR\nn)∥1\n∥ρRn∥1≤1. (A8)\nNext, we prove some lemmas on the norm of quantum channels. The Shatten p-norm of the square matrix Ais\n∥A∥p:= X\niσp\ni!1\np\n, (A9)\nwhere σirepresents the singular values of A. It satisfies the monotonicity\n∥A∥∞≤ ··· ≤ ∥ A∥2≤ ∥A∥1. (A10)\n|| · || 1is called the trace norm and|| · ||∞is called the spectral norm . The H¨ older’s inequality\n∥AB∥1≤ ∥A∥p∥B∥q (A11)\nholds for p, q∈[1,∞] satisfying1\np+1\nq= 1.\nLemma 2. The second singular value moment of a quantum channel is equivalent to the purity of its Choi state;\nS2= Tr(EB)2(A12)\nProof.\nS2:=1\nd2X\niσ2\ni= Trh\nR(EB)†R(EB)i\n=1\nd2\n=1\nd2\n= Tr (EB)2. (A13)\nLemma 3. Consider a quantum channel Eacting on a d-dimensional system. The operator norm of its Liouville\nrepresentation satisfies [6]\n1≤ ∥E A∥∞≤√\nd. (A14)\nThe lower bound is saturated by unital channels, while the upper bound is saturated by trace and replace channels such\nasE(ρ) = Tr( ρ)|0⟩⟨0|.\nProof. As mentioned in Lemma 1, every quantum channel has a fixed point. The spectral radius r= max {|λi|}for\nany square matrix Agives a lower bound on the spectral norm. For quantum channels, the spectral radius is 1, which\nmeans\n1≤ ∥E A∥∞= max\niσi. (A15)\n\n10\nThe equality holds if and only if Eis unital (see Theorem 4.27 of [27]) Next, consider the Frobenius norm\n∥A∥F=sX\nij|aij|2. (A16)\nFor a vectorized density matrix\n|ρ⟩⟩=X\nijρij|i⟩ ⊗ |j⟩, (A17)\nThis corresponds to the usual inner product of vectors. Thus, it can be shown that\n∥EA∥∞= sup\nρ̸=0∥|E(ρ)⟩⟩∥2\n∥|ρ⟩⟩∥2= sup\nρ̸=0∥E(ρ)∥F\n∥ρ∥F= sup\nρ̸=0s\nTrE(ρ)2\nTrρ2≤√\nd, (A18)\nwhich follows from the fact that1\nd≤Trρ2≤1. Therefore, we conclude that\n1≤max\niσi=∥EA∥∞≤√\nd. (A19)\nThe upper bound is saturated if ρ=I/d, andE(ρ) is a pure state. This is achieved, for instance, by the trace and\nreplace channel E(ρ) = Tr( ρ)|0⟩⟨0|.\nLemma 4. Consider a quantum channel Eacting on a d-dimensional system. The trace norm of its Liouville\nrepresentation satisfies\n∥EA∥1≤d2. (A20)\nProof. From Lemma 2, we have\n∥EA∥2=vuutd2X\ni=1σ2\ni=q\nd2Tr (EB)2≤d\nFrom the Cauchy-Schwartz inequality ||EA||1≤d||EA||2, we obtain\n∥EA∥1≤d2. (A21)\nThe upper bound is saturated by unitary channels.\nFinally, we also bound the operator norm of arbitrary reshuffled states.\nLemma 5. LetRdenote the reshuffling operation. For a bipartite density matrix ρin aD=d2dimensional system,\n∥R(ρ)∥∞≤1. (A22)\nProof.\n∥R(ρ)∥∞≤ ∥R (ρ)∥2=p\nTrρ2≤1. (A23)\nThe upper bound ∥R(ρ)∥∞= 1 is saturated by pure product states.\n\n11\nAppendix B: Review of some basic quantum algorithm tools\nHere we review some basic algorithmic tools used in the main text.\n1. The Hadamard test\nThe Hadamard test calculates Tr( ρU) by repeatedly running the following circuit.\n|0⟩ H H\nρ U\nMore specifically, the binary measurement outcome obeys the probability distribution\np(0) =1 + Re[Tr( ρU)]\n2\np(1) =1−Re[Tr( ρU)]\n2(B1)\nso if we record +1 when the measurement outcome is 0, and −1 when the measurement outcome is −1, taking the\naverage produces an estimate ˆXfor Re[Tr( ρU)];E[ˆX] = Tr( ρU). When the input ρis the maximally mixed state, it\nis also called the DQC1 [24] circuit. Repeating the circuit Ntimes, we know from the Chernoff bound that\nPr(|ˆX−E[ˆX]| ≤ϵ)≤1−2exp(−2Nϵ2). (B2)\nThus, O(log(1\nδ)\nϵ2) repetition of the circuit allows one to estimate Tr( ρU) up to ϵ-precision with probability P≥1−δ.\nMoreover, if Uis an n+aqubit unitary that block-encodes an n-qubit operator A, Tr( ρA) can be analogously\nestimated up to a fixed precision with O(log(1\nδ)\nϵ2) repetitions.\n2. Quantum singular value transformation (QSVT)\nThe quantum singular value transformation (QSVT) is a framework that encompasses many well-known algorithms\nsuch as amplitude amplification, matrix inversion, and Hamiltonian simulation [1, 28]. Specifically, one can show\nthat the core results of quantum signal processing (QSP) [2, 22, 29], a statement about products of parameterized\nSU(2) elements, can be ‘lifted’ to describe permissible manipulation of linear operators encoded into sub-blocks of\na larger unitary. While the details of this argument are somewhat precise, relying on properties of the cosine-sine\ndecomposition exposited quite cleanly in [30], we quote important definitions and results here for the unfamiliar\nreader. To start, we reproduce the commonly cited definition of a block-encoding given in the main text.\nDefinition 2. ((α, a, ϵ ) block encoding). If Ais an n-qubit operator and Uis an ( n+a) qubit unitary operator that\nsatisfies\n∥A−α(⟨0|⊗a⊗I)U(|0⟩⊗a⊗I)∥ ≤ϵ, (B3)\nthen we say that Uis an ( α, a, ϵ )-block encoding ofA. In other words, the (2 ×2)-block unitary Ucontains an\nα-subnormalized copy of an ε-approximation to Ain its top-left block (indexed by |0⟩⊗afor both row and column).\nGiven the definition above, we can define precisely what we mean by transforming the singular values of A, after\nwhich we can provide a useful workhorse theorem from QSVT, which will be all that is required in our application.\nSpecifically, given the simplest case of a (1 ,1,0) block-encoding of some linear operator A, QSVT performs the\ntransformation\n\u0014\nA∗\n∗ ∗\u0015\n7−→\u0014\nP(SV)(A)∗\n∗ ∗\u0015\n, (B4)\n\n12\nwhere the blocks here have been implicitly labeled by a single auxiliary qubit state, or more specifically\nA=X\nξξk|ℓk⟩⟨rk| 7→P(SV)(A) :=X\nξP(ξk)|ℓk⟩⟨rk|, (B5)\nwith ξk,|ℓk⟩,|rk⟩thek-th singular value, left singular vector, and right singular vector of A, respectively. The key\ninsight and surprise of QSVT will be that we can induce this transformation on the singular values without having\nto know the singular values orsingular vectors [31], meaning these manipulations can, in many common cases, be\nextremely efficient. Moreover, while the referenced theorem below makes mention only of query complexity to the\nblock encoding, the additional required gates are simple and their number scales linearly with the query complexity.\nThe following theorem, which describes permissable transformations of the singular values of block encoded (sub-\nnormalized, approximate, Hermitian) linear operators, is necessary and sufficient for our applications. Note that\nextensive results exist on the construction and manipulation of block encodings, some of which may be useful in\nextensions to this work; the interested reader is directed to common pedagogical texts [1, 28, 30].\nTheorem 4. (QSVT for Hermitian matrices [1]) Suppose Uis an (α, a, ϵ )-block encoding of a Hermitian matrix A,\nandP(x)∈R[x]is a polynomial that satisfies |P(x)| ≤1\n2for all x∈[−1,1]. Then, we can implement a unitary eU\nthat is a (1, a+ 2,4(deg P)pϵ\nα+δ)-block-encoding of P(A/α), using O(degP)queries to U. The description of the\ncircuit can be computed with a classial computer in time O(poly(deg P,log1\nδ)). Note also that if the parity of P(x)is\ndefinite, then the same statement holds for |P(x)| ≤1onx∈[−1,1].\nTo calculate spectral moments, we need a suitable polynomial approximation of the power function xq. The most\ncommon choice for the approximating polynomial is the degree dtruncated Chebyshev polynomial\n˜Pd(x)≡c0\n2+dX\nk=1ckTk(x), (B6)\nwhich can exhibit poor approximation around x= 0. For our setting, a more well-behaved approximation (i.e., one\nwhich uniformly converges across all x∈[−1,1] at a near optimal rate) will be obtained by averaging over dsuch\npolynomials\nPd′(x) =1\nd2d−1X\nk=d˜Pd(x). (B7)\nThis modified polynomial was employed in [12] to obtain an estimate for the Tsallis entropy of states to avoid having\nto make assumptions on the spectrum of the density matrix (e.g., bounds on its condition number). In particular, for\na fixed positive integer rand a real number in ( −1,1), there is a degree d=⌈(β/ϵ)1\nr+α⌉polynomial for any ϵ∈(0,1\n2]\nsuch that\nmax\nx∈[−1,1]\f\f\f\f1\n2xr−1|x|1+α−Pd(x)\f\f\f\f≤ϵ, max\nx∈[−1,1]∥Pd(x)∥ ≤1. (B8)\nThe same reference also notes that a polynomial of degree O(1/ϵ′′1\nq−1) is generally required to obtain the desired\nϵ′′-uniform approximation to1\n2xq−1overx∈[−1,1].\n3. Samplizer\nHere we review the notion of the samplizer, which generalizes the density matrix exponentiation trick [20], originally\nintroduced in [12]. This method converts a block-encoding unitary access model into a quantum state access model.\nTheorem 5. (Samplizer) Suppose C={C[U]}is a quantum circuit family with access to (1, m,0)-block encoding of\nρ/2with query complexity Q. If m≥4, then, for every δ >0, there is a quantum channel family Samplizeδ⟨C⟩with\nsample access to ρwith sample complexity O\u0010\nQ2\nδlog2\u0010\nQ\nδ\u0011\u0011\nsatisfying for every ρ, there is a specific unitary operator\nUρthat is a (2, m,0)-block-encoding of ρsuch that\n∥Samplizeδ⟨C⟩(ρ)−C[Uρ]∥⋄≤δ (B9)\n\n13\nProof. Suppose the quantum circuit has the following form\nC[U] =GQUQ···G2U2G1U1G0 (B10)\nwhere Giconsists of one and two qubit gates, and Uis are either unitary, controlled unitary or their inverse. Utilizing\nthe state exponentiation protocol in [20], one can show that for ϵ=δ/Qwe can implement the channel E,Einvup to\nprecision ϵ\n∥Eρ− Uρ∥⋄≤ϵ,∥Einv\nρ− Uinv\nρ∥⋄≤ϵ (B11)\nwhere U(σ) =UρσU†\nρ,Uinv(σ) =U†\nρσUρ, using O(1\nϵlog2\u00001\nϵ\u0001\n) samples. By appropriately enlarging the Hilbert space,\none can show that\n∥C′[ρ]−C[Uρ⊗I⊗(m−4)]∥⋄≤Qϵ=δ (B12)\nsince\n∥E1+E2∥⋄≤ ∥E 1∥⋄+∥E2∥⋄,∥E2E1∥⋄≤ ∥E 2∥⋄∥E1∥⋄ (B13)\nAppendix C: Block-encoding via reshuffling circuit\nHere we discuss a different scheme for approximately block-encoding the Hermitized Liouville representation. Our\noverall scheme proceeds by first constructing a SWAP circuit implementing the reshuffling operation in a Hermitized\nform, followed by density matrix exponentiation. We then see how the reshuffling circuit is useful for directly estimating\nthe full-singular value spectrum of unital quantum channels.\nTheorem 6. Given a D=d×d-dimensional bipartite state ρ∈CD×D, we can construct a quantum channel eUthat\nisδ-close in diamond norm to a (1,3,0)-block encoding unitary of2\nπH, where\nH:=1\n2d1−k\u0014OR(ρ)\nR(ρ)†O\u0015\n(C1)\nusingeO(d2k\nδlog21\nδ)copies of ρ. In particular, when ρ=EB, namely, when ρis the Choi state of the unknown channel\nE,His the Hermitized Liouville representation. k∈[0,1],k∈[0,3\n2], and k∈[0,2]ensures ∥H∥∞≤1\n2for arbitrary\nstates, Choi states of general channels, and Choi states of unital channels, respectively.\nProof. A key observation is that the reshuffling operation Rcan be obtained via the relation\n1\ndR(ρ) = Tr 23\u0002\u0000\nρ⊗ |Φ+\nd⟩⟨Φ+\nd|\u0001\nF23F14F34\u0003\n. (C2)\nHere,Fijrepresents the SWAP operator between system iandj. The corresponding circuit is\n|±⟩\nρ±ρ\n\f\fΦ+\nd\u000b\nwhere the white caps represent the trace and the subsystem labels read from top to bottom, following the control\nqubit Cwhich labels the uppermost system. This circuit prepares a state ˜ ρ±∈S(C2⊗Cd⊗Cd) given by\n˜ρ±=|0⟩⟨0| ⊗1\n2dTr2[ρ]⊗I± |0⟩⟨1| ⊗1\n2dR(ρ)\n± |1⟩⟨0| ⊗1\n2dR(ρ)†+|1⟩⟨1| ⊗1\n2dI⊗Tr1[ρ]. (C3)\n\n14\nNext, we use density matrix exponentiation [20] to approximate the unitary e−iρ±tfrom many copies of ρ±as\nTr1[e−iF∆t(˜ρ±⊗σ)eiF∆t] = (cos2∆t)σ+ (sin2∆t)˜ρ±−isin ∆ tcos ∆ t[˜ρ±, σ]\n=σ−i∆t[˜ρ±, σ] +O(∆t2)\n=e−i˜ρ±∆tσei˜ρ±∆t+O(∆t2). (C4)\nThis allows us to obtain an approximation of the unitary\neUR(ρ):=ei˜ρ−∆te−i˜ρ+∆t\n= exp\u0012\n−i\nd\u0014OR(ρ)\nR(ρ)†O\u0015\n∆t\u0013\n+O(∆t2). (C5)\nChoosing ∆ t=2δ\ndkand applying the above procedure O(d2k\n4δ) times, we arrive at a O(∆t2)×O(d2k\n4δ) = δ-close\napproximation of the unitary\nUR(ρ)= exp\u0012\n−i\n2d1−k\u0014OR(ρ)\nR(ρ)†O\u0015\u0013\n=e−iH. (C6)\nIn particular, when ρis the Choi state EB,R(EB) =1\ndEA, soHis the Hermitized Liouville representation of the\nunknown channel E. Note that control- UR(ρ)can be similarly obtained by replacing ˜ ρ±with|1⟩⟨1| ⊗˜ρ±[21]. Now,\nHsatisfies ∥H∥∞≤1\n2as long as k≤1 (see Appendix A, Lemma 5). When restricted to Choi states, k≤3\n2and\nk≤2 ensures ∥H∥∞≤1\n2for general channels and unital channels, respectively (see Appendix A, Lemma 3). Then,\nfollowing the exact same argument provided in the proof of Theorem 1, we can obtain a quantum channel eUthat is\nδ-close in diamond norm to a (1 ,3,0)-block-encoding of2\nπH, using O\u0010\nd2k\nδlog21\nδ\u0011\nsamples of ρ.\nNext, we discuss how our reshuffling circuit in (C3) can be used to estimate the full-singular value spectrum of unital\nquantum channels. When the input is the Choi state of a unital channel, the circuit produces the following state;\nρ+=1\n2d2|0⟩⟨0| ⊗I⊗I+1\n2d2|0⟩⟨1| ⊗ E A+1\n2d2|1⟩⟨0| ⊗ E†\nA+1\n2d2|1⟩⟨1| ⊗I⊗I. (C7)\nThe eigenspectrum {λi}of this state coincides with {1\n2d2±1\n2d2σi}, where σi’s are the singular values of the quan-\ntum channel. By applying conventional spectrum estimation algorithms [32, 33], we see that the full-singular value\nspectrum can be estimated to ϵprecision in total variation distance with O(d6/ϵ2) samples. Furthermore, since\n\f\f\f\f\f\f\n2d2X\ni=d2+1ˆλi−d2X\ni=1ˆλi\n−1\nd∥R(EB)∥1\f\f\f\f\f\f=\f\f\f\f\f\f\n2d2X\ni=d2+1ˆλi−d2X\ni=1ˆλi\n−\n2d2X\ni=d2+1λi−d2X\ni=1λi\n\f\f\f\f\f\f≤2d2X\ni=1|ˆλi−λi| ≤ϵ,(C8)\nwe can also estimate the 1st moment ∥R(EB)∥1up to precision ϵusing O(d6/ϵ2) samples of ρ+.\nAppendix D: Lower bound for the query complexity\nLet us define the unitary channel U(t) as\nU(t)[·] =U(t)[·]U(t)†, U (t) = exp\u0012\n−i\u0014OEA\nE†\nAO\u0015\nt\u0013\n. (D1)\nNow, suppose that there exists a protocol that universally creates a quantum channel eU(δ, t) that is δ-close in diamond\nnorm to U(t), given the ability to apply the unknown channel EforNtimes.\n∥eU(δ, t)− U(t)∥⋄≤δ. (D2)\nThis process Pcan be seen as a quantum superchannel [34, 35] which takes Ncopies of Eas input and produces an\noutput eU. Most generally, Pis asequential superchannel . Since Pis applicable to arbitrary black-box channels, it\n\n15\ncan be used to solve the following channel discrimination task; Given the ability to apply a quantum channel that is\npromised to be either E1orE2forNtimes, determine whether it is E1orE2. Indeed, by producing a single copy of\neUiby applying PtoEi, the optimal success probability of discriminating between eU1andeU2is [27]\nePsuc.\nN=1\n2+1\n4∥eU1(δ, t)−eU2(δ, t)∥⋄. (D3)\nIn channel discrimination, the most general strategy allows feedback from the output, which is referred to as an\nadaptive strategy. If we denote with Psuc.\nNthe success probability of discriminating between E1andE2with an\nadaptive strategy querying the channel Ntimes,\nePsuc.\nN≤Psuc.\nN. (D4)\nAdaptive strategies can strictly outperform parallel strategies, so in general\n1\n2+1\n4∥E⊗N\n1− E⊗N\n2∥⋄< Psuc.\nN. (D5)\nHowever, it has been shown [36] that the success probability for discriminating between two unknown channels with\nadaptive strategies reduces to the N-copy diamond distance and trace distance of Choi states for the special case of\nteleportation-covariant channels ;E(UρU†) =VE(ρ)V†, where Uis a teleportation unitary U∈UdandVis some\nunitary [37]. For these channels,\n1\n2+1\n4∥eU1(δ, t)−eU2(δ, t)∥⋄=ePsuc.\nN≤Psuc.\nN=1\n2+1\n4∥E⊗N\n1− E⊗N\n2∥⋄=1\n2+1\n4∥E⊗N\n1B− E⊗N\n2B∥1, (D6)\nwhere we denote the Choi states of E1andE2byE1BandE2B. It is important to emphasize that Nonly depends\non the parameters δ, t, and is independent of the channel discrimination task. Thus, any lower bound for Nderived\nfrom a specific choice of the channel in the discrimination task applies generally to the original task of creating eU\nfrom Ncopies of the black-box channel E.\nWith this in mind, let us focus on the channel discrimination task of two teleportation-covariant channels E1,E2.\nFirst, let f(δ, t) denote the query complexity for obtaining eU(δ, t) and let t∗denote the minimum tsuch that ∥U1(t)−\nU2(t)∥⋄= 2. We also denote with Θ( X) the smallest arc containing all the eigenvalues of the unitary X. Then,\n∥U1(t)− U2(t)∥⋄− ∥U 1(t)−eU1(δ, t)∥⋄− ∥U 2(t)−eU2(δ, t)∥⋄≤ ∥eU1(δ, t)−eU2(δ, t)∥⋄≤ ∥E⊗f(δ,t)\n1B− E⊗f(δ,t)\n2B∥1.(D7)\nwhich gives\n2−2\n3≤ ∥E⊗f(1\n3,t∗)\n1B − E⊗f(1\n3,t∗)\n2B ∥1≤2q\n1−F(E⊗f(1\n3,t∗)\n1B ,E⊗f(1\n3,t∗)\n2B ) = 2q\n1−F(E1B,E2B)f(1\n3,t∗). (D8)\nHere, F(ρ, σ) =||√ρ√σ||2\n1denotes the fidelity, and we have used the property F(ρ1⊗ρ2, σ1⊗σ2) =F(ρ1, σ1)F(ρ2, σ2)\nas well as the Fuchs-van de Graaf inequality\n1−p\nF(ρ, σ)≤1\n2∥ρ−σ∥1≤p\n1−F(ρ, σ). (D9)\nIf we denote H1:=\u0014OE1A\nE†\n1AO\u0015\n, H2:=\u0014OE2A\nE†\n2AO\u0015\n,\n[H1, H2] =\u0014\nE1AE†\n2A− E2AE†\n1A O\nO E†\n1AE2A− E†\n2AE1A\u0015\n. (D10)\nIf [H1, H2] = 0, we have\nΘ(U†\n1(t)U2(t)) = 2 d∥R(∆EB)∥∞t (D11)\nfor sufficiently small ∆ EB:=E1B− E2B. Then, we have\n1−F(E1B,E2B)f(1\n3,t∗)≥4\n9, t∗=π\n2d∥R(∆EB)∥∞. (D12)\n\n16\nTo proceed further, let us specifically choose\nE1(ρ) =Ep(ρ) :=Dp◦Φ(ρ),E2(ρ) =Eq(ρ) :=Dq◦Φ(ρ),(p= 1, q= 1−ϵ) (D13)\nwhere Da(ρ) := aI\n2+ (1−a)ρ,(a=p, q) denotes the qubit depolarizing channel and Φ( ρ) is defined as Φ( ρ) :=\nTrE(ρ)⊗ |ΦE⟩⟨ΨE|, where Tr Edenotes the partial trace over n−1 qubits and |ΨE⟩⟨ΨE|:=|00···0⟩⟨00···0|is an\nn-qubit pure state. Since the teleportation unitary U∈Udconsists of nqubit Pauli operators for d= 2n, Φ becomes\nteleportation covariant. Thus, E1,E2are also teleportation covariant channels. We can confirm that\nEaB=\u0012\naI2\n2⊗I2\n2+ (1−a)|Φ+\n2⟩⟨Φ+\n2|\u0013\n⊗\u0012\n|ΨE⟩⟨ΨE| ⊗I2n−1\n2n−1\u0013\n(D14)\nR(EaB) =\u0012a\n2|Φ+\n2⟩⟨Φ+\n2|+ 2(1−a)I2\n2⊗I2\n2\u0013\n⊗\u00121√\n2n−1(|ΨE⟩ ⊗ |ΨE⟩)⟨Φ+\n2n−1|\u0013\n, (D15)\nsoE1AE†\n2A=E2AE†\n1A,E†\n1AE2A=E†\n2AE1A. This implies [ H1, H2] = 0, and\n\n\n∥∆EB∥1=3\n2|p−q|\nd∥R(∆EB)∥∞=r\nd\n2|p−q|\nF(E1B,E2B) =\"r\n1−3\n4pr\n1−3\n4q+√p\n2√q\n2×3#2\n= 1−3\n4ϵ2+O(ϵ3)(D16)\nUsing the Bernoulli inequality (1 + x)r≥1 +rxforx≥ −1, and r∈N,\nf(1\n3, t∗)≥16\n27ϵ2=4\n31\n∥∆EB∥2\n1=32\n27π2\u0010√\ndt∗\u00112\n. (D17)\nFinally, if we set m=⌈1\n6δ⌉, we have mδ≤1\n3, so for t≥t∗/m, we have\nf(δ, t)≥1\nmf(mδ, mt )≥1\nmf(1\n3, mt)≥32\n27π2m(√\ndt)2≥16\n81π2·(√\ndt)2\nδ. (D18)\nAppendix E: Query complexity for learning singular value moments\nSuppose we are given access to the exact block-encoding of the Hermitized Liouville representation H, and utilized\nQSVT to obtain a matrix Mthat is a (1 ,5, ϵF)-block encoding of P\u00002\nπH\u0001\n, where P(x) is the approximation of the\nfunction f(x) =1\n2xr−1|x|1+αup to precision ϵ′′, where r+α=q−2,ris a positive integer, and α∈(−1,1), namely,\n|P(x)−1\n2xr−1|x|1+α| ≤ϵ′′. (E1)\nP(x) has the same parity as r−1. Therefore, we can choose P(x) and f(x) to be an even function. Noting that\n\u0014\nΣO\nO−Σ\u0015\n=1\n2\u0014\nU†V†\nU†−V†\u0015\u0014\nO U ΣV†\nVΣU†O\u0015\u0014\nU U\nV−V\u0015\n, (E2)\nfor any even function f, we see that\nf\u0012\u0014OEA\nE†\nAO\u0015\u0013\n=1\n2\u0014\nU U\nV−V\u0015\u0014\nf(Σ) O\nO f (−Σ)\u0015\u0014\nU†V†\nU†−V†\u0015\n=\u0014\nUf(Σ)U†O\nO V f (Σ)V†\u0015\n. (E3)\nNow, we consider the circuit depicted below;\n\n17\n|±⟩ H\n|0a⟩\nU|1⟩\nI\nd E\n|Φ+\nd⟩\nE\nWe can show that the probability distribution is\n\n\np(0) =1\n2+1\n2Re\u0010\nTrh\nE†\nAEA⟨1|M|1⟩i\u0011\np(1) =1\n2−1\n2Re\u0010\nTrh\nE†\nAEA⟨1|M|1⟩i\u0011\n.(E4)\nRepeating the above circuit O(log(1\nδ)\nϵ2\nH) times gives an estimate ˆS′\nqsuch that\nPr(|ˆS′\nq−1\nd2Trh\nE†\nAEA⟨1|M|1⟩i\n| ≤ϵH)≥1−δ. (E5)\nSince Tr |AB| ≤ ∥A∥1∥B∥∞,∥1\nd2E†\nAEA∥1= TrE2\nB≤1,∥⟨1|X|1⟩∥∞≤∥X∥∞, we have\n1\nd2\f\f\f\fTrh\nE†\nAEA⟨1|M|1⟩i\n−Tr\u0014\nE†\nAEA⟨1|P\u00122\nπH\u0013\n)|1⟩\u0015\f\f\f\f≤\r\r\r\rM−P\u00122\nπH\u0013\r\r\r\r\n∞≤ϵF. (E6)\nWe also have\nTr\u0014\nE†\nAEA⟨1|F\u0012\u0014OEA\nE†\nAO\u0015\u0013\n|1⟩\u0015\n= Tr\u0002\nVΣ2V†V F(Σ)V†\u0003\n=X\niσ2\niF(σi) (E7)\nfor any even function F. Therefore, we obtain\n1\nd2\f\f\f\fTr\u0014\nE†\nAEA⟨1|P\u00122\nπH\u0013\n)|1⟩\u0015\n−Tr\u0014\nE†\nAEA⟨1|f\u0012\u0014OEA\nE†\nAO\u0015\u0013\n|1⟩\u0015\f\f\f\f≤1\nd2d2X\ni=1σ2\ni\f\f\fP\u0010σi\nπd1−k\u0011\n−f\u0010σi\nπd1−k\u0011\f\f\f\n≤ϵ′′, (E8)\nwhere we have used the fact1\nd2P\niσ2\ni= TrE2\nB≤1 (see Appendix A, Lemma 2). We also have\n1\nd2Tr\u0012\nE†\nAEA⟨1|f\u00122\nπH\u0013\n|1⟩\u0013\n=1\nd2\u00121\nπd1−k\u0013q−2\nd2X\ni=1σq\ni\n. (E9)\nUtilizing the notion of the samplizer [12], we can obtain a quantum channel Uthat is δ′-close in diamond norm to\nthe unitary implemented in the above QSVT circuit CQSVT . Performing the Hadamard test with this approximated\nchannel Uproduces the estimate ˆSqsuch that\n|ˆSq−ˆS′\nq| ≤δ′(E10)\nsince Tr[ M(CQSVT [ρ0])−M(U(ρ0))]≤ ||M||∞||C[ρ0]− U(ρ0)||1≤ ||C QSVT− U|| ⋄=δ′for any measurement operator\nM. Thus, we arrive at an ϵH+ϵF+ϵ′′+δ′estimate ˆSqfor the quantity1\nd2+(1−k)(q−2)Pd2\ni=1σq\niwith\nO\n(dkQ)2log2\u0010\nQ\nδ′\u0011\nlog1\nδ\nδ′·ϵ2\nH\n (E11)\n\n18\nqueries to the black-box channel. Substituting ϵH=ϵF=ϵ′′=δ′=ϵ\n4andQ=O(1/ϵ1\nq−2\nF), the overall query\ncomplexity for estimating Sqto additive precision ϵis\nO\nd2klog1\nδlog2\u0012\n1\nϵ1+1\nq−1\u0013\n\u0000dq\nd2+(1−k)(q−2)ϵ\u00013+2\nq−2\n\nwith k≤1\n2for general channels and k≤1 for unital channels. Therefore, we obtain the following query complexity;\n\n\neO\u0012log1\nδ\nd3\n2(q−2)ϵ3+2\nq−2\u0013\n(general channels)\neO\u0012log1\nδ\nd3(q−2)ϵ3+2\nq−2\u0013\n(unital channels)(E12)\nAppendix F: SWAP circuit\nWhile in the main text we have discussed the general singular value moment q >1, q∈R, when qis an even integer,\nit is also possible to calculate the singular value moments with SWAP circuits implemented on many copies of the Choi\nstateEB. The SWAPs in the circuit are chosen so as to realize the following permutation pattern JandKin system 1\nand 2, respectively:\nJ=\u0012\n1 2 3 4 5 6 7 ···2k\n2k3 2 5 4 7 6 ···1\u0013\n, K =\u0012\n1 2 3 4 ···2k−1 2 k\n2 1 4 3 ···2k2k−1\u0013\n. (F1)\nIn a concrete form, this is expressed as\n1\nd2X\niσ2\ni= Tr\u0002\nF(12)⊗F(12)ρ⊗2\u0003\n= Tr ρ2\n1\nd4X\niσ4\ni= Tr\u0002\nF(4321)⊗F(2143) ρ⊗4\u0003\n1\nd6X\niσ6\ni= Tr\u0002\nF(632541) ⊗F(214365) ρ⊗6\u0003\n1\nd8X\niσ8\ni= Tr\u0002\nF(83254761) ⊗F(21436587) ρ⊗8\u0003\n(F2)\n|0⟩ H H\n|0⟩⊗nH E\n|0⟩⊗n\n|0⟩⊗nH E\n|0⟩⊗n\n|0⟩⊗nH E\n|0⟩⊗n\n|0⟩⊗nH E\n|0⟩⊗n\nFIG. 2. A quantum circuit for calculating the 4-th singular value moment.\n\n19\nHere,Fσrepresents the SWAP operator that implements the permutation (1 ,2,···,2k)→σ. The query complexity\nfor estimating Sq=1\ndqP\niσqup to precision ϵisO\u0010log1\nδ\nϵ2\u0011\n. We note that the above circuit was originally discussed for\ngeneral mixed states, rather than focusing on the Choi state of channels in the context of entanglement detection [10].\nWhile this scheme allows us to calculate Sqfor the special case where qis an even integer, other moments such as\nodd integer or more general real moments q∈Rare unlikely to be calculated by the above strategy.\nAppendix G: 1st moment via Fourier-cosine expansion\nHere we follow the methods employed in [15] to estimate the 1st singular value moment. First, the Fourier-cosine\nexpansion of the absolute function reads\n|x|=π\n2−∞X\nℓ=14\nπ(2ℓ−1)2cos((2 ℓ−1)x), x∈[−1,1]. (G1)\nSince∥R(EB)∥∞≤1, (see Appendix A, Lemma 5), we can substitute the Hermitized matrix to obtain\n2∥R(EB)∥1=\r\r\r\r\u0014OR(EB)\nR(EB)†O\u0015\r\r\r\r\n1=π\n2(2d2)−∞X\nℓ=14\nπ(2ℓ−1)2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\n. (G2)\nTruncating the series expansion at level Lgives\n\f\f\f\f\f∞X\nℓ=L+14\nπ(2ℓ−1)2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\f\f\f\f\f<∞X\nℓ=L+1\f\f\f\f4\nπ(2ℓ−1)2\f\f\f\f·(2d2)\n<8d2\nπZ∞\nLdx1\n(2x−1)2\n=4d2\nπ(2L−1)(G3)\nwhich can be bounded as4d2\n(2L−1)π≤ϵ1by choosing L=⌈2d2\nπϵ1+1\n2⌉. Next, consider a random variable Xthat takes the\nvalue−π\n2(2d2),π\n2(2d2),0 with the probabilityPL\nℓ=18\nπ2(2ℓ−1)2p(0),PL\nℓ=18\nπ2(2ℓ−1)2p(1),1−P∞\nℓ=L+18\nπ2(2ℓ−1)2, where\n\n\np(0) =1\n2+1\n4d2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\np(1) =1\n2−1\n4d2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\n.\nThen, we obtain\nE[X] =−LX\nℓ=14\nπ(2ℓ−1)2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\n. (G4)\nHere, p(0), p(1) corresponds to the probability of obtaining the measurement outcomes 0 ,1 in the DQC1 circuit with\nthe ideal block-encoding unitary. Let eXdenote the corresponding random variable when we replace the unitary with\nthe approximate channel constructed by our block-encoding scheme. Defining the precision parameter\nϵ′\n2ℓ−1=π(2ℓ−1)\n2(2 + log(2 L−1))ϵ2\n2d2, (G5)\none obtains\n|E[eX]−E[X]| ≤LX\nℓ=14(2d2)\nπ(2ℓ−1)2ϵ′\n2ℓ−1\n=2ϵ2\n2 + log(2 L−1)LX\nℓ=11\n2ℓ−1\n<2ϵ2\n2 + log(2 L−1) \n1 +ZL\n11\n2x−1dx!\n=ϵ2 (G6)\n\n20\nwitheO(1/ϵ′\n2ℓ−1) queries to the unknown channel, utilizing Theorem 1. Computing the variance, one obtains\nVar[eX] =E[eX2]−E[eX]2\n≤ |E[eX2]−π2\n4(2d2)2|+|E[eX]2−π2\n4(2d2)2|\n=π2(2d2)2\n4∞X\nℓ=L+18\nπ2(2ℓ−1)2+|E[eX]−π\n2(2d2)||E[eX] +π\n2(2d2)|\n≤π2(2d2)2\n4∞X\nℓ=L+18\nπ2(2ℓ−1)2+|E[eX]−π\n2(2d2)|\u0010\n|E[eX]−E[X]|+|E[X] +π\n2(2d2)|\u0011\n< πϵ 1d2+ 2πd2(ϵ2+ 2∥R(EB)∥1+ϵ1) =O(d2∥R(EB)∥1), (G7)\nwhere we have used |E[eX]| ≤π\n2(2d2),|E[eX]−E[X]|< ϵ2and\n\f\f\fE[X] +π\n2(2d2)\f\f\f=\f\f\f\f\f2∥R(EB)∥1−∞X\nℓ=L+14\nπ(2ℓ−1)2Tr\u0014\ncos\u0012\n(2ℓ−1)\u0014OR(EB)\nR(EB)†O\u0015\u0013\u0015\f\f\f\f\f\n≤2∥R(EB)∥1+ϵ1. (G8)\nEmploying the median of means estimation, it suffices to have O\u0010\nVar[eX]log1\nδ\nϵ2\n3\u0011\n=O(d2||R(EB)||1log1\nδ\nϵ2\n3) repetitions to\nproduce an ϵ3-precise estimation with probability Psuc.>1−δ. Thus, choosing ϵ1=ϵ2=ϵ3=2ϵ\n3, we can produce\nanϵ-close estimate for ||R(EB)||1with probability Psuc.≥1−δ. The expectation value for the query complexity\nbecomes\nO LX\nℓ=18\nπ2(2ℓ−1)2(2ℓ−1)2\nϵ′\n2L−1·d2∥R(EB)∥1log1\nδ\nϵ2\n3!\n=O LX\nℓ=132(2 + log(2 L−1))\nπ3(2ℓ−1)d4\nϵ2∥R(EB)∥1log1\nδ\nϵ2\n3!\n=O\u001216(2 + log(2 L−1))2\nπ3d4\nϵ2∥R(EB)∥1log1\nδ\nϵ2\n3\u0013\n=eO\u0012d4∥R(EB)∥1log1\nδ\nϵ3\u0013\n, (G9)\nwhere eOignores logarithmic factors in d. Since ∥R(EB)∥1≤d(see Appendix A, Lemma 4), the query complexity is\neO\u0010d5log1\nδ\nϵ2\u0011\nin the worst case.\nAppendix H: Comparison with full-state tomography\nHere we roughly analyze the sample complexity of estimating the first moment via full-state tomography of Choi\nstates. We consider bipartite states ρ∈Cd2×d2. Let the spectral decomposition of ρ1−ρ2beρ1−ρ2=Pd2\ni=1λi|i⟩⟨i|.\nFrom the triangle inequality of the Schatten p-norms, we obtain\n|∥R(ρ1)∥p− ∥R (ρ2)∥p| ≤ ∥R (ρ1)− R(ρ2)∥p\n≤d2X\ni=1|λi|∥R(|i⟩⟨i|)∥p. (H1)\nFrom this norm inequality and ∥R(ρ)∥1≤d∥R(ρ)∥2=dp\nTrρ2≤d, we know that\n|∥R(ρ1)∥1− ∥R (ρ2)∥1| ≤dd2X\ni=1|λi|=d∥ρ1−ρ2∥1. (H2)\nTo estimate ∥ρ1−ρ2∥1up to precision ϵ, it takes O(d4/ϵ2) samples. Therefore, substituting ϵ→ϵ/d, it takes O(d6/ϵ2)\nsamples to precisely estimate the 1st moment ∥R(ρ)∥1up to precision ϵvia full-state tomography in the worst case.\n\n",
    "source": "http://arxiv.org/abs/2506.24112v1",
    "authors": [
      "Ryotaro Niwa",
      "Zane Marius Rossi",
      "Philip Taranto",
      "Mio Murao"
    ],
    "categories": [
      "quant-ph"
    ],
    "type": "content"
  },
  {
    "id": "2506.24111v1_abstract",
    "title": "Pricing Fractal Derivatives under Sub-Mixed Fractional Brownian Motion with Jumps",
    "content": "Title: Pricing Fractal Derivatives under Sub-Mixed Fractional Brownian Motion with Jumps\n\nAbstract: We study the pricing of derivative securities in financial markets modeled by\na sub-mixed fractional Brownian motion with jumps (smfBm-J), a non-Markovian\nprocess that captures both long-range dependence and jump discontinuities.\nUnder this model, we derive a fractional integro-partial differential equation\n(PIDE) governing the option price dynamics.\n  Using semigroup theory, we establish the existence and uniqueness of mild\nsolutions to this PIDE. For European options, we obtain a closed-form pricing\nformula via Mellin-Laplace transform techniques. Furthermore, we propose a\nGrunwald-Letnikov finite-difference scheme for solving the PIDE numerically and\nprovide a stability and convergence analysis.\n  Empirical experiments demonstrate the accuracy and flexibility of the model\nin capturing market phenomena such as memory and heavy-tailed jumps,\nparticularly for barrier options. These results underline the potential of\nfractional-jump models in financial engineering and derivative pricing.",
    "source": "http://arxiv.org/abs/2506.24111v1",
    "authors": [
      "Nader Karimi"
    ],
    "categories": [
      "q-fin.PR"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24111v1_content",
    "title": "Pricing Fractal Derivatives under Sub-Mixed Fractional Brownian Motion with Jumps",
    "content": "arXiv:2506.24111v1  [q-fin.PR]  30 Jun 2025Pricing Fractal Derivatives under Sub-Mixed\nFractional Brownian Motion with Jumps\nNader Karimi∗\nJuly 1, 2025\nAbstract\nWe study the pricing of derivative securities in financial markets modeled by a\nsub-mixed fractional Brownian motion with jumps (smfBm–J), a non-Markovian\nprocess that captures both long-range dependence and jump discontinuities. Un-\nder this model, we derive a fractional integro–partial differential equation (PIDE)\ngoverning the option price dynamics.\nUsing semigroup theory, we establish the existence and uniqueness of mild\nsolutions to this PIDE. For European options, we obtain a closed-form pricing\nformula via Mellin–Laplace transform techniques. Furthermore, we propose a\nGr¨ unwald–Letnikov finite-difference scheme for solving the PIDE numerically and\nprovide a stability and convergence analysis.\nEmpirical experiments demonstrate the accuracy and flexibility of the model in\ncapturing market phenomena such as memory and heavy-tailed jumps, particularly\nfor barrier options. These results underline the potential of fractional-jump models\nin financial engineering and derivative pricing.\n1 Introduction\nFinancial time series exhibit two robust stylised facts that defy the assumptions of the\nclassical Black–Scholes framework: (1) long-range dependence slowly decaying autocorre-\nlations attributed to market micro-structure, algorithmic trading and behavioural feed-\nback loops; and (2) discontinuous jumps caused by macro-economic announcements, liq-\nuidity shocks or flash crashes. Standard Brownian motion captures neither, while pure\nfractional Brownian motion (fBm) accounts for the first but remains a continuous Gaus-\nsian process with unbounded arbitrage opportunities under the usual semimartingale\nsetting [1]. Jump–diffusion models [2] address the second feature but ignore memory.\nRecent strands of literature have sought to bridge this gap. [3] introduce sub-fractional\nBrownian motion (sfBm) to temper the strong covariance structure of fBm. [4] blend fBm\nwith Brownian motion to form a mixed model that displays both short- and long-memory\nregimes. [5] incorporate pure-jump L´ evy noise into a mixed-fractional setting but leave\nopen the questions of risk-neutral measure construction and numerical valuation for path-\ndependent pay-offs.\n∗Department of Mathematics and Computer Science, Amirkabir University of Technology, Tehran\n1591634311, Iran, e-mail: nkarimi@aut.ac.ir\n1\n\nContributions. Building on these ideas, we develop a full pricing machinery for a market\ndriven by a smfBm–J . Our contributions are:\n1. A fractional Girsanov–Esscher theorem that yields an equivalent martingale mea-\nsure without semimartingale assumptions, extending [6].\n2. Derivation of a fractal Black–Scholes integro–PDE whose time-fractional derivative\nof order 1 −β(β= 1−H) captures long-memory while a non-local jump generator\nmodels discontinuities.\n3. A closed-form Mellin–Laplace representation for European options expressed through\nthe two-parameter Mittag–Leffler function, subsuming Black–Scholes and Merton\nas limiting cases.\n4. A fully implicit Gr¨ unwald–Letnikov scheme with unconditional L2\nw-stability and\nconvergence rate O(∆t1+H) for barrier options, with proof via a discrete energy\nargument.\n5. Empirical calibration to S&P 500 data and extensive Monte-Carlo validation con-\nfirming both pricing accuracy and theoretical convergence.\nOrganisation of the paper. Section 2 introduces the smfBm–J process and its covari-\nance structure. Section 3 constructs the equivalent martingale measure and proves The-\norem 3.2. Section 4 derives the fractal Black–Scholes integro–PDE and establishes exis-\ntence and uniqueness of mild solutions. Section 5 provides the closed-form Mellin–Laplace\nprices for European calls (Theorem 5.1). Section 5 also develops the Gr¨ unwald–Letnikov\nfinite-difference method and proves stability (Theorem 6.1). Section 7 reports calibra-\ntion results and Section 6.6 presents barrier-option experiments. Section 8 concludes and\noutlines avenues for future research, including stochastic-volatility extensions and rough-\njump calibration for cryptocurrencies.\n2 Preliminaries\n2.1 Sub-Mixed Fractional Brownian Motion with Jumps\nReal financial time series frequently display both long–range dependence (LRD) in the\nform of slowly–decaying autocovariances andsudden discontinuities or jumps. The stan-\ndard Brownian motion Wtis unable to capture either of these features; fractional Brow-\nnian motion (fBm) BH\ntwith Hurst index H∈(0,1) incorporates LRD for H > 1/2\nbut remains a continuous process, while pure jump L´ evy processes lack the correlation\nstructure required for LRD. To model the simultaneous presence of short–memory noise,\nlong–memory dependence and rare jumps, we consider the sub-mixed fractional Brow-\nnian motion with jumps (smfBm–J ) originally proposed by [5].\nDefinition 2.1 (smfBm–J) .Let\n•W={Wt:t≥0}be a standard Brownian motion with variance parameter σ2\n0,\n2\n\n•SH={SH\nt:t≥0}be a sub-fractional Brownian motion (sfBm) with Hurst index\nH∈(0,1), zero mean and covariance\nE\u0002\nSH\ntSH\ns\u0003\n=t2H+s2H−1\n2\u0000\n(t+s)2H+|t−s|2H\u0001\n,\nscaled by σH>0,\n•J={Jt:t≥0}be a compensated compound Poisson process Jt=PNt\nk=1Yk−\nλtE[Y1],where Nt∼Poisson (λt)counts the jumps and {Yk}are i.i.d. log-jump\nsizes with c.g.f. ΨY(u) = log E[euY1].\nAll three drivers are assumed independent. The sub-mixed fractional Brownian motion\nwith jumps is defined by\nBsmfJ\nt:=σ0Wt+σHSH\nt+Jt, t ≥0.\nSecond-order structure. Because WandSHare centred Gaussian and independent,\nBsmfJhas mean zero and covariance\nCov\u0002\nBsmfJ\nt, BsmfJ\ns\u0003\n=σ2\n0min{t, s}+σ2\nH\u0000\nt2H+s2H−1\n2((t+s)2H+|t−s|2H)\u0001\n,\nwhile the jump part only contributes to the variance through its compensator λtVar(Y1).\nSelf–similarity and long-memory. Setting β= 1−Hwe obtain the scaling relation\n{BsmfJ\nct}t≥0d=σ0√c W+σHcHSH+Jct,which is mixed-self-similar : the Gaussian compo-\nnent inherits the exact self-similarity of order Hfrom sfBm and 1 /2 from BM, whereas\nthe Poisson component scales linearly with time. For H >1\n2the sfBm contributes LRD,\ni.e.P∞\nk=1Cov(∆ hSH\nkh,∆hSH\n0) =∞, so the overall process exhibits long memory despite\nthe short-memory Brownian and jump parts.\nIncrement representation. The smfBm–J admits the decomposition BsmfJ\nt+h−BsmfJ\nt=\nσ0(Wt+h−Wt)+σH(SH\nt+h−SH\nt)+(Jt+h−Jt),where the increments of SHarenotstationary\nbut are asymptotically stationary when h≪t. This property is crucial in Section 3 where\na fractional Girsanov theorem is applied.\nSemimartingale and integration. ForH̸=1\n2neither fractional nor sub-fractional\nBrownian motion is a semimartingale, hence the mixed driver\nBsmfJ\nt=σ0Wt+σHSH\nt+Jt\nis not amenable to classical Itˆ o integration. We therefore use the fractional Wick–Itˆ o–Skorokhod\n(F–WIS) integral for the sfBm part and classical Itˆ o integrals for the Brownian–jump\ncomponents:\n1.Kernel isometry. Define IH:L2([0, T])→HHby\n(IHφ)(t) :=Zt\n0KH(t, s)φ(s)ds, K H(t, s) =cH\u0002\n(t−s)H−1\n2−(−s)H−1\n2\u0003\n,\nso every square–integrable kernel maps into the Cameron–Martin space HH.\n3\n\n2.F–WIS integral. Forϕ∈D1,2(Malliavin differentiable and adapted) set\nZT\n0ϕs⋄dSH\ns:=δH(ϕ) =X\nn≥0In+1\u0000eϕ(n)\u0001\n,\nwhere Indenotes the n-th multiple Wiener integral and eϕ(n)is the symmetrisation\nof (I⊗n\nH∂sϕ).\n3.Isometry. The F–WIS integral preserves L2–norms:\nEh\f\f\fZT\n0ϕs⋄dSH\ns\f\f\f2i\n=∥ϕ∥2\nL2([0,T]).\n4.Trading filtration. With the enlarged filtration Ft=σ{Ws, SH\ns, Js: 0≤s≤t},\na trading strategy is predictable w.r.t. the semimartingale subfiltration generated\nby (W, J); gains from SHenter valuation only through risk-neutral expectations\ncomputed via the F–WIS integral.\nLimit cases.\n1. Setting σH= 0 recovers the mixed Brownian motion with jumps (standard Merton\nmodel with an extra Brownian factor).\n2. Setting σ0= 0 produces the pure sfBm with jumps, suitable for markets where\nmicro-structure noise is negligible.\n3. Taking λ→0 yields the continuous sub-mixed fBm studied by [4]; conversely letting\nH→1\n2reduces the model to Brownian motion plus jumps.\nThese nested cases facilitate diagnostic testing and calibration, as discussed in Section 3.\n2.2 Fractal Calculus\nClassical option–pricing models rely on Itˆ o calculus, which is well–suited to semimartin-\ngales such as Brownian motion. Once a memory component such as sfBm is introduced,\nthe kernel of the integral becomes non–local in time and Itˆ o’s rule no longer applies. In\nthis work we therefore adopt tools from fractal calculus , i.e. the fractional–order general-\nisations of integration and differentiation.\nFractional integrals. For a function f∈L1[0, T] and fractional order α∈(0,1) the\nleft–sided Riemann–Liouville (R–L) fractional integral is defined by\n(Iα\n0+f)(t) :=1\nΓ(α)Zt\n0(t−s)α−1f(s) ds, 0≤t≤T,\nwhere Γ( ·) denotes the Euler gamma–function. The operator is linear and non–local; the\nentire past of finfluences ( Iα\n0+f)(t) through the power–law kernel ( t−s)α−1.\n4\n\nRiemann–Liouville derivative. Applying an ordinary derivative to I1−β\n0+fyields the\nR–L fractional derivative of order β∈(0,1):\n\u0000\n0Dβ\ntf\u0001\n(t) :=d\ndt(I1−β\n0+f)(t) =1\nΓ(1−β)d\ndtZt\n0(t−s)−βf(s) ds.\nIntuitively,0Dβ\ntmeasures a weighted history derivative : recent observations of fcarry\nmore weight than distant ones, but all past values contribute.\nCaputo derivative. For applications with non–smooth initial data the Caputo deriva-\ntive is often preferred because it replaces f(s) in the kernel with f′(s), allowing classical\nboundary conditions. It is given by\n\u0000C\n0Dβ\ntf\u0001\n(t) :=1\nΓ(1−β)Zt\n0(t−s)−βf′(s) ds,\nand satisfiesC\n0Dβ\ntconst = 0, a property important when discounting cash–flows.\nLaplace–transform and semigroup properties. Both derivatives have simple Laplace\ntransforms: L{0Dβf}(u) =uβ˜f(u)−uβ−1f(0),which facilitates analytical solutions of lin-\near fractional ODEs such as the time–fractional Black–Scholes PDE (4). Moreover, Iα\n0+\nforms a semigroup, Iα\n0+Iβ\n0+=Iα+β\n0+, enabling incremental time–stepping schemes (Sec-\ntion 6).\nGr¨ unwald–Letnikov discretisation. For numerical purposes we approximate the R–\nL derivative via the backward Gr¨ unwald–Letnikov series\n\u0000\n0Dβ\ntf\u0001\n(tn)≈1\n∆tβnX\nk=0ω(β)\nkf(tn−k), ω(β)\nk:= (−1)k\u0012β\nk\u0013\n.\nThis representation naturally leads to the fully–implicit finite–difference scheme analysed\nin Section Section 5; order 1 + Hconvergence is established in Section 5.\nFractional Itˆ o formula. A cornerstone of our option–pricing derivation is the frac-\ntional Itˆ o (or Wick–Itˆ o–Skorokhod ) formula, which reads for an admissible functional\nF(t, St),\n0D1−β\ntF=∂tF+1\n2σ2\n0S2∂2\nSSF+σ0σHS2∂S0DH\ntF+···,\nwhere the dots denote the jump operator. The proof follows [7] and is reproduced in\nAppendix B for completeness.\nDetailed reviews of these operators can be found in [8] and [7].\n3 Market Model and Equivalent Martingale Measure\n3.1 Market set–up\nFix a finite horizon T >0 and let (Ω ,F,{Ft}0≤t≤T,P) be a filtered probability space that\nsatisfies the usual conditions and supports\n5\n\n•a standard Brownian motion W={Wt}t∈[0,T],\n•an independent sub–fractional Brownian motion SH={SH\nt}t∈[0,T]with Hurst index\nH∈(0,1),\n•an independent Poisson process N={Nt}t∈[0,T]with intensity λ > 0 and i.i.d.\njumps Yk∼FY.\nThe filtration is generated by the three drivers, i.e. Ft=σ{Ws, SH\ns, Ns: 0≤s≤t}P.\nThe money–market account evolves deterministically via\ndBt=r Btdt, B 0= 1, r > 0.\nThe risky asset price S={St}follows the mixed–fractional SDE with jumps\ndSt\nSt=µ dt+σ0dWt+σHdSH\nt+dJt, S 0>0,\nwhere Jt=PNt\nk=1Yk−λtE[Y1] is the compensated jump process. We assume µ, σ 0, σH>0\nandE[eηY1]<∞for some η >1 to guarantee exponential moments.\nChange of measure. To derive risk–neutral prices in a market driven jointly by Brown-\nian noise, sub-fractional memory and compound Poisson jumps, we first need an equivalent\nmartingale measure Qunder which the discounted asset price becomes a martingale. Clas-\nsical Girsanov theory handles Brownian drift shifts, and the Esscher transform neutralises\npure jumps, but neither framework alone can cope with the non-semimartingale nature of\nsub-fractional Brownian motion (sfBm). The next theorem extends the Girsanov–Esscher\nmachinery to our hybrid setting: it constructs a joint density that simultaneously (i)shifts\nthe drift of the Brownian part, (ii)“tilts” the sfBm via its Cameron–Martin space, and\n(iii) reweights jump sizes so that the overall drift equals the risk-free rate r. The de-\ntailed proof is provided to guarantee integrability and to verify that the resulting measure\npreserves covariance structures of all three driving processes.\nTheorem 3.1 (Fractional Girsanov–Esscher) .Let(Wt)t≥0be a standard Brownian mo-\ntion, (SH\nt)t≥0a sub-fractional Brownian motion with Hurst index H∈(0,1), independent\nofW, and Jt=PNt\nk=1Yk−λtE[Y1]a compensated compound Poisson process with intensity\nλ >0and i.i.d. jump sizes (Yk)satisfying E[eηY1]<∞for some real η. Fix deterministic\ndrift controls θ0∈RandθH∈L2([0, T]). Define the Radon–Nikodym density\nZT=Z(G)\nTZ(J)\nT, Z(G)\nT= exp\u0010\n−θ0WT−1\n2θ2\n0T−ZT\n0θH(s)dSH\ns−1\n2∥θH∥2\nHH\u0011\n,\nZ(J)\nT= exp\u0010\nη∗NTX\nk=1Yk−λT\u0000\nE[eη∗Y1]−1\u0001\u0011\n,\nwhere HHis the canonical Hilbert space of the sfBm and η∗is the unique solution of\nE[e(η∗+1)Y1] =E[eη∗Y1](Esscher drift-neutrality). If\n1\n2θ2\n0T+1\n2∥θH∥2\nHH<∞,E\u0002\neη∗Y1\u0003\n<∞, (1)\n6\n\nthenEP[ZT] = 1 and the probability measure dQ=ZTdPis equivalent to P. Under Q\nfWt:=Wt+θ0t,eSH\nt:=SH\nt+Zt\n0θH(s)ds, eJt:=Jt+λt\u0000\nE[eη∗Y1]−1\u0001\nhave the same covariance structures as their P-counterparts, and the discounted price\nprocess e−rtStis aQ-martingale.\nProof. Step 1 (Gaussian density). For sfBm we recall the kernel representation\nSH\nt=Rt\n0KH(t, s)dBswith a standard Brownian B. Define φ(s) =θH(s)1[0,T](s) and\nnote∥θH∥2\nHH=RT\n0RT\n0φ(u)φ(v)KH(T, u)KH(T, v)du dv < ∞by (1). Hence Z(G)\nT=\nET\u0000\n−θ0dW−φ dB\u0001\nis an exponential martingale, and Novikov’s criterion E[e1\n2⟨θ0W+φ∗B⟩T]<\n∞holds by (1); thus EP[Z(G)\nT] = 1.\nStep 2 (Esscher density for jumps). Write Z(J)\nT= exp\u0000\nη∗XT−ψ(η∗)T\u0001\nwhere\nXT=PNT\nk=1Ykandψ(η) =λ(E[eηY1]−1) is the log-MGF of Jt. Because ψ(η) is finite at\nη∗,EP[Z(J)\nT] =e−ψ(η∗)Texp\u0000\nψ(η∗)T\u0001\n= 1.\nStep 3 (Equivalence and quadratic-covariation shift). SetZT=Z(G)\nTZ(J)\nT.\nBoth factors are strictly positive local martingales with unit mean, so ZTis itself a\npositive martingale and defines an equivalent measure Q.\nUnder Qthe Girsanov theorem for classical Brownian motion yields dfWt=dWt+θ0dt,\na standard Q-Brownian motion. For the sfBm component, Hu–Øksendal (2003, Thm 3.6)\nimplies that eSH\nt:=SH\nt+Rt\n0θH(s)dsretains sfBm covariance EQ[eSH\nteSH\ns] =cH(t2H+s2H−\n|t−s|2H).\nLikewise the Esscher tilt shifts jump intensity to λ∗=λE[eη∗Y1] but keeps the com-\npensated process eJta square-integrable martingale.\nStep 4 (Drift cancellation in price SDE). The physical-measure dynamics are\ndSt/St=µ dt+σ0dWt+σHdSH\nt+dJt. Substituting dWt=dfWt−θ0dt,dSH\nt=deSH\nt−\nθH(t)dtanddJt=deJt−λ(E[eY1]−1)dtgives\ndSt\nSt= (µ−σ0θ0−σHθH(t)−λκ)dt+σ0dfWt+σHdeSH\nt+deJt,\nwhere κ=E[eY1]−1. Setting the drift equal to r dtand discounting yields d\u0000\ne−rtSt\u0001\n=\ne−rtSt\u0000\nσ0dfWt+σHdeSH\nt+deJt\u0001\n, an Itˆ o integral with respect to Q-martingales, hence a\nlocal martingale. Integrability follows fromRT\n0S2\ntdt <∞in exponential-moment models;\ntherefore the local martingale is a true martingale.\n3.2 Objective\nTo preclude arbitrage we seek an equivalent martingale measure (EMM) Q∼Psuch that\nthe discounted price process ˜St:=e−rtStis aQ–martingale. The presence of a memory\ncomponent and jumps requires a fractional version of Girsanov’s theorem combined with\nan Esscher transform for L´ evy jumps.\n3.3 Fractional Girsanov transform\nA shift of the Brownian part is performed in the classical way via\nθ0:=µ−r−λκ\nσ0, κ :=E[eY1−1].\n7\n\nFor the sfBm we employ the Cameron–Martin space HHof kernels KH(see Appendix A).\nChoosing a deterministic control θH(t)∈L2([0, T]) we define the Radon–Nikodym deriva-\ntive\nZ(G)\nT:= exp\u0010\n−θ0WT−1\n2θ2\n0T−ZT\n0θH(t)(s)dSH\ns−1\n2∥θH(t)∥2\nHH\u0011\n,\nwhich satisfies EP[Z(G)\nT] = 1 under a Novikov–type condition [6].\n3.4 Esscher transform for jumps\nFor the jump component we apply the exponential tilting (Esscher) transform with pa-\nrameter η∗determined by λE[e(η∗+1)Y1−eη∗Y1] =λκ.Set\nZ(J)\nT:=NTY\nk=1eη∗Yk\nE[eη∗Y1]exp\u0000\nλT(E[eη∗Y1]−1−η∗E[Y1])\u0001\n.\nThen Z(J)\nTis anP–martingale and under the tilted measure the compensated process Jt\nturns into a Q–martingale.\n3.5 Construction of Q\nDefine the density process\ndQ\ndP\f\f\f\f\nFT:=Z(G)\nTZ(J)\nT.\nSince Z(G)\nTandZ(J)\nTare independent P–martingales with expectation one, their product\nalso has unit expectation; thus Q∼P.\nTheorem 3.2. LetSt=S0exp\u0010Rt\n0(µ−1\n2σ2\n0)ds+σ0Wt+σHSH\nt+Jt\u0011\n,with a Brownian\nmotion W, an independent sub-fractional Brownian motion SH(H∈(0,1))and a com-\npensated compound-Poisson process Jt=PNt\nk=1Yk−λt κ,κ:=E[eY1]−1. Fix controls\nθ0∈RandθH∈L2([0, T]). Assume the Novikov–type integrability1\n2θ2\n0T+1\n2∥θH∥2\nHH<∞\nandE[eη∗Y1]<∞for the Esscher root η∗. If\nµ=r+λκ−σ0θ0−σHZT\n0θH(s)KH(T, s)ds (2)\nthen the discounted price ˜St:=e−rtStis a true martingale under the probability measure\nQdefined below.\nProof.\nStep 0 (Notation). The kernel representation of sfBm is SH\nt=Rt\n0KH(t, s)dBs,\nwhere KH(t, s) =cH[(t−s)H−1\n2−(−s)H−1\n2] and Bis an independent Brownian motion. Its\nCameron–Martin space is HH={φ: [0, T]→R,∥φ∥2\nHH=RT\n0RT\n0φ(u)φ(v)KH(T, u)KH(T, v)du dv <\n∞}.\nStep 1 (Density for Gaussian part). Define\nZ(G)\nt= exp\u0010\n−θ0Wt−1\n2θ2\n0t−Zt\n0θH(s)dSH\ns−1\n2∥θH∥2\nHH(0,t)\u0011\n,0≤t≤T.\n8\n\nBecause ∥θH∥2\nHH(0,t)is finite by hypothesis, the exponential is square-integrable. Novikov’s\ncondition for the 2-dimensional Brownian vector ( W, B ) reads E\u0002\nexp\u00001\n2θ2\n0T+1\n2∥θH∥2\nHH\u0001\u0003\n<\n∞,which holds; hence Z(G)\ntis a true martingale with unit mean [6, Thm. 3.6].\nStep 2 (Esscher density for jumps). Letψ(η) =λ(E[eηY1]−1) be the L´ evy\nexponent. Choose η∗solving ψ′(η∗) =λE[Y1eη∗Y1] =λκ, i.e. Esscher drift neutrality. Set\nZ(J)\nt= exp\u0000\nη∗Xt−ψ(η∗)t\u0001\n, Xt=PNt\nk=1Yk.Because ψ(η∗)<∞,EP[Z(J)\nt] = 1 for every t.\nStep 3 (Equivalent measure). Define the product density Zt=Z(G)\ntZ(J)\nt, dQ=\nZTdP.Both factors are positive martingales with expectation one, so Qis equivalent to\nP.\nStep 4 (Shifted drivers). Under Qwe have\ndfWt=dWt+θ0dt,\ndeSH\nt=dSH\nt+θH(t)dt,\ndeJt=dJt−λκ dt,\nwherefWis a Brownian motion, eSHan sfBm with identical covariance, and eJa compen-\nsated Poisson martingale with intensity λ∗=λE[eη∗Y1] and tilted jump law F(η∗)\nY.\nStep 5 (Drift cancellation). Insert the shifted differentials into dSt/St=µ dt+\nσ0dWt+σHdSH\nt+dJt:\ndSt\nSt=\u0002\nµ−σ0θ0−σHθH(t)−λκ\u0003\ndt+σ0dfWt+σHdeSH\nt+deJt.\nWith µchosen by (2) the bracket equals r, sod˜St=˜St\u0000\nσ0dfWt+σHdeSH\nt+deJt\u0001\n,a local\nQ-martingale.\nStep 6 (True martingale). Square-integrability of ˜Stfollows from the exponential-\nmoment bound EQ[exp( α|WT|+β|SH\nT|+γ|JT|)]<∞for some α, β, γ , ensured by the\ngaussian moments and E[eη∗Y1]<∞. Hence ˜Sthas bounded expectation and is a true\nmartingale.\nTherefore e−rtSt=˜Stis aQ-martingale.\n3.6 Economic interpretation\nCondition µ=r+λκ−σ0θ0−σHθH(t) states that the expected excess return of the asset\nequals a linear combination of three risk premia: (i) the diffusive market price of risk θ0,\n(ii) the fractional market price of risk encoded by the kernel θH(t), and (iii) the classical\njump risk premium λκ. In Section 7 we discuss empirical estimation of ( θ0, θH(t), λ, κ)\nfrom option implied–volatility surfaces.\n4 Fractal Black–Scholes Equation\nThe central pricing result of this paper is a time–fractional, non-local generalisation\nof the classical Black–Scholes PDE that simultaneously incorporates long-memory and\njump discontinuities. In this section we derive the equation rigorously via the fractional\nItˆ o formula introduced in Appendix B, interpret each operator financially, and discuss\nwell–posedness and limiting cases.\n9\n\n4.1 Derivation via the fractional Itˆ o formula\nLetV: [0, T]×R+→Rbe a twice differentiable pricing functional with polynomial\ngrowth. Applying the Wick–Itˆ o–Skorokhod formula to V(t, St) under the martingale\nmeasure Qobtained in Section 3 yields\n0D1−β\ntV=∂tV+1\n2σ2\n0S2∂2\nSSV+σ0σHS2∂S\u0000\n0DH\ntV\u0001\n+ (µ−λκ)S∂SV+λEY\u0002\nV(t, SeY)−V(t, S)\u0003\n. (3)\nSubstituting µ=r+λκ(risk–neutral drift) and collecting terms gives the Fractal\nBlack–Scholes (FBS) equation\n0D1−β\ntV+1\n2σ2\n0S2∂2\nSSV+σ0σHS2∂S\u0000\n0DH\ntV\u0001\n+ (r−λκ)S∂SV−rV+λEY\u0002\nV(t, SeY)−V(t, S)\u0003\n= 0\n(4)\nwith terminal condition V(T, S) = Φ( S) for a given payoff function Φ.\n4.2 Interpretation of each term\n•Fractional drift0D1−β\ntV.The Riemann–Liouville derivative of order 1 −β=\nHcreates temporal memory: the option value at tdepends on the entire past\ntrajectory of the underlying via a power-law kernel.\n•Gaussian diffusion1\n2σ2\n0S2∂2\nSSV.This is the familiar risk from instantaneous\nBrownian fluctuations.\n•Fractional–Gaussian cross term σ0σHS2∂S(0DH\ntV).A mixed term coupling the\nlocal Brownian and fractional components; disappears if either σH= 0 or H= 1/2.\n•Jump generator λEY[V(t, SeY)−V(t, S)].A non-local integral operator account-\ning for Poisson jumps with distribution FY.\n•Discounting terms (r−λκ)S∂SV−rV.Standard cost-of-carry adjusted for the\njump drift κ=E[eY−1].\n4.3 Limiting cases\n1.Classical Black–Scholes: σH= 0 and λ= 0 reduce (4) to the usual BS PDE.\n2.Merton jump–diffusion: σH= 0 but λ >0 recovers the integro–PDE of [2].\n3.Time-fractional BS: σ0= 0,λ= 0 yields the Caputo-type model of [9].\n4.Rough volatility limit: letting H→0 increases memory length and leads to ultra-\nslow diffusion as studied in [10].\n4.4 Existence and uniqueness\nFunctional setting. Following [3] we work in the weighted Banach space\nV=n\nv∈C1,2\u0000\n[0, T)×R+\u0001\f\f∥v∥V:= sup\n(t,S)∈[0,T)×R+|v(t, S)|\n1 +S2<∞o\n.\nThe factor (1 + S2)−1ensures uniform decay for large prices and yields compactness\nproperties similar to C0(R+).\n10\n\nGenerator of the diffusion part. Define the second–order operator ( Av)(S) :=\n1\n2σ2\n0S2vSS+ (r−λκ)S vS−rv,with domain D(A) ={v∈ V :v, Sv S, S2vSS∈ V} .\nA standard Lyapunov argument shows ℜ⟨v,Av⟩V≤C∥v∥2\nV,soAis sectorial and gener-\nates an analytic C0–semigroup T(t) =etAonV[11, Thm. 6.1.5].\nJump operator. For any v∈ Vset\n(Jv)(S) :=λ\u0000\nEY[v(SeY) ]−v(S)\u0001\n.\nAssuming E[eγ|Y|]<∞for some γ >0, one has\n∥Jv− Jw∥V≤λE\u0002\n(1 +e2Y)\u0003\n∥v−w∥V=LJ∥v−w∥V,\nhence Jis Lipschitz on Vwith constant LJ<∞.\nFractional abstract Cauchy problem. The pricing PDE can now be phrased as\n0D1−β\ntV(t) =AV(t) +JV(t), V (T) = Φ ,\nwhich is an inhomogeneous Caputo–type abstract Volterra equation. Using the fractional\nHille–Yosida theorem [12] [Prop. 2.4] and the analyticity of T(t), the problem admits a\nunique mild solution given by\nV(t) =Eβ\u0000\n−(T−t)βA\u0001\nΦ +ZT\nt(s−t)β−1Eβ,β\u0000\n−(s−t)βA\u0001\nJV(s)ds. (5.3)\nHere EβandEβ,βare the one– and two–parameter Mittag–Leffler functions and the\nintegral is Bochner–integrable in V.\nFixed-point argument. Define the map F[V](t) to be the right–hand side of (5.3).\nUsing the semigroup estimate ∥Eβ(−(T−t)βA)∥ ≤Cand the Lipschitz constant LJ, we\nderive\n∥F[V]−F[W]∥C([0,T];V)≤C L JTβ\nΓ(β+ 1)∥V−W∥C([0,T];V).\nForT(maturity) fixed, the factor C L JTβ/Γ(β+ 1) <1, soFis a contraction; hence a\nunique fixed point V∈C([0, T];V) exists by Banach’s fixed-point theorem.\nClassical differentiability. Since T(t) is analytic and Jis bounded, V(t) inherits\nC1-regularity in t<T andC2inS, soV∈C1,2([0, T)×R+)∩ V, i.e. a classical solution\nas well.\nHence the fractional Black–Scholes operator admits a unique mild (and in fact classi-\ncal) solution in the weighted space V, with explicit representation (5.3).\n4.5 Energy estimate and maximum principle\nBy multiplying (4) with (1 + S2)−1Vand integrating over S∈(0,∞), we obtain the\na-priori estimate ∥V(t,·)∥L2w≤C∥Φ∥L2w,where w(S) = (1 + S2)−1, ensuring numerical\nstability of the finite-difference scheme in Section 6.\n11\n\n4.6 Summary\nEquation (4) forms the mathematical backbone of our pricing framework, generalising\nseveral well-known models. The next section provides closed-form Laplace–Mellin so-\nlutions for European payoffs and develops an efficient Gr¨ unwald–Letnikov scheme for\nbarrier options.\n5 Closed-Form European Prices\nAlthough the FBS integro–fractional PDE (4) generally requires numerical methods, Eu-\nropean vanilla options admit a semi–analytic formula expressed through the two–parameter\nMittag–Leffler function. In this section we derive the result using a Mellin transform in\nthe spatial variable and a Laplace transform in time; the mixed fractional term trans-\nlates into a polynomial in the Laplace domain, while the jump integral yields a simple\nmultiplicative factor.\n5.1 Transform strategy\nLetx= log( S/K) denote the log–moneyness and u(T−t) =τthe time to maturity.\nDefine\nv(τ, x) :=e−rτV(T−τ, Kex), 0≤τ≤T, x∈R,\nso that v(0, x) = (Kex−K)+=K(ex−1)+. Under these variables equation (4) becomes\n0D1−β\nτv=Lxv−λEY\u0002\nv(τ, x+Y)−v(τ, x)\u0003\n, (5)\nwith spatial operator Lx:=1\n2σ2\n0∂xx+σ0σH∂x0DH\nτ+ (r−1\n2σ2\n0−λκ)∂x.\nLaplace transform in τ.Taking Lτ{·}(s) =R∞\n0e−sτ(·)dτand using L{0D1−β\nτv}(s) =\ns1−βˆv(s, x)−s−βv(0, x),we obtain\ns1−βˆv=Lxˆv−λEY\u0002\nˆv(s, x+Y)−ˆv(s, x)\u0003\n+s−βv(0, x).\nMellin transform in x.SetM{f}(z) :=R∞\n−∞e−zxf(x)dx. For ˆ v(s, x) this yields\ns1−β˜v(s, z) =\u00021\n2σ2\n0(z2+z)+σ0σHz2s−β+(r−λκ)z\u0003\n˜v(s, z)−λ(ΦY(z)−1)˜v(s, z)+s−β˜v0(z),\nwhere Φ Y(z) =E[e−zY] is the bilateral Laplace transform of jump sizes. Solving for\n˜v(s, z) we arrive at\n˜v(s, z) =s−β˜v0(z)\ns1−β−1\n2σ2\n0(z2+z)−σ0σHz2s−β−(r−λκ)z+λ(ΦY(z)−1).\n5.2 Analytic inversion\nFor log–normally distributed jumps Y∼ N(µY, σ2\nY) we have Φ Y(z) = exp {µYz+1\n2σ2\nYz2}.\nChoosing the Esscher parameter η∗such that κ= 0 simplifies the denominator to a\nquadratic function in zplus a fractional power in s. After algebraic manipulation we\nobtain\n˜v(s, z) =s−β ˜v0(z)\ns1−β+az2+bz+c, a =1\n2σ2\n0+σ0σHs−β, b=1\n2σ2\n0+r, c=λ\u0002\n1−ΦY(z)\u0003\n.\n12\n\nThe denominator factorises and its inverse Laplace transform is the two–parameter Mit-\ntag–Leffler function Eβ,1; subsequently the Mellin inversion follows residue calculus similar\nto [8].\nTheorem 5.1 (Closed–form price) .For a European call with strike Kand maturity T\nthe time– 0price satisfies\nC(S0,0) = S0M−1\nz→x\u0002\nϕ(z)Mβ(az2)\u0003\n−Ke−rTM−1\nz→x\u0002\nϕ(z−1)Mβ(a(z−1)2)\u0003\n,\nwhere x= log( S0/K),ϕ(z) = exp\u0002\n−1\n2σ2\n0Tz2+ (r−1\n2σ2\n0)Tz\u0003\nandMβ(ξ) :=Eβ,1(−ξTβ)is\nthe Mittag–Leffler kernel.\nProof. We sketch the main steps; full details appear in Section 5.\nStep 1 (Laplace inversion). Invert the Laplace transform using L−1\ns→τ{(s1−β+q)−1}=\nτβ−1Eβ,β(−qτβ).Substituting q=az2+bz+cyields v(τ, z) =τβ−1Eβ,β\u0002\n−(az2+bz+\nc)τβ\u0003\n˜v0(z).\nStep 2 (Mellin inversion). Because v0(x) =Kmax( ex−1,0) its Mellin transform\nis ˜v0(z) =K\u0002\nΓ(z−1)−Γ(z−1,1)\u0003\n,analytic in ℜ(z)∈(0,1). Closing the contour to the\nright and summing residues at the poles z= 0,−1,−2, . . .reproduces two inverse Mellin\nintegrals weighted by ϕ(z) and Mβ(az2), completing the formula.\nStep 3 (convergence). Uniform convergence of the integrals follows from the Eβ,1\nasymptotics |Eβ,1(−ξ)| ≤C/(1 + ξ) and standard Mellin–Barnes bounds, ensuring the\nprice is finite for any T >0.\n5.3 Numerical evaluation\nBoth Mellin inversions are computed via a Talbot contour with 16 nodes; the Mittag–\nLeffler function is evaluated using the modified Lagrange algorithm with absolute error\nbelow 10−8. Table 2 in Section 6 confirms consistency with finite–difference prices.\n5.4 Remarks\n•The formula reduces to Black–Scholes when β→1 and λ→0, in which case\nMβ(ξ)→e−ξ.\n•For puts, exchange the roles of S0andKvia the usual put–call parity.\n•The methodology extends to tempering of the long–memory kernel by replacing\nEβ,1with the three–parameter Prabhakar function.\n6 Numerical Scheme for Barrier Options\nAmerican–style and path–dependent derivatives such as down–and–out calls cannot ex-\nploit the closed–form solution of Section 5; we therefore construct a robust finite–difference\nmethod for the integro–fractional PDE (4). The spatial domain (0 , Smax) is truncated\nat a sufficiently large Smaxand transformed to the log–price grid xi=xmin+i∆xwith\ni= 0, . . . , I ; the time interval is partitioned as τn=n∆t,n= 0, . . . , N .\n13\n\n6.1 Implicit Gr¨ unwald–Letnikov discretisation\nDefine Vn\ni≈V(τn, xi). The Riemann–Liouville derivative is approximated by the back-\nward Gr¨ unwald–Letnikov series\n0D1−β\nτV(τn, xi)≈1\n∆t1−βnX\nk=0ω(1−β)\nkVn−k\ni, ω(γ)\nk:= (−1)k\u0012γ\nk\u0013\n.\nSpatial derivatives are approximated by centered differences ∂xV≈δxVn\ni:= (Vn\ni+1−\nVn\ni−1)/(2∆x) and ∂xxV≈δxxVn\ni:= (Vn\ni+1−2Vn\ni+Vn\ni−1)/(∆x)2.The non–local jump term\nJV:=EY[V(τ, x+Y)−V(τ, x)] is evaluated by Gauss–Hermite quadrature on the\ntransformed grid; linear interpolation is used if xi+Ylies between nodes.\nCollecting terms yields the fully–implicit update\n1\n∆t1−βnX\nk=0ω(1−β)\nkVn−k\ni=1\n2σ2\n0δxxVn\ni+σ0σHδxh1\n∆tHnX\nk=0ω(H)\nkVn−k\nii\n+ (r−λκ)δxVn\ni−rVn\ni+λJh[Vn]i, (6)\nwhere Jhdenotes the quadrature interpolation operator.\nThe pseudocode for the Grunwald–Letnikov algorithm is presented below for a clearer\nunderstanding of its numerical implementation\nAlgorithm 1: Implicit Gr¨ unwald–Letnikov solver for barrier options\nInput: grid ( xi, τn), payoff Φ, barrier B, parameters σ0, σH, H, λ\nOutput: option prices V0\niatτ= 0\n1fori= 0, . . . , I do\n2 VN\ni←Φ(xi); // terminal condition\n3end\n4forn=N−1to0do\n5 fori= 1toI−1do\n6 assemble RHS using fractional convolutions ω(γ)\nk;\n7 end\n8 solve tridiagonal+Toeplitz system A Vn=bn(BiCG–STAB);\n9 apply barrier: ifSi< Bthen\n10 Vn\ni←0\n11 end\n12end\n13return V0\n6.2 Boundary and barrier conditions\nFor a down–and–out call with barrier B < K < S maxwe impose V(τ, x) = 0 if S=\nKex≤B, V (τ, xmax) =Smax−Ke−rτ,andV(0, x) = max( K(ex−1),0). These\ntranslate into Dirichlet conditions for Vn\n0andVn\nI, updated each timestep.\n6.3 Matrix form\nEquation (6) can be written A Vn=bnwith a tri–diagonal diffusion matrix plus a\ndense Toeplitz–like fractional matrix determined by {ω(γ)\nk}. The system is solved by\n14\n\nthe preconditioned BiCG–STAB method; the cost per step is O(IlogI) owing to an\nFFT–accelerated convolution for the fractional weights.\n6.4 Stability and convergence analysis\nLetwi= 1 + e2xiand define the discrete inner product ⟨u, v⟩h=PI−1\ni=1wiuivi∆xwith\ninduced norm ∥ · ∥ h.\nTheorem 6.1 (Unconditional stability and convergence) .If∆t≤c0(∆x)2with c0<\n1\n2σ2\n0, the implicit scheme (6)is unconditionally L2\nw–stable and the numerical solution Vn\ni\nconverges to the mild solution of (4)with global error\nmax\n0≤n≤N∥V(τn,·)−Vn∥h≤C\u0000\n(∆x)2+ ∆t1+H\u0001\n,\nwhere Cis independent of ∆tand∆x.\nProof. Step 1 (Energy identity). Multiply (6) by wiVn\ni∆xand sum over ito obtain\n1\n∆t1−βnX\nk=0ω(1−β)\nk\u0000\n∥Vn−k∥2\nh− ∥Vn−k−1∥2\nh\u0001\n=−σ2\n0∥δxVn∥2\nh+Rn,\nwhere Rncollects mixed and jump terms. Jensen’s inequality and Young’s convolution\ninequality yield |Rn| ≤ε∥δxVn∥2\nh+Cε∥Vn∥2\nh. Choosing ε < σ2\n0and applying the discrete\nGr¨ onwall lemma proves ∥Vn∥h≤ ∥V0∥hfor all n(unconditional stability).\nStep 2 (Consistency). A Taylor expansion shows that the truncation error τn\niof (6)\nsatisfies |τn\ni| ≤C\u0000\n(∆x)2+ ∆t1+H\u0001\n.\nStep 3 (Convergence). LetEn\ni=V(τn, xi)−Vn\nibe the error. The discrete equation for\nEnhas identical coefficients as (6) with forcing term τn. Repeating the energy argument\nand summing the geometric series of fractional weights yields\n∥En∥h≤C\u0000\n(∆x)2+ ∆t1+H\u0001\n,\nwhich proves the stated order.\n6.5 Implementation details\n•A non–uniform grid clustered near the barrier improves accuracy; we employ 400\nnodes with geometric spacing xi+1−xi=q(xi−xi−1),q= 0.97.\n•The fractional weights ω(γ)\nkare pre–computed once with high–precision arithmetic\nand stored.\n•For calibration we match model prices to market quotes via a least–squares routine\nthat leverages the linearity of the scheme with respect to σ0andσH.\n6.6 Numerical experiment\nWe consider a down–and–out European call with strike K= 4,200 and barrier B= 3,800\non an index with spot S0= 4,050. The contractual maturity is T= 0.5 years, risk–free\nrater= 2%, and dividend yield zero. Model parameters are taken from the calibration\nin Section 7: σ0= 0.14,H= 0.35,σH= 0.10,λ= 0.85 and log–normal jump sizes\nY∼ N(−4%,11%2).\n15\n\nGrid specification. The spatial grid is truncated at Smax= 8 000 and mapped onto\nx∈[log(B),log(Smax)] with I= 400 nodes using a geometric refinement factor q= 0.97\nnear the barrier. Time is discretised with ∆ t= 5×10−4resulting in N= 1 000 steps,\nwhich satisfies the stability restriction of Theorem 6.1 with c0= 1/(4σ2\n0).\nMonte–Carlo benchmark. We simulate 106trajectories of the smfBm–J process us-\ning: (i) Cholesky factorisation for correlated ( W, SH) increments on a 2 000–point fine\ngrid followed by Brownian bridge refinement, and (ii) Poisson thinning for jumps. Con-\ntrol–variates are applied by subtracting the analytic price of the same barrier option\nunder the Merton model ( σH= 0) and adding it back as a constant ([13]). The resulting\nstandard error is below 5 ×10−3.\nResults. Table 1 reports the option values and relative errors. CPU time refers to a\nsingle core of an Intel i9–13900K.\nTable 1: Down–and–out call: finite–difference vs. Monte–Carlo\nMethod Price Rel. error (%) CPU (s)\nGr¨ unwald–Letnikov (∆ x= 0.012) 131.42 0.21 1.7\nGr¨ unwald–Letnikov (∆ x= 0.008) 131.09 0.00 3.9\nMonte–Carlo (106) 131.10 – 82.0\nConvergence verification. Figure 1 plots the log–log error ∥V∆t,∆x−VMC∥∞versus\n∆tfor ∆ x= (∆ t)1/2. A linear regression yields slope 1 .34≈1 +Hconsistent with\nTheorem 6.1.\nFigure 1: Convergence rate of GL scheme\n16\n\n7 Emprical Experiments\nWe calibrate the model to weekly closing levels of the S&P 500 index and corresponding\noption implied volatilities between January 2015 and December 2024. The classical least–\nsquares objective min ΘPM\nj=1\u0000\nCmodel(Θ;Kj, Tj)−Cmkt\nj\u00012is solved with parameter vector\nΘ = ( σ0, σH, H, λ, µ Y, σY);M= 620 option quotes are used after filtering for moneyness\n0.8<S0/K < 1.2 and maturities below 1 year.\nThe global optimum found via differential evolution is\nσ0= 0.14, σ H= 0.10, H = 0.35, λ = 0.85, µ Y=−4%, σ Y= 11% .\nThe root–mean–square percentage error is 1 .9% versus 5 .4% for the classical Merton\nmodel.\nTable 2: European Call Prices ( T= 0.5 yr) under calibrated parameters\nStrike KBlack–Scholes smfBm–J Rel. Diff.%\n3 800 524.9 530.8 1.1\n4 200 326.7 334.2 2.3\n4 600 158.4 173.4 9.5\n5 000 57.6 66.4 15.3\nTable 2 illustrates that ignoring long–memory and jumps leads to under–pricing of\ndeep out–of–the–money calls by more than 15%.\n7.1 RMSE surface\nFigure 2 maps the calibration error as a function of Handλ; the valley confirms the\nidentifiability of the two effects.\n17\n\nFigure 2: Calibration RMSE surface\n7.2 Sensitivity analysis\nGreeks via algorithmic differentiation. Once the option price V(t, S;θ) is obtained\non the finite–difference grid, we apply adjoint algorithmic differentiation (AAD) to the\nsolver’s residual map, which yields machine–precision values of all first– and second–order\nGreeks in a single reverse sweep [14]. In particular we extract\nDelta ∆ =∂V\n∂Sprice sensitivity to spot, hedging ratio,\nGamma Γ =∂2V\n∂S2curvature, affects rebalancing cost,\nVega ν0=∂V\n∂σ0,νH=∂V\n∂σHsensitivity to short–/long-memory volatilities,\nVanna V=∂2V\n∂S ∂σ 0cross–sensitivity driving skew dynamics.\nFixT= 0.5,r= 3%, σ0= 0.14,H= 0.35,σH= 0.10,λ= 0.85, and jump distribution\nY∼ N(−0.05,0.252). We compare three scenarios:\n1.Baseline: σH= 0,λ= 0 (Black–Scholes);\n2.Memory–only: σH>0,λ= 0 ( smfBm );\n3.Full model: σH>0,λ >0 (smfBm–J ).\n18\n\nTable 3: Selected Greeks for an at–the–money call ( S=K= 4200)\nScenario ∆ Γ ν0V(Vanna)\nBaseline (BS) 0.527 1 .15×10−4239.8 0.014\nMemory–only 0.525 1 .09×10−4268.6 0.013\nFull (smfBm–J) 0.522 0 .98×10−4268.9 0.019\nInterpretation.\n•Vega amplification. Introducing long–memory volatility ( σH>0) enlarges the\ntotal Vega by268.6−239.8\n239.8≈12%. Higher sensitivity to volatility shocks is intuitive:\npersistent variance fluctuations increase future uncertainty, which the option price\nmust reflect.\n•Vanna in the wings. Figure 3 plots Vanna versus moneyness S/K∈[0.8,1.2].\nJumps steepen the wings— for deep OTM strikes, Vgrows by 35–40 % relative to\nmemory–only. Empirically, index options display steeper skew (higher Vanna) in\ncrash regions, so adding jumps aligns the model with market data.\n•Gamma dampening. Both memory and jumps slightly decrease Γ, flattening\nthe replication cost profile—consistent with rough volatility models that “smooth”\ndelta curvature.\nFigure 3: Vanna across strikes. smfBm (dashed) vs. smfBm–J (solid).\nHedging implication. A desk hedging with only ∆ and classical Vega would system-\natically under-hedge volatility risk when memory is present, and misprice skew when\njumps dominate. Calibration therefore requires at least two orthogonal volatility factors\n(σ0, σH) plus a jump skew factor associated with V.\nOverall, the sensitivity analysis confirms that long-memory amplifies pure volatility\nrisk, while jumps govern cross–sensitivities that shape the smile/skew—insights useful for\nGreeks-based risk management and calibration.\n19\n\n8 Conclusion\nWe have developed a comprehensive framework for pricing fractal derivatives under sub–\nmixed fractional Brownian motion with jumps. Theoretical contributions include (i) a\nnew fractional Girsanov theorem with jump Esscher tilting, (ii) derivation of a fractal\nBlack–Scholes integro–PDE, (iii) a convergent Gr¨ unwald–Letnikov scheme of order 1+ H,\nand (iv) closed–form European prices via Mellin–Laplace transforms.\nManagerial insights. Calibration to S&P 500 options reveals that neglecting either\njumps or long–memory results in material mis–pricing of out–of–the–money options and\nbarrier derivatives. Risk metrics such as Vega and Vanna are strongly amplified, suggest-\ning higher hedging costs in rough–jump markets.\nFuture research. Extending the model to stochastic volatility in the fractional ker-\nnel, investigating American early exercise via fractional free–boundary problems, and\nembedding regime–switching jumps constitute promising directions.\nA Fractional Wick–Itˆ o–Skorokhod Integral\nThis appendix gives a self-contained construction of the F–WIS integral used for the sub-\nfractional Brownian component. Let (Ω ,F,P) carry an independent standard Brownian\nmotion Band define the sub-fractional Brownian motion\nSH\nt=Zt\n0KH(t, s)dBs, K H(t, s) =cH\u0002\n(t−s)H−1\n2−(−s)H−1\n2\u0003\n, H∈(0,1).\nCameron–Martin space. The Hilbert space HHis the closure of step functions under\nthe inner product ⟨1[0,t],1[0,s]⟩HH=cH(t2H+s2H− |t−s|2H).\nIsometry and divergence operator. Forφ∈L2([0, T]) set ( IHφ)(t) =Rt\n0KH(t, s)φ(s)ds.\nGiven an adapted process ϕ∈D1,2(Malliavin derivative in L2), the F–WIS integral is\nthe divergenceZT\n0ϕs⋄dSH\ns:=δH(ϕ) =X\nn≥0In+1\u0000eϕ(n)\u0001\n,\nwhere Indenotes the n-fold Wiener integral and eϕ(n)is the symmetrisation of ( I⊗n\nH∂sϕ).\nIsometry. Ifϕis adapted then\nEh\f\f\fZT\n0ϕs⋄dSH\ns\f\f\f2i\n=∥ϕ∥2\nL2([0,T]),\nenabling the stochastic Fubini proofs used in the main text.\nB Fractional Hille–Yosida Well-posedness\nWe prove existence and uniqueness of the mild solution presented in Section 4.4. Let V\nandAbe defined as there.\n20\n\nStep 1 (Sectoriality of A).Forv∈D(A) we have\n⟨v,Av⟩V= sup\nS>0v(S)\u00001\n2σ2\n0S2v′′(S) + (r−λκ)Sv′(S)−r v(S)\u0001\n1 +S2≤C∥v∥2\nV,\nsoAis sectorial and generates an analytic semigroup T(t) onV.\nStep 2 (Lipschitz jump operator). Set (Jv)(S) = λ(E[v(SeY)]−v(S)). Given\nE[eγ|Y|]<∞,∥Jv− Jw∥V≤LJ∥v−w∥V.\nStep 3 (Fractional abstract Cauchy problem). Write the pricing PDE as 0D1−β\ntV(t) =\nAV(t) +JV(t), V(T) = Φ .Apply [12][Prop. 2.4] to obtain a unique mild solution\nV(t) =Eβ\u0000\n−(T−t)βA\u0001\nΦ +ZT\nt(s−t)β−1Eβ,β\u0000\n−(s−t)βA\u0001\nJV(s)ds.\nStep 4 (Classical differentiability). Analyticity of T(t) implies V∈C1,2((0, T)×\nR+), completing the subsection 4.4.\nReferences\n[1] B. Mandelbrot and J. W. Van Ness. Fractional brownian motions, fractional noises\nand applications. SIAM Review , 10(4):422–437, 1968.\n[2] R. C. Merton. Option pricing when underlying stock returns are discontinuous.\nJournal of Financial Economics , 3(1–2):125–144, 1976.\n[3] L. Wang. European option pricing under sub-fractional brownian motion. Fractal\nand Fractional , 8(1):13, 2024.\n[4] M. Eini and H. Khodadadi. Pricing asian options under the mixed fractional brow-\nnian motion with jumps. Mathematics and Computers in Simulation , 226:172–183,\n2024.\n[5] C. Shen and C. Yue. Fractal barrier option pricing under sub-mixed fractional\nbrownian motion with jump processes. AIMS Mathematics , 9(11):1496–1519, 2024.\n[6] Y. Hu and B. Øksendal. Fractional white noise calculus and applications to finance.\nInfinite Dimensional Analysis, Quantum Probability and Related Topics , 6(1):1–32,\n2003.\n[7] T. Benson and X. Li. A generalized fractional calculus approach to financial mod-\nelling. Journal of Computational Finance , 27(2):45–78, 2023.\n[8] A. A. Kilbas, H. M. Srivastava, and J. J. Trujillo. Theory and Applications of\nFractional Differential Equations , volume 204 of North-Holland Mathematics Studies .\nElsevier, 2006.\n[9] W. Wyss. The fractional black–scholes equation. Fractional Calculus and Applied\nAnalysis , 3:51–61, 1986.\n21\n\n[10] O. El Euch and M. Rosenbaum. Rough fractional stochastic volatility models. Pro-\nceedings of the Royal Society A , 476:20200208, 2020.\n[11] A. Pazy. Semigroups of Linear Operators and Applications to Partial Differential\nEquations , volume 44 of Applied Mathematical Sciences . Springer-Verlag, New York,\n1983.\n[12] E. Bazhlekova. Fractional Evolution Equations in Banach Spaces . Ph.D. thesis,\nEindhoven University of Technology, Eindhoven, The Netherlands, 2001.\n[13] P. Glasserman. Monte Carlo Methods in Financial Engineering . Applications of\nMathematics 53. Springer, 2003.\n[14] Mike Giles. An extended collection of matrix derivative results for forward and\nreverse mode algorithmic differentiation. Oxford University Computing Laboratory ,\n2012. https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf .\n22\n\n",
    "source": "http://arxiv.org/abs/2506.24111v1",
    "authors": [
      "Nader Karimi"
    ],
    "categories": [
      "q-fin.PR"
    ],
    "type": "content"
  },
  {
    "id": "2506.24093v1_abstract",
    "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies",
    "content": "Title: Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies\n\nAbstract: Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.",
    "source": "http://arxiv.org/abs/2506.24093v1",
    "authors": [
      "Paul Wachter",
      "Lukas Niehaus",
      "Julius Schöning"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1; I.2.0; F.2.3"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24093v1_content",
    "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies",
    "content": "arXiv:2506.24093v1  [cs.LG]  30 Jun 2025Development of Hybrid Artificial Intelligence\nTraining on Real and Synthetic Data\nBenchmark on Two Mixed Training Strategies\nPaul Wachter1[0000−0002−6224−6140], Lukas Niehaus2[0009−0009−9978−6851], and\nJulius Schöning1[0000−0003−4921−5179]\n1Faculty of Engineering and Computer Science, Osnabrück University of Applied\nSciences, Osnabrueck, Germany\n{p.wachter,j.schoening}@hs-osnabrueck.de\n2Institute of Cognitive Science, Osnabrück University, Osnabrueck, Germany\nluniehaus@uni-osnabrueck.de\nAbstract. Synthetic data has emerged as a cost-effective alternative to\nreal data for training artificial neural networks (ANN). However, the dis-\nparity between synthetic and real data results in a domain gap . That gap\nleads to poor performance and generalization of the trained ANN when\napplied to real-world scenarios. Several strategies have been developed to\nbridge this gap, which combine synthetic and real data, known as mixed\ntrainingusinghybriddatasets.Whilethesestrategieshavebeenshownto\nmitigate the domain gap, a systematic evaluation of their generalizability\nand robustness across various tasks and architectures remains underex-\nplored. To address this challenge, our study comprehensively analyzes\ntwo widely used mixing strategies on three prevalent architectures and\nthree distinct hybrid datasets. From these datasets, we sample subsets\nwith varying proportions of synthetic to real data to investigate the im-\npact of synthetic and real components. The findings of this paper provide\nvaluable insights into optimizing the use of synthetic data in the training\nprocess of any ANN, contributing to enhancing robustness and efficacy\ncf.a\nKeywords: Hybrid Datasets ·Mixed Training ·Artificial Intelligence ·\nBenchmark ·Synthetic Data ·Domain Gap ·Reality Gap\n1 Introduction\nSynthetic data has recently gained considerable attention as a cost-effective way\nto generate training data [14] for artificial neural networks (ANN). Synthetic\ndata,definedasinformationnotderivedfromreal-worldsources,inherentlylacks\nrealism.Thisabsencecreatesadiscrepancybetweensyntheticandrealexamples,\nwhich can be termed the reality gap [25]. The reality gap is a subset of the\naSupplementary code and results: https://hs-osnabrueck.de/prof-dr-julius-\nschoening/ki2025\n\n2 P. Wachter et al.\nbroaderdomain gap , which limits the exclusive use of synthetic data in ANN\ntraining since the target domain usually is the real-world. As a result, applying\nANNs,whicharesolelytrainedonsyntheticdata,toreal-worldscenariosrequires\ndomain adaptation to transfer knowledge from one domain to the other [17].\nOne key area of domain adaptation research focuses on enhancing the qual-\nity and diversity of synthetic data to bridge the domain gap [14]. This can be\nachieved by increasing the realism of synthetic data either during its generation\nor through post-processing techniques. Enhancing realism during generation re-\nquires an advanced simulation framework that accurately models real-world con-\nditions [10]. Such frameworks often prove costly and labor-intensive [28]. Alter-\nnatively, image transformation methods, such as neural style transfer, improve\nrealism in post-processing [6], although these methods often suffer from training\ninstabilities and may introduce artifacts [32].\nAnother research focus targets the architecture of ANNs, aiming to enable\nthem to learn domain-invariant features, thereby reducing the domain gap [30].\nHowever, designing such architectures is inherently challenging because they\nmust effectively abstract away domain-specific details to maintain generality.\nMoreover, many of these architectures are developed for multi-domain adap-\ntation [16], including domains with no available data. These assumptions may\nlimit their effectiveness in addressing the specific reality gap, where real data is\nusually accessible.\nConsequently, this paper examines a mixed training approach for domain\nadaptation,whichintegratessyntheticandrealdataintoaunifiedhybriddataset.\nByintegratingbothtypesintoahybridset,mixedtrainingallowsmodelstolearn\nfeatures from synthetic part, while real-world data introduces authentic visual\nand contextual variations, thereby mitigating the lack of realism in the syn-\nthetic data and bridging the domain gap. This approach can be used with every\nANN architecture, thus making it broadly applicable, while omitting the need\nfor highly specialized ANNs when relaying on synthetic sources. Mixed training\nprominently follows two strategies: simple mixed (SM), where both data types\nare used simultaneously, and sequentially fine-tuned (FT), in which ANNs are\npretrained on synthetic data and afterwards fine-tuned with real data. Although\nthese methods are widely applied, their underlying mechanisms and performance\nunder varying conditions remains only partially understood.\nIn response to these limitations, this study systematically evaluates both\nmixed training strategies across three structurally distinct datasets, varying ra-\ntios of synthetic-to-real data and three different ANN architectures on image\nclassification tasks. Unlike previous studies that boost model performance by\nexpanding the hybrid dataset with additional synthetic examples, our approach\nmaintains a constant dataset size while gradually increasing the synthetic-to-real\nproportion. Thus, our goal is not limited to improving the overall performance,\nbut to conduct a precise assessment of each strategies efficacy under varying con-\nditions. This design offers actionable insights to guide the practical application\nof synthetic data in real-world settings.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 3\n2 Related Work\nAlthough many publications have successfully applied mixed training to reduce\nthe domain gap [9,23,21,19], most studies concentrate on the generation of syn-\ntheticdataratherthanexaminingthemixedtrainingstrategiesthemselves[7,29].\nNowruzi et al. [15] evaluated an object detector for cars and pedestrians us-\ning three synthetic and three real datasets. They created hybrid training sets\nby combining synthetic data with four different, low ratios of real data. More-\nover, they compared the SM strategy with the FT approach for their tasks and\nconcluded that FT more effectively reduces the domain gap. However, the study\ndid not consider the effects of varying dataset sizes and class distributions. In\ncontrast, Burdorf et al. [3,18] did not confirm the advantages of FT for a very\nsimilar task as reported by Nowruzi et al. [15].\nIn [26], Vanherle et al. compared the SM and FT strategies across subsets\nwith the same overall size but different synthetic-to-real ratios; notably, the\nsubsets were imbalanced. They employ the DIMO dataset [4], which contains\nreal and synthetic images, to train and test a Mask-Region Based Convolutional\nNeural Networks (Mask R-CNN) for object detection. Although the FT strategy\nyieldedbetterperformance,thestudyinvolvedpretrainingontheCOCOdataset\nbefore introducing synthetic examples. Moreover, synthetic data was used solely\nto train the heads of the pretrained model. These methodological choices impact\nnetwork performance [12] and may interact with the mixed training strategies,\npotentially confounding true differences.\nTogether, these studies leave several questions unresolved. Reported gains\ncould be due to uncontrolled factors, like dataset size, class imbalance, or pre-\nraining on large scale datasets with freezing of layers, that obscure the true effect\nof the mixing strategy. Most experiments rely on a single hybrid dataset and one\nANN, making it unclear whether conclusions generalize across synthetic data\ntypes or ANN architectures. A more controlled, cross-dataset, cross-architecture\nanalysis is therefore required, which we address in this study.\n3 Methodology\nThe complete code, configurations and numerical results of our study can be\nfound at 1. In general, mixed training using real and synthetic data can be ap-\nplied to any neural network and hybrid dataset, regardless of the architecture,\nsynthetic data type or synthetic-to-real ratio. Consequently, this study covers a\nbroad range of possible configurations of these key factors to evaluate their influ-\nence on the two training strategies SM and FT. Our study is designed to expose\nand isolate the effects on model performance, instead of aiming for maximum\naccuracy in each configuration. Accordingly, we minimize other factors which\ncan influence the training process, such as advanced optimization algorithms,\nregularization heuristics, data augmentation or hyperparameter tuning. In the\nfollowing, we lay out the choices of our ANN architectures, hybrid datasets and\nsynthetic-to-real proportions, as well as the implementation of the SM and FT\ntraining strategies.\n\n4 P. Wachter et al.\n3.1 Application Task\nWe choose image classification as the application task of our study. Image clas-\nsification is considered one of the most fundamental tasks within the domain\nof AI-based and classical computer vision [22]. It was selected, because it’s sig-\nnificance extends beyond mere categorization: more intricate tasks, like object\ndetection, semantic segmentation, and image generation, are derived from the\nsame principles [24]. This allows to assume a similar behavior of the mixed train-\ning strategies on a broad range of tasks in computer vision.\n3.2 Artificial Neural Networks\nAsnotedbefore,mixedtrainingcanbeusedirrespectiveoftheANNarchitecture.\nTherefor, we decided to assess the impact of three commonly used architectures\nin AI-based computer vision. Namely, the multilayer perceptron (MLP) [22], the\nconvolutional neural network (CNN) [8], and the vision transformer (ViT) [5].\nEach of these ANNs is implemented in the original form, without additional\nmechanisms,likenormalizationorregularizationlayers.Thisreducesthenumber\nof confounding variables and allows to focus on the respective ANN’s core mech-\nanism. However, normalization layers were incorporated into the transformer\nblocksandthepatchencoderoftheViT,giventheircriticalroleinpreservingthe\nfundamental functionality and performance of transformer-based networks [31].\nThe exclusion of these layers would constitute a deviation from the fundamental\ndesign principles of transformers [27]. The general design choices, such as the\nwidth of the layers, the activation functions, the initialization, etc., reflect com-\nmon practices. A detailed overview of the network configurations can be found\nin 1.\n3.3 Datasets\nSynthetic data can be generated through several distinct mechanisms [14]. For\nthis comprehensive study on mixed training strategies, we curated three hybrid\ndatasets, each created using a different synthetic data generation mechanism.\nThese are generative Ai (GenAI), computer aided design (CAD) and hand draw-\nings, which results in distinct structures and characteristics of each dataset.\nCifar-10 and CiFake comprise the first dataset in which, as illustrated in\nFig. 1 (a) and (d), Cifar-10 [11] represents the real and CiFake [1] the synthetic\npart. Cifar-10 and CiFake both comprise 60,000 32 ×32RGB images across ten\nobject classes, each with 6,000images. The CiFake dataset is generated using\ntheCompVis Stable Diffusion model [20], an open-source latent diffusion model.\nAdditional prompt modifiers enhanced the diversity, while still producing images\nthat closely resemble Cifar-10’s characteristics. Importantly, 668duplicate image\npairs were found in the CiFake dataset, and one from each was removed to ensure\ndataset integrity.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 5\n(a) Cifar-10 (real)\n (b) Lego (real)\n (c) DomainNet (real)\n(d) CiFake (synth.)\n (e) Lego (synth.)\n (f) DomainNet (synth.)\nFig.1: Real and synthetic example images of the datasets.\nLEGOBricks [2]fortrainingclassificationnetworkswasselectedasthesecond\ndataset. It’s real part is shown in Fig. 1 (b), while the synthetic images, shown\nin Fig. 1 (e) were generated using the computer-aided design (CAD) program\nLDraw [13]. The synthetic images introduce additional colors and angles, but\nstill try to mimic the real images accurately. The dataset features 447 unique\nLEGO brick classes with a total of 50,000 real and 560,000 synthetic images.\nThe classes are identified by the official LEGO IDs based on shape, regardless of\ncolor or decorations. All images are RGB with varying sizes, and both datasets\nare highly, yet differently, unbalanced, with class samples ranging from 6to over\n600.\nDomainNet [16] is the third dataset used in our study. In it’s original form,\nit consists of six domains: Clipart, Infograph, Painting, Quickdraw, Real, and\nSketch, with a total of 596,000samples across 345categories. We selected the\nReal domain as our real, and the Quickdraw domain as our synthetic data, as\nvisualized in Fig. 1 (c) and (f). The former consists of roughly 176,000RGB\nimages of varying sizes and class distributions, while the latter is composed of\n500hand-drawn, black-and-white images per class, each with a size of 300×300.\nThis provides a third, distinct hybrid dataset compared to CiFake (GenAI) and\nLegoBricks (CAD), because it’s synthetic part is not designed to be as similar\nto the real part as possible.\n\n6 P. Wachter et al.\n3.4 Dataset Creation\nTo examine the effect of synthetic-to-real image ratios, we generated 11 equally\nsized subsamples from each of the three datasets introduced in Section 3.3. For\nevery original dataset we produced one subset made entirely of real images, one\nmade entirely of synthetic images, and nine hybrid subsets whose composition\nranges from 90 % synthetic / 10 % real to 10 % synthetic / 90 % real in 10-\npercentage-point steps. To eliminate potential confounding factors, we matched\nthe class distribution across all subsets. For the nine hybrid subsets, the same\nclass balance was enforced separately within the real and synthetic parts. The 11\nsynthetic-to-real ratios on the three original datasets resulted in 33subsampled\ndatasets for this study, with the attributes summarized in Table 1.\nFurthermore, the images were rescaled to the same spatial dimensions, to\nmatch the ANN’s fixed input sizes. No rescaling was necessary for Cifar-10 and\nCiFake, since all images have the shape 32×32. The LegoBricks images were\nrescaled to 256×256, and all DomainNet images to 300×300. Additionally,\nthe grayscale images from the synthetic part of the DomainNet dataset were\nconverted into the 3-channel RGB format, for the same reason. Lastly, all images\nwerenormalizedtoliewithintherange [0,1],inordertoensureconsistencyacross\nall RGB channels and image sources.\nTable 1: Attribute overview of the (hybrid) datasets\nName # Images # Classes Image Dimensions Synth.\nSource\nCifar-10 [11]/Cifake [1] 55,000 10 32×32×3GenAI\nLegoBricks [2] 20,100 134 256×256×3CAD\nDomainNet [16] 30,000 60 300×300×3Drawing\n3.5 Mixed Training Strategies\nTwo mixed training strategies are compared in this study, which we termed\nsimple mixed (SM)and fine-tuned (FT).Theformertreatsthesyntheticandreal\npart entirely equivalent. It does not not distinguish between synthetic and real\ndata during training; data from both domains are utilized in the same manner,\nas if they were part of a single dataset. The inputs and their corresponding labels\nare sampled randomly from both datasets, regardless of their size or distribution.\nThis provides the ANN with a broader range of examples, increasing its ability\nto adapt to various scenarios and improving its robustness.\nConversely, the FT strategy utilizes the real and synthetic parts sequentially.\nOnly the synthetic part is used to train the network in the first step. This\npretraining is stopped when there is no more improvement on the real evaluation\nset, implemented through validation-based early stopping. In the second step,\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 7\nthe real part is used to retrain the network obtained after step one. The FT\nstrategy acknowledges the differences between data types and their relation to\nthe real test data, and adjusts the training process accordingly.\n3.6 Training Procedure\nAll models were training using vanilla Stochastic Gradient Descent (SGD) with\na learning rate of 0.01and mini-batches of 64samples. Datasets were split\n60% /20% /20%into training, validation, and test sets. The validation and test\nsets are sampled exclusively from the real part of the dataset and remained\nunchanged. In both SM and FT training strategies, the MLP, CNN and ViT\nwere trained for 100epochs, ensuring convergence. After training, the model\nwas restored to the state where it showed the highest validation accuracy. For\nthe FT strategy, the first training step proceeded until the validation accuracy\ndid not improve for 10 epochs; at that moment, early stopping triggered, and\nthe weights from the best-performing model were preserved. In the second step,\nthe network was trained for the remaining number of epochs. All parameters\nremained unchanged throughout this procedure.\nTo summarize, we trained three neural network architectures, each with two\ntraining strategies, on 27 hybrid datasets and six non-hybrid datasets (all-real\nor all-synthetic). One full training cycle therefore produced 162 models trained\non hybrid data and 18 models trained on non-hybrid data, yielding 180 trained\nnetworks in total. To obtain statistically robust results, we repeated this cycle\nten times, resulting in 1,800 trained networks overall. At the beginning of each\ntraining cycle, all 33 datasets were resampled.\n4 Results\nThe two baselines that guide the evaluation of the mixed training strategies\nare the purely synthetic and purely real dataset settings. While the former pro-\nvides information about the quality of the synthetic data, the latter shows the\nperformance when no domain gap exists between the training and test data.\nTable 2 reports the test accuracy of the baselines, averaged over ten repetitions.\nAll purely synthetic setups achieve performance above chance level. This result\nindicates their applicability as a proxy for the real-world, although a large gap\nto the performance of the purely real setting is evident, highlighting the domain\ngap. The domain gap can be quantified as the difference between the purely real\nand purely synthetic results. The goal of mixed training strategies is to mini-\nmize this gap, which would result in a comparable performance to the purely\nreal setups.\nInthemixedtrainingsetting,thetwotrainingstrategies,SMandFT,havere-\nsulted in divergent performances. Across all combinations of datasets, synthetic-\nto-real proportions, and ANN architectures, FT has outperformed SM in 635\nout of 810cases; when averaged over the ten repetitions, in 69out of 81.\n\n8 P. Wachter et al.\nTable 2: MLP, CNN, and ViT average test accuracy without mixing (0.0=100%\nreal, 1.0=100% synthetic) averaged over ten runs.\nCifar-10/CiFake LegoBricks DomainNet\nReal Synthetic Real Synthetic Real Synthetic\nMLP 0.5090 0 .1328 0.6699 0 .1423 0.2064 0 .0244\nCNN 0.6413 0 .1430 0.5582 0 .0130 0.2870 0 .0353\nViT 0.5165 0 .1255 0 .6296 0 .0125 0 .2660 0 .0345\nThe synthetic-to-real ratio of the train data strongly influences the perfor-\nmance of the ANNs. This influence is two-fold: it impacts performance in general\nand determines the magnitude of the difference between the FT and SM strate-\ngies. The general impact on the performance was observed throughout all set-\ntings. Every increase in the real proportion leads to an improved performance.\nThis finding was expected since more real data in the training set leads to a\nreduction of the domain gap to the real test data. Notably, the most significant\nimprovement was observed for the first 10%increase. The improvement made\nthrough the next increments gradually decreased. In other words, even a small\namount of real data in the hybrid dataset leads to a drastic improvement in\nperformance, and the bigger the proportion of real data in the dataset, the less\nimprovement is made by increasing the real data proportion further.\nIn addition to the general role of the synthetic-to-real ratio on the perfor-\nmance, it also determines the magnitude of the difference between the FT and\nSM strategies. Fig. 2 shows a ViT trained on the Cifar-10/Cifake dataset using\nboth strategies. Here, the difference between the two strategies increases parallel\nto the synthetic data proportion. In the setup with only 10% synthetic data,\nFT only marginally outperforms SM. With every increase of the synthetic data\nproportion, this difference increases as well, up to the point of 90% synthetic\ndata, where the FT strategy performs roughly twice as good as the SM strategy.\nWhat follows from this is that although the performance of the ANNs de-\ncrease with an increase of the synthetic proportion, this degradation of perfor-\nmance is less drastic for the FT strategy. Fig. 3 depicts the test accuracy gra-\ndients of the ViT trained on the Cifar-10/CiFake dataset with both strategies.\nThe gradient shows the rate at which the performance is changing in relation\nto the synthetic-to-real ratio. The gradient of the FT strategy is constantly less\nnegative compared to the SM strategy, highlighting the slower degradation of\nperformance. In other words, the higher the proportion of the synthetic data\nin the dataset, the bigger is the gain in performance of the FT over the SM\nstrategy. This tendency caused by the synthetic-to-real ratio was observed for\nall networks trained on the Cifar-10/CiFake and LegoBricks datasets, although\nless pronounced for the CNN.\nIn the setups that included the DomainNet dataset, FT did not clearly out-\nperform SM, as it was the case for Cifar-10/CiFake and LegoBricks. Here, the\narchitecture of the ANN played a crucial role. For the ViT, the FT strategy\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 9\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.150.200.250.300.350.400.450.50AccuracyDataset: cifake  |  Network: vit\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.2: ViT test accuracy on the Cifar-10/CiFake dataset (0.0=100% real,\n1.0=100% synthetic) averaged over ten runs shown as boxplots.\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nProportion   f synthetic data−0.08−0.07−0.06−0.05−0.04−0.03−0.02−0.010.00Gradient  f AccuracyDataset: cifake | Netw rk: vit\nfine -tuned\nsimple_mixed\nFig.3: Test accuracy gradients of the ViT trained on Cifar-10/CiFake datasets.\nThe proportion of 0.0represents 100%real, and 1.0represents 100%synthetic\ndata.\n\n10 P. Wachter et al.\nconsistently resulted in marginally better performances. In the setups, including\nthe MLP shown in Fig. 4, the FT strategy was better only for cases with a higher\nsynthetic proportion. The CNN consistently performed better when trained with\nthe SM strategy, as illustrated in Fig. 5. Notably, the more synthetic data is\npresent, the smaller is the advantage of the SM strategy.\nIn addition to this preference for the SM strategy, the CNN had a higher vari-\nation throughout the 10 repetitions. The interquartile range for the SM strategy\nshowed a high variation around the mean and the FT strategy showed signif-\nicant outliers. This was observed here, as well as in the case of CNN trained\non Cifar-10/Cifake and LegoBricks datasets. In Section A of the Appendix, the\nresults of all setups can be found.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.0250.0500.0750.1000.1250.1500.1750.200AccuracyDataset: domain  |  Network: mlp\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.4: MLP test accuracy on the DomainNet dataset (0.0=100% real, 1.0=100%\nsynthetic) averaged over ten runs shown as boxplots.\n5 Discussion\nThe results of our study indicate that FT is a preferable strategy for mixed\ntraining using real and synthetic data. Nevertheless, the CNN/DomainNet set-\nting constitutes a clear counter-example: here simultaneous exposure to both\ndomains (SM) yielded better performance. In this section we are going to elab-\norate our theories for the reasons leading to this outlier.\nA convolutional layer extracts localized patterns with learnable kernels; each\nkernel sees only its receptive field, a region whose size equals the kernel di-\nmensions. Stacking convolutional layers (with larger kernels, strides, or pooling)\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 11\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.050.100.150.200.250.30AccuracyDataset: domain  |  Network: cnn\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.5: CNN test accuracy on the DomainNet dataset (0.0=100% real, 1.0=100%\nsynthetic) averaged over ten runs shown as boxplots.\nexpands the receptive field, such that deeper layer can capture increasingly com-\nplex structures. This, however, relies entirely on the patterns captured by earlier\nlayers; if those first layers miss important structures, later layers have nothing\nto build on. High-quality, diverse features at the network’s outset are therefore\ncritical to the overall performance.\nDomainNet exhibits a larger domain gap compared to Cifar-10/CiFake and\nLegoBricks. The latter two contain colored, texture-matched synthetic images,\nmeticulously designed to mimic their real counterpart, while DomainNet’s syn-\nthetic subset consists solely of black-and-white sketches with only two pixel\nvalues, sharp edges and no texture (Fig. 3.3).\nPre-training a CNN on these simple, synthetic images biases its early layers\ntoward extreme black-white edges. Kernels aligned with these simple pattern re-\nceive large gradients during training, increasing a few weights rapidly, while sup-\npressing the rest. The result is comparable to Sobel or Prewitt kernels, whose few\nlarge coefficients emphasize a single-oriented edge, whereas kernels that would\nencode more fine grained patterns need many smaller, finely balanced coeffi-\ncients. Once dominated by such coarse detectors, the early layers fail to capture\nmore complex patterns, leaving deeper layers without the information necessary\nto form complex features. The resulting kernels make subsequent fine-tuning dif-\nficult: gradients become ill-conditioned and struggle to reshape the early layers\nonce real data are introduced in the fine-tuning step. In summary, the network\nis effectively trapped in a local minimum shaped by oversized and diminishing\nsmall weights.\n\n12 P. Wachter et al.\n6 Conclusion\nOur study reveals three findings that refine the current understanding of mixed\ntraining using real and synthetic data. First, the fine-tuning strategy usually,\nbut not always, outperforms the simple mixed strategy. Across 89 individual\nconfigurations FT surpassed SM in 69 ( 78 %), particularly when synthetic data\ncomprisedalargeportionofthedataset.Thisindicatesthatitismoreeffectivein\nutilizing real data to bridge the domain gap. Nevertheless, the CNNDomainNet\nsettingconstitutesaclearcounter-example.Thus,architecture-datainteractions,\nas outlined in 5, influence the strategies’ success.\nSecond, diminishing returns of real data. The largest performance gains oc-\ncurred with an initial 10% increase in real data; further increases yielded pro-\ngressively less. Practically, adding a modest subset of real data to a synthetic set\ncan markedly boosts performance, providing a possible cost-effective solution in\nmany cases.\nThird, Interaction of strategy with domain gap For the two synthetic sources\nthat explicitly mimic the real domain (GenAI and CAD), FT was consistently\nadvantageous. Conversely, hand-drawn sketches of the DomainNet dataset con-\nstitute large discrepancy to the real images, semantically as well as visually.\nUnder such conditions the SM strategy can outperform FT. This suggests that\nthe “optimal” strategy could be chosen based on a quantifiable domain gap mea-\nsure.\nFuture work should test mixed training on richer tasks, like object detection,\ninstance or semantic segmentation, and with production-grade networks that\nincludemodernregularizationandoptimizationmethods.Furthermore,itshould\ndeveloparobustdomain-gapmetricthatcombineslow-levelvisualstatisticswith\nhigh-level semantic information. A reliable score would enable the prediction\nof the required real-data fraction and select the appropriate mixing strategy,\nturning today’s trial-and-error process into a more systematic workflow.\nAcknowledgments. ThisworkispartoftheAgrifoodTEF-DEproject.AgrifoodTEF-\nDE is supported by funds of the Federal Ministry of Agriculture, Food and Regional\nIdentity (BMLEH) based on a decision of the Parliament of the Federal Republic of\nGermany via the Federal Office for Agriculture and Food (BLE) under the research\nand innovation program ‘Climate Protection in Agriculture’.\nThe computation of this research was done, using computing resources of the High-\nPerformance Computing (HPC) cluster of the Osnabrück University of Applied Sci-\nences, which were provided by the German Federal Ministry of Research, Technology\nand Space (BMFTR) within the HiPer4All@HSOS project.\nDisclosure of Interests. The authors have no competing interests to declare that\nare relevant to the content of this article.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 13\nA Appendix\nA.1 Results on Cifar-10/CiFake\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.150.200.250.300.350.400.450.50AccuracyDataset: cifake  |  Network: mlp\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.6: MLP test accuracy on the Cifar-10/Cifake dataset (0.0=100% real,\n1.0=100% synthetic) averaged over ten runs shown as boxplots.\n\n14 P. Wachter et al.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.10.20.30.40.50.6AccuracyDataset: cifake  |  Network: cnn\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.7: CNN test accuracy on the Cifar-10/Cifake dataset (0.0=100% real,\n1.0=100% synthetic) averaged over ten runs shown as boxplots.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.150.200.250.300.350.400.450.50AccuracyDataset: cifake  |  Network: vit\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.8: ViT test accuracy on the Cifar-10/Cifake dataset (0.0=100% real,\n1.0=100% synthetic) averaged over ten runs shown as boxplots.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 15\nA.2 Results on LegoBricks\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.00.10.20.30.40.50.60.7AccuracyDataset: lego  |  Network: mlp\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.9: MLP test accuracy on the LegoBricks dataset (0.0=100% real, 1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n\n16 P. Wachter et al.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.00.10.20.30.40.50.6AccuracyDataset: lego  |  Network: cnn\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.10:CNNtestaccuracyontheLegoBricksdataset(0.0=100%real,1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.00.10.20.30.40.50.6AccuracyDataset: lego  |  Network: vit\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.11: ViT test accuracy on the LegoBricks dataset (0.0=100% real, 1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 17\nA.3 Results on DomainNet\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.0250.0500.0750.1000.1250.1500.1750.200AccuracyDataset: domain  |  Network: mlp\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.12:MLPtestaccuracyontheDomainNetdataset(0.0=100%real,1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n\n18 P. Wachter et al.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.050.100.150.200.250.30AccuracyDataset: domain  |  Network: cnn\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.13:CNNtestaccuracyontheDomainNetdataset(0.0=100%real,1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nProportion  of synthetic data0.050.100.150.200.25AccuracyDataset: domain  |  Network: vit\nStrategy\nbaseline\nfine-tuned\nsimple_mixed\nFig.14: ViT test accuracy on the DomainNet dataset (0.0=100% real, 1.0=100%\nsynthetic) averaged over ten runs shown as a boxplot.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 19\nReferences\n1. Bird,J.J.,Lotfi,A.:Cifake:Imageclassificationandexplainableidentificationofai-\ngenerated synthetic images. IEEE Access 12, 15642–15650 (2024). https://doi.\norg/10.1109/access.2024.3356122\n2. Boiński, T., Zaraziński, S., Śledź, B.: Lego bricks for training classification network\n(2021). https://doi.org/10.34808/RCZA-JY08\n3. Burdorf, S., Plum, K., Hasenklever, D.: Reducing the amount of real world data for\nobject detector training with synthetic data (2022). https://doi.org/10.48550/\nARXIV.2202.00632\n4. De Roovere, P., Moonen, S., Michiels, N., Wyffels, F.: Dataset of industrial metal\nobjects (2022). https://doi.org/10.48550/ARXIV.2208.04052\n5. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby,\nN.: An image is worth 16x16 words: Transformers for image recognition at scale.\nIn: International Conference on Learning Representations (ICLR) (2021), https:\n//openreview.net/forum?id=YicbFdNTTy\n6. Ettedgui, S., Abu-Hussein, S., Giryes, R.: ProCST: Boosting semantic segmen-\ntation using progressive cyclic style-transfer (2022). https://doi.org/10.48550/\nARXIV.2204.11891\n7. Eversberg, L., Lambrecht, J.: Generating images with physics-based rendering for\nan industrial object detection task: Realism versus domain randomization. Sensors\n21(23), 7901 (Nov 2021). https://doi.org/10.3390/s21237901\n8. Fukushima, K.: Neocognitron: A self-organizing neural network model for a mech-\nanism of pattern recognition unaffected by shift in position. Biological Cybernetics\n36(4), 193–202 (Apr 1980). https://doi.org/10.1007/bf00344251\n9. Kim,J.,Kim,D.,Lee,S.,Chi,S.:Hybriddnntrainingusingbothsyntheticandreal\nconstruction images to overcome training data shortage. Automation in Construc-\ntion149, 104771 (May 2023). https://doi.org/10.1016/j.autcon.2023.104771\n10. Klein, J., Waller, R., Pirk, S., Pałubicki, W., Tester, M., Michels, D.L.: Synthetic\ndata at scale: a development model to efficiently leverage machine learning in\nagriculture. Frontiers in Plant Science 15(Sep 2024). https://doi.org/10.3389/\nfpls.2024.1360113\n11. Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny im-\nages. Tech. rep., University of Toronto, Toronto, Ontario (2009), https://www.cs.\ntoronto.edu/~kriz/learning-features-2009-TR.pdf\n12. Lambrecht, J., Kästner, L.: Towards the usage of synthetic data for marker-less\npose estimation of articulated robots in rgb images. In: 2019 19th International\nConference on Advanced Robotics (ICAR). pp. 240–247. IEEE (Dec 2019). https:\n//doi.org/10.1109/icar46387.2019.8981600\n13. LDraw.org: Content and licensing information (2020), https://www.ldraw.org/ ,\naccessed: 2025-04-24\n14. Nikolenko, S.I.: Synthetic Data for Deep Learning. Springer International Publish-\ning (2021). https://doi.org/10.1007/978-3-030-75178-4\n15. Nowruzi,F.E.,Kapoor,P.,Kolhatkar,D.,Hassanat,F.A.,Laganiere,R.,Rebut,J.:\nHow much real data do we actually need: Analyzing object detection performance\nusing synthetic and real data (2019). https://doi.org/10.48550/ARXIV.1907.\n07061\n16. Peng, X., Bai, Q., Xia, X., Huang, Z., Saenko, K., Wang, B.: Moment matching\nfor multi-source domain adaptation. In: 2019 IEEE/CVF International Conference\n\n20 P. Wachter et al.\non Computer Vision (ICCV). pp. 1406–1415. IEEE (Oct 2019). https://doi.org/\n10.1109/iccv.2019.00149\n17. Peng, X., Usman, B., Kaushik, N., Wang, D., Hoffman, J., Saenko, K.: Visda: A\nsynthetic-to-real benchmark for visual domain adaptation. In: 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW).\npp. 2102–21025. IEEE (Jun 2018). https://doi.org/10.1109/cvprw.2018.00271\n18. Poucin, F., Kraus, A., Simon, M.: Boosting instance segmentation with synthetic\ndata: A study to overcome the limits of real world data sets. In: 2021 IEEE/CVF\nInternational Conference on Computer Vision Workshops (ICCVW). pp. 945–953.\nIEEE (Oct 2021). https://doi.org/10.1109/iccvw54120.2021.00110\n19. Rajpura, P.S., Bojinov, H., Hegde, R.S.: Object detection using deep cnns trained\non synthetic images (2017). https://doi.org/10.48550/ARXIV.1706.06782\n20. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: 2022 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). pp. 10674–10685. IEEE (Jun\n2022). https://doi.org/10.1109/cvpr52688.2022.01042\n21. Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M.: The synthia\ndataset: A large collection of synthetic images for semantic segmentation of urban\nscenes. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR). IEEE (Jun 2016). https://doi.org/10.1109/cvpr.2016.352\n22. Schmidhuber, J.: Annotated history of modern ai and deep learning (2022). https:\n//doi.org/10.48550/ARXIV.2212.11279\n23. Staniszewski, M., Kempski, A., Marczyk, M., Socha, M., Foszner, P., Cebula, M.,\nLabus, A., Cogiel, M., Golba, D.: Searching for the ideal recipe for preparing syn-\nthetic data in the multi-object detection problem. Applied Sciences 15(1), 354\n(Jan 2025). https://doi.org/10.3390/app15010354\n24. Szeliski,R.:ComputerVision:AlgorithmsandApplications.SpringerInternational\nPublishing, 2nd edn. (2022). https://doi.org/10.1007/978-3-030-34372-9\n25. Tremblay, J., Prakash, A., Acuna, D., Brophy, M., Jampani, V., Anil, C., To, T.,\nCameracci, E., Boochoon, S., Birchfield, S.: Training deep networks with synthetic\ndata: Bridging the reality gap by domain randomization. In: 2018 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW).\npp. 1082–10828. IEEE (Jun 2018). https://doi.org/10.1109/cvprw.2018.00143\n26. Vanherle, B., Moonen, S., Reeth, F.V., Michiels, N.: Analysis of training object\ndetection models with synthetic data. In: 33rd British Machine Vision Confer-\nence 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press (2022),\nhttps://bmvc2022.mpi-inf.mpg.de/0833.pdf\n27. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N.,\nKaiser, L.u., Polosukhin, I.: Attention is all you need. In: Guyon, I., Luxburg,\nU.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.)\nAdvances in Neural Information Processing Systems. vol. 30. Curran Associates,\nInc. (2017), https://proceedings.neurips.cc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n28. Vuletic, J., Polic, M., Orsag, M.: Procedural generation of synthetic dataset for\nrobotic applications in sweet pepper cultivation. In: 2022 International Conference\non Smart Systems and Technologies (SST). pp. 309–314. IEEE (Oct 2022). https:\n//doi.org/10.1109/sst55530.2022.9954643\n29. Wachter, P., Kruse, N., Schöning, J.: Synthetic fields, real gains: Enhancing\nsmart sgriculture through hybrid datasets. In: Informatik in der Land-, Forst-und\nErnährungswirtschaft-Fokus: Biodiversität fördern durch digitale Landwirtschaft.\n\nDevelopment of Hybrid AI Training on Real and Synthetic Data 21\npp. 437–442. Gesellschaft für Informatik e.V. (2024). https://doi.org/10.18420/\nGILJT2024_44\n30. Wang, M., Deng, W.: Deep visual domain adaptation: A survey. Neurocomputing\n312, 135–153 (Oct 2018). https://doi.org/10.1016/j.neucom.2018.05.083\n31. Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y.,\nWang, L., Liu, T.Y.: On layer normalization in the transformer architecture. In:\nProceedings of the 37th International Conference on Machine Learning. ICML’20,\nJMLR.org (2020), https://dl.acm.org/doi/10.5555/3524938.3525913\n32. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In: 2017 IEEE International Con-\nference on Computer Vision (ICCV). pp. 2242–2251. IEEE (Oct 2017). https:\n//doi.org/10.1109/iccv.2017.244\n\n",
    "source": "http://arxiv.org/abs/2506.24093v1",
    "authors": [
      "Paul Wachter",
      "Lukas Niehaus",
      "Julius Schöning"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "I.2.1; I.2.0; F.2.3"
    ],
    "type": "content"
  },
  {
    "id": "2506.24081v1_abstract",
    "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
    "content": "Title: SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks\n\nAbstract: We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.",
    "source": "http://arxiv.org/abs/2506.24081v1",
    "authors": [
      "Rahul Kumar",
      "Wenqi Wei",
      "Ying Mao",
      "Junaid Farooq",
      "Ying Wang",
      "Juntao Chen"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24081v1_content",
    "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks",
    "content": "arXiv:2506.24081v1  [quant-ph]  30 Jun 2025SQUASH: A SWAP-Based Quantum Attack to Sabotage\nHybrid Quantum Neural Networks\nRahul Kumar∗, Wenqi Wei∗, Ying Mao∗, Junaid Farooq†, Ying Wang‡, and Juntao Chen∗\nAbstract —We propose a circuit-level attack, SQUASH, a\nSWAP-Based Quantum Attack to sabotage Hybrid Quantum\nNeural Networks (HQNNs) for classification tasks. SQUASH is\nexecuted by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based\nor adversarial input attacks, SQUASH directly manipulates the\ncircuit structure, leading to qubit misalignment and disrupting\nquantum state evolution. This attack is highly stealthy, as it\ndoes not require access to training data or introduce detectable\nperturbations in input states. Our results demonstrate that\nSQUASH significantly degrades classification performance, with\nuntargeted SWAP attacks reducing accuracy by up to 74.08%\nand targeted SWAP attacks reducing target class accuracy by\nup to 79.78%. These findings reveal a critical vulnerability in\nHQNN implementations, underscoring the need for more resilient\narchitectures against circuit-level adversarial interventions.\nIndex Terms —Quantum Machine Learning, Hybrid Quantum\nNeural Networks, SWAP Test, Fidelity, Circuit-level Attack\nI. I NTRODUCTION\nQuantum Machine Learning (QML) represents a rapidly\nevolving field at the intersection of quantum computing and\nartificial intelligence. By harnessing quantum phenomena such\nas superposition and entanglement, QML algorithms aim to\noutperform classical counterparts in complex tasks like opti-\nmization, pattern recognition, and generative modeling [1], [2],\n[3]. Among the various QML approaches, Hybrid Quantum\nNeural Networks (HQNNs) have emerged as powerful tools,\ndemonstrating their potential in tackling a variety of complex\nchallenges across domains such as medical image classifica-\ntion [4], [5], speech recognition [6], and cybersecurity threat\ndetection [7]. By leveraging the strengths of both quantum and\nclassical computing, HQNNs offer enhanced computational\nefficiency and improved pattern recognition capabilities. The\nopen-sourcing of quantum computing frameworks and QNNs\nhas accelerated innovation, enabling researchers and develop-\ners to experiment with and deploy quantum-enhanced machine\nlearning models on a large scale [8]. However, this openness\nalso introduces significant security vulnerabilities [5]. Publicly\navailable implementations often lack robust protections against\nadversarial manipulations, making them susceptible to attacks\nthat can compromise their integrity. While classical neural\n∗The authors are with the Department of Computer and Information Sci-\nences, Fordham University, New York, NY , 10023 USA. E-mail: {rkumar42,\nwwei23, ymao41, jchen504 }@fordham.edu\n†The author is with the Department of Electrical & Computer Engineering,\nCollege of Engineering and Computer Science, University of Michigan-\nDearborn, Dearborn, MI 48128 USA. E-mail: mjfarooq@umich.edu\n‡The author is with the Department of Systems and Enterprises, Stevens In-\nstitute of Technology, Hoboken, NJ 07030 USA. E-mail: ywang6@stevens.edunetworks have been extensively studied for their susceptibility\nto adversarial threats, the security of HQNNs remains largely\nunexplored, particularly at the circuit level. This oversight\npresents a critical risk, as attackers can exploit architectural\nweaknesses in quantum circuits to degrade model performance\nwithout leaving detectable traces.\nIn this paper, we propose a novel circuit-level attack, which\nwe call SQUASH, specifically targeting HQNNs used for\nclassification tasks. Unlike conventional attack strategies that\nmight focus on altering input data [9], [10], SQUASH oper-\nates by directly manipulating the quantum circuit’s structure\nthrough the insertion of SWAP gate(s) into the variational\nquantum circuit of the targeted HQNN. SWAP gates are\nfundamental quantum operations used to exchange the states\nof two qubits, facilitating proper qubit connectivity in quantum\ncircuits, especially on hardware with limited qubit interactions.\nTheir strategic insertion can misalign qubits, disrupting the\nintended evolution of quantum states and making them a potent\ntool for structural attacks like SQUASH.\nOur proposed attack method is designed to be highly stealthy\nfor several key reasons. Firstly, access to the original training\ndata used to train the HQNN is not necessary. Secondly,\nSQUASH does not introduce any detectable perturbations\nin the input states that are fed into the network; instead,\nthe disruption occurs within the quantum processing itself.\nWe present two key attack strategies based on SWAP gate\ninsertion. 1) Untargeted SWAP attacks aim to systematically\ndegrade overall classification accuracy by perturbing fidelity\nmeasurements across all qubits (including the ancilla qubit).\n2) Targeted SWAP attacks are designed to induce specific\nmisclassifications, ensuring a given input is assigned to an\nadversary-controlled incorrect category by selectively manip-\nulating fidelity measurements. This is done after correlating\nfidelity measurements to prediction outcomes and targeting\nspecific qubits. In this two-pronged attack approach, SQUASH\nis able to reduce classification accuracy on average by 74.08%\nusing our untargeted SWAP attack and 79.78% using our\ntargeted SWAP attack. These findings expose a critical vulner-\nability present in publicly available HQNN implementations\nand open-source quantum computing frameworks at the circuit\nlevel.\nThe main contribution of this paper is summarized below:\n1) We propose a novel two-pronged attack strategy that\nutilizes SWAP gates to directly interfere with model\nperformance using quantum entanglement.\n2) We highlight vulnerabilities within the HQNN architec-\n\nture that have not been explored extensively in existing\nliterature.\n3) By using the targeted SWAP attack strategy, we demon-\nstrate the stealth of this attack by showing the classifi-\ncation performance becoming less distinguishable from\nnormal behavior, with only subtle degradations in ac-\ncuracy and misclassifications occurring in a targeted\nmanner.\nII. R ELATED WORK\nA. Backdoor Attacks\nData-Poisoning-based Backdoor Attacks (DPBAs) and Tro-\njaning attacks are common threats to neural networks, where\nadversaries manipulate training data or modify network com-\nponents to embed hidden behaviors [11]. These attacks com-\npromise model integrity, triggering malicious actions only\nunder specific conditions. Related work [12] addresses the\nunexplored threat of data poisoning attacks on QML models in\ncloud settings. They propose a novel quantum indiscriminate\ndata poisoning attack, QUID, which leverages intra-class en-\ncoder state similarity to flip labels of training data, effectively\ndegrading QML model performance. Through experiments,\nQUID demonstrates significant accuracy degradation, even in\nnoisy environments and against classical defenses, highlighting\nthe vulnerability of QML to data poisoning. QDoor is a\nbackdoor attack framework for QNNs that differs from DPBAs\nby leveraging approximate synthesis [9]. It trains QNNs to\nbehave normally until synthesis activates malicious behavior,\nexploiting unitary differences between uncompiled and synthe-\nsized models. QDoor enhances stealth, is robust against noise,\nand improves both attack success rate and clean data accuracy,\nmaking it harder to detect than traditional DPBAs.\nB. Quantum Attacks\nQTrojan is a circuit-level backdoor attack on QNNs that\ninserts quantum gates into the variational quantum circuit\nrather than modifying inputs or training data [13]. It exploits\nquantum compiler configuration files as triggers, enabling\nstealthy attacks with improved clean data accuracy (21%)\nand attack success rate (19.9%). However, its effectiveness\ndepends on users frequently updating configuration files and\nattackers successfully inserting triggers, limiting its reliability.\nInvestigating SWAP tests for attacks has been investigated\nrecently, with the authors proposing a SWAP-based fault injec-\ntion model in multi-tenant quantum computing environments,\ntargeting superconducting hardware systems [14]. The attack\ninjects SWAP operations by strategically occupying qubits,\nincreasing SWAP overhead by up to 55% (median 25%). They\nintroduce a qubit quality metric for adversary selection and\npropose a machine learning model to detect anomalous user\nbehavior as a countermeasure.\nIn contrast, our work focuses on injecting SWAP gates\ninto smaller scale models that are more accessible through\nopen-source projects. The average user is more likely to\ninteract with these smaller-scale, publicly available HQNNimplementations rather than the more complex multi-tenant\ncomputing environments [14]. This makes the vulnerabilities\nexposed by SQUASH potentially more relevant and accessible\nto a wider range of users and developers experimenting with\nhybrid quantum machine learning [15].\nIII. B ACKGROUND\nA. Qubits and Quantum Circuits\nA qubit (quantum bit) is the basic unit of quantum infor-\nmation, representing a quantum analogue to the classical bit.\nUnlike a classical bit, which can only take the value 0 or 1, a\nqubit can exist in a superposition of both states simultaneously.\nThe general state of a qubit is described as:\n|ψ⟩=α|0⟩+β|1⟩, (1)\nwhere αandβare complex probability amplitudes such\nthat|α|2+|β|2=1. These squared magnitudes represent the\nprobability of measuring the qubit in the corresponding basis\nstate.\nThe state of a qubit is often visualized using the Bloch\nsphere, where any pure state corresponds to a point on the\nsurface of a unit sphere. This representation is useful for un-\nderstanding how quantum gates transform qubit states through\nrotations and phase shifts. Furthermore, qubits can become\nentangled, meaning the state of one qubit cannot be described\nindependently of the state of another. Entanglement is a\nuniquely quantum resource that enables powerful correlations\nand is critical for quantum algorithms and communication\nprotocols [1], [16]. Qubit superposition and entanglement are\nessential for implementing the SWAP test, which relies on\ncoherent interference between qubit states to measure their\nsimilarity.\nTo manipulate qubits, quantum computers use quantum\ncircuits, which are composed of quantum gates. These gates are\nunitary operators that evolve the state of qubits in a reversible\nand deterministic manner [17]. For a single-qubit gate U, the\ntransformation is:\n|ψ⟩ →U|ψ⟩. (2)\nSingle-qubit gates act on individual qubits and perform basic\noperations. For example, the Pauli-X gate behaves like a\nclassical NOT gate by flipping |0⟩to|1⟩and vice versa. The\nHadamard gate (H) puts a qubit into an equal superposition\nof|0⟩and|1⟩, while the phase gates (S and T) apply phase\nshifts.\nMulti-qubit gates, such as the Controlled-NOT (CNOT)\ngate, are essential for enabling interactions and entanglement\nbetween qubits. A CNOT gate flips the target qubit if the\ncontrol qubit is in state |1⟩. These gates are the backbone of\nconditional operations and entanglement generation in quan-\ntum circuits. A quantum circuit is constructed by applying\na series of single- and multi-qubit gates to an initial state\n(typically |0⟩⊗nfornqubits). As the gates are applied, the\nqubit states evolve to encode the solution of a computational\ntask.\n\nFig. 1: Illustration of the user sequence that results in SQUASH being successful. After a user has successfully downloaded the\nattacker’s HQNN code, the configuration file injects SWAP tests (based on the type of attack) after the user has initiated the\ntraining execution phase.\nB. SWAP Tests\nThe SWAP test is a fundamental quantum algorithm used to\ndetermine the similarity between two quantum states, making\nit particularly valuable in HQNNs for tasks such as quantum\nkernel estimation and entanglement detection [18], [19], [20].\nThe test operates by introducing an ancilla qubit and applying\na controlled-SWAP (CSWAP) gate to swap between Ntarget\nstates, followed by a Hadamard transformation on the ancilla\nqubit [21]. Mathematically, given two quantum states |ψ⟩\nand|φ⟩, and an ancilla initialized as |0⟩, the total system is\nprepared as:\n|Ψ⟩=|0⟩⊗|ψ⟩⊗|φ⟩. (3)\nApplying a Hadamard gate to the ancilla results in:\n1√\n2(|0⟩+|1⟩)⊗|ψ⟩⊗|φ⟩. (4)\nThe CSWAP gate is then applied, which swaps |ψ⟩and|φ⟩if\nthe ancilla is in state |1⟩, yielding:\n1√\n2(|0⟩⊗|ψ⟩⊗|φ⟩+|1⟩⊗|φ⟩⊗|ψ⟩). (5)\nA final Hadamard transformation on the ancilla leads to a\nprobability of measuring |0⟩given by:\nP(0) =1+|⟨ψ|φ⟩|2\n2. (6)\nThis probability directly encodes the squared fidelity be-\ntween the two states, making the SWAP test an essential\ntool in HQNNs for assessing quantum state similarity in\nclassification and clustering tasks. However, the SWAP test is\nvulnerable to noise and decoherence, especially in near-termquantum devices where gate errors impact fidelity estimation\n[22]. As the number of qubits increases, maintaining coherence\nbecomes more challenging due to the exponentially larger\nHilbert space and increased potential for qubit interactions that\ninduce decoherence. This noise can distort quantum operations,\nincluding the SWAP test, by introducing gate errors that affect\nsuperposition states and lead to inaccurate fidelity measure-\nments. These inaccuracies compromise the SWAP test’s ability\nto measure quantum state overlaps, making it susceptible\nto exploitation in quantum attacks where adversaries extract\ninformation about unknown states by measuring their overlap\nwith known references [19].\nIV. SQUASH\nA. Overview of SQUASH\nWe propose SQUASH as a circuit-level attack to sabotage\nHQNNs designed for classification tasks. More specifically,\nwe achieve this by inserting SWAP gate(s) directly into the\nvariational quantum circuit of the targeted HQNN. Unlike\nconventional attacks that might introduce noise or manipu-\nlate input data, SQUASH operates by directly altering the\nquantum circuit’s structure, leading to qubit misalignment and\nconsequently disrupting performance. This method of attack is\nconsidered highly stealthy for several reasons. Firstly, access\nto the original training data used to train the HQNN is\nnot necessary. Secondly, SQUASH does not introduce any\ndetectable perturbations in the input states that are fed into\nthe network. Instead, the disruption occurs within the quantum\nprocessing itself.\nFor these experiments, we assume that the user are in need\nof an open-sourced QNN model for a classification task. Given\n\nAlgorithm 1 SWAP Attack for HQNN\n1:/* Step 1: Initialize parameters */\n2:Set initial perturbation δ←0\n3:foreach qubit ψin the quantum circuit do\n4: /* Step 2: Check attack type */\n5: ifattack type == untargeted then\n6: Apply random SWAP gates between circuit\nqubits\n7: else\n8: Compute initial fidelities:\nFt←F(|ψt⟩,|x⟩)\nFc←F(|ψc⟩,|x⟩)\n9: /* Step 3: Optimize perturbation for qubit ψi\n*/\n10: while Ft≤Fcdo\n11: Compute gradient ∇δF(|ψt⟩,|x+δ⟩)\n12: Update perturbation: δ←δ+η·∇δ\n13: Generate perturbed state: |x′⟩ ←eiδ|x⟩\n14: Recalculate fidelities:\nFt←F(|ψt⟩,|x′⟩)\nFc←F(|ψc⟩,|x′⟩)\n15: end while\n16: end if\n17:end for\n18:/* Step 4: Return adversarial state */\n19:return |ψt⟩\nthe model efficiency of utilizing quantum and classical com-\nputations, the user finds an implementation of an HQNN on\nGitHub and decides to download it for their task. The user, un-\naware of the potential risks, downloads the model directly from\na public repository. While the model itself appears to be open-\nsourced and reputable, the repository also includes a malicious\nconfiguration file that is designed to exploit vulnerabilities\nwithin the user’s system [10], [13]. This configuration file is\ncarefully crafted to inject harmful code when the user attempts\nto load the model into their environment. Once the HQNN\nmodel is downloaded and the configuration file is executed,\nthe malicious code triggers specific actions specified by the\nattacker, such as degrading model performance or intentionally\nmispredicting specific classes. The user unknowingly opens the\ndoor to a security breach simply by downloading the model\nfrom GitHub without properly reviewing the associated files.\nThe complete flow is shown in Figure 1.\nB. Adversarial SWAP-Based Attacks on Hybrid Quantum Neu-\nral Networks\nHybrid quantum-classical neural networks leverage quantum\nfidelity measurements as a fundamental component of their\nclassification process [23], [24]. Specifically, the SWAP test\nserves as a mechanism to evaluate the similarity between\nquantum states, enabling the construction of class boundariesbased on quantum feature representations [22]. However, this\nreliance introduces vulnerabilities that adversaries can exploit\nto degrade classification accuracy or induce targeted mis-\nclassification. In this work, we utilize SWAP tests to alter\nclassification performance by leveraging measured probabil-\nities to reinforce quantum state grouping within HQNNs. Our\napproach consists of two key attack strategies: untargeted\nSWAP attacks and targeted SWAP attacks, both designed to\nmanipulate classification outcomes by influencing quantum\nfidelity measurements. The algorithm for inserting the SWAP\ntests into the circuit is outlined in Algorithm 1.\n1) Untargeted SWAP Attacks: An untargeted SWAP at-\ntack aims to systematically degrade classification accuracy by\ncovertly injecting multiple SWAP gates across all qubits, as\nshown in Figure 2. Given an input qubit state |x⟩, the attack\ndisrupts the similarity computations essential for accurate label\nassignment for classification. The similarity metric is given by:\nSi=∑\njP(j)\ni(0) =∑\nj1+|⟨x|r(i)\nj⟩|2\n2, (7)\nwhere P(j)\ni(0)is the probability obtained from the SWAP test\nbetween |x⟩and the reference state |r(i)\nj⟩. The classifier assigns\n|x⟩to the category maximizing Si, ensuring it is grouped with\nthe most similar reference states.\nBy injecting SWAP gates indiscriminately across all\nqubits—including ancilla qubits—the adversary introduces\nsubtle interference that degrades fidelity across multiple class\ncomparisons. Unlike targeted attacks, which aim to misclas-\nsify specific inputs, this broad perturbation reduces overall\nconfidence in predictions, leading to increased classification\nuncertainty. Repetitive SWAP tests cause discrepencies with\nfidelities of qubit measurements since the entanglement be-\ntween qubits can be disrupted, leading to a loss of correlation\nthat results in discrepancies across fidelities. Since quantum\nclassification is often embedded within a hybrid learning\nframework, these perturbations propagate through subsequent\nlayers, distorting extracted features and degrading the overall\nperformance of the model. Additionally, as hybrid models\nemploy iterative refinement strategies for ambiguous classifica-\ntions, an untargeted SWAP attack can amplify model instabil-\nity, forcing redundant evaluations and increasing computational\noverhead.\n2) Targeted SWAP Attacks: In contrast to untargeted at-\ntacks, a targeted SWAP attack is designed to induce a specific\nmisclassification, ensuring that a given input state is system-\natically assigned to an incorrect but adversary-controlled cat-\negory. The attack selectively manipulates the fidelity function\nto ensure:\nF(|ψt⟩,|χ⟩)>F(|ψc⟩,|χ⟩), (8)\nwhere |ψt⟩represents a reference state from the adversary-\nchosen target class, and |ψc⟩corresponds to the reference state\nfrom the correct class as shown in Figure 3. By artificially\nincreasing the measured fidelity between |χ⟩and|ψt⟩, while\n\nMALICIOUS SWAP|0⟩ H H\n|χ⟩\n|ψ⟩\nFig. 2: An untargeted attack on a 2-qubit quantum circuit.\nThe first connection between |0⟩with the crosses on |χ⟩\nand|ψ⟩represents a healthy SWAP test. The connections\nin the MALICIOUS SWAP container represent SWAP tests\nthat contaminate the fidelities of |χ⟩and|ψ⟩due to improper\nconfiguration.\nsimultaneously reducing its similarity to |ψc⟩, the adversary\nforces the classifier to misattribute the input to the incorrect\ncategory as described in Algorithm 1. Targeted SWAP attacks\nin SQUASH only use two SWAP tests to carry out the attack,\nas the qubit fidelities are measured using SWAP before and\nafter the circuit tampering, hence reducing the overhead of the\ncircuit execution on the user’s machine.\nC. Circuit Design\nThis architecture combines classical convolutional layers\nwith a QNN to leverage both classical and quantum computing\nfor classification tasks. It begins with two convolutional layers\nthat process input data through feature extraction, detecting\npatterns and spatial hierarchies in the data. A dropout layer is\nincluded for regularization, preventing overfitting by randomly\ndeactivating a fraction of neurons during training.\nAfter the convolutional layers, the output is flattened and\npassed through two fully connected layers, which reduce\nthe dimensionality and prepare the data for input into the\nquantum network. The second fully connected layer outputs\na 2-dimensional tensor, which is then passed to the quantum\nnetwork. The quantum processing begins with a feature map\nlayer, which is responsible for encoding the classically pro-\ncessed data into the quantum states of the network’s qubits.\nThe number of qubits used in this encoding layer is 2 qubits.\nFollowing the encoding layer is a variational ansatz, which\nconsists of parameterized quantum blocks. These blocks apply\na sequence of adjustable quantum gates to the encoded quan-\ntum state, allowing the network to learn complex relationships\nin the data. The quantum output is then passed through a\nfinal fully connected layer, producing a 1-dimensional output\nthat is normalized to form a 2-dimensional result, which is\nconcatenated as the final classification output.\nA crucial and distinctive element of this HQNN architecture\nis the inclusion of a swap test performed on a specific pair of\nqubits. This SWAP Test serves as the measurement layer in\nthis context, as it determines the fidelity (similarity) between\nquantum states, which is then used for classification. The\nswap test involves an ancilla qubit, Hadamard gates, and a\nCSWAP gate. The probability of measuring the ancilla qubit|0⟩ H H\n|χ⟩\n|ψc⟩ fidelity noise\n|χ⟩\n|ψt⟩\nFig. 3: A targeted attack on a 2-qubit quantum circuit. |ψc⟩\nrepresents the correct classification and |ψt⟩represents the\nattack target classification.\nin a certain state provides a measure of the fidelity. Finally, a\nfully connected layer converts the qubit outputs and fidelities\nto an MNIST classification.\nV. E XPERIMENTAL RESULTS\nA. Experimental Setup\nWe utilize Qiskit, IBM’s open-source quantum software\ndevelopment kit, in conjunction with a custom Python wrapper\nto facilitate quantum circuit simulations. To assess the perfor-\nmance of the HQNN across different data complexities, we\ntrain the model on both the MNIST and CIFAR-10 datasets.\nMNIST serves as a standard benchmark for handwritten digit\nrecognition, consisting of 70,000 grayscale images of digits\n(0–9) with a resolution of 28×28 pixels [25]. CIFAR-10,\nin contrast, comprises 60,000 RGB images of size 32×32\npixels, spanning 10 classes that include vehicles and animals\n[26]. To evaluate performance under different classification\ncomplexities, we extract both binary and multiclass subsets\nfrom each dataset. For MNIST-2, we select digits 0 and 1\nto form a binary classification task with minimal intra-class\nvariation. For CIFAR-2, we use the cat and dog classes,\nrepresenting a more visually complex binary problem due to\noverlapping textures and poses. These binary tasks help high-\nlight the HQNN’s sensitivity to visual abstraction in low- and\nhigh-complexity domains. The full multiclass tasks (MNIST-\n10 and CIFAR-10) retain all 10 classes from each dataset,\noffering a broader perspective on generalization across diverse\ncategories. In all cases, class-balanced splits are maintained\nbetween training and testing to ensure fair evaluation. Both\ndatasets are preprocessed using PyTorch’s torchvision library\n[27], transformed into tensors via torchvision.transforms , and\nloaded using PyTorch’s DataLoader , with a fixed random seed\napplied to ensure reproducibility during training.\nB. Evaluation Metrics\nNegative Log Loss (NLL) is a widely used metric for evalu-\nating classification models, particularly in HQNNs. This metric\nis suitable for HQNN-based image classification because quan-\ntum circuits inherently produce probabilistic outputs. The mea-\nsurement results from a PQC can be interpreted as probability\n\nDataset SWAP Count Clean Training Untargeted SWAP Attack\nNLL Accuracy (%) NLL Accuracy (%)\nMNIST-21 SWAP Test 0.0451 96.01% 0.512 43.91%\n2 SWAP Tests 0.0510 95.68% 0.649 35.22%\n3 SWAP Tests 0.0687 95.30% 0.778 21.78%\nCIFAR-21 SWAP Test 0.1102 90.78% 0.599 38.04%\n2 SWAP Tests 0.1127 91.41% 0.769 30.92%\n3 SWAP Tests 0.1210 91.87% 0.932 18.12%\nMNIST-101 SWAP Test 0.0991 93.11% 0.598 38.27%\n2 SWAP Tests 0.0904 93.88% 0.701 32.64%\n3 SWAP Tests 0.0904 93.89% 0.880 19.31%\nCIFAR-101 SWAP Test 0.1723 77.42% 0.651 34.07%\n2 SWAP Tests 0.1771 78.36% 0.744 27.18%\n3 SWAP Tests 0.1817 78.92% 0.911 15.84%\nTABLE I: Classification results across MNIST and CIFAR using the 2-qubit HQNN under clean training and untargeted SWAP\nattack conditions.\ndistributions over class labels, making NLL a natural choice\nfor assessing model confidence. In an image classification task\nwith Cpossible classes, let p(yc|x)represent the predicted\nprobability of class cfor an input image x. The NLL for a\nsingle sample is calculated as:\nL=−C\n∑\nc=1yclogp(yc|x), (9)\nwhere ycis a binary indicator (1 for the correct class, 0\notherwise). The total loss across a dataset with Nsamples is\ngiven by:\nLN=−1\nNN\n∑\ni=1C\n∑\nc=1yi,clogp(yi,c|xi), (10)\nwhere yi,cand p(yi,c|xi)represent the ground truth and\npredicted probability for sample i, respectively.\nNLL is particularly useful for HQNNs due to its com-\npatibility with probabilistic quantum measurements. Quantum\ncircuits produce output distributions according to the Born\nrule, where probabilities are obtained from quantum state\namplitudes [28]. Since NLL penalizes incorrect classifications\nmore heavily when assigned high confidence, it effectively\nguides HQNNs to refine their probability estimates.\nC. Classification Analysis\nThe performance of the HQNN under clean training con-\nditions—absent any adversarial SWAP interference—is sum-\nmarized in Table I. Across all datasets, the model demon-\nstrated strong classification capability, with accuracy values\nconsistently high and NLL values low, even as the number\nof inserted SWAP tests increased. On simpler datasets such\nas MNIST-2, accuracy remained above 95%, while CIFAR-\n2 yielded slightly lower but still robust results around 91%.\nFor the more complex multiclass tasks, MNIST-10 maintained\nperformance above 93%, and CIFAR-10 achieved accuracy\nnear 79%. Interestingly, for CIFAR datasets, the accuracy\nslightly increased with more SWAP tests under clean train-\ning. This may reflect a beneficial effect of additional circuitentanglement when processing richer, high-dimensional data,\nin contrast to MNIST where performance stays consistent due\nto the simplicity of the input space.\nUpon the introduction of untargeted SWAP attacks, a pro-\nnounced vulnerability in the HQNN framework was revealed.\nAcross all datasets, increasing the number of maliciously\ninserted SWAP tests resulted in a consistent degradation of\nmodel performance, evidenced by declining classification ac-\ncuracy and rising negative log-likelihood (NLL). For instance,\non the MNIST-2 dataset, accuracy dropped from 96.01% under\nclean training to 43.91%, 35.22%, and 21.78% as one, two, and\nthree SWAP tests were applied, respectively. A similar pattern\nwas observed on CIFAR-2, where clean accuracy of 91.87%\nfell sharply to 38.04%, 30.92%, and 18.12% under increasing\nSWAP interference. The multiclass settings, MNIST-10 and\nCIFAR-10, also exhibited substantial degradation: MNIST-10\naccuracy declined from 93.89% to 19.31%, while CIFAR-\n10 dropped from 78.92% to just 15.84%. Concurrently, NLL\nvalues rose across all scenarios, indicating increasing model\nuncertainty. These results reinforce the hypothesis that untar-\ngeted SWAP attacks broadly perturb the fidelity measurements\nby inducing structural misalignments in the quantum circuit,\nas shown in Figure 4. Here, |ψu⟩is the qubit with randomized\nfidelity on the faulty Hadamard gate, where its arbitrary\nlocation leads to severely degraded classification performance\nfor classes relying on that specific qubit.\nThe targeted SWAP attacks exhibited a distinct pattern,\nspecifically designed to manipulate the accuracy of a single\ntarget class as shown in Table II. For the MNIST-2 dataset,\nthe accuracy for the target class dropped significantly to\n19.14% under the targeted SWAP attack. Similar trends were\nobserved for MNIST-10, where target class accuracy reduced\nto 20.94%. These drastic drops in accuracy demonstrate the\nattack’s effectiveness in inducing targeted misclassifications\nby manipulating fidelity measurements for the chosen class,\nas illustrated by |ψt⟩in Figure 4. In this figure, |ψt⟩lies\nat the intersection of the clean Hadamard gate (blue) and\nthe Hadamard gate from the SWAP attack circuit (red). This\n\nClean Training Targeted SWAP Attack\nNLL Total Accuracy (%) NLL Target Class Accuracy (%) Total Accuracy (%)\nMNIST-2 0.0510 95.68% 0.823 19.14% 57.075%\nCIFAR-2 0.1123 91.38% 0.932 18.12% 54.062%\nMNIST-10 0.952 92.3% 0.801 20.94% 85.164%\nCIFAR-10 0.1817 78.92% 0.911 15.84% 73.919%\nTABLE II: Results for classification of MNIST and CIFAR datasets (2 and 10 classes) under clean training and targeted SWAP\nattack. Target Class Accuracy measures performance specifically on the attacker’s chosen class, while Total Accuracy reflects\noverall model performance. A successful attack lowers both, with a more severe drop in the target class.\noverlap conceals the qubit’s altered state by making it ap-\npear consistent with the clean circuit, thereby rendering the\nattack stealthy at the quantum level. Despite the significant\ndrop in target class accuracy, the overall accuracy showed a\nmore moderate decline when compared to clean training. For\ninstance, for MNIST-2, the total accuracy under a targeted\nSWAP attack decreased to 57.075%, a considerable drop from\nthe clean training accuracy of 95.01%. Similarly, for MNIST-\n10, the total accuracy dropped to 85.164%, compared to the\nclean training accuracy of 92.3%. This is a result of the\ncorrect classifications of other classes masking the incorrect\nclassification of the target class, making the attack stealthy\nto the end user. These results indicate that while the attack\nis highly effective at targeting and misclassifying a specific\nclass, the network’s overall ability to classify other instances\nremains largely intact.\nD. SWAP Overhead Analysis\nTo evaluate the overhead of conducting SWAP operations\nthrough the SQUASH attack, we measure the SWAP overhead\nas the percentage increase in execution time per epoch for\nboth MNIST and CIFAR combined. In our experiments, we\nconducted 100 training iterations for each model and recorded\nthe execution time for each epoch. From Figure 5, the targeted\nSWAP attack model exhibited an average execution time of\n44.38 seconds, corresponding to a 5.96% increase compared to\nthe clean HQNN model, which averaged 42.07 seconds. The\nuntargeted SWAP attack model demonstrated a significantly\nhigher average execution time of 48.82 seconds, reflecting a\n16.05% increase over the clean HQNN model. The increased\nexecution time for both attack models can be attributed pri-\nmarily to the additional CNOT gates required by the SWAP\ngate decomposition (typically involving three CNOT gates per\nSWAP). This, in turn, increases the circuit depth and the\nnumber of operations necessary during training. Furthermore,\ndue to the inherent nature of the SQUASH attack, untargeted\nattacks require more SWAP tests to degrade classification\nperformance, whereas targeted attacks strategically add fidelity\nonly utilizing two SWAP tests. As a result, the overhead\nassociated with untargeted attacks is more pronounced.\nE. Potential Defenses\nGiven that SQUASH operates by directly inserting SWAP\ngates to induce qubit misalignment and disrupt quantum state\nFig. 4: Visualization of a Bloch sphere with an untargeted\nand targeted SQUASH attack. The blue ring represents a clean\nHadamard gate, whereas the red ring represents a Hadamard\ngate from a SWAP attack. The qubits represent the correct\nclass from clean training ( |ψc⟩), the targeted class from the\ntargeted attack ( |ψt⟩), and the randomized initialization from\nthe untargeted attack ( |ψu⟩).\nevolution, monitoring quantum circuit structure and execution\nfor anomalous activity represents a key defensive strategy.\nThis could involve the development of anomaly detection\nsystems capable of identifying unexpected increases in the\nnumber of SWAP gates or deviations from the intended circuit\narchitecture during the execution phase [14], [29]. Solutions\nlike QML-IDS aim to detect attacks before hitting the circuit\nlayer, which can potentially pose as a problem for attackers\nwanting to use SQUASH [30]. Furthermore, the exploitation\nof publicly available QML implementations through malicious\nconfiguration files underscores the need for enhanced security\nmeasures concerning the provenance and validation of quan-\ntum code and associated configuration files before deployment\nand training [31]. Ultimately, addressing the circuit-level vul-\nnerabilities highlighted by SQUASH requires the development\nand integration of robust defense mechanisms specifically\n\nFig. 5: Visualization of the execution times across 100 execu-\ntions for each model.\ndesigned to detect and prevent unauthorized modifications to\nthe quantum components.\nVI. C ONCLUSION\nThe presence of SWAP-based adversarial vulnerabili-\nties highlights several critical security challenges in hy-\nbrid quantum-classical learning architectures. Perturbations to\nquantum fidelity scores directly impact classification reliability,\nincreasing model uncertainty and reducing overall predictive\naccuracy. As fidelity-based decisions propagate through hybrid\nlayers, errors introduced at the quantum stage distort feature\nrepresentations, exacerbating misclassification risks. Further-\nmore, adaptive learning mechanisms that iteratively refine\nclassification boundaries may reinforce adversarial misclassi-\nfications over time, further degrading model robustness.\nThe stark contrast in the impact of untargeted and targeted\nSWAP attacks underscores the nuanced vulnerabilities present\nin HQNNs at the circuit level. The significant performance\ndegradation under untargeted attacks, even with minimal cir-\ncuit modifications, highlights a critical security gap in publicly\navailable HQNN implementations and open-source quantum\ncomputing frameworks. This vulnerability stems from the\nreliance of HQNNs on quantum fidelity measurements for\nclassification. The insertion of SWAP gates, as implemented\nin the SQUASH attack, proves to be a stealthy and effective\nmethod for sabotaging HQNNs without necessitating access\nto sensitive training data or introducing detectable input per-\nturbations. The fact that increasing the number of inserted\nSWAP tests generally amplifies the detrimental effects of\nuntargeted attacks further emphasizes the sensitivity of these\nhybrid architectures to seemingly minor structural alterations\nin their quantum components. The findings of this work sug-\ngest that while hybrid quantum-classical architectures leverage\nthe strengths of both classical neural networks and QNNs to\nenhance high-dimensional feature representation, their reliance\non fidelity-based classification creates vulnerabilities that can\nbe exploited.REFERENCES\n[1] J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe, and\nS. Lloyd, “Quantum machine learning,” Nature , vol. 549, no. 7671, pp.\n195–202, 2017.\n[2] M. Cerezo, G. Verdon, H.-Y . Huang, L. Cincio, and P. J. Coles,\n“Challenges and opportunities in quantum machine learning,” Nature\ncomputational science , vol. 2, no. 9, pp. 567–576, 2022.\n[3] C. Ciliberto, M. Herbster, A. D. Ialongo, M. Pontil, A. Rocchetto,\nS. Severini, and L. Wossnig, “Quantum machine learning: a classical\nperspective,” Proceedings of the Royal Society A: Mathematical, Physi-\ncal and Engineering Sciences , vol. 474, no. 2209, p. 20170551, 2018.\n[4] G. De Luca, “A survey of nisq era hybrid quantum-classical machine\nlearning research,” Journal of Artificial Intelligence and Technology ,\nvol. 2, no. 1, pp. 9–15, 2022.\n[5] S. Huang, Y . Chang, Y . Lin, and S. Zhang, “Hybrid quantum–classical\nconvolutional neural networks with privacy quantum computing,” Quan-\ntum Science and Technology , vol. 8, no. 2, p. 025015, 2023.\n[6] Y . Thakar, B. Ghosh, V . Adeshra, and K. Srivastava, “Performance anal-\nysis of hybrid quantum-classical convolutional neural networks for audio\nclassification,” in 2024 15th International Conference on Computing\nCommunication and Networking Technologies (ICCCNT) . IEEE, 2024,\npp. 1–7.\n[7] H. Suryotrisongko and Y . Musashi, “Evaluating hybrid quantum-classical\ndeep learning for cybersecurity botnet dga detection,” Procedia Com-\nputer Science , vol. 197, pp. 223–229, 2022.\n[8] M. Fingerhuth, T. Babej, and P. Wittek, “Open source software in\nquantum computing,” PloS one , vol. 13, no. 12, p. e0208561, 2018.\n[9] C. Chu and et al., “Qdoor: Exploiting approximate synthesis for back-\ndoor attacks in quantum neural networks,” in 2023 IEEE International\nConference on Quantum Computing and Engineering (QCE) , vol. 1.\nIEEE, 2023.\n[10] J. John, L. Golla, and Q. Wang, “Quantum trojan insertion: Con-\ntrolled activation for covert circuit manipulation,” arXiv preprint\narXiv:2502.08880 , 2025.\n[11] Y . Liu and et al., “Trojaning attack on neural networks,” in 25th Annual\nNetwork And Distributed System Security Symposium (NDSS 2018) .\nInternet Society, 2018.\n[12] S. Kundu and S. Ghosh, “Adversarial poisoning attack on quantum\nmachine learning models,” arXiv preprint arXiv:2411.14412 , 2024.\n[13] C. Chu and et al., “Qtrojan: A circuit backdoor against quantum neural\nnetworks,” in ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2023.\n[14] S. Upadhyay and S. Ghosh, “Stealthy swaps: Adversarial swap injection\nin multi-tenant quantum computing,” 2023. [Online]. Available:\nhttps://arxiv.org/abs/2310.17426\n[15] V . Reers and M. Maußner, “Comparative analysis of vulnerabilities\nin classical and quantum machine learning,” in INFORMATIK 2024 .\nGesellschaft f ¨ur Informatik eV , 2024, pp. 555–571.\n[16] M. A. Nielsen and I. L. Chuang, Quantum computation and quantum\ninformation . Cambridge university press, 2010.\n[17] M. Schuld, A. Bocharov, K. M. Svore, and N. Wiebe, “Circuit-centric\nquantum classifiers,” Physical Review A , vol. 101, no. 3, p. 032308,\n2020.\n[18] S. Foulds, V . Kendon, and T. Spiller, “The controlled swap test for\ndetermining quantum entanglement,” Quantum Science and Technology ,\nvol. 6, no. 3, p. 035002, 2021.\n[19] W. Liu, Y .-Z. Li, H.-W. Yin, Z.-R. Wang, and J. Wu, “Quantum multi-\nstate swap test: an algorithm for estimating overlaps of arbitrary number\nquantum states,” EPJ Quantum Technology , vol. 11, no. 1, p. 46, 2024.\n[20] A. Maldonado-Romo, J. Y . Montiel-P ´erez, V . Onofre, J. Maldonado-\nRomo, and J. H. Sossa-Azuela, “Quantum k-nearest neighbors: Utilizing\nqram and swap-test techniques for enhanced performance,” Mathematics ,\nvol. 12, no. 12, p. 1872, 2024.\n[21] A. Kay, “Tutorial on the quantikz package,” arXiv preprint\narXiv:1809.03842 , 2018.\n[22] P. Ripper, G. Amaral, and G. Tempor ˜ao, “Swap test-based charac-\nterization of decoherence in universal quantum computers,” Quantum\nInformation Processing , vol. 22, no. 5, p. 220, 2023.\n[23] W. El Maouaki, A. Marchisio, T. Said, M. Bennai, and M. Shafique,\n“Advqunn: A methodology for analyzing the adversarial robustness of\nquanvolutional neural networks,” in 2024 IEEE International Conference\non Quantum Software (QSW) . IEEE, 2024, pp. 175–181.\n\n[24] W. El Maouaki, A. Marchisio, T. Said, M. Shafique, and M. Bennai,\n“Robqunns: A methodology for robust quanvolutional neural networks\nagainst adversarial attacks,” in 2024 IEEE International Conference on\nImage Processing Challenges and Workshops (ICIPCW) . IEEE, 2024,\npp. 4090–4095.\n[25] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based learning\napplied to document recognition,” Proceedings of the IEEE , vol. 86,\nno. 11, pp. 2278–2324, 1998.\n[26] A. Krizhevsky, V . Nair, G. Hinton et al. , “The cifar-10 dataset,” online:\nhttp://www. cs. toronto. edu/kriz/cifar. html , vol. 55, no. 5, p. 2, 2014.\n[27] A. Paszke, “Pytorch: An imperative style, high-performance deep learn-\ning library,” arXiv preprint arXiv:1912.01703 , 2019.\n[28] A. Neumaier, “Born’s rule and measurement,” arXiv preprint\narXiv:1912.09906 , 2019.\n[29] A. Ghosh and S. Ghosh, “Ai-driven reverse engineering of qml models,”\narXiv preprint arXiv:2408.16929 , 2024.\n[30] D. Abreu, C. E. Rothenberg, and A. Abel ´em, “Qml-ids: Quantum\nmachine learning intrusion detection system,” in 2024 IEEE Symposium\non Computers and Communications (ISCC) . IEEE, 2024, pp. 1–6.\n[31] H. Li, Q. Liu, and J. Zhang, “A survey of hardware trojan threat and\ndefense,” Integration , vol. 55, pp. 426–437, 2016.\n\n",
    "source": "http://arxiv.org/abs/2506.24081v1",
    "authors": [
      "Rahul Kumar",
      "Wenqi Wei",
      "Ying Mao",
      "Junaid Farooq",
      "Ying Wang",
      "Juntao Chen"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "type": "content"
  },
  {
    "id": "2506.24108v1_abstract",
    "title": "Navigating with Annealing Guidance Scale in Diffusion Space",
    "content": "Title: Navigating with Annealing Guidance Scale in Diffusion Space\n\nAbstract: Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.",
    "source": "http://arxiv.org/abs/2506.24108v1",
    "authors": [
      "Shai Yehezkel",
      "Omer Dahary",
      "Andrey Voynov",
      "Daniel Cohen-Or"
    ],
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24108v1_content",
    "title": "Navigating with Annealing Guidance Scale in Diffusion Space",
    "content": "arXiv:2506.24108v1  [cs.GR]  30 Jun 2025Navigating with Annealing Guidance Scale in Diffusion Space\nSHAI YEHEZKEL∗,Tel Aviv University, Israel\nOMER DAHARY∗,Tel Aviv University, Israel\nANDREY VOYNOV, Google DeepMind, Israel\nDANIEL COHEN-OR, Tel Aviv University, Israel\nCFG Ours CFG Ours CFG Ours\n\"A man juggles flaming hats.\" \"Two giraffes in astronaut suits repairing a spacecraft on Mars.\" \"A dragon and a knight playing cards in a tavern.\"\n\"Arobot painting a portrait.\" \"A cake with onions on top of it.\" \"A photo of a ballerina flamingo dancing on the beach.\"\n\"A donkey in a clown costume giving a lecture at the front of a lecture hall...\nThere are many students in the lecture hall.\"\"A mid-air dog practicing karate in a Japanese dojo, wearing a white gi\nwith a black belt on wooden floors...\"\"A man is seated on a wooden stool against a white background,\ndressed in a blue suit with a tie and brown shoes.\"\nFig. 1. Our annealing guidance scheduler significantly enhances image quality and alignment with the text prompt.\nDenoising diffusion models excel at generating high-quality images condi-\ntioned on text prompts, yet their effectiveness heavily relies on careful guid-\nance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice\nof the guidance scale has a critical impact on the convergence toward a\nvisually appealing and prompt-adherent image. In this work, we propose an\nannealing guidance scheduler which dynamically adjusts the guidance scale\nover time based on the conditional noisy signal. By learning a scheduling\npolicy, our method addresses the temperamental behavior of CFG. Empirical\nresults demonstrate that our guidance scheduler significantly enhances im-\nage quality and alignment with the text prompt, advancing the performance\nof text-to-image generation. Notably, our novel scheduler requires no addi-\ntional activations or memory consumption, and can seamlessly replace the\ncommon classifier-free guidance, offering an improved trade-off between\nprompt alignment and quality.1 Introduction\nDenoising diffusion models [Ho et al .2020; Nichol and Dhariwal\n2021; Sohl-Dickstein et al .2015; Song et al .2020a; Song and Ermon\n2019; Song et al .2020b] have shown outstanding abilities in text-\nbased generation of images [Dhariwal and Nichol 2021; Podell et al .\n2023; Ramesh et al .2022; Rombach et al .2022; Saharia et al .2022].\nAt training, these models learn to denoise a noisy signal 𝑧𝑡, e.g.\nan image latent, based on its existing lower frequency structure,\nand a text prompt 𝑐. However, while these models are tasked with\niteratively pushing the signal towards the conditional distribution\n𝑝(𝑧|𝑐), in practice, step corrections must be applied to sample high-\nquality results.\nThe widely used approach, classifier-free guidance (CFG) [Ho and\nSalimans 2022], suggests corrections by extrapolating predictions\naway from the unconditional distribution 𝑝(𝑧). In practice, this\nis performed by guiding the latent in the direction of difference\nbetween the conditional and unconditional predictions 𝛿𝑡(𝑧𝑡)≡\n𝜖𝑐\n𝑡(𝑧𝑡)−𝜖∅\n𝑡(𝑧𝑡), using a step size of 𝑤. This mechanism requires\n*Denotes equal contribution.\n\n2 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nspecial care setting the guidance scale 𝑤, which directly affects\nimage quality, diversity and prompt alignment of the generated\nimage. Selecting a proper guidance scale is extremely challenging.\nThe VAE latent space, which we refer to as diffusion space , has a\ncomplex high-dimensional landscape with non-uniform densities.\nProperly navigating through this landscape requires skipping low-\nlikelihood regions towards a nearby mode which aligns well with\nthe prompt.\nFrom this perspective, we can think of CFG as a tool that assists\nnavigating in diffusion space. CFG applies correction steps, by which\nthe latent is iteratively refined to agree with both the prompt and the\nprior distribution of the diffusion model. The size 𝑤of the correction\nsteps fundamentally affects the success of proper convergence to an\nimage which is both visually appealing and adheres to the prompt.\nRecent works have attempted to address the instability of CFG\nby proposing schedulers for the guidance scale 𝑤, typically defined\nas functions of the timestep 𝑡. However, these schedules are often\nmanually designed and based on opposing heuristics. Crucially, such\nmethods do not adapt to the initial noise or the evolving denoising\ntrajectory—factors that are essential for navigating the diffusion\nspace effectively.\nTo address this limitation, we propose a learning-based scheduler\nthat adapts the guidance scale throughout the generation process.\nOur approach leverages the signal 𝛿𝑡=𝜖𝑐\n𝑡−𝜖∅\n𝑡, which captures\nthe discrepancy between the model’s conditional and unconditional\npredictions at each step. We train a lightweight MLP to predict 𝑤as a\nfunction of both the timestep 𝑡and∥𝛿𝑡∥, enabling trajectory-aware,\nsample-specific guidance.\nOur method builds upon CFG++ [Chung et al .2024], an improved\nvariant of CFG that casts the sampling process as an optimization\nproblem. Specifically, it views guidance as a gradient descent step\nthat minimizes the Score Distillation Sampling (SDS) loss [Poole\net al.2022], which measures the model’s accuracy in predicting\nthe true noise based on the prompt. In this framework, the signal\n𝛿𝑡naturally emerges as a proxy for the gradient of the SDS loss,\nproviding a principled way to steer the denoising trajectory toward\nprompt-consistent samples.\nFig. 1 presents examples where our annealing scheduler enhances\nprompt alignment and corrects generation artifacts, resulting in\nvisually pleasing images that more accurately reflect the user’s\nintent.\nFig. 2 illustrates the behavior of our annealing scheduler. As\nshown in the plots, the predicted scale 𝑤evolves differently across\ntwo generations ( AandB), exhibiting non-monotonic fluctuations\nthat adapts to each denoising trajectory. This adaptive behavior\ncontrasts with the fixed guidance scales used in CFG and CFG++,\nwhich cannot account for such variations. For scene A, our scheduler\ncorrects artifacts present in the baselines, most notably the distorted\nanatomy of the woman’s hands, resulting in a higher-quality image.\nFor scene B, our method produces an image that is more faithfully\naligned with the prompt, accurately capturing the specified number\nof objects, unlike the generations produced by the baselines.\nWe further explore the behavior of the annealing scheduler over a\ntoy example, and demonstrate quantitatively and qualitatively that\nour navigation scheme improves the quality and prompt alignment\n0 200 400 600 800 1000\nTimestep0.91.01.11.21.3\nGuidance ScaleCFG++ (A+B)\nAnnealing (A)\nAnnealing (B)CFG CFG++ Annealing (Ours)\nA:\"Woman in black dress on the red carpet wearing a ring on the finger.\"\nB:\"Two dogs, one cat\"\nFig. 2. Guidance Scale Over Time. Top: Guidance scale trajectories for\ntwo prompts: AandB. CFG++ uses a constant scale for both prompts, while\nour annealing scheduler dynamically adapts the scale per prompt. CFG is\nomitted from the plot for clarity but uses a fixed scale of 𝑤=10. Bottom:\nComparison of generations from CFG (left), CFG++ (center) and our method\n(right). Our scheduler improves both quality and alignment: resolving visual\nartifacts (distorted hands, scene A) and correcting object counts (scene B).\nof generated images. Notably, our scheduler achieves state-of-the-\nart performance on FID/CLIP and FD-DINOv2/CLIP when evaluated\non MSCOCO17 [Lin et al .2014], outperforming prior methods by a\nconsiderable margin.\n2 Related works\n2.1 Guidance in Diffusion Models\nDiffusion-based models have emerged as the driving force behind\nadvanced generative modeling, defining the state-of-the-art in the\nsynthesis of high-quality, diverse, and coherent data across various\ndomains. A significant aspect of diffusion-based generative models\nis their ability to perform sampling guided by specific conditions,\nwith text-based conditioning being the most commonly employed.\nThe conditioning mechanism in diffusion-based generative mod-\nels can be implemented in various ways, with classifier-free guid-\nance (CFG) [Ho and Salimans 2022] emerging as a foundational\nand widely adopted technique. CFG replaces the use of external\n\nNavigating with Annealing Guidance Scale in Diffusion Space •3\ngradients [Dhariwal and Nichol 2021] by combining conditional\nand unconditional model outputs in a linear manner, offering a\npowerful and flexible method for controlling generation. This ap-\nproach has become a standard in most modern sampling algorithms,\nsignificantly enhancing both the quality and controllability of gen-\nerated outputs. Additionally, other approaches extend conditioning\nthrough internal feature corrections [Voynov et al .2023], domain-\nspecific architectural adaptations [Ye et al .2023; Zhang et al .2023],\nand alternative strategies [Liu et al .2023; Tumanyan et al .2023],\nfurther enriching the capabilities of diffusion-based models.\n2.2 Advanced Sampling\nClassifier-Free Guidance (CFG) sampling with a simple solver pro-\nduces plausible results; however, models often struggle to generate\ncomplex scenes, such as those with intricate compositions or mul-\ntiple elements [Chefer et al .2023; Dahary et al .2025]. Despite its\nwidespread use, CFG introduces an inherent tradeoff between faith-\nfulness to the desired prompt and diversity, where increasing the\nguidance scale enhances alignment with the conditioning but re-\nduces output variability. Moreover, simply increasing the guidance\nscale is not always effective, as it can result in unnatural artifacts\nor over-saturated images that compromise realism. Additionally,\ncertain seeds have been shown to consistently produce low-quality\nimages [Xu et al. 2024].\nSeveral works have proposed improved sampling techniques\nto address these challenges. One approach considers various non-\nlearnable hyperparameter configurations for the noise scheduler\nand guidance scales [Karras et al .2022a]. Another method intro-\nduces guidance distillation, enabling the use of a single model to\nstreamline the sampling process [Meng et al .2023]. To mitigate\nissues at higher guidance scales, some techniques suggest clipping\nthe guidance step size to prevent over-saturation [Lin et al .2024;\nSadat et al .2024], while others propose controlling the step size\nusing empirically designed schedulers [Kynkäänniemi et al .2024;\nSadat et al. 2023; Wang et al. 2024].\nOther studies have proposed modifications to CFG to address\nits limitations. Some approaches restrict the guidance to the image\nmanifold, ensuring more coherent outputs [Chung et al .2024], while\nothers redefine the guidance process by introducing a new basis that\nbetter separates the denoising and prompt-guidance components\n[Sadat et al .2024]. More relevant to our work are techniques that\nemploy non-constant guidance, such as adjusting the steps at which\nguidance is applied [Dinh et al .2024; Kynkäänniemi et al .2024],\nmodifying guidance based on segmentation of generated objects\n[Shen et al .2024], or altering the unconditional component in the\nCFG formulation [Karras et al. 2024].\n3 Overview\nThe Classifier-Free Guidance (CFG) sampling equation in the sim-\nplest case is given by:\nˆ𝜖𝑡=𝜖∅\n𝑡+𝑤·\u0000𝜖𝑐\n𝑡−𝜖∅\n𝑡\u0001, (1)\nwhere ˆ𝜖𝑡is the guided noise prediction at time step 𝑡,𝜖∅\n𝑡is the\nunconditional model output, 𝜖𝑐\n𝑡is the conditional model output,\nand𝑤is the guidance scale that controls the extent to which we\nFig. 3. Classifier-Free Guidance step. The denoising step of a sample 𝑧𝑡is\nillustrated as a linear combination of the unconditional noise prediction 𝜖∅\n𝑡\nand the conditional noise prediction 𝜖𝑐\n𝑡. The dashed line represents possible\n𝑧𝑡−1predictions using CFG, for the figure simplicity, we don’t depict the\nrescaling of 𝑧𝑡which is performed at each denoising step. 𝑧(1)\n𝑡−1and𝑧(2)\n𝑡−1\ndenote predictions corresponding to two different guidance scales, 𝑤1and\n𝑤2, respectively. The blue manifold represents the density 𝑝𝑡(𝑧), while the\norange manifold illustrates the conditional distribution density 𝑝𝑡(𝑧|𝑐).\nextrapolate from the unconditional to the conditional outputs (see\nsupplement for a detailed algorithm).\nThe guidance scale 𝑤determines the strength of alignment with\nthe conditioning input, with higher values improving alignment\nbut potentially reducing diversity or introducing artifacts. This is\nillustrated in Fig. 3, which depicts a denoising step of 𝑧𝑡over the\ndensity manifold 𝑝𝑡(𝑧)toward the density manifold 𝑝𝑡−1(𝑧). We\nshow the unconditional noise direction 𝜖∅\n𝑡and the conditional noise\ndirection𝜖𝑐\n𝑡.\nThe CFG operation aims to increase the probability 𝑝𝑡(𝑐|𝑧)while\nstaying on the manifold defined by natural images by extrapolating\nbetween𝜖𝑐\n𝑡and the unconditional prediction 𝜖∅\n𝑡, weighted by a\nfactor𝑤. The figure illustrates extrapolations with two scales: one\nwith𝑤1, which undershoots the target distribution, and another\nwith𝑤2, which overshoots.\nDetermining the optimal size of the guidance scale 𝑤is a non-\ntrivial task, as it depends on the distribution’s local geometry, the\ntarget prompt, the initial noise, and the model itself.\nThe commonly used approach is to keep 𝑤constant throughout\nthe generation process. While other works have explored relations\nbetween𝑤and timesteps— we argue that 𝑤should also depend on\nthe difference defined as\n𝛿𝑡=𝜖𝑐\n𝑡−𝜖∅\n𝑡. (2)\nSpecifically, 𝛿𝑡is affected by the model’s predictions on the cur-\nrent noisy latent in relation to the prompt, and thus encapsulates\ninformation specific to the denoising trajectory. This dependency\nsuggests that a fixed or simplistic scheduling of 𝑤may not be suf-\nficient for achieving optimal results, stressing the need for more\nadaptive approaches.\n\n4 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\n(a)𝜆=0.6\n (b)𝜆=0.7\n (c)𝜆=0.8\nFig. 4. Heatmaps showing the predicted guidance scale 𝑤𝜃as a function of timestep 𝑡and∥𝛿𝑡∥, for three values of 𝜆. The color represents the value of\n𝑤𝜃(𝑡,∥𝛿𝑡∥, 𝜆), with the colormap shown on the right. Larger 𝑡corresponds to earlier diffusion steps, with 𝑡=0marking the end of denoising. At each step,\n∥𝛿𝑡∥is recomputed and used to dynamically predict the guidance scale, forming a trajectory over time as demonstrated in Fig. 2.\nGiven the temperamental behavior of 𝑤, we propose a learning-\nbased approach to determine its optimal value. Specifically, we learn\n𝑤as a function of the timestep 𝑡and∥𝛿𝑡∥, enabling a more adaptive\nand context-aware guidance scale.\n3.1 SDS and CFG++\nScore Distillation Sampling (SDS) [Poole et al .2022] is a technique\nfor aligning input data with a target distribution defined by a pre-\ntrained diffusion model, by leveraging gradients extracted from the\nmodel. It operates using the explicit SDS loss [Zhu et al. 2024]:\n𝐿SDS(𝑧0)=E𝑡,𝜖\r\r𝜖𝑐\n𝑡(𝑧𝑡)−𝜖\r\r2\n2, (3)\nwhich encourages the optimized input 𝑧0to both align with the con-\nditional signal 𝑐and remain consistent with the model distribution.\nHere,𝑧𝑡is the noisy latent corresponding to 𝑧0,𝜖is the sampled\ntrue noise, and 𝜖𝑐\n𝑡is the conditional prediction of the model.\nRecent work [Chung et al .2024] adopts this loss formulation to\nreinterpret guidance as a diffusion-based inverse problem [Chung\net al.2022a]. By solving for 𝑧0under the constraint that it lies on the\nclean data manifold 𝑝0(𝑧), this approach yields a sampling scheme\nsimilar to CFG.\nThis reformulation, termed CFG++, introduces two key modifi-\ncations to ensure 𝑧0is on the image manifold: (1) it restricts the\nguidance scale 𝑤*to the interval[0,1]; and (2) in contrast to CFG,\nwhich uses the guided noise prediction ˆ𝜖𝑡for both denoising and\nrenoising when computing 𝑧𝑡−1from𝑧𝑡, CFG++ uses ˆ𝜖𝑡for denois-\ning but reintroduces noise using the unconditional prediction 𝜖∅\n𝑡.\nWe refer to the supplement for the full algorithm.\nWith these adjustments, Eq. (1)can be interpreted as a manifold-\nconstrained gradient descent (MCG) step [Chung et al .2023, 2022b]\ntoward minimizing the SDS loss, thereby enhancing prompt align-\nment. Notably, the MCG is approximated at each step by\n∇𝑧0|𝑡𝐿SDS=2𝛾𝑡(𝜖𝑐\n𝑡−𝜖∅\n𝑡), (4)\nwhere𝑧0|𝑡is the current estimate of denoised latent, and 𝛾𝑡=√¯𝛼𝑡/√1−¯𝛼𝑡is a time-dependent coefficient that scales the current\nnoise level to the latent space.\nSubstituting Eq. (2)into Eq. (4)reveals that 𝛿𝑡can serve as a time-\nnormalized proxy for the SDS gradients. Consequently, smaller\nvalues of∥𝛿𝑡∥indicate proximity to stationary points of the SDS\nloss. Intuitively, if 𝑧𝑡is within the model’s distribution, stronger\n*For simplicity, we use 𝑤to interchangeably denote the guidance scale of both CFG\nand CFG++.alignment between the conditional and unconditional predictions\ncorresponds to better adherence to the prompt.\nIn the following section, we build upon this insight to design our\nscheduler. While constraining the guidance scale 𝑤to the interval\n[0,1]is theoretically well-motivated, we argue that this restriction\ncan hinder the guidance mechanism’s ability to explore diverse\nmodes of the conditional distribution 𝑝(𝑧|𝑐), ultimately limiting\nprompt adherence. To overcome this limitation, we lift the constraint\non𝑤and instead train our scheduler to robustly balance between\nmode exploration and fidelity to the data manifold.\n4 Annealing Scheduler\nBuilding upon our insight that 𝛿𝑡captures trajectory-specific infor-\nmation and that its norm is representative of the SDS convergence,\nwe propose a learnable model 𝑤𝜃(𝑡,∥𝛿𝑡∥,𝜆)that maps the timestep\n𝑡and the magnitude ∥𝛿𝑡∥to a guidance scale. The scalar 𝜆∈[0,1]\nserves as a user-defined input that controls the trade-off between\nimage quality and prompt alignment, offering an interpretable al-\nternative to manually selecting a fixed guidance scale 𝑤. Instead of\ndirectly tuning 𝑤as in vanilla CFG and CFG++, the user specifies\na high-level preference via 𝜆, and the scheduler adaptively deter-\nmines the optimal 𝑤throughout the generation process. Through\nexperimentation (Sec. 6), we show that this formulation yields more\nconsistent and controllable outcomes.\nDuring inference, we incorporate our scheduler to the CFG++\nsampling mecahnism by replacing the constant guidance scale 𝑤in\nEq. (1) to achieve:\nˆ𝜖𝑡=𝜖∅\n𝑡+𝑤𝜃(𝑡,𝛿𝑡,𝜆)·\u0000𝜖𝑐\n𝑡−𝜖∅\n𝑡\u0001. (5)\nWe implement 𝑤𝜃as a lightweight MLP and train it with a subset\nof the LAION-POP dataset [Schuhmann et al .2022], which was\ncurated for high resolution and high prompt-aligned images. We\nprovide full implementation details in the supplement.\nDuring training, the pre-trained diffusion model is kept frozen. At\neach iteration, we sample an image with its corresponding caption\n𝑐, together with a random timestep 𝑡and noise𝜖to compute𝑧𝑡. The\nguided noise prediction ˆ𝜖𝑡is obtained from Eq. (5). The parameter\n𝜆∈[0,1]is sampled uniformly.\nOur training loss balances between two objectives, as governed\nby𝜆:\nL=𝜆𝐿𝛿\n𝑡+(1−𝜆)𝐿𝜖\n𝑡. (6)\nHere,𝐿𝛿\n𝑡and𝐿𝜖\n𝑡are loss terms that promote prompt alignment and\nimage quality, respectively. We now turn to formally define these\nlosses, and refer to the supplement for the full training algorithm.\n\nNavigating with Annealing Guidance Scale in Diffusion Space •5\nCFG APG CFG++ Annealing\n\"Aphoto of unicorn driving a jeep in the desert\"\n\"Aknight in rainbow armor riding a dragon made of fire \"\n\"A cat looking through a glass of water \"\n\"Ayellow dog runs to grab a yellow frisbee in the grass.\"\n\"Bear cubs play among the fallen tree limbs.\"\n\"A traffic sign that has a picture of a man holding a surfboard on it.\"\nFig. 5. Qualitative comparison of our Annealing method 𝜆=0.8(right\ncolumn) vs. three guidance methods: CFG ( 𝑤=15), APG ( 𝑤=20) and\nCFG++ ( 𝑤=1.2).\n𝛿-loss. Following our observation in section (3.1), we introduce\na novel loss, leveraging ∥𝛿𝑡∥as a proxy value that aims to reflect\nprompt alignment. This loss is designed to encourage the scheduler\nto select guidance scales that move the denoising trajectory toward\nregions where the model’s conditional and unconditional predic-\ntions begin to agree, indicating proximity to a prompt-consistent\nstationary point of the SDS loss.\nIn practice, for a given 𝑧𝑡, we perform denoising with ˆ𝜖𝑡and\nrenoising with 𝜖∅\n𝑡to obtain𝑧𝑡−1. By evaluating∥𝛿𝑡−1∥at this point,\nwe introduce our 𝛿-loss:\n𝐿𝛿\n𝑡=∥𝛿𝑡−1∥2\n2. (7)\nThis loss leverages the diffusion model’s prior of the alignment with\nthe target prompt. However, solely optimizing on 𝐿𝛿\n𝑡results in veryhigh guidance scales, leading to out-of-distribution samples, similar\nto𝑧(2)\n𝑡−1in Fig. 3 (see Sec. 5,6 for further analysis). Therefore, we opt\nto maintain fidelity to the data manifold using the second loss term\n𝐿𝜖\n𝑡.\n𝜖-loss. To ensure that the predicted guided noise ˆ𝜖𝑡from Eq. (5)\nmatches the sampled noise 𝜖, we introduce a denoising objective,\nnamely, the reconstruction loss:\n𝐿𝜖\n𝑡=∥ˆ𝜖𝑡−𝜖∥2\n2. (8)\nThis loss resembles the standard denoising diffusion objective, but\ninstead of applying to the conditional model prediction, it operates\non the guided prediction ˆ𝜖𝑡, which combines both conditional and\nunconditional signals. Its primary role is to regularize the 𝛿-loss by\npreventing the guidance scale from pushing the generation toward\nimplausible regions. By encouraging ˆ𝜖𝑡to remain close to the true\nnoise𝜖, this loss helps preserve visual quality and ensures that the\ndenoising trajectory remains within realistic bounds.\nPrompt Perturbation. During training, each latent 𝑧𝑡is paired with\na prompt𝑐that closely matches the corresponding image. Even after\napplying noise to obtain 𝑧𝑡, semantic information about the prompt\nremains encoded in the latent [Lin et al .2024], preserving alignment\nthroughout the denoising trajectory. In contrast, inference begins\nfrom pure noise, and the prompt is injected through the denoising\nprocess. As shown by prior work [Ma et al .2025; Samuel et al .2024;\nSinghal et al .2025], the alignment of complex prompts remains\nhighly sensitive to the initial seed, often leading to greater variability\nat inference time.\nTo simulate this mismatch, we inject Gaussian noise into the\nprompt embeddings during training (see supplement for details).\nThis exposes the scheduler to imperfect prompt-image alignment,\nimproving its robustness.\nOur approach was motivated by CADS [Sadat et al .2023], where\nnoise is injected into the prompt embeddings during inference to\nencourage mode diversity. Their analysis showed that this pertur-\nbation smooths the conditional score ∇𝑧𝑡log𝑝(𝑐|𝑧𝑡), acting as\na regularizer that prevents the model from collapsing onto domi-\nnant modes. In contrast, we apply this principle during training to\nenhance robustness, enabling the scheduler to generalize across a\nrange of prompt-image alignment scenarios.\nThis technique improves the scheduler’s behavior across different\nguidance regimes. When 𝜆is low and𝐿𝜖\n𝑡dominates, it promotes the\ngeneration of high-quality images even under imprecise alignment.\nWhen𝜆is high and𝐿𝛿\n𝑡dominates, it helps the scheduler adaptively\nshift toward nearby modes that better satisfy the prompt.\nPredicted guidance scales. We present the learned guidance scales\npredicted by the trained scheduler in Figure 4. As shown, the sched-\nuler adapts its annealing strategy based on different values of the\nuser-specified parameter 𝜆.\n5 2D Toy Example\nWe now turn to investigating the behavior of our annealing sched-\nuler in a controlled and interpretable setting using a 2D toy example.\n\n6 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nzT(start)\nz0(end)\n(a)𝑤=0.1\n (b)𝑤=0.15\n (c)𝑤=0.2\n (d)Annealing\nFig. 6. A 2D diffusion toy example with a distribution density shaped as a wide ring. Random seeds conditioned on 𝑐=3𝜋/4are plotted, with their denoising\ntrajectories shown in gray. The dashed section highlights a region within a tolerance of ±𝜋/64from 𝑐=3𝜋\n4where the manifold density is high. (a)𝑤=0.1:\nSampling with low guidance scale shows sub-optimal condition adherence. (b)𝑤=0.15: Moderate guidance improves alignment, though some samples\nremain out of distribution. (c)𝑤=0.2: Stronger guidance overfits the condition, at the expense of the sample quality. (d)Our Annealing scheduler achieves\nbetter condition alignment while remaining on the sample manifold.\nlog(t)\n𝑡=1 𝑡=24 𝑡=49\nFig. 7. log||𝛿𝑡||heatmap for 𝑐=3𝜋/4.This measures the alignment\nbetween conditional and unconditional predictions across timesteps. The\nregion between black circles indicates high sample density; the blue dashed\nline marks the target condition. 𝑡=49: Noise dominates, and predictions\ncluster near the source distribution center. 𝑡=24: Alignment improves\nnear the target, though lower values persist off-distribution. 𝑡=1: A local\nminimum emerges at the target location on the ring.\nIn Fig. 6, we illustrate the behavior of a diffusion model trained to\napproximate a target distribution shaped as a wide ring. The condi-\ntional distribution is defined over the angular variable 𝑐∼𝑈(0,2𝜋),\nwhile the initial noise samples 𝑧𝑇are drawn from a standard normal\ndistribution. We condition generation on 𝑐=3𝜋\n4and visualize the\ndenoising trajectories under different constant guidance scales with\nCFG++, as well as our adaptive scheduler.\nThe formed trajectories demonstrate both the strengths and limi-\ntations of classifier-free guidance. Increasing the guidance scale en-\nforces stronger adherence to the conditioning signal but also pushes\nsamples away from the data manifold. In contrast, our annealing\nscheduler (Fig. 12d) adaptively modulates the guidance strength\nduring denoising, resulting in improved condition alignment while\npreserving fidelity to the data manifold. As can be seen, our sched-\nuler also achieves better coverage of the conditional distribution,\nreflecting more diverse and representative generations.\nTo support our insight into the 𝛿𝑡-loss (Eq. 7), we display the\nnorm∥𝛿𝑡∥across different denoising steps in Fig. 7, for the same\nconditioning value 𝑐=3𝜋\n4. As𝑡decreases, we observe that ∥𝛿𝑡∥\nbecomes small near the correct region of the ring, indicating that the\nconditional and unconditional predictions are well aligned and thesample is approaching the target mode. This implies that promot-\ning low∥𝛿𝑡∥throughout the denoising process can lead to better\nalignment with the conditioning signal.\nHowever,∥𝛿𝑡∥also tends to have low values away from the ring\n(e.g., Fig. 7, 𝑡=1), suggesting that minimizing ∥𝛿𝑡∥alone may\nguide samples off the data manifold. This highlights the need for an\nadditional regularization term, such as the 𝜖-loss (Eq. (8)), to ensure\nthat generations remain faithful to the data manifold.\n6 Experiments and Results\nTo evaluate our annealing guidance scheduler, we conduct a com-\nprehensive set of experiments, including qualitative comparisons,\nquantitative evaluations, and ablation studies. We compare our\nmethod against existing guidance scheduling approaches, including\nAPG [Sadat et al .2024], CFG++ [Chung et al .2024], and the com-\nmonly used CFG [Ho and Salimans 2022] baseline. All experiments\nare performed using SDXL [Podell et al .2023]. In the supplementary\nmaterial, we provide additional experiments demonstrating the ef-\nfectiveness of our scheduler when applied with different solvers and\nnoise schedules, as well as its extension to flow matching models,\nfurther highlighting its generalizability.\n6.1 Qualitative comparisons\nWe compare our annealing guidance scheduler qualitatively in Fig. 5.\nAs shown, our method consistently delivers superior results both in\nimage quality and prompt alignment.\nIn the first row, where the prompt specifies a photo of a unicorn\ndriving a jeep, baseline methods produce cartoonish results or in-\ntroduce visual artifacts, and none correctly place the unicorn inside\nthe jeep. Our method, by contrast, generates a photo-realistic image\nthat is both prompt-aligned and compositionally accurate.\nIn the second row, our approach is the only one to correctly render\nthe knight in rainbow armor. Other methods leak the rainbow onto\nthe dragon’s torso, with CFG and APG even hallucinating an extra\ndragon head.\nIn the third row, only our scheduler generates the water in the\nglass, and in the fourth row, generates a dog with yellowish fur.\n\nNavigating with Annealing Guidance Scale in Diffusion Space •7\n(a)FID/CLIP similarity (b)FD-DINOv2/CLIP similarity\n32.10 32.15 32.20 32.25 32.3024.50\n25.00\n25.50\n26.00\n26.50\n27.000.60\n0.80\n1\n1.2010\n15 17.50\n20\n250\n0.05\n0.10\n0.40\n0.60\n0.70\n0.80\n0.90\n17.5\n9\n10\n12.5\n15ClipFID\nOurs/uni00A0( )\nCFG++/uni00A0(w)\nAPG/uni00A0(w)\nCFG/uni00A0(w)\n32.10 32.15 32.20 32.25 32.30265.00\n270.00\n275.00\n280.00\n285.00\n290.00\n295.000.60\n0.80\n1\n1.2010\n15\n17.50\n20\n250\n0.05\n0.10\n0.40\n0.60\n0.70\n0.80\n0.90\n17.5\n9\n10\n12.5\n15\nClipFD/uni00ADDINOv2Ours/uni00A0( )\nCFG++/uni00A0(w)\nAPG/uni00A0(w)\nCFG/uni00A0(w)\nFig. 8. Quantitative Metrics. (a) FID versus CLIP. (b)FD-DINOv2 versus CLIP.\nIn the fifth row, our method successfully separates the bear from\nthe tree, whereas other methods blend the two together, producing\nunrealistic results.\nAdditional qualitative comparisons are shown in Figs. 9 and 10,\nagainst CFG++ and CFG, respectively, with more results provided\nin the supplementary material.\n6.2 Quantitative comparisons\nWe conduct a quantitative evaluation on the MSCOCO 2017 valida-\ntion set by generating 5,000 images per model using identical seeds.\nImage quality is assessed using FID [Heusel et al .2018] and the\nrecently proposed FD-DINOv2 [Oquab et al .2024], while prompt\nalignment is measured via CLIP similarity [Radford et al. 2021].\nTo visualize the trade-off between image quality and prompt align-\nment at commonly used CFG guidance scales ( 𝑤≥7.5), we plot FID\nvs. CLIP and FD-DINOv2 vs. CLIP in Figure 8. As shown, APG does\nnot improve over CFG across these metrics, while CFG++ provides\ngains only in the FID/CLIP space. In contrast, our method consis-\ntently enhances both alignment and image quality, outperforming\nall baselines across both evaluation criterias.\nFor direct comparison, we select multiple operating points of\nour scheduler by varying 𝜆, and match each to the closest config-\nuration of CFG, CFG++, or APG in terms of FD-DINOv2. Table 1\nreports the corresponding FID, CLIP similarity, and additionally\nImageReward [Xu et al .2023] for human-preference, and precision\nand recall [Kynkäänniemi et al .2019] for quality and diversity re-\nspectively. Across all settings, our scheduler achieves the lowest FID,\nthe highest CLIP similarity, and consistently outperforms baselines\nin recall at higher guidance strengths. Notably, it also attains the\nhighest ImageReward in two out of four matched configurations. A\nfull table including FD-DINOv2 scores, and implementation details\nfor evaluation is provided in the supplementary material.\n6.3 Ablation Studies\nTo understand the contribution of each component in our method,\nwe conduct ablation studies by retraining the scheduler from scratch\nunder different configurations. In all cases, we fix the prompt align-\nment parameter to 𝜆=0.8during evaluation, and report FID, CLIP,\nand ImageReward to assess the trade-off between visual quality and\nprompt alignment. Table 2 summarizes the results.Method FID ↓CLIP ↑IR↑ P↑ R↑\nCFG (𝑤=7.5) 25.13 32.12 0.817 0.863 0.630\nAPG (𝑤=10) 25.25 32.08 0.818 0.862 0.631\nCFG++ (𝑤=0.6) 24.97 32.12 0.808 0.859 0.629\nOurs (𝜆=0.05) 24.76 32.16 0.809 0.860 0.620\nCFG (𝑤=10) 26.06 32.22 0.859 0.859 0.594\nAPG (𝑤=15) 26.60 32.19 0.865 0.864 0.592\nCFG++ (𝑤=0.8) 25.61 32.20 0.857 0.855 0.601\nOurs (𝜆=0.4) 25.35 32.25 0.865 0.859 0.606\nCFG (𝑤=12.5) 26.61 32.25 0.881 0.850 0.570\nAPG (𝑤=17.5) 26.58 32.21 0.887 0.861 0.586\nCFG++ (𝑤=1) 26.33 32.26 0.882 0.848 0.570\nOurs (𝜆=0.7) 25.95 32.26 0.884 0.852 0.594\nCFG (𝑤=15) 27.15 32.27 0.883 0.844 0.570\nAPG (𝑤=20) 26.85 32.23 0.893 0.855 0.577\nCFG++ (𝑤=1.2) 26.84 32.28 0.894 0.847 0.551\nOurs (𝜆=0.8) 26.40 32.29 0.898 0.846 0.586\nTable 1. Comparison of CFG, APG, CFG++, and our method across FID,\nCLIP similarity, Image Reward (IR), Precision (P), and Recall (R). Arrows\nindicate whether higher ( ↑) or lower ( ↓) values are better.\nWe assess the role of inputs to the scheduler. Omitting timestep\ninformation ( w/o𝑡) or the alignment signal ( w/o𝛿𝑡) inputs leads\nto lower performance in all metrics, indicating that both inputs\ncontribute to the overall effectiveness of our scheduler.\nDropping CFG++’s renoising step ( w/o CFG++ Renoise ) results in\na significant drop in CLIP and ImageReward.\nRemoving prompt perturbation during training ( w/o Perturbation )\ndegrades performance across all metrics, indicating its importance\nfor robustness, and constraining the predicted guidance scale 𝑤to\nthe range[0,1], as done in CFG++ ( Constrained 𝑤), achieves the\nlowest FID, but at the cost of reduced alignment and reward. Given\nthis trade-off, we deliberately opt to leave 𝑤unconstrained, as it\nenables a better overall balance across metrics—maintaining strong\nprompt alignment and perceptual quality.\n7 Conclusions\nWe have presented an annealing guidance scheduler that adaptively\nadjusts the guidance scale throughout the denoising process. Unlike\nthe widely used CFG, which relies on a fixed guidance scale, and\n\n8 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nConfiguration FID ↓CLIP ↑ImageReward ↑\nAnnealing ( 𝜆=0.8) 26.40 32.29 0.898\nw/o𝑡 26.86 32.27 0.896\nw/o𝛿𝑡 26.97 32.28 0.896\nw/o CFG++ Renoise 26.34 32.18 0.831\nw/o Perturbation 27.01 32.25 0.884\nConstrained 𝑤 26.15 32.25 0.880\nTable 2. Ablation study results. Each variant removes or modifies a key\ncomponent of our model.\nits improved variant CFG++, our method dynamically determines\nstep sizes based on the evolving structure of the latent space. This\napproach is grounded in viewing guidance as an optimization prob-\nlem aimed at minimizing the SDS loss, steering latents to better\nmatch the prompt while remaining faithful to the model’s prior\ndistribution.\nWe find that this adaptive strategy is particularly beneficial for\ncomplex prompts, where balancing prompt fidelity and sample qual-\nity is most challenging. Nonetheless, our results highlight a funda-\nmental trade-off between strict adherence to the prompt and staying\nwithin the data manifold.\nNavigating the high-dimensional diffusion space remains inher-\nently difficult due to its intricate and multimodal structure. Nev-\nertheless, our work opens the door to future exploration of more\nprincipled, context-aware guidance mechanisms that better adapt\nto the geometry of the denoising trajectory.\nAcknowledgments\nWe would like to thank Oren Katzir, Daniel Garibi, Or Patashnik,\nand Shelly Golan for their early feedback and insightful discussions.\nWe also thank the anonymous reviewers for their thorough and\nconstructive comments, which helped improve this work.\nReferences\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-\nand-excite: Attention-based semantic guidance for text-to-image diffusion models.\nACM Transactions on Graphics (TOG) 42, 4 (2023), 1–10.\nHyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul\nYe. 2022a. Diffusion posterior sampling for general noisy inverse problems. arXiv\npreprint arXiv:2209.14687 (2022).\nHyungjin Chung, Jeongsol Kim, Geon Yeong Park, Hyelin Nam, and Jong Chul Ye. 2024.\nCfg++: Manifold-constrained classifier free guidance for diffusion models. arXiv\npreprint arXiv:2406.08070 (2024).\nHyungjin Chung, Suhyeon Lee, and Jong Chul Ye. 2023. Decomposed diffusion sampler\nfor accelerating large-scale inverse problems. arXiv preprint arXiv:2303.05754 (2023).\nHyungjin Chung, Byeongsu Sim, Dohoon Ryu, and Jong Chul Ye. 2022b. Improving\ndiffusion models for inverse problems using manifold constraints. Advances in\nNeural Information Processing Systems 35 (2022), 25683–25696.\nOmer Dahary, Or Patashnik, Kfir Aberman, and Daniel Cohen-Or. 2025. Be your-\nself: Bounded attention for multi-subject text-to-image generation. In European\nConference on Computer Vision . Springer, 432–448.\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\nsynthesis. Advances in neural information processing systems 34 (2021), 8780–8794.\nAnh-Dung Dinh, Daochang Liu, and Chang Xu. 2024. Compress Guidance in Condi-\ntional Diffusion Sampling. arXiv preprint arXiv:2408.11194 (2024).\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp\nHochreiter. 2018. GANs Trained by a Two Time-Scale Update Rule Converge to a\nLocal Nash Equilibrium. arXiv:1706.08500 [cs.LG] https://arxiv.org/abs/1706.08500\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in neural information processing systems 33 (2020), 6840–6851.\nJonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint\narXiv:2207.12598 (2022).Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022a. Elucidating the de-\nsign space of diffusion-based generative models. Advances in neural information\nprocessing systems 35 (2022), 26565–26577.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. 2022b. Elucidating the Design\nSpace of Diffusion-Based Generative Models. arXiv:2206.00364 [cs.CV] https:\n//arxiv.org/abs/2206.00364\nTero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, and\nSamuli Laine. 2024. Guiding a Diffusion Model with a Bad Version of Itself. arXiv\npreprint arXiv:2406.02507 (2024).\nTuomas Kynkäänniemi, Miika Aittala, Tero Karras, Samuli Laine, Timo Aila, and Jaakko\nLehtinen. 2024. Applying guidance in a limited interval improves sample and\ndistribution quality in diffusion models. arXiv preprint arXiv:2404.07724 (2024).\nTuomas Kynkäänniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. 2019.\nImproved precision and recall metric for assessing generative models. Advances in\nneural information processing systems 32 (2019).\nShanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. 2024. Common diffusion\nnoise schedules and sample steps are flawed. In Proceedings of the IEEE/CVF winter\nconference on applications of computer vision . 5404–5411.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in\ncontext. In Computer vision–ECCV 2014: 13th European conference, zurich, Switzerland,\nSeptember 6-12, 2014, proceedings, part v 13 . Springer, 740–755.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. 2022.\nFlow matching for generative modeling. arXiv preprint arXiv:2210.02747 (2022).\nXihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao\nHu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. 2023. More control for free!\nimage synthesis with semantic diffusion guidance. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision .\nNanye Ma, Shangyuan Tong, Haolin Jia, Hexiang Hu, Yu-Chuan Su, Mingda Zhang,\nXuan Yang, Yandong Li, Tommi Jaakkola, Xuhui Jia, et al .2025. Inference-time scaling\nfor diffusion models beyond scaling denoising steps. arXiv preprint arXiv:2501.09732\n(2025).\nChenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan\nHo, and Tim Salimans. 2023. On distillation of guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition . 14297–14306.\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion\nprobabilistic models. In International conference on machine learning . PMLR, 8162–\n8171.\nMaxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil\nKhalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,\nMahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang,\nShang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu\nXu, Hervé Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bo-\njanowski. 2024. DINOv2: Learning Robust Visual Features without Supervision.\narXiv:2304.07193 [cs.CV] https://arxiv.org/abs/2304.07193\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas\nMüller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion\nmodels for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023).\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion:\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022).\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural\nLanguage Supervision. arXiv:2103.00020 [cs.CV] https://arxiv.org/abs/2103.00020\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 1, 2 (2022), 3.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition . 10684–10695.\nSeyedmorteza Sadat, Jakob Buhmann, Derek Bradley, Otmar Hilliges, and Romann M\nWeber. 2023. CADS: Unleashing the diversity of diffusion models through condition-\nannealed sampling. arXiv preprint arXiv:2310.17347 (2023).\nSeyedmorteza Sadat, Otmar Hilliges, and Romann M Weber. 2024. Eliminating Oversat-\nuration and Artifacts of High Guidance Scales in Diffusion Models. arXiv preprint\narXiv:2410.02416 (2024).\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al.2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in neural information processing systems 35 (2022), 36479–36494.\nDvir Samuel, Rami Ben-Ari, Simon Raviv, Nir Darshan, and Gal Chechik. 2024. Gener-\nating images of rare concepts using pre-trained diffusion models. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , Vol. 38. 4695–4703.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wight-\nman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al .2022. Laion-5b: An open large-scale dataset for training next generation\n\nNavigating with Annealing Guidance Scale in Diffusion Space •9\nimage-text models. Advances in neural information processing systems 35 (2022),\n25278–25294.\nDazhong Shen, Guanglu Song, Zeyue Xue, Fu-Yun Wang, and Yu Liu. 2024. Rethinking\nthe Spatial Inconsistency in Classifier-Free Diffusion Guidance. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition . 9370–9379.\nRaghav Singhal, Zachary Horvitz, Ryan Teehan, Mengye Ren, Zhou Yu, Kathleen\nMcKeown, and Rajesh Ranganath. 2025. A general framework for inference-time\nscaling and steering of diffusion models. arXiv preprint arXiv:2501.06848 (2025).\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015.\nDeep unsupervised learning using nonequilibrium thermodynamics. In International\nconference on machine learning . PMLR, 2256–2265.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020a. Denoising diffusion implicit\nmodels. arXiv preprint arXiv:2010.02502 (2020).\nYang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients of\nthe data distribution. Advances in neural information processing systems 32 (2019).\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Er-\nmon, and Ben Poole. 2020b. Score-based generative modeling through stochastic\ndifferential equations. arXiv preprint arXiv:2011.13456 (2020).\nNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. 2023. Plug-and-Play\nDiffusion Features for Text-Driven Image-to-Image Translation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) . 1921–\n1930.\nAndrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2023. Sketch-guided text-to-image\ndiffusion models. In ACM SIGGRAPH 2023 Conference Proceedings . 1–11.\nXi Wang, Nicolas Dufour, Nefeli Andreou, Marie-Paule Cani, Victoria Fernández Abre-\nvaya, David Picard, and Vicky Kalogeiton. 2024. Analysis of Classifier-Free Guidance\nWeight Schedulers. arXiv preprint arXiv:2404.13040 (2024).\nJiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and\nYuxiao Dong. 2023. ImageReward: Learning and Evaluating Human Preferences\nfor Text-to-Image Generation. arXiv:2304.05977 [cs.CV] https://arxiv.org/abs/2304.\n05977\nKatherine Xu, Lingzhi Zhang, and Jianbo Shi. 2024. Good Seed Makes a Good\nCrop: Discovering Secret Seeds in Text-to-Image Diffusion Models. arXiv preprint\narXiv:2405.14828 (2024).\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. IP-Adapter: Text Compatible\nImage Prompt Adapter for Text-to-Image Diffusion Models. (2023).\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding Conditional Control to\nText-to-Image Diffusion Models.\nJunzhe Zhu, Peiye Zhuang, and Sanmi Koyejo. 2024. HiFA: High-fidelity Text-to-\n3D Generation with Advanced Diffusion Guidance. arXiv:2305.18766 [cs.CV]\nhttps://arxiv.org/abs/2305.18766\n\n10 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nCFG++ Annealing CFG++ Annealing\n“Photo of a bear wearing colorful glasses:\n“left glass is red, right is blue . ““Agloved hand holding a strawberry milkshake...\nEarth visible in the distance on the moon horizon. ”\n“Three Young Foxes by Kain Shannon. “ “A dogchasing a cat in a desert, at high speed. ”\n“A ghost sitting on a living room chair. ” “A yellow diamond-shaped sign with a deer silhouette. ”\n“A photo of a ram and a polar bear walking in London. ” “There is a stop sign outside of a window. ”\n“A whale in an open ocean jumps over a small boat . ” “An old man lifts a barbell above his head. ”\nFig. 9. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG++ 𝑤=0.8(left).\n\nNavigating with Annealing Guidance Scale in Diffusion Space •11\nCFG Annealing CFG Annealing\n“A statue of Abraham Lincoln wearing an opaque and\nshiny astronaut’s helmet . The statue sits on the moon. ”“A baby sitting on a female ’s lap\n“staring into the camera. ”\n“A bride and groom cutting their wedding cake. ” “ A small boy trying to fly a small kite. ”\n“five red balls on a table. ” “ A man and child next to a horse. ”\n“A demonic looking chucky like doll standing next to a white clock . ” “Older woman hula hooping in backyard. ”\n“A dog running with a stick in its mouth , Eiffel tower in the background. “ “A girl riding a giant bird over a futuristic city. ”\nFig. 10. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG 𝑤=10(left).\n\n12 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nA Supplementary Material\nA.1 Implementation Details\nWe provide our annealing scheduler training algorithm in Alg.1, and\nits inference algorithm in Alg.4. Our annealing scheduler 𝑤𝜃(𝑡,𝑧𝑡,𝜆)\nis implemented as a lightweight MLP with three hidden layers of\ndimension 128, resulting in a total of 52K trainable parameters. The\nmodel takes as input sinusoidal embeddings of three features: (1) the\nnormalized timestep 𝑡/𝑇, (2) the normalized guidance magnitude\n∥𝛿𝑡∥/∥𝛿∥max, where∥𝛿∥maxcorresponds to the typical maximum\nnorm of𝛿𝑡observed empirically across the training set and set to 5.0\nin SDXL, and (3) the prompt-alignment parameter 𝜆. Each embed-\nding is 4-dimensional, and the three embeddings are concatenated\nbefore being passed through the first layer. ReLU activations are\napplied after each layer. The network outputs a single scalar corre-\nsponding to the predicted guidance scale. When we constrain the\nguidance scale 𝑤in the ablation to[0,1]we add a sigmoid layer at\nthe output. Training is performed for a maximum of 20,000 steps\nusing the AdamW optimizer with a learning rate of 1e−3and weight\ndecay of 0.01. We train with a per-device batch size of 2 and accu-\nmulate gradients for 8 steps before performing an optimizer update.\nWe use the default Kaiming-uniform initialization. All training runs\ncomplete within approximately 4.5 hours on a single NVIDIA A6000\nGPU (48GB).\nALGORITHM 1: Annealing Scheduler - Training\nRequire:\n𝑤𝜃:trainable guidance scale model;\n𝜖(·)\n𝑡:frozen noise predictor, accepts ∅or a condition, at timestep 𝑡;\n𝑇: total number of denoising steps\nrepeat\n// — Sample data and noise —\nSample(z0,c)∼𝑝(z0,c),𝑡∼𝑈[1,𝑇],𝝐∼N( 0,I),𝜆∼[0,1];\nz𝑡←AddNoise(z0, 𝑡,𝝐);\n˜c←Perturb(c);\n// — Step at time 𝑡—\n𝛿𝑡←𝜖˜𝑐\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡);\nˆ𝝐𝑡←𝜖∅\n𝑡(z𝑡)+𝑤𝜃(𝑡, 𝛿𝑡, 𝜆)·\u0010\n𝜖˜𝑐\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡)\u0011\n;// CFG\n𝑧0|𝑡=(𝑧𝑡−√1−¯𝛼𝑡ˆ𝜖𝑡)/√¯𝛼𝑡; // Denoise\n𝑧𝑡−1=√¯𝛼𝑡−1𝑧0|𝑡+√1−¯𝛼𝑡−1𝜖∅\n𝑡(𝑧𝑡); // Renoise\n// — Step at time 𝑡−1—\n𝛿𝑡−1←𝜖˜𝑐\n𝑡(z𝑡−1)−𝜖∅\n𝑡(z𝑡−1);\n// — Compute loss and update —\nL← 𝜆∥𝛿𝑡−1∥2+(1−𝜆)∥𝝐−ˆ𝝐𝑡∥2;\nTake gradient step on ∇𝜃L; // Update scheduler\nuntil converged ;\nA.2 Training Data\nWe use the LAION-POP subset of LAION-5B dataset [Schuhmann\net al.2022] with high-resolution images with detailed descriptions,\nand selected 20,000 images based on the highest similarity scores.ALGORITHM 2: CFG - Inference (DDIM)\nRequire:\n𝑇: total number of denoising steps;\n𝑤: guidance scale;\n𝜖(·)\n𝑡:frozen noise predictor, accepts ∅or a condition, at timestep 𝑡;\nc: condition;\nz𝑇∼N( 0,I);\nfor𝑡=𝑇to1do\nˆ𝜖𝑡←𝜖∅\n𝑡(z𝑡)+𝑤·(𝜖c\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡)); // CFG\n𝑧0|𝑡←(z𝑡−√1−¯𝛼𝑡·ˆ𝜖𝑡)/√¯𝛼𝑡; // Denoise\nz𝑡−1←√¯𝛼𝑡−1·𝑧0|𝑡+√1−¯𝛼𝑡−1·ˆ𝜖𝑡; // Renoise\nend\nreturn z0\nALGORITHM 3: CFG++ - Inference (DDIM)\nRequire:\n𝑇: total number of denoising steps;\n𝑤∈[0,1]: guidance scale;\n𝜖(·)\n𝑡:frozen noise predictor, accepts ∅or a condition, at timestep 𝑡;\nc: condition;\nz𝑇∼N( 0,I);\nfor𝑡=𝑇to1do\nˆ𝜖𝑡←𝜖∅\n𝑡(z𝑡)+𝑤·(𝜖c\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡)); // CFG\n𝑧0|𝑡←(z𝑡−√1−¯𝛼𝑡·ˆ𝜖𝑡)/√¯𝛼𝑡; // Denoise\nz𝑡−1←√¯𝛼𝑡−1·𝑧0|𝑡+√1−¯𝛼𝑡−1·𝝐∅\n𝒕(z𝒕);// Renoise\nend\nreturn z0\nALGORITHM 4: Annealing Scheduler - Inference (DDIM)\nRequire:\n𝜆∈[0,1]: prompt alignment weighting parameter;\n𝑇: total number of denoising steps;\n𝑤𝜃:trained guidance scale model;\n𝜖(·)\n𝑡:frozen noise predictor, accepts ∅or a condition, at timestep 𝑡;\nc: condition;\nz𝑇∼N( 0,I);\nfor𝑡=𝑇to1do\n𝛿𝑡←𝜖˜𝑐\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡);\nˆ𝜖𝑡←𝜖∅\n𝑡(z𝑡)+𝒘𝜽(𝒕,∥𝜹𝒕∥, 𝝀)·(𝜖c\n𝑡(z𝑡)−𝜖∅\n𝑡(z𝑡));// CFG\n𝑧0|𝑡←(z𝑡−√1−¯𝛼𝑡·ˆ𝜖𝑡)/√¯𝛼𝑡; // Denoise\nz𝑡−1←√¯𝛼𝑡−1·𝑧0|𝑡+√1−¯𝛼𝑡−1·𝝐∅\n𝒕(z𝒕);// Renoise\nend\nreturn z0\nA.3 Memory and Time Consumption\nWe evaluated the inference time of our lightweight model, which\nhas a footprint of only 700KB. Running the model 10,000 times on a\nNVIDIA RTX A5000 yielded a mean inference time of 0.001434 sec-\nonds with a standard deviation of 0.000123 seconds. Given that the\nmodel is activated for 50 timesteps during a typical diffusion process,\nthis results in an additional computational cost of approximately\n0.0717 seconds per sample.\n\nNavigating with Annealing Guidance Scale in Diffusion Space •13\n𝒛𝟎|𝒕𝝐𝒕∅𝜹𝒕\n𝝐𝒕𝒄𝝐𝒕−𝟏∅𝝐𝒕−𝟏𝒄𝜹𝒕−𝟏\n𝝐𝒕−𝟏∅𝝐𝒕−𝟏𝒄𝜹𝒕−𝟏Mode B\nMode A\n𝒛𝟎|𝒕−𝟏(𝟏)𝒛𝟎|𝒕−𝟏(𝟐)𝑝(𝒛𝟎|𝒄) log\nFig. 11. Intuition behind the 𝛿-loss. A 2D illustration showing how the\nmagnitude of 𝛿𝑡=𝜖𝑐\n𝑡−𝜖∅\n𝑡reflects alignment with the prompt. At time 𝑡,\nthe sample 𝑧0|𝑡lies near mode A, which partially aligns with the prompt,\nresulting in a small ∥𝛿𝑡∥. As the denoising progresses, following the direction\nof𝛿𝑡leads toward an augmented mode B that even better reflects the prompt\nsemantics. Among the candidate points, 𝑧(2)\n0|𝑡−1lies closest to mode B, where\nthe conditional and unconditional predictions are best aligned, yielding a\nminimal∥𝛿𝑡−1∥. The 𝛿-loss encourages such behavior.\nA.4 Intuition for 𝛿-loss\nTo provide further intuition into ∥𝛿𝑡∥as a navigational tool, we\npresent a 2D illustration depicting two modes of the conditional\ndistribution 𝑝(𝑧0|𝑐)in Figure 11.\nThe point𝑧0|𝑡represents the estimated clean image at time 𝑡,\nand the vectors 𝜖𝑐\n𝑡and𝜖∅\n𝑡denote the conditional and unconditional\nnoise predictions, respectively (scaling factors omitted for clarity).\nTheir difference, 𝛿𝑡=𝜖𝑐\n𝑡−𝜖∅\n𝑡, shown in blue, reflects the guidance\ndirection.\nAt time𝑡, the sample 𝑧0|𝑡is close to mode A, which corresponds\nto a high-quality image that partially matches the prompt 𝑐. As\na result,𝜖𝑐\n𝑡is only slightly biased toward mode B relative to 𝜖∅\n𝑡,\nleading to a small∥𝛿𝑡∥.\nFrom a navigation perspective, we aim to reach mode B, which\neven better aligns with the prompt. We consider candidate estimates\nalong the blue dashed line.\nThe point𝑧(1)\n0|𝑡is the clean image estimate at the next step when\na small guidance scale 𝑤is used. At this location, there is a larger\ngap between 𝜖𝑐\n𝑡−1and𝜖∅\n𝑡−1, indicating misalignment.\nIn contrast, the point 𝑧(2)\n0|𝑡, which lies near mode B, represents a\nmore optimal solution. Here, both 𝜖𝑐\n𝑡−1and𝜖∅\n𝑡−1are already aligned\ntoward mode B, resulting in a minimal ∥𝛿𝑡−1∥.\nOur𝛿-loss leverages this geometric insight during training by\nencouraging smaller values of ∥𝛿𝑡−1∥through the adaptive selection\nof the guidance scale 𝑤.Noise Scale 𝑠FID↓CLIP ↑ImageReward ↑\n0 27.01 32.25 0.884\n0.025 26.40 32.29 0.898\n0.1 27.17 32.27 0.880\n0.25 28.14 32.27 0.873\nTable 3. Ablation over noise scaling parameter 𝑠in the mode augmentation\nscheme.\nA.5 Prompt Perturbation\nWe perturb the conditioning signal solely during training, following\nCADS [Sadat et al .2023]. In practice, we apply the noise directly to\nthe prompt embedding 𝑐, using the corruption rule:\nˆ𝑐=√︁\n𝛾(𝑡)𝑐+𝑠√︁\n1−𝛾(𝑡)𝑛, 𝑛∼N( 0,𝐼),\nwhere𝛾(𝑡)is a schedule and 𝑠controls the noise level. We adopt a\nlinear schedule with 𝜏1=0,𝜏2=𝑇, such that𝛾(𝑡)decays from 1 to\n0 over the course of denoising, thus inducing higher corruption in\nearlier timesteps. To maintain the norm of the noised embedding,\nwe rescale the signal as proposed in CADS:\nˆ𝑐rescaled =ˆ𝑐−mean(ˆ𝑐)\nstd(ˆ𝑐)std(𝑐)+mean(𝑐),˜𝑐=𝜓ˆ𝑐rescaled+(1−𝜓)ˆ𝑐,\nand set the mixing factor to 𝜓=1.\nWe set the noise scale 𝑠to0.025. We ablate this scale by fixing\n𝜆=0.8and reporting the performance of the trained scheduler in\nterms of FID, CLIP similarity, and ImageReward on the COCO2017\nValidation set in Table 3.\nA.6 Metrics Calculation\nFor assessment of fidelity and diversity, we report Precision and\nRecall [Kynkäänniemi et al .2019] in the DINOv2 [Oquab et al .2024]\nfeature space.\nPrecision measures the fraction of generated samples that lie\nwithin the support of the real image distribution. This is estimated by\nchecking whether each generated sample has a real image among its\n𝑘nearest neighbors in feature space. Conversely, recall quantifies the\nfraction of real images that lie within the support of the generated\ndistribution, also based on their nearest neighbors among generated\nsamples. Higher precision indicates better sample fidelity, while\nhigher recall reflects greater diversity in generation. We used 𝑘=\n5in our reports. Additionally, we report FD-DINOv2, a feature\ndistance metric computed in the same feature space for image quality\nassessment.\nTo construct the generated dataset, we use the same captions\nas the COCO 2017 validation set. Each image in this set has five\nhuman-provided annotations; we consistently use the first caption\nper image for generation. We use a unique random seed for each\nimage, setting it to the corresponding image_id from the COCO\nvalidation set.\n\n14 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nMethod FID ↓FD-DINOv2 ↓CLIP ↑Image Reward ↑Precision ↑Recall ↑\nCFG (𝑤=7.5) 25.13 269.44 32.12 0.817 0.863 0.630\nAPG (𝑤=10) 25.25 268.00 32.08 0.818 0.862 0.631\nCFG++ (𝑤=0.6) 24.97 267.91 32.12 0.808 0.859 0.629\nOurs (𝜆=0.05) 24.76 267.17 32.16 0.809 0.860 0.620\nCFG (𝑤=10) 26.06 281.04 32.22 0.859 0.859 0.594\nAPG (𝑤=15) 26.60 282.09 32.19 0.865 0.864 0.592\nCFG++ (𝑤=0.8) 25.61 279.69 32.20 0.857 0.855 0.601\nOurs (𝜆=0.4) 25.35 279.30 32.25 0.865 0.859 0.606\nCFG (𝑤=12.5) 26.61 288.13 32.25 0.881 0.850 0.570\nAPG (𝑤=17.5) 26.58 286.67 32.21 0.887 0.861 0.586\nCFG++ (𝑤=1) 26.33 288.55 32.26 0.882 0.848 0.570\nOurs (𝜆=0.7) 25.95 285.52 32.26 0.884 0.852 0.594\nCFG (𝑤=15) 27.15 293.93 32.27 0.883 0.844 0.570\nAPG (𝑤=20) 26.85 290.93 32.23 0.893 0.855 0.577\nCFG++ (𝑤=1.2) 26.84 294.22 32.28 0.894 0.847 0.551\nOurs (𝜆=0.8) 26.40 290.33 32.29 0.898 0.846 0.586\nTable 4. Comparison of CFG, APG, CFG++, and our method across FID, FD-DINOv2, CLIP score, Image Reward (IR), Precision, and Recall. Arrows indicate\nwhether higher ( ↑) or lower ( ↓) values are better.\nA.7 Implementation Details for Other Methods\nForCFG++ , we followed the official implementation*and evaluated\nthe method using 𝜆values ranging from 0.4 to 1.2.\nForAPG , we adopted the settings provided in the original pa-\nper [Sadat et al .2024]. Specifically, we used the recommended hy-\nperparameters for SDXL: 𝜂=0,𝑟=15, and𝛽=−0.5, and varied\nguidance scales 𝑤from 7.5through 25.\nA.8 Other Solvers & Noise Schedules\nTo evaluate the robustness of our method across different samplers\nand noise schedules, we adopt the CFG++ renoising step general-\nized to both the Euler sampler [Karras et al .2022b] and the Euler\nAncestral sampler, following the formulations in CFG++ [Chung\net al.2024]. For each sampler, we fix the learned annealing sched-\nuler and evaluate it with 𝜆=0.4, comparing against a CFG++\nbaseline using the same sampler with a fixed guidance weight\n𝑤=0.8. We additionally report results using DDIM for complete-\nness, using a scaled_linear beta schedule with 𝛽start=0.00085\nand𝛽end=0.012, while Euler and Euler Ancestral use a linear\nschedule with 𝛽start=0.0001 and𝛽end=0.02.\nWe report FID, CLIP, and ImageReward in Table 5. Notably, our\nscheduler outperforms the CFG++ baseline in all metrics across\ndifferent solvers.\nA.9 Extension to Flow Matching\nOur scheduler can be naturally extended to flow-based models by\nleveraging the continuous-time formulation of Flow Matching [Lip-\nman et al .2022]. In this setting, we model the trajectory of samples\n*https://github.com/CFGpp-diffusion/CFGppMethod FID ↓CLIP ↑IR↑\nDDIM (CFG++, w=0.8) 25.61 32.20 0.857\nDDIM (Ours, 𝜆=0.4) 25.35 32.25 0.865\nEuler (CFG++, w=0.8) 26.17 32.23 0.867\nEuler (Ours, 𝜆=0.4) 25.92 32.21 0.873\nEuler Ancestral (CFG++, w=0.8) 28.57 32.32 0.900\nEuler Ancestral (Ours, 𝜆=0.4)28.09 32.34 0.906\nTable 5. Comparison across solvers, CFG++ and ours. We consistently\nachieve better metrics in terms of FID, CLIP and IR.\n𝑥(𝑡)using a learned velocity field 𝑣𝜃(𝑥,𝑡,𝑐), governed by the ordi-\nnary differential equation:\n𝑑𝑥\n𝑑𝑡=𝑣𝜃(𝑥,𝑡,𝑐),\nwhere𝑐denotes the conditioning signal. The model is trained to\nmatch the true velocity 𝑥1−𝑥0by sampling intermediate points\n𝑥(𝑡)=𝑥0+𝑡(𝑥1−𝑥0)and minimizing a velocity prediction loss,\nanalogous to the diffusion-based 𝜖-prediction loss. Specifically, the\nequivalent of the 𝜖-loss becomes:\nL𝜖=∥𝑣𝜃(𝑥(𝑡),𝑡,𝑐)−(𝑥1−𝑥0)∥2.\nFurthermore, we define a 𝛿-loss similar to the diffusion model case.\nAfter an integration step to a future point 𝑥(𝑡+Δ𝑡), we compute\nthe discrepancy between the conditional and unconditional velocity\npredictions:\n𝛿𝑡+Δ𝑡=𝑣𝜃(𝑥(𝑡+Δ𝑡),𝑡+Δ𝑡,𝑐)−𝑣𝜃(𝑥(𝑡+Δ𝑡),𝑡+Δ𝑡,∅),\nand define the loss as:\nL𝛿=∥𝛿𝑡+Δ𝑡∥2.\n\nNavigating with Annealing Guidance Scale in Diffusion Space •15\nzT(start)\nz0(end)\n(a)𝑤=1\n (b)𝑤=1.5\n (c)𝑤=2\n (d)Annealing ( 𝜆=0.9)\nFig. 12. A 2D flow matching toy example with a target distribution shaped as a wide ring. Random seeds conditioned on 𝑐=3𝜋/4are shown, along with their\ntrajectories in gray. The dashed region indicates a tolerance band of ±𝜋/64around the target condition 𝑐, where the manifold density is high. (a)𝑤=1.0: Low\nguidance scale results in weak condition adherence. (b)𝑤=1.5: Moderate guidance slightly improves alignment, but some samples still deviate from the\ntarget region. (c)𝑤=2.0: Strong guidance leads to overfitting the condition, pulling samples off the true manifold. (d)Ours: The trained annealing scheduler\nachieves accurate condition alignment while preserving sample quality.\n][t]\nlog(t)\n𝑡=1 𝑡=0.5 𝑡=0\nFig. 13. log||𝛿𝑡||heatmap for 𝑐=3𝜋/4.This quantity illustrates how\nwell unconditional and conditional predictions align in magnitude and\ndirection across the latent space for varying timesteps. The region between\nthe black circles marks areas of high sample density, while the blue dashed\nline represents the desired conditional region. 𝑡=1: At large 𝑡’s, noise\ndominates the samples, with model predictions tending toward the center\nof the source distribution, albeit slightly noisy. 𝑡=0.5: As𝑡increases,\nunconditional and conditional predictions align more closely, showing lower\nvalues near the desired condition, but even lower values remain farther from\nthe distribution in the direction of 𝑐.𝑡=1: By the final timestep, a local\nminimum is achieved at the target location within the ring.\nSimilarly to the toy example presented for diffusion models, we\ntrain a 2D toy flow matching model that predicts the conditional ve-\nlocity field𝑣𝜃(𝑥,𝑡,{𝑐,∅}), and afterwards train an annealing sched-\nuler using the equivalent velocity matching objectives described\nabove (L𝛿andL𝜖). At inference time, as a baseline, we apply guid-\nance by combining the conditional and unconditional velocity pre-\ndictions using a scaled interpolation (namely the guidance scale 𝑤),\nand finally present our scheduler guidance. For the target condition\n𝑐=3𝜋/4, we visualize the resulting trajectories and sample align-\nments in Figure 12. The results show that our proposed annealing-\nbased scheduler achieves better condition alignment and sample\nquality compared to constant guidance scales, confirming the effec-\ntiveness of our approach in the flow matching setting as well. To\nfurther analyze the behavior of the model, we visualize the quantity∥𝛿𝑡∥across different timesteps. As shown in Figure 13, we observe a\npattern similar to that in the diffusion model, highlighting the desir-\nability of low∥𝛿𝑡∥values, which indicate the desirability of better\nagreement between conditional and unconditional predictions.\nA.10 Additional Results\nWe present additional qualitative comparisons to further highlight\nthe differences between methods. Figures 14 and 15 show compar-\nisons against CFG, while Figs. 16 and 17 present comparisons against\nCFG++.\n\n16 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nCFG Annealing CFG Annealing\n“A statue of a pharaoh wearing steampunk glasses,\nwhite t-shirt and leather jacket. ”“Child with curly hair sitting on a wooden swing in a green park,\nholding a red balloon. ”\n“an antique chest with three drawers. “ “Couple on sailboat with a dog on open waters. ”\n“A photo of a young girl playing piano. ” “ Three elephants in a field next to each other. ”\n“The woman holding an umbrella smiles... beside the sidewalk. ” “A woman is carrying many packages in an office building. ”\n“A toilet that has three sea shells on top. ” “A diplodocus standing in front of the Millennium Wheel. ”\nFig. 14. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG 𝑤=10(left).\n\nNavigating with Annealing Guidance Scale in Diffusion Space •17\nCFG Annealing CFG Annealing\n“A woman bending over and looking inside of a mini fridge. ” “A coffee cup that is full of holes . ”\n“A cat that is standing looking through a glass. “ “ Two giraffes moving very quickly in the woods. ”\n“a man standing next to an elephant next to his trunk“ “An airplane leaving a trail in the sky. ”\n“A tiger dancing on a frozen lake. ” “A tropical bird. ”\n“A woman standing next to a young man near a pile of fruit. ” “A grand piano next to the net of a tennis court. ”\nFig. 15. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG 𝑤=10(left).\n\n18 •Shai Yehezkel∗, Omer Dahary∗, Andrey Voynov, and Daniel Cohen-Or\nCFG++ Annealing CFG++ Annealing\n“A child in a yellow raincoat jumping into a puddle,\n“holding a red balloon. ”“A white bear in glasses, wearing tuxedo, glowing hat,\n“and with cigare at the British queen reception. ”\n“A man is shaking hands with another man. “ “A man stands beside his black and red motorcycle near a park. ”\n“An elephant with sun glasses plays with a flute. “ “Two samurai cats... katanas drawn, petals swirling in the background. ”\n“A horse in a field. ” “An owl delivering mail at a snowy train station. ”\n“A lavender backpack with a triceratops stuffed animal head on top. ” “A light blue bicycle chained to a pole... in front of a red building. ”\nFig. 16. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG++ 𝑤=0.8(left).\n\nNavigating with Annealing Guidance Scale in Diffusion Space •19\nCFG++ Annealing CFG++ Annealing\n“A giant snail race through the streets of an old European town,\nonlookers cheering, mid-day sun. ”“Photo of alpines ski resort with yeti instead of humans.\n“It wears a red helmet . ”\n“A giant meteorite with the words ’hello people’ approaching the earth. “ “ A man standing next to a bikes and a motorcycle. ”\n“Long-exposure night shot of neon... huge ghostly animal. “ “A present. ”\n“A ballet dancer next to a waterfall. ” “Bear drinking coffee on a sunny morning street, in Italy. ”\n“A man riding a motorcycle while eating food . ” “Two rams trying to solve math equation. ”\nFig. 17. Qualitative comparison of our Annealing method 𝜆=0.4(right) vs. CFG++ 𝑤=0.8(left).\n\n",
    "source": "http://arxiv.org/abs/2506.24108v1",
    "authors": [
      "Shai Yehezkel",
      "Omer Dahary",
      "Andrey Voynov",
      "Daniel Cohen-Or"
    ],
    "categories": [
      "cs.GR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "type": "content"
  },
  {
    "id": "2506.24097v1_abstract",
    "title": "Ruelle-Pollicott resonances of diffusive U(1)-invariant qubit circuits",
    "content": "Title: Ruelle-Pollicott resonances of diffusive U(1)-invariant qubit circuits\n\nAbstract: We study Ruelle-Pollicott resonances of translationally invariant\nmagnetization-conserving qubit circuits via the spectrum of the\nquasi-momentum-resolved truncated propagator of extensive observables.\nDiffusive transport of the conserved magnetization is reflected in the Gaussian\nquasi-momentum $k$ dependence of the leading eigenvalue (Ruelle-Pollicott\nresonance) of the truncated propagator for small $k$. This, in particular,\nallows us to extract the diffusion constant. For large $k$, the leading\nRuelle-Pollicott resonance is not related to transport and governs the\nexponential decay of correlation functions. Additionally, we conjecture the\nexistence of a continuum of eigenvalues below the leading diffusive resonance,\nwhich governs non-exponential decay, for instance, power-law hydrodynamic\ntails. We expect our conclusions to hold for generic systems with exactly one\nU(1) conserved quantity.",
    "source": "http://arxiv.org/abs/2506.24097v1",
    "authors": [
      "Urban Duh",
      "Marko Žnidarič"
    ],
    "categories": [
      "cond-mat.stat-mech",
      "nlin.CD",
      "quant-ph"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24097v1_content",
    "title": "Ruelle-Pollicott resonances of diffusive U(1)-invariant qubit circuits",
    "content": "arXiv:2506.24097v1  [cond-mat.stat-mech]  30 Jun 2025Ruelle-Pollicott resonances of diffusive U(1)-invariant qubit circuits\nUrban Duh∗and Marko ˇZnidariˇ c\nPhysics Department, Faculty of Mathematics and Physics,\nUniversity of Ljubljana, 1000 Ljubljana, Slovenia\n(Dated: July 1, 2025)\nWe study Ruelle-Pollicott resonances of translationally invariant magnetization-conserving qubit\ncircuits via the spectrum of the quasi-momentum-resolved truncated propagator of extensive ob-\nservables. Diffusive transport of the conserved magnetization is reflected in the Gaussian quasi-\nmomentum kdependence of the leading eigenvalue (Ruelle-Pollicott resonance) of the truncated\npropagator for small k. This, in particular, allows us to extract the diffusion constant. For large k,\nthe leading Ruelle-Pollicott resonance is not related to transport and governs the exponential decay\nof correlation functions. Additionally, we conjecture the existence of a continuum of eigenvalues\nbelow the leading diffusive resonance, which governs non-exponential decay, for instance, power-law\nhydrodynamic tails. We expect our conclusions to hold for generic systems with exactly one U(1)\nconserved quantity.\nCONTENTS\nI. Introduction 1\nA. Ruelle-Pollicott resonances 2\nB. Summary of results 2\nII. Truncated quasi-momentum-dependent\npropagator of extensive observables 3\nA. Brief description 4\nB. Truncation and Ruelle-Pollicott resonances 4\nC. Example: Brickwall quantum circuits with no\nconserved quantities 5\nIII. The leading Ruelle-Pollicott resonance 6\nA. Small quasi-momenta and magnetization\ntransport 7\nB. Quasi-momenta far from zero and correlation\nfunction decay 8\nIV. Beyond the leading Ruelle-Pollicott resonance 9\nA. Powers of the conserved quantity 9\nB. Hydrodynamic tails 10\n1. Extensive spin current 11\nV. Conclusion 12\nReferences 12\nA. Details about the quasi-momentum-dependent\npropagator 15\n1. Space of extensive observables 15\n2. Propagator of extensive observables 15\n3. Space-time symmetries 16\nB. Numerical methods 16\n1. Spectrum of the truncated propagator 16\n2. Correlation functions 16\n3. Domain wall quench 17\n∗urban.duh@fmf.uni-lj.siC. Ruelle-Pollicott resonances through weak\ndephasing 17\nD. Plateaus in correlation functions due to the\npowers of the conserved quantity 18\nE. Stretched-exponential decay of local correlation\nfunctions 19\nF. Local spin current 20\nI. INTRODUCTION\nOne of the reasons why physics is so successful in de-\nscribing Nature is because oftentimes simple descriptions\nsuffice. This is so because even simple rules, given for in-\nstance by a Hamiltonian, can lead to complicated behav-\nior. How and which simple rules lead to complex behavior\nis studied within a theory of dynamical systems. Some\nsystems (rules) lead to predictable regular dynamics, the\nextreme case being integrable systems, while other lead\nto complex behavior, the extreme case being chaotic sys-\ntems for which predicting future for a specific initial con-\ndition becomes impossible due to exponential complexity\ngrowth [1].\nIn many-body quantum systems complexity can grow\nexponentially also with the system size. Such exponen-\ntially growing Hilbert space size is what allows for outper-\nformance of quantum devices over classical, and indeed,\none of premier usages of present day digital quantum\ncomputers and analog simulators is to simulate quantum\nmany-body systems [2]. An interesting question is what\nnew effects can exponential complexity due to chaos and\nthe one from many-body nature bring? To begin with,\nit is not entirely clear how to define or quantify quan-\ntum chaos in many body quantum systems. One can\ntry to use theoretically appealing concepts, like dynam-\nical entropies, which are difficult to use in practice, or\none can use a concept that is easy to study numerically,\nfor instance various spectral quantifiers [3]. Both have a\n\n2\ndrawback that in a many-body system they are hardly\nobservable quantities.\nOne can, however, ditch theoretical elegance for prac-\ntical relevance and try to directly study measurable ob-\njects. One such are correlation functions\nCAB(t):=⟨A(t)B(0)⟩ − ⟨A(t)⟩⟨B(0)⟩, (1)\nwhere ⟨•⟩denotes the average over an appropriate en-\nsemble. A defining property of mixing systems is that\ncorrelation functions decay to zero, while for chaotic sys-\ntems we expect more, namely, that CAB(t) decay expo-\nnentially at long enough times, CAB(t)∼e−νt. Time\ndependent correlation functions specifically allow us to\nstudy dynamics, and through linear response also get ac-\ncess to non-equilibrium properties. A prime example is\ntransport of globally conserved quantities, like energy,\ncharge, or magnetization.\nWhile many different methods have been proposed\nand employed to study transport in many-body quan-\ntum systems, for a review see Ref. [4], here we propose\nand test a new one that is based on dynamics of op-\nerators. It directly works in the thermodynamic limit,\ni.e., in infinitely-sized systems, with the only approxi-\nmation being truncating dynamics to operators with a\nfinite local support size. It builds on a decades old idea\nof Ruelle-Pollicott (RP) resonances [5–7] that we upgrade\nand adapt to translationally invariant many-body quan-\ntum circuits so that it allows for an easy identification\nof the transport dynamical exponent, and, for instance,\ncalculation of the diffusion constant.\nA. Ruelle-Pollicott resonances\nThe idea of RP resonances is to get the decay rate ν\nof exponential asymptotic decay of correlation functions,\nCAB(t)∼e−νt=λt, (2)\nwhere we introduced λ:= e−ν. While specific observables\ncan decay with their own rate, there is the slowest decay\nrateν, the inverse of which is nothing but the system’s\nrelaxation time. It was shown in Refs [5, 6] that in a class\nof strongly chaotic classical systems λdoes not dependent\non the chosen observable, but is rather an intrinsic prop-\nerty of the system. In such cases, λis referred to as the\n(leading) RP resonance of the system. While one could\ngetλby simply calculating the appropriate correlation\nfunction, the idea is to get λdirectly from the generator\nof dynamics (after all, λis an inherent property of the\nsystem, not observables).\nWith classical systems in mind, the starting point is\nthe propagator U, either the Frobenius-Perron propaga-\ntor of densities, or the Koopman propagator of observ-\nables [7, 8]. Uis unitary in the appropriate space (e.g.,\nin the space of L2functions), its eigenvalues lying on the\nunit circle. One can, however, find poles in the analytic\ncontinuation of the resolvent R(z):= 1/(z−U) inside theunit circle. It was shown in specific systems [9] that they\ncorrespond exactly to RP resonances, the largest being\nthe mentioned λ.\nPerforming the analytic continuation for generic sys-\ntems is not feasible. A more practical approach, which\ncan be carried out either analytically or numerically, is\nintroducing some kind of dissipation or coarse-graining\n(i.e., non-unitarity) in our system. This leads to a non-\nunitary propagator U(ε), where εmeasures the strength\nof the non-unitarity. The eigenvalues of U(ε) now lie in-\nside the unit circle and, analogous to the RP resonances\nin the unitary case, govern the decay of generic correla-\ntion functions. It is conjectured that in the unitary limit\nε→0, contrary to a naive expectation that the eigenval-\nues uniformly converge to the unit circle, this does not\nhappen – they converge to a smaller radius inside the\nunit circle, which corresponds to an RP resonance. This\nprocedure was used in classical systems, with the non-\nunitarity being either white noise [10] or a phase space\ncoarse-graining procedure [11].\nLess is known about RP resonances of quantum sys-\ntems. Initially, they have been studied in the semi-\nclassical regime [12–14]. More recently, weak Lindblad\ndissipation (an analog of noise in classical systems) has\nbeen suggested [15] as a method to extract λvia the\nLiouvillian gap in the limit of infinite systems size and\nzero noise, with the results for few specific solvable mod-\nels [16–18] agreeing with λcalculated directly.\nWe shall use a different path by considering the Heisen-\nberg propagator of observables U(A):=U†AU, where U\nis the unitary propagator, truncating Uto an appropri-\nate operator subspace. In this setting, the RP resonance\nis expected to correspond to the leading eigenvalue of\nU. The method was first used on translationally invari-\nant local operators in Ref. [19] (which can be though of\nus akin to a phase space coarse-graining), in the kicked\nIsing model [19–21], and more recently followed by ad-\nditionally resolving a quasi-momentum [22]. We shall\nuse this quasi-momentum resolved truncated propagator.\nThe truncated propagator can be very handy not just in\nstudies of chaotic systems, but more broadly, for instance,\nidentifying unknown constants of motion in cellular au-\ntomata [23, 24], or discovering that all homogeneous U(1)\nconserving quantum circuits are integrable [25, 26].\nWe note that the recently much discussed Krylov space\nmethod can be viewed as a particular way of trunca-\ntion (in powers of time in the Taylor expansion, i.e.,\nnested commutators) and can thus be related to RP res-\nonances [27, 28].\nB. Summary of results\nIn this work we use the momentum-resolved truncated\npropagator approach to study RP resonances in systems\nwith one conserved quantity. We focus on many-body\nquantum circuits with a local U(1) conserved quantity\nand local gates. Our generic conclusions are demon-\n\n3\n(a)\nNo conserved quantities\nk/π1|λ|\n1\n00exponential decay (b)\n1 conserved quantity\nk/π1|λ|\n1\n00∝e−Dk2\ndiffusion\nexponential decay\nkc\nFIG. 1. Diagram of the RP resonance spectrum in systems without (a) and with (b) one conserved quantity. Labels in figure\n(b) show which parts of the spectrum are responsible for which physical behavior of the system. In the thermodynamic limit,\nkis continuous and periodic, we normalize it in a way that k∈[−π, π). We show only positive k, since |λ(k)|=|λ(−k)|, for\ndetails see Sec. II B. The shaded region in figure (b) denotes the location of conjectured continuums of RP resonances, which\nwe cannot observe numerically with used methods.\nstrated on magnetization-conserving circuits with 3-site\ninteraction and 3-site translational invariance. Such sys-\ntems with their associated magnetization transport are of\ndirect experimental interest for NISQ machines [29–31],\nand are the simplest theoretical setting with a conserved\nquantity [32].\nAs mentioned, we use the truncated operator propaga-\ntor [19] in its momentum-resolved form [22], which will\nallow us to extract transport properties from the momen-\ntum dependence of RP resonances. Some work has been\ndone on the connection between RP resonances and dif-\nfusion in classical systems [33, 34], and conserved quan-\ntities have been briefly mentioned in previous Lindblad\nweak dissipation works [15, 18], as well as for a trun-\ncated propagator of the kicked Ising model in a prether-\nmalized regime (or the Hamiltonian limit) in a recent\npreprint [35].\nWe provide the first comprehensive study of the k-\ndependence of RP resonances, showing that (i) the quasi-\nmomentum dependence of the leading eigenvalue of the\ntruncated propagator and thus the leading RP resonance\nλ1(k) is in general non-trivial (Fig. 1a), even in a sys-\ntem without any conserved quantities where |λ1(k)|is\ngapped away from 1. For instance, the decay need not\nbe always the fastest at k= 0, i.e., for translationally in-\nvariant observables. (ii) In systems with conserved mag-\nnetization one expectedly has λ1(0) = 1 with the corre-\nsponding eigenvector being the conserved magnetization,\nand, more interestingly, from λ1(k) for small kone can\nextract the transport dynamical exponent, and, specifi-\ncally in the case of diffusion, the value of the diffusion\nconstant due to |λ1(k)|= e−Dk2(Fig. 1b). At large k,\nthe diffusive nature of the leading RP resonance ends\ndue to an eigenvalue crossing at kc, resulting in an ex-\nponential decay of generic observables with sufficiently\nlarge quasi-momentum k. (iii) We discuss and conjec-\nture a RP continuum below |λ1(k)|(Fig. 1b), that are\nrelated to non-exponential decay of correlation functions\nand corrections to diffusive transport.II. TRUNCATED\nQUASI-MOMENTUM-DEPENDENT\nPROPAGATOR OF EXTENSIVE OBSERVABLES\nIn this section, we describe the quasi-momentum-\ndependent propagator of local observables, first intro-\nduced for k= 0 in Ref. [21] and extended to any kin\nRef. [22]. We are considering qubit circuits with propa-\ngator for 1 unit of time Uthat has translational invari-\nance by ssites, and that can, due to a sharp light-cone,\nspread local operators supported on rconsecutive sites\nby at most δrsites at each edge. For instance, a brickwall\ncircuit composed of all the same gates (Fig. 2a) is invari-\nant under translation by s= 2 sites and has δr= 2, while\na 3-layered brickwall circuit with 3-qubit gates (Fig.3a)\nhass= 3 and δr= 6. Previous implementations [21, 22]\nwere for the kicked Ising model that has s= 1 and δr= 1,\nor for 2-layer brickwall [25, 26] circuits that have s= 2\nandδr= 2. We generalize it to work for translationally\ninvariant quantum circuits with any geometry, i.e., any s\nandδr.\nWe first express the Heisenberg propagator of observ-\nables\nU(A):=U†AU (3)\nin a basis of an infinite system where the quasi-\nmomentum kis a good quantum number. In quantum\ncircuits, the local support rof the density of observable\nAcan increase only by at most δrsites at each end, to\nr+ 2δr. See for example Fig. 2 for the brickwall circuit\nwhere δr= 2. We then truncate (i.e., project) the observ-\nableU(A) back to observables with densities supported\non at most rsites, thus obtaining a non-unitarity trun-\ncated propagator U(r). In the limit of the non-unitarity\ngoing to zero, that is r→ ∞ , we expect to extract RP\nresonances as the “frozen” eigenvalues of the truncated\npropagator U(r).\n\n4\nA. Brief description\nIn order to study the Heisenberg propagator with good\nquasi-momentum k, we must first define a basis of opera-\ntors with good k. Any observable in a finite system of N\nlattice sites with good quasi-momentum kcan be written\nin the following way\nA=N/s−1X\nj=0e−ikjSsj(a), (4)\nwhere Sis the 1-site translation super-operator to the\nright (i.e., S(a⊗ 1) = 1⊗a),ais a local operator and\nthe sum runs over all j∈ {0, . . . , N/s −1}possible trans-\nlations by ssites. In a finite system, ktakes values2πs\nNj,\nhowever, at the end we will be interested in the thermo-\ndynamic limit N→ ∞ [36], where k∈[0,2π). One can\ncheck that Areally is an eigenvector of the translation\nsuper-operator, S(A) = eikA. Such Awill be referred to\nas an extensive observable .\nWe wish to work with local operators aas proxies for\nextensive observables A. It is thus crucial to restrict our-\nselves to a space of local operators in which the corre-\nspondence between local operators and extensive observ-\nables is one-to-one. Namely, in Eq. (4), different acan\nresult in the same A. For example, both a=σz\n1and\na=1\n2\u0000\nσz\n1+σz\n1+s\u0001\nrepresent the same A. Here σα\njde-\nnotes the αPauli matrix acting on site j. To avoid that\nand have a one-to-one correspondence, such translations\nof components of abyssites must be prohibited. This\ncan be done in various ways, we choose to write the ex-\ntensive observable in the following form\nA=s−1X\nm=0N/s−1X\nj=0e−ikjSsj+m(am), (5)\nwhere Ssj+m(am) can be thought of as operators starting\non sites sj+m(i.e., on all sites i, where i(mod) s=m).\nIn other words, due to translational invariance by ssites\nwe have a local unit call of ssites, so that the sum over\nmruns over sites within a unit cell, while jruns over all\nunit cells. In this formulation it is now easy to see that\nwe will have a one-to-one correspondence between the set\nof{am}s−1\nm=0and extensive observable Aif we in addition\nenforce that amdo not contain components acting triv-\nially on the first site. Explicitly, ammust be in the space\nspanned by the set (basis) P(r)of Pauli strings without\nidentities acting on the first site:\np0:={ 1} ∪p, where p:={σx, σy, σz},(6)\nP(r):={p1⊗p2⊗ ··· ⊗ pr|p1∈p∧pj̸=1∈p0}.\nIn this notation, therefore,\nam∈Span\u0010\nP(r)\u0011\nfor some r. (7)\nSuch amwill be referred to as local densities . Addition-\nally, we define their support to be the smallest rsuchthat am∈Span\u0000\nP(r)\u0001\n. Analogously, the support of an\nextensive observable Ais defined to be the smallest r,\nsuch that am∈Span\u0000\nP(r)\u0001\nfor all m∈ {0, . . . , s −1}.\nThe basis of extensive observables can now be written\ndown, one merely takes all possible local basis elements\nP(r)on all possible starting positions m. The basis ele-\nments are therefore\nB(m,b)\nk:=N/s−1X\nj=0e−ikjSsj+m(b), (8)\nwhere b∈P(r)for some r. Moreover, B(m,b)\nkfor all k, m\nandb∈P(r)are an orthonormal basis of the space of\nextensive observable w.r.t. the extensive Hilbert-Schmidt\ninner product ⟨⟨A, B⟩⟩:=s\nN2NtrA†B. The number of ba-\nsis elements of support r(or less) is3s\n44r. Importantly,\nthe identity operator 1is excluded from this basis. This\nis convenient, since it is always a conserved quantity in\nunitary dynamics and is, therefore, not of interest in the\npresent application. For details and derivations see Ap-\npendix A 1.\nIt is now easy to calculate the matrix elements of the\nHeisenberg propagator in the basis of extensive observ-\nables by simply taking the inner product\n[Uk](m,b),(m′,b′)=DD\nB(m,b)\nk,U\u0010\nB(m′,b′)\nk\u0011EE\n.(9)\nIn this context, we call Uthepropagator of extensive ob-\nservables (i.e., the Heisenberg propagator in a particular\nbasis).\nB. Truncation and Ruelle-Pollicott resonances\nThe Heisenberg propagator in Eq. (9) evaluated in the\nentire (infinitely dimensional) Hilbert space is a unitary\noperator. We wish to introduce a non-unitary by restrict-\ning ourselves to some subspace. There are different op-\ntions, one of the most natural ones is restricting ourselves\nto the space of support rdefined in Eq. (6). Projections\nof the Heisenberg propagator to this space results in the\ntruncated propagator of extensive observables U(r)\nkwith\nmatrix elements\nh\nU(r)\nki\n(m,b),(m′,b′)=1X\nj=−1eikjD\nSsj+m(b),U\u0010\nSm′(b′)\u0011E\n,\n(10)\nwhere b, b′∈P(r)(6), m, m′∈ {0, . . . , s −1}and\nk∈[0,2π). The RHS is expressed only in terms of\nlocal densities and the angled brackets denote the local\nHilbert-Schmidt inner product ⟨a, b⟩:=1\n2Ntra†b. The\nwritten form is valid if δr≤s. Importantly, the trun-\ncated propagator is block-diagonal with blocks indexed\nby the quasi-momentum k. More generally, the range of\nvalues of jover which one has to sum in Eq. (10) depends\non the spreading δrof the circuit. For the derivation and\nfurther details see Appendix A 2.\n\n5\nrr δr δr\nt\nxt= 0t= 1light cone\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nk/π0.50.60.70.80.91.0|λ1||λ1|≈0.845\n|λ1|≈0.726r= 12\nr= 11\nr= 10\nCase i\nCase ii (b)\n0 10 20 30 40 50\nt10−510−410−310−210−1100const.×|CAA(t)|/N\n|λ1|≈0.845\n|λ1|≈0.726σz,k= 0\nσz,k=π\nGUE,k= 0\nGUE,k=π (c)\nFIG. 2. Circuits with no conserved quantities. (a) Diagram of operator spreading in a brickwall circuit with the same gate\nVacting between all nearest neighbors (the shown circuit has s= 2, δr= 2). (b) The leading eigenvalue of the propagator\ntruncated to the space of extensive observables with support rfor two cases of a circuit, with Vin each case chosen randomly\naccording to the unitary Haar measure. (c) Infinite temperature autocorrelation functions of extensive observables A(4) in\nthe circuit denoted as Case i in (b). The local density ais either σz, or chosen randomly according to the Gaussian unitary\nensemble (GUE) [37]. Dashed lines show the RP prediction ∝ |λ1|t, while full curves are exact calculation in a circuit with\nN= 30 qubits; the σz, k= 0 case is multiplied by 15 for better presentation.\nThe truncated propagator U(r)\nkis non-unitary and its\neigenvalues lie inside the unit disc. One might think that\nits eigenvalues smoothly converge back to the unit circle\nin the unitary limit r→ ∞ , but in many cases this is not\ntrue. If the local correlations decay exponentially one ex-\npects that the leading eigenvalue converges (or “freezes”)\nto some value inside the unit disc, which one interprets\nas the leading RP resonance λ1(k).\nFor the finite-sized truncated propagator, assuming it\nis diagonalizable, we can motivate that more explicitly.\nLetλ(r)\nibe the eigenvalues of U(r)\nk(sorted by decreasing\nmagnitude) and L(r)\ni/R(r)\niits left/right eigenvectors. A\ngeneric infinite temperature correlation function of ob-\nservables A,Bwith good quasi-momentum kcan then\nbe decomposed as\n⟨A(0)B(t)⟩=\nA†,Ut\nk(B)\u000b\n(11)\n= lim\nr→∞X\ni\u0010\nλ(r)\ni\u0011tD\nA†,R(r)\niED\nL(r)\ni, BE\n− − − →\nt→∞λt\n1D\nA†,R(∞)\n1ED\nL(∞)\n1, BE\n.\nTherefore, if A, B are not orthogonal to the eigenvectors\nassociated with the leading RP resonance, their corre-\nlation function will decay as |λ1|tat long times, which\nmatches with the definition of an RP resonance in Eq. (2).\nHere on the LHS ⟨•⟩:=1\n2Ntr (•) (i.e., an infinite temper-\nature average in a finite system) and, for clarity, A, B are\nassumed to be traceless. If A, B are not traceless (i.e.,\nthey overlap with identity, which is also the eigenvalue\nofU), a similar argument can be made for the connected\ncorrelation function defined in Eq. (1). In all the follow-\ning sections we shall discuss connected infinite temper-\nature autocorrelation functions, we will usually shorten\nour language and simply say correlation functions. A\nsimilar procedure can also be done at finite temperatureor chemical potential by introducing an appropriate inner\nproduct with a non-trivial measure.\nIf a circuit has a conserved quantity, correlation func-\ntions of observables overlapping with a conserved quan-\ntity will converge to a non-zero value. In the RP reso-\nnance language, this would imply the presence of λi= 1\nin the decomposition in Eq. (11). Additionally, also all\npowers of the conserved quantity are conserved quan-\ntities, although typically non-local. While they cannot\ncause correlation functions of extensive (or local) observ-\nables to converge to the non-zero value in the thermody-\nnamic limit, they can introduce plateaus in finite systems\nthat decay as a power law in system size. We discuss this\neffect in Appendix D.\nC. Example: Brickwall quantum circuits with no\nconserved quantities\nLet us demonstrate the above RP formalism with a\nsimple example of a circuit with no conserved quantities.\nThe main idea is to compare the exactly calculated cor-\nrelation function in a finite system with the asymptotic\ndecay obtained from the truncated propagator. In addi-\ntion, we will show that the quasi-momentum dependence\nofλ1(k) can be non-monotonous, specifically, the fastest\ndecay can happen at nonzero k.\nWe take a brickwall circuit with nearest-neighbor 2-\nqubit gates, all being the same. The gate is picked ran-\ndomly according to the unitary Haar measure. In Fig. 2b\nwe show the leading RP resonance |λ1(k)|for two real-\nizations (cases) of a random local gate. For details on\nnumerical methods see Appendix B 1. The circuit’s Flo-\n\n6\nquet propagator can be written as\nU=UevenUodd, (12)\nUodd=V1,2V3,4. . . V N−1,N,\nUeven=V2,3V4,5. . . V N−2,N−1VN,1,\nwhere Vi,jare local gates, i.e., 2-site unitary operators\nacting on sites iandj. All Vare taken to be the same.\nIn both cases we see that the kdependence of λ1(k) is\nhighly non-trivial and depends strongly on the choice of\nV. We show kdependence only for k∈[0, π], since\n|λ1(k)|=|λ1(−k)|. This is a consequence of the fact that\u0010\nU(r)\nk\u0011∗\n=U(r)\n−k, which follows from the form in Eq. (10).\nOne must only show that theD\nSsj+m(b),U\u0010\nSm′(b′)\u0011E\nis\na real number, which quickly follows from the hermiticity\nof the Pauli basis, b=b†, b′=b′†.\nFig. 2c shows the decay of correlation functions of ex-\ntensive observables with k= 0 and π. For details about\nnumerical calculation of correlation functions see Ap-\npendix B 2. Their long-time decay rate matches well with\nthe leading RP resonance determined from the truncated\npropagator. If the observables do not have good quasi-\nmomentum k, for instance strictly local observables, their\ncorrelation function expands over the whole spectrum of\neigenvalues with different momenta k. The long time de-\ncay is then governed by the biggest λ1(k). It is often\nassumed that this happens at k= 0, i.e., for translation-\nally invariant observables, but as we see in Fig. 2b for\ncase ii, this is not always the case. There the gap seems\nto be the smallest around k≈2.\nIn the following sections we shall focus on the main\nsubject of our work, namely, study how a conserved quan-\ntity changes the RP spectrum and what can we extract\nfrom it. As mentioned in Sec. I A, RP resonances can also\nbe studied in the presence of conservation laws by intro-\nducing Lindbladian dissipation [15], though we find the\ntruncated propagator more convenient; we briefly discuss\nthe Lindblad approach in Appendix C.\nIII. THE LEADING RUELLE-POLLICOTT\nRESONANCE\nWe now focus on the main result of this paper, the\neffect of a single conserved quantity on the leading RP\nresonance. In order to study this, we need a model with\nonly 1 conserved quantity. A natural choice for the con-\nserved quantity in the context of spin-1\n2chains is the\nmagnetization\nM:=X\njσz\nj. (13)\nTherefore, our system, in addition to a translational sym-\nmetry, also has a U(1) symmetry. A natural way to\nachieve that would be to take a brickwall circuit with a 2-\nqubit gate that conserves magnetization. However, if thegate is the same everywhere such circuits are all special\n– they are integrable [25, 26], including for non-brickwall\ngeometries [38]. Second possibility that also preserves\ntranslational symmetry would be to take a brickwall con-\nfiguration with two different magnetization-conserving\ngates, one in each brickwall layer, though it turns out\nthat such circuits have slow (possibly non-diffusive) ther-\nmalization [39].\nBecause we want good diffusive thermalization, we\ntherefore consider one of the next simplest options, which\nare circuits with 3-site gates. The gates are arranged in\na brickwall-like geometry with s= 3 site translational\ninvariance, see Fig. 3a. The Floquet propagator is given\nby the following expression\nU=U3U2U1, (14)\nUi=N/3Y\nj=0Vi+3j,i+3j+1,i+3j+2\nwith periodic boundary conditions (indices are taken\nmodulo N). 3-site gates Vare taken the same every-\nwhere, so that we have translational symmetry. In order\nfor the circuit to conserve magnetization, each local gate\nmust conserve it as well. One can thus quickly deduce\nthat the local gate Vmust have 4 independent blocks,\nwhich are shown on the diagram in Fig. 3a. We choose\nthe matrix elements in each block independently accord-\ning to the unitary Haar measure in the corresponding\nblock, which results in a unitary V.\nThe considered circuit spreads observables by δr= 6 in\none period (see Fig. 3) and, importantly, we have δr > s .\nTherefore, Eq. (10) for the matrix elements of the trun-\ncated propagator is not applicable. While one can use the\nmore general formula derived in Appendix A 2, this ap-\nproach gives access only up to r= 6, since the evaluation\nof matrix elements is numerically performed on observ-\nables of support r+ 2δr. A substantial improvement can\nbe made by noticing that successive layers are transla-\ntions of the previous layers, that is S(Ui) =Ui+1. This\nspace-time symmetry [40] allows us to redefine the first\nlayer (and a translation to the left) to be the “new” Flo-\nquet propagator for a three times shorter period, leading\nto less spreading δr= 2 and allowing us to use Eq. (10).\nFor details see Appendix A 3.\nThe leading eigenvalue of the truncated propagator\n(and thus the leading RP resonance) for 3 different real-\nizations (cases with different gate V) is shown in Fig. 3b.\nWe will use the same cases throughout this paper. Its k\ndependence has two regimes, kclose to 0 (small k) and\nkfar from zero (around k=π). At k= 0, the leading\neigenvalue is exactly 1 and its eigenvector is the local\ndensity corresponding to magnetization (i.e., the con-\nserved quantity). Eigenvalues at small quasi-momenta\nhave smooth dependence on kand converge fast with\nr. We will show that they are related to transport and\ncontain information about the spin diffusion constant in\nSec. III A. Eigenvalues far from k= 0 converge slower and\nshow more varied behavior, similarly to what we have\n\n7\nrr+ 2δr\nt\nxt= 0t= 1\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nk/π0.60.70.80.91.0|λ1|r= 12\nr= 11\nr= 10Case 1\nCase 2\nCase 3 (b)\nFIG. 3. Circuits with conserved magnetization. (a) Circuit diagram with magnetization preserving 3-qubit gate V(the shown\ncircuit has s= 3 and δr= 6), whose structure with non-zero blocks is shown on the left. (b) The leading RP eigenvalue of the\ntruncated propagator with support rfor 3 choices of V(chosen according to the Haar measure). The same cases will be used\nthroughout the paper.\nseen in cases without symmetries (cf. Fig. 2b), includ-\ning non-smooth dependence due to collisions with smaller\neigenvalues. In Sec. III B we will numerically verify that\na finite gap there indeed signals exponential decay of cor-\nresponding observables that are unrelated to transport.\nA. Small quasi-momenta and magnetization\ntransport\nIn generic, i.e., chaotic, quantum systems with 1 con-\nserved quantity, one typically expects diffusive trans-\nport [4]. Diffusive transport can be probed by considering\nthe correlation function of the local density of the con-\nserved quantity, also called the dynamic structure factor,\nfor which one obtains\n⟨σz(x, t)σz(0,0)⟩=1√\n4πDte−x2\n4Dt, (15)\nwhere Dis the model-dependent (spin) diffusion con-\nstant. We have adjusted our notation to σz(x, t) meaning\nthez-Pauli matrix acting on space coordinate xand prop-\nagated to time t. For clarity, we can also consider space\nto be continuous with ∆ x= 1 corresponding to three\nsites of our model (i.e., we average our system over one\ntranslational period). Eq. (15) follows from the system\nobeying the diffusion equation on hydrodynamic scales.\nThe diffusion equation can be derived by assuming lin-\near relationship between expectation values of the spin\ncurrent and the gradient of the density of the conserved\nquantity [41].\nAssuming Eq. (15) holds, one can derive the correla-\ntion function of magnetization with quasi-momentum k,\nMk:=P\nje−ikj/3σz\nj[factor 3 is included to match thedefinition in Eq. (5)]. That is,\n1\nN\nMk(t)Mk(0)†\u000b\n=1\nNX\nj,j′e−ik(j−j′)/3\nσz\nj(t)σz\nj′(0)\u000b\n=X\nje−ikj/3\nσz\nj(t)σz\n0(0)\u000b\n→Z\n⟨σz(x, t)σz(0,0)⟩e−ikxdx= e−Dk2t. (16)\nThe correlation function of magnetization in momentum\nspace therefore decays exponentially, with the decay time\ndiverging as 1 /(Dk2) in the long wavelength limit k→0.\nThis is a standard indicator of diffusion [41]. Because\nthere are evidently exponentially decaying correlation\nfunctions with their rate having quadratic kdependence\nthis implies that there must exist a leading RP resonance\n|λ1(k)|= e−Dk2. (17)\nThis can alternatively be interpreted as an RP continuum\ngoverning the decay of the local magnetization correla-\ntion function. We discuss other potential RP continuums\ngreater detail in Sec. IV B.\nSince the RP resonance predicted in Eq. (16) converges\nto 1 as k→0, it must be the leading RP resonance for\nsmall k. It is, therefore, precisely the RP resonance ob-\nserved for small kin Fig. 3b. This is confirmed numeri-\ncally by fitting a function e−Dkztoλ1for small k, where\nzis usually referred to as the transport dynamical expo-\nnent. This is shown in Fig. 4, insets (i). The obtained\nvalues of zmatch the predicted diffusive z= 2 up to\naccessible precision. One can also check the correspond-\ning eigenvectors, which, according to the calculation in\nEq. (16), should be the local density of magnetization.\nWe have confirmed that this is the case at big enough\nr. At smaller rthe corresponding eigenvector includes\nother components. It would be interesting to extend this\nto a different type of transport, perhaps a non-diffusive\n\n8\n0 20 40 60x\n−1.0−0.50.00.51.0/angbracketleftσz/angbracketright/µ\n0100\nt\n100101102\nt100101∆/angbracketleftσz/angbracketright/µ\n∆/angbracketleftσz/angbracketright\nµ= 2/radicalBig\nD\nπt1/z,\nz≈1.99,\nD≈0.29(1)\n0.0 0.2\nk/π0.800.850.900.951.00|λ1|\n|λ|=e−Dkz,\nz≈2.00,\nD≈0.302(2)\n(a)\n(a.i)\n(a.ii)\n0 20 40 60x\n−1.0−0.50.00.51.0/angbracketleftσz/angbracketright/µ\n0100\nt\n100101102\nt100101∆/angbracketleftσz/angbracketright/µ\n∆/angbracketleftσz/angbracketright\nµ= 2/radicalBig\nD\nπt1/z,\nz≈1.99,\nD≈0.42(1)\n0.0 0.2\nk/π0.70.80.91.0|λ1|\n|λ|=e−Dkz,\nz≈1.99,\nD≈0.421(2)(b)\n(b.i)\n(b.ii)\nFIG. 4. Transport of magnetization in circuits case 1 (a) and case 2 (b). In both cases we show the leading RP resonance\n(r= 11) and the fit to e−Dk2(insets i), and the domain-wall evolution (TEBD, N= 204 with µ= 10−3andχ= 256) at\ntimes with equal spacing ∆ t= 22 up to t= 200 (main plots) with the associated transferred magnetization and the diffusion\nconstant fit (insets ii).\none (z̸= 2) with a one-parameter scaling relation xz∼t.\nWe expect the quasi-momentum dependence of λ1(k) for\nsmall kto also hold information about zin that case,\nalthough the exact form of λ1(k) depends not just on z,\nbut also on the scaling function of the local correlation\nfunction of the conserved quantity.\nThe fit also extracts the spin diffusion constant Dof\nthe model. In order to verify that this is indeed the\ncorrect value, we compare it to the diffusion constant\nobtained by a completely different method. Namely, from\nthe evolution of an initial weakly-polarized domain wall\nstate [42]. We prepare the system in the state with the\nfollowing density matrix\nρ∝N/2Y\nj=1eµσz\njNY\nj=N/2+1e−µσz\nj, (18)\nfor small µ, and evolve it with U. Diffusion equation\ndictates the transferred magnetization ∆ ⟨σz⟩ρshould be\n(at long times in our system)\n∆⟨σz⟩ρ(t):=NX\nj=N/2+1\nσz\nj(t)\u000b\nρ= 2µr\nD\nπt, (19)\nwhere ⟨•⟩ρ:= tr ( ρ•) denotes the expectation value in\nstate ρ. The dynamics can be simulated using time-\nevolving block decimation (TEBD) [43, 44], for details see\nAppendix B 3. Fitting a curve scaling as t1/zto ∆⟨σz⟩\none can numerically confirm diffusion and extract the\nspin diffusion constant D.\nBoth methods of determining Dare shown in Fig. 4.\nIn the transferred magnetization from the domain-wall\n(bottom left (ii) insets), we again see zmatching the ex-\npected z= 2. The diffusion constants determined fromdifferent approaches also match up to accessible preci-\nsion. We have thus shown the leading RP resonance for\nsmall ktruly does contain information about transport.\nA natural next question, which we leave for future work,\nis how competitive is the extraction of Dthrough RP\nresonances compared to the current state-of-the-art.\nB. Quasi-momenta far from zero and correlation\nfunction decay\nThe leading RP resonances far from k= 0 do not follow\ne−Dk2anymore and are not connected to diffusive trans-\nport, see Fig. 3b. We thus expect them to merely be\nindicators of exponential decay, like in the case without\nsymmetries in Fig. 2. We demonstrate this numerically\nfork=πin Fig. 5. Convergence with ris slower than\nat small kand at the largest available r= 13 the lead-\ning eigenvalue has not yet fully converged, however, it\ndoes seem that the extrapolated values [45] are strictly\nsmaller than 1. Therefore, generic correlation functions\nare expected to decay exponentially with a finite rate. In\nFig. 5c we show correlation functions of xmagnetization\nand of a random observable. While exponential decay at\navailable system size N= 30 is not super nice, the decay\nis compatible with the calculated extrapolated λ1, e.g., it\nis faster for the circuit with smaller |λ1|. Also, different\nobservables decay with the same rate.\nThe fact that the exponential decay in the present case\nis not as evident from numerics is compatible with slow\nconvergence of the RP resonance with r. Namely, the\nleading RP resonance at truncation rcan be intuitively\nunderstood to govern the decay of correlations when the\noperator has spread to rsites. Slower convergence of\nRP resonances with rthus implies “nice” exponential\n\n9\n(a)\n0 2 4 6 8 10 12\nr0.20.40.60.81.0|λ1|\nCase 1,|λ(r→∞)\n1|≈0.82\nCase 3,|λ(r→∞)\n1|≈0.73 (b)\n0 10 20 30\nt10−510−410−310−210−1100|CAA(t)|/N\n|λ1|≈0.82\n|λ1|≈0.73Case 1,σx\nCase 1, GUE\nCase 3,σx\nCase 3, GUE (c)\nFIG. 5. Leading RP resonance and correlation function decay in the k=πspectrum for magnetization-conserving circuits.\n(a) The whole spectrum of the truncated propagator in k= 0, πsectors for case 3 and r= 7. (b) Convergence of the leading\nRP resonances in the k=πsector with r, with an extrapolation to the r→ ∞ limit. (c) Infinite temperature autocorrelation\nfunctions of extensive observables Adefined in Eq. (5) with k=π. The local densities are either σx(i.e., xmagnetization) or\nchosen randomly according to the Gaussian unitary ensemble [37]. The correlation functions are calculated in a finite circuit\nwith N= 30 sites. Dashed lines show the RP predictions ∝ |λ1|t.\ndecay only at later times and consequently smaller val-\nues of CAA. Therefore, it is expected that exponential\ndecay will be harder to see numerically at finite Npre-\ncisely when RP resonances will also be hard to extract\nnumerically. Additionally, correlation functions in Fig. 5c\nalso visibly oscillate, making numerical checks less clear.\nThis, however, is merely a consequence of λ1having an\nimaginary component seen for the example of case 3 in\nFig. 5a.\nIV. BEYOND THE LEADING\nRUELLE-POLLICOTT RESONANCE\nWe have shown that the leading RP resonance λ1(k)\nfor small kis connected to transport of the conserved\nquantity. How about subleading eigenvalues; they could\ncontain information about other correlation functions.\nOne might expect correlation functions of observables or-\nthogonal to all transport-related quantities decay expo-\nnentially, like in a system without any conserved quanti-\nties, which would imply that there must exist subleading\nRP resonances.\nThe first few largest eigenvalues of the truncated prop-\nagator can be calculated numerically and are shown in\nFig. 6. We observe many subleading eigenvalues in a\nnarrow interval and one might think that they are iso-\nlated RP resonances. It turns out this is not true; these\neigenvalues have not yet converged with rand, more-\nover, many of them look to be converging to λ= 1. We\nshall propose in Sec. IV A that such eigenvalues close\nto 1 come from the powers of the conserved magnetiza-\ntionM. Furthermore, we shall argue in Sec. IV B that\npower-law hydrodynamic tails in transport-related corre-\nlation functions imply an RP continuum in the spectrum\nof the truncated propagator as sketched in Fig. 1b. Both\n0.0 0.2 0.4 0.6 0.8 1.0\nk/π0.60.70.80.91.0|λi|\nFIG. 6. 20 leading eigenvalues of the truncated propagator at\ndifferent k. Calculated for case 3 and support r= 10.\nof these results make it hard to numerically determine\nwhether subleading discrete RP resonances exist.\nA. Powers of the conserved quantity\nEigenvectors corresponding to λ= 1 are conserved\nquantities and while our system does have only 1 lo-\ncal conserved quantity – the magnetization M– all its\npowers are also conserved. Because our truncated oper-\nator propagator is geared towards local operators, while\nM2=P\ni,kσz\niσz\nkis non-local, it does not have an eigen-\nvalue λ= 1 that would correspond to M2. However, as\nwe will now argue [46], it does have an eigenvalue that is\nclose to 1.\nWithout truncation M2is conserved. This can be also\nseen explicitly, U(M2) =P\ni,kU(σz\ni)U(σz\nk), which, after\n\n10\ninvoking definition of the spin current jk[see Eq. (21)],\nU(σz\nk) =σz\nk+jk−1−jk, can be rewritten as U(M2) =P\ni,k(σz\ni+ji−1−ji)(σz\nk+jk−1−jk) =M2. Conserva-\ntion comes about because the many-qubit terms jijk(and\nlikewise σz\nijk) sum to 0 due to alternating signs stemming\nfrom the continuity equation. Importantly, the cancella-\ntions occur only between terms with distance |k−i|dif-\nfering by 1, i.e., the term jijkcan only be generated by\nthe propagation of σz\niσz\nk,σz\ni+1σz\nk,σz\niσz\nk+1andσz\ni+1σz\nk+1\n(and similarly for σz\nijk).\nIn the basis truncated to support r, some of the terms\nneeded for cancellation are not included, resulting in an\nalmost-conservation. Let us denote the truncated M2by\nQ(r), explicitly Q(r)=P\ni,k,|k−i|<rσz\niσz\nk. In particular,\nthejijkterms stemming from the propagation of σz\niσz\nk\nwith|k−i|=r−1, that would cancel out with the same\n(but negative) terms stemming from σz\niσz\nk+1, do not can-\ncel out, since σz\niσz\nk+1is not included in Q(r)(and simi-\nlarly for σz\nijkterms). Importantly, for |k−i|< r−1 the\ncancellation still occurs and all such terms are conserved.\nThe upshot of this is that U(Q(r)) does not conserve only\ntheσz\niσz\nkterms with distance |k−i|=r−1, the number\nof which does not scale with r.\nThe number of terms in Q(r)is equal to the number\nof indices with |k−i|< r, i.e., it scales as ∼r(Nrin a\nsystem of length N). The number of the conserved terms\ninU(Q(r)) scales as ∼r−c, where cis some constant not\nscaling with r. In the limit r→ ∞ , the two are almost\nthe same, and one can estimate that Q(r)is an almost\neigenvector with eigenvalue\n|λ| ≈r−c\nr= 1−c\nr. (20)\nWe expect this eigenvalue also in the spectrum of the\ntruncated propagator U(r)\nk=0. A similar informal calcula-\ntion can be done for an arbitrary power of magnetization.\nNumerical tests of Eq. (20) by calculating 50 sublead-\ning eigenvalues of the truncated propagator are shown in\nFig. 7. A few largest subleading eigenvalues indeed con-\nverge to the unit circle according to a power law. The\nfirst subleading eigenvalue λ2in fact seems to converge\neven faster than the predicted 1 /r. This might be a\nconsequence of small raccessible numerically. Further\nanalysis reveals that the eigenvector corresponding to λ2\nhas a large overlap with Q(r). Similarly, the second sub-\nleading eigenvector (corresponding to λ3) seems to have\nsubstantial overlaps with the truncation of M3.\nThe truncated propagator in k= 0 therefore seems to\nhave a number of eigenvalues that converge to 1 as 1 /r\nthat come from powers of M. With the truncation that\nwe use it is, therefore, hard to numerically see if there\nare any genuine isolated subleading resonances. Further-\nmore, it is not clear if subleading RP resonances should\neven exist due to power-law hydrodynamic tails in corre-\nlation functions, as we discuss in the next section.\n3 4 6 8 10 12\nr10−11001−|λ(r)\ni|\nr−n,n≈1.72\nr−n,n≈1.13\nr−n,n≈1.04FIG. 7. Convergence of 50 subleading eigenvalues of the trun-\ncated propagator in the k= 0 sector to the unit circle. A\npower-law fit for the 3 subleading eigenvalues is shown.\nB. Hydrodynamic tails\nIn systems with conserved quantities certain correla-\ntion functions related to transport exhibit power-law de-\ncays, i.e., the so-called hydrodynamic tails [41, 47], due to\nslow power-law spreading of inhomogeneities of the con-\nserved density (distance and time scaling as xz∼t). A\ntypical example is the correlation function of local mag-\nnetization (15), i.e., the Green’s function of the diffu-\nsion equation, decaying as ∼1/t1/2atx= 0. Similar\npower-law tails occur also in higher-order corrections to\ndiffusion equation [41, 47], more recently studied in the\ncontext of effective field theories [48–51].\nPower-law decay C(t)∼t−α, for some α > 0, being\nslower than exponential, cannot be governed by a dis-\ncrete RP resonance. If we nonetheless assume it expands\nover a spectrum of eigenvalues as C(t) =P\njcjλt\nj, where\ncjare expansions coefficients [cf. Eq. (11)], it must ex-\npand over a continuum of eigenvalues (an “RP contin-\nuum”) with closing gap to λ= 1 [52]. This is precisely\nwhat we observed for the correlation function of magne-\ntization in momentum space (16), with the gap closing at\nk= 0. Additionally, the quasi-momentum dependence of\nthe RP continuum was exactly determined in that case,\n|λ(k)|= e−Dk2. The expansion over eigenvalues for all k\nis expected only for correlation functions of local observ-\nables. For extensive observables, the whole continuum\nmust lie in one ksector. A potential RP continuum gov-\nerning a power-law decay of a k= 0 extensive observable\nis depicted in the diagram in Fig. 8.\nPower-law decay is not the only type of non-\nexponential decay expected in systems with one con-\nserved quantity. One other possibility is a power-law\ncorrection to exponential decay C(t)∼t−αe−ν0t, for\nsome αandν0>0, which is governed by a continuum\nλ∈[0,e−ν0] [53], also depicted in Fig. 8. Such decay\nwere observed in corrections to the correlation function\nof magnetization with quasi-momentum ⟨Mk(t)Mk(0)⟩\n\n11\nk/π1|λ|\n1\n00power-law ∼t−α\nor stretched-exponential ∼e−tα\npower-law corrections\nto exponential decay\n∼e−ν0t(1 + t−α)\nFIG. 8. Diagrams of possible RP continuums and the type\nof correlation function decay of extensive observables they\ngovern.\n(see Eq. (16) for the exponential diffusive prediction),\nrecently explicitly calculated with the methods of effec-\ntive field theory [51]. The final possibility is a stretched-\nexponential decay C(t)∼e−tαfor some 0 < α < 1, which\nwas recently predicted to occur for all local correlation\nfunctions unrelated to transport in 1d [54]. Such decay is\ngoverned by a continuum λ∈[0,1], we discuss it further\nin Appendix E.\nBased on the above arguments we conjecture that there\nis an RP continuum of eigenvalues for all |λ|below the\nleading RP resonance governing the transport of magne-\ntization. In other words, all |λ| ≤e−Dk2are part of an\nRP continuum.\n1. Extensive spin current\nPower-law tails are expected also in the spin current\njdefined via the (discrete space and time) continuity\nequation\nU(M[i,l])−M[i,l]=ji−1−jl, (21)\nwhere jiis the current operator between sites iandi+1,\nand we introduced the magnetization acting on sites itol\nasM[i,l]:=Pl\nj=iσz\nj. The closed-form expression for jiis\ncomplicated to derive analytically for our 3 −layer brick-\nwall circuit, but can be obtained numerically for a given\ngate by evaluating the LHS of Eq. (21) in a big enough\nfinite circuit. Current densities jiare different for differ-\nent values of imodulo 3, and have support on at most 9\nsites. For a 2-layer brickwall circuit (e.g., Fig. 2a) the ex-\npressions are more manageable (2 or 4 site operators) and\nare written out for any magnetization preserving gate in\nRef. [26].\nThe asymptotics of the local spin current correlation\nfunction can be deduced from the local magnetization\ncorrelation function and the continuity equation. The\ndiffusive local magnetization correlation function (15)\n100101\nt10−310−210−1100|CJJ(t)|/N∼t−n\nN= 18,n≈1.07\nN= 21,n≈1.82\nN= 24,n≈2.42\nN= 27,n≈2.61\nN= 30,n≈2.99FIG. 9. Infinite temperature autocorrelation of extensive spin\ncurrent. The correlation functions are calculated in a case 3\nfinite circuit with different number of sites N. Dashed lines\nshow power-law fits.\n(i.e., the diffusion equation Green’s function) implies\n⟨j(x, t)j(0,0)⟩=x2−2Dt\n8√\nπDt5/2e−x2\n4Dt. (22)\nFor the derivation see Appendix F. At x= 0 this again\ndecays as a power-law ∼1/t3/2.\nOf particular interest is the extensive current\nJ:=X\niji, (23)\nbecause it appears in the Green-Kubo [55] equation for\nthe diffusion constant [56]. Using Eq. (22) and the fact\nthat the equilibrium correlation function is translation-\nally invariant in space, we get for the extensive current\nin the thermodynamic limit\nlim\nN→∞1\nN⟨J(t)J(0)⟩=Z∞\n−∞⟨j(x, t)j(0,0)⟩dx= 0.(24)\nThe prediction coming from the diffusive (i.e., first or-\nder) local magnetization correlation function, thus gives\nexactly 0, which can be traced back to the fact that\n⟨j(x, t)j(0,0)⟩is a total derivative in x[see Eq. (22)].\nThis is very interesting as it means that the leading be-\nhavior of the correlation function of the extensive current\nis given by subleading corrections to the local magnetiza-\ntion correlation function. Autocorrelation function of J\nis therefore a sensitive probe of subleading corrections to\ndiffusion that would be perhaps hard to see in other cor-\nrelation functions that are nonzero already in the leading\norder.\nNumerical calculation of the extensive current correla-\ntion function for one case is shown in Fig. 9. It decays\nas a power law for any finite system size N, however, the\npower increases with N. It is not clear what will happen\nin the thermodynamic limit; its power-law dependence\n\n12\nmay converge to some finite power or the decay may be-\ncome exponential.\nIn addition to the inconclusive numerics, it is also not\nclear what is theoretically expected. Power-law decay\n∼t−3/2is expected in systems with an additional con-\nserved quantity, e.g., in Hamiltonian systems [47], and\ncan be explained by cross transport of the conserved\nquantities. In systems with only one U(1) conserved\nquantity a power-law decay with a higher power was pre-\ndicted [51], although exponential decay is also sometimes\nobserved, e.g., in the kicked Ising model [57].\nAll in all, the fundamental question of the asymptotics\nof the extensive current correlation function CJJstill re-\nmains. Is it a power law, like the local current correla-\ntion function, or are the hydrodynamic tails absent and,\nif they are, when and why?\nV. CONCLUSION\nWe have studied the (leading) Ruelle-Pollicott reso-\nnances in 3-site translationally invariant qubit circuits\nwith conserved magnetization, which correspond to the\n(leading) eigenvalues of the truncated operator propa-\ngator in the limit of no truncation. Our main result is\nthat the quasi-momentum kdependence of the leading\nRuelle-Pollicott resonance λ1can be used to extract the\ntransport properties. Specifically, for diffusive systems,\nwe have |λ1(k)|= e−Dk2for small k, allowing us to nu-\nmerically determine the spin diffusion constant D. For\nlarge k, the leading resonance is not related to trans-\nport and governs the decay of the respective correlation\nfunctions. We also argue for the existence of a contin-\nuum of eigenvalues below the leading diffusive resonance,\n|λ|<e−Dk2, which controls non-exponentially decaying\ncorrelation functions, for instance, due to power-law hy-\ndrodynamic tails.\nEven though our analysis was limited to a specific\nqubit circuit model, we expect our conclusions about the\nRuelle-Pollicott resonances to be generic for systems with\nexactly one U(1) conserved quantity. In particular, the fi-\nnal expression for matrix elements of the truncated prop-\nagator in Eq. (10) [or, in case of larger operator spread-ing, Eq. (A4)] hold for any quantum circuit, for example,\nwith a different arrangement of gates or higher local di-\nmension. An interesting open question is generalizing the\ntruncated propagator to Hamiltonian systems.\nAnother compelling direction is benchmarking the\ntruncated propagator method for extracting the diffu-\nsion constant against other standard methods. While we\nhave shown that the Ruelle-Pollicott method produces\nthe same results as a tensor-network simulation and the\nGreen-Kubo formula, it is not clear whether and when\none method might be preferable to others. Additionally,\nthe leading Ruelle-Pollicott resonance can hold informa-\ntion about non-diffusive transport, i.e., ballistic, sub- or\nsuper-diffusive. However, the exact dependence of the\nleading resonance on kdepends on the asymptotic (space\nand time) behavior of the correlation function of the local\ndensity of the conserved quantity and might be compli-\ncated. This can hinder the effectiveness of identifying\nthe transport type and extracting model-dependent con-\nstants. Especially interesting are integrable systems with\nan extensive number of conserved quantities, where we\nexpect an extensive number of RP continuums governing\ntransport.\nA focus of future work can also be to better understand\nthe conjecture about subleading Ruelle-Pollicott contin-\nuums. A promising direction seems to be developing\na different truncation scheme that separates asymptot-\nically differently behaving observables, i.e., those which\nare governed by hydrodynamics and those which are not.\nThis can also shed additional light on the stretched-\nexponential decay recently predicted for correlation func-\ntions of local non-hydrodynamic observables in 1d sys-\ntems [54]. A particularly interesting physical question in\nthis context is also the asymptotic behavior of the exten-\nsive current correlation function. We have shown that\nit is governed by the corrections to the diffusive corre-\nlation function of local magnetization recently argued to\nbe universal for systems with exactly one U(1) conserved\nquantity [51].\nAcknowledgements . U.D. would like to thank\nRustem Sharipov, Matija Koterle, Lenart Zadnik, and\nTomaˇ z Prosen for insightful discussions. We also ac-\nknowledge support by Grants No. J1-4385 and No. P1-\n0402 from the Slovenian Research Agency (ARIS).\n[1] E. Ott, Chaos in Dynamical Systems (Cambridge Uni-\nversity Press, 2002).\n[2] A. Daley, I. Bloch, C. Kokail, S. Flannigan, N. Pearson,\nM. Troyer, and P. Zoller, Practical quantum advantage in\nquantum simulation , Nature 607, 667 (2022).\n[3] F. Haake, Quantum Signature of Chaos (Springer Verlag,\nBerlin, 2010).\n[4] B. Bertini, F. Heidrich-Meisner, C. Karrasch, T. Prosen,\nR. Steinigeweg, and M. ˇZnidariˇ c, Finite-temperature\ntransport in one-dimensional quantum lattice models ,\nRev. Mod. Phys. 93, 025003 (2021).[5] D. Ruelle, Resonances of chaotic dynamical systems ,\nPhys. Rev. Lett. 56, 405 (1986).\n[6] M. Pollicott, On the rate of mixing of Axiom A flows ,\nInvent. Math. 81, 413 (1985).\n[7] P. Gaspard, Chaos, scattering and statistical mechanics\n(Cambridge University Press, 1998).\n[8] D. Braun, Dissipative quantum chaos and decoherence\n(Springer Verlag, 2001).\n[9] H. H. Hasegawa and W. C. Saphir, Unitarity and ir-\nreversibility in chaotic systems , Phys. Rev. A 46, 7401\n(1992).\n\n13\n[10] P. Gaspard, G. Nicolis, A. Provata, and S. Tasaki, Spec-\ntral signature of the pitchfork bifurcation: Liouville equa-\ntion approach , Phys. Rev. E 51, 74 (1995).\n[11] J. Weber, F. Haake, and P. ˇSeba, Frobenius-Perron res-\nonances for maps with a mixed phase space , Phys. Rev.\nLett. 85, 3620 (2000).\n[12] K. Pance, W. Lu, and S. Sridhar, Quantum fingerprints\nof classical Ruelle-Pollicott resonances , Phys. Rev. Lett.\n85, 2737 (2000).\n[13] C. Manderfeld, Classical resonances and quantum scar-\nring, J. Phys. A: Math. & Gen. 36, 6379 (2003).\n[14] I. Garc´ ıa-Mata, M. Saraceno, R. A. Jalabert, A. J.\nRoncaglia, and D. A. Wisniacki, Chaos signatures in the\nshort and long time behavior of the out-of-time ordered\ncorrelator , Phys. Rev. Lett. 121, 210601 (2018).\n[15] T. Mori, Liouvillian-gap analysis of open quantum many-\nbody systems in the weak dissipation limit , Phys. Rev. B\n109, 064311 (2024).\n[16] J. A. Jacoby, D. A. Huse, and S. Gopalakrishnan, Spectral\ngaps of local quantum channels in the weak-dissipation\nlimit (2024), arXiv: 2409.17238 [quant-ph].\n[17] C. Zhang, L. Nie, and C. von Keyserlingk, Thermal-\nization rates and quantum Ruelle-Pollicott resonances:\ninsights from operator hydrodynamics (2024), arXiv:\n2409.17251 [quant-ph].\n[18] T. Yoshimura and L. S´ a, Theory of irreversibility in quan-\ntum many-body systems (2025), arXiv: 2501.06183 [cond-\nmat.stat-mech].\n[19] T. Prosen, Ruelle resonances in quantum many-body dy-\nnamics , J. Phys. A: Math. & Gen. 35, L737 (2002).\n[20] T. Prosen, Ruelle resonances in kicked quantum spin\nchain , Physica D 187, 244 (2004).\n[21] T. Prosen, Chaos and complexity of quantum motion , J.\nPhys. A: Math. & Gen. 40, 7881 (2007).\n[22] M. ˇZnidariˇ c, Momentum-dependent quantum Ruelle-\nPollicott resonances in translationally invariant many-\nbody systems , Phys. Rev. E 110, 054204 (2024).\n[23] K. Klobas, Exact time-dependent solutions of interacting\nsystems , PhD Thesis, (University of Ljubljana, 2020).\n[24] R. Sharipov, M. Koterle, S. Grozdanov, and T. Prosen,\nErgodic behaviors in reversible 3-state cellular automata\n(2025), arXiv: 2503.16593 [cond-mat.stat-mech].\n[25] M. ˇZnidaric, U. Duh, and L. Zadnik, Integrability is\ngeneric in homogeneous U(1)-invariant nearest-neighbor\nqubit circuits (2024), arXiv: 2410.06760 [quant-ph].\n[26] M. ˇZnidariˇ c, Inhomogeneous SU(2) symmetries in homo-\ngeneous integrable U(1) circuits and transport , Nat. Com-\nmun. 16(2025).\n[27] A. Teretenkov, F. Uskov, and O. Lychkovskiy, Pseu-\ndomode expansion of many-body correlation functions ,\nPhys. Rev. B 111, 174308 (2025).\n[28] N. Loizeau, B. Buˇ ca, and D. Sels, Opening Krylov space\nto access all-time dynamics via dynamical symmetries\n(2025), arXiv: 2503.07403 [quant-ph].\n[29] N. Keenan, N. F. Robertson, T. Murphy, S. Zhuk, and\nJ. Goold, Evidence of Kardar-Parisi-Zhang scaling on\na digital quantum simulator , npj Quantum Inf. 9, 72\n(2023).\n[30] K. Maruyoshi, T. Okuda, J. W. Pedersen, R. Suzuki,\nM. Yamazaki, and Y. Yoshida, Conserved charges in the\nquantum simulation of integrable spin chains , J. Phys. A\n56, 165301 (2023).\n[31] E. Rosenberg et al. ,Dynamics of magnetization at infi-\nnite temperature in a Heisenberg spin chain , Science 384,48 (2024).\n[32] Implementation of the truncated propagator method is\nsimpler for quantum circuits compared to Hamiltonian\nsystems due to a sharp light-cone. In addition, one has\na 1-local conserved magnetization density instead of a\n2-local energy density.\n[33] P. Gaspard, Diffusion, effusion, and chaotic scattering:\nAn exactly solvable Liouvillian dynamics , J. Stat. Phys.\n68, 673 (1992).\n[34] M. Khodas, S. Fishman, and O. Agam, Relaxation to the\ninvariant density for the kicked rotor , Phys. Rev. E 62,\n4769 (2000).\n[35] M. ˇZnidariˇ c, Prethermalization, shadowing breakdown,\nand the absence of Trotterization transition in quantum\ncircuits (2025), arXiv: 2505.15521 [quant-ph].\n[36] Formally, RP resonances can only exist in systems with\ninfinitely dimensional Hilbert spaces. In finite systems,\ncorrelation functions always converge to a nonzero con-\nstant.\n[37] M. L. Mehta, Random matrices , 3rd ed., Pure and applied\nmathematics (Academic Press, Amsterdam, 2004).\n[38] C. Paletta, U. Duh, B. Pozsgay, and L. Zadnik, Integrabil-\nity and charge transport in asymmetric quantum-circuit\ngeometries (2025), arXiv: 2503.04673 [cond-mat.stat-\nmech].\n[39] C. Jonay, J. F. Rodriguez-Nieva, and V. Khemani, Slow\nthermalization and subdiffusion in U(1) conserving Flo-\nquet random circuits , Phys. Rev. B 109, 024311 (2024).\n[40] U. Duh and M. ˇZnidariˇ c, Classification of same-gate\nquantum circuits and their space-time symmetries with\napplication to the level-spacing distribution , Phys. Rev.\nRes.6, 023068 (2024).\n[41] D. Forster, Hydrodynamic fluctuations, broken symmetry,\nand correlation functions (Adison-Wesley, 1990).\n[42] M. Ljubotina, M. ˇZnidariˇ c, and T. Prosen, Spin diffusion\nfrom an inhomogeneous quench in an integrable system ,\nNat. Commun. 8, 16117 (2017).\n[43] G. Vidal, Efficient classical simulation of slightly entan-\ngled quantum computations , Phys. Rev. Lett. 91, 147902\n(2003).\n[44] G. Vidal, Efficient simulation of one-dimensional quan-\ntum many-body systems , Phys. Rev. Lett. 93, 040502\n(2004).\n[45] The extrapolation is done through an exponential fit. We\nhave no good theoretical justification for the exponential\nfit, it was simply the best match for the data.\n[46] We thank Rustem Sharipov for this suggestion.\n[47] S. Mukerjee, V. Oganesyan, and D. Huse, Statistical the-\nory of transport by strongly interacting lattice fermions ,\nPhys. Rev. B 73, 035113 (2006).\n[48] B. Doyon, Diffusion and Superdiffusion from Hydrody-\nnamic Projections , J. Stat. Phys. 186, 25 (2022).\n[49] A. Matthies, N. Dannenfeld, S. Pappalardi, and\nA. Rosch, Thermalization and hydrodynamic long-time\ntails in a Floquet system (2024), arXiv: 2410.16182\n[quant-ph].\n[50] M. Crossley, P. Glorioso, and H. Liu, Effective field theory\nof dissipative fluids , J. High Energy Phys. 2017 (9), 95.\n[51] A. A. Michailidis, D. A. Abanin, and L. V. Delacr´ etaz,\nCorrections to diffusion in interacting quantum systems ,\nPhys. Rev. X 14, 031020 (2024).\n[52] The argument can be illustrated more explicitly by\nwriting the decomposition as an integral C(t) =P\njcje−νjt=R∞\n0c(ν)e−νtdν, where λ= e−ν,c(ν) is\n\n14\nthe continuous version of the expansion coefficients and\nwe assumed λto be real for simplicity. In other words,\nC(t) is the Laplace transform of c(ν). By employing the\ninverse Laplace transform, we see that C(t) =t−αis gov-\nerned by c(ν) =1\nΓ(α)να−1θ(ν), where Γ is the Gamma\nfunction and θis the Heaviside step function. We observe\nthat c(ν)̸= 0 for all ν >0 - an RP continuum over all\nλ∈[0,1].\n[53]C(t) =t−αe−ν0tleads to c(ν) =1\nΓ(α)(ν−ν0)α−1θ(ν−ν0)\nvia the inverse Laplace transform.\n[54] E. McCulloch, J. A. Jacoby, C. von Keyserlingk, and\nS. Gopalakrishnan, Subexponential decay of local corre-\nlations from diffusion-limited dephasing (2025), arXiv:\n2504.05380 [quant-ph].\n[55] R. Kubo, M. Toda, and N. Hashtisume, Statisti-\ncal Physics II: Nonequilibrium Statistical Mechanics ,\nSpringer Series in Solid-State Sciences (Springer, New\nYork, 1991).\n[56] For discrete-time dynamics, the\nGreen-Kubo formula reads D =\nlimtmax→∞limN→∞1\nN\u00001\n2⟨J(0)J(0)⟩+Ptmax\nt=1⟨J(t)J(0)⟩\u0001\n.\nIn the considered circuits, it gives D≈0.31 in case 1 and\nD≈0.44 in case 2, which matches our predictions with\nother methods up to accessible precision (see Fig. 4).\n[57] S. Yi-Thomas, B. Ware, J. D. Sau, and C. D. White,\nComparing numerical methods for hydrodynamics in a\none-dimensional lattice spin model , Phys. Rev. B 110,\n134308 (2024).\n[58] R. B. Lehoucq, D. C. Sorensen, and C. Yang, ARPACK\nUsers’ Guide (Society for Industrial and Applied Math-\nematics, 1998).\n[59] L. N. Trefethen and M. Embree, Spectra and pseudospec-\ntra: The behavior of nonnormal matrices and operators\n(Princeton University Press, N.J., 2005).\n\n15\nAppendix A: Details about the\nquasi-momentum-dependent propagator\nIn this appendix, we more carefully define the math-\nematical structure of the space of extensive observables\nbriefly explained in Sec. II A and use it to derive the ma-\ntrix elements of the truncated propagator, Eq. (10).\n1. Space of extensive observables\nAs explained in words in Sec. II A, the space of exten-\nsive observables of support ris spanned by\nP(r)\nk:=s−1[\nm=0n\nB(m,b)\nk|b∈P(r)o\n, (A1)\nwhere B(m,b)\nkis defined in Eq. (8) and P(r)in Eq. (6).\nP(r)\nkis an orthonormal basis w.r.t. the extensive Hilbert-\nSchmidt inner product, ⟨⟨A, B⟩⟩=s\nN2NtrA†B, defined in\nSec. II A. This can be deduced from a direct calculation,\nDD\nB(m,b)\nk, B(m′,b′)\nkEE\n= (A2)\n=s\nN2Ntr\nX\njeikjSsj+m(b)X\nj′e−ikj′Ssj′+m′(b′)\n=\n=s\nN2NX\nj,j′eik(j−j′)trh\nSsj+m(b)Ssj′+m′(b′)i\n=\n=s\nNX\nj,j′eik(j−j′)δm,m′δj,j′δb,b′=δm,m′δb,b′,\nwhere δis the Kronecker delta. We used that\nD\nSj(b),Sj′(b′)E\n=δj,j′⟨b, b′⟩ (A3)\nforb, b′∈Span( P(r)). Here the local Hilbert-Schmidt\ninner product is used, ⟨a, b⟩=1\n2Ntra†b, defined already\nin Sec. II A. This is a consequence of the fact that b, b′\nact with a traceless operator on the first site. For any\nj̸=j′we thus get a traceless operator acting on either\nsitejorj′, rendering the whole inner product zero. If\nadditionally b, b′∈P(r), we have ⟨b, b′⟩=δb,b′.\n2. Propagator of extensive observables\nWe now express the Heisenberg propagator Uin the\nbasis Pk:= lim r→∞P(r)\nk. Since we are working with an\northonormal basis, the matrix elements [ Uk](m,b),(m′,b′)are simply\n[Uk](m,b),(m′,b′)=DD\nB(m,b)\nk,U\u0010\nB(m′,b′)\nk\u0011EE\n=\n=s\nNX\nj,j′eik(j−j′)D\nSsj+m(b),U\u0010\nSsj′+m′(b′)\u0011E\n=s\nNX\nj,j′eik(j−j′)D\nSs(j−j′)+m(b),U\u0010\nSm′(b′)\u0011E\n=X\njeikjD\nSsj+m(b),U\u0010\nSm′(b′)\u0011E\n, (A4)\nwhere we took into account translational invariance and\nthe cyclic property of trace.\nIn many cases, the sum in Eq. (A4) has only a few\nnon-zero terms. The reason for that is similar to the one\nemployed in showing Eq. (A3): since b, b′act with a trace-\nless on the first site, their translations must overlap on it\nin order to give a non-zero inner product. In addition to\nthat, Eq. (A4) contains propagation U\u0010\nSm′(b′)\u0011\n, which\ncan change the first site on which b′acts non-trivially.\nAs mentioned already at the beginning of Sec. II, in\nquantum circuits, the support of observables can increase\nby at most a finite amount in one period, i.e., the light\ncone is exact. In general, the spreading of operators de-\npends on the geometry, they can even spread to all sites\nin one time period, e.g., in staircase circuits. Addition-\nally, it depends on the site at which they act, e.g., a two\nsite operator in a brickwall circuit shown in Fig. 2 spreads\nby 1 site to the left and right if it begins at an odd site\nand by 2 sites of the left and right if it begins at an even\nsite (the case depicted in the figure). Furthermore, it can\nalso be different to the left and to the right, e.g., in the\ncircuit considered in the bulk of this paper, the operator\nshown in Fig. 3 spreads by 6 sites to the left and 4 sites\nto the right. To simplify the discussion, we define δrto\nbe the maximum spreading for any site and any direc-\ntion (left/right) in a considered circuit. One can easily\nsee that if δr≤s, only three terms in Eq. (A4) can be\nnon-zero, and we can simplify it to\n[Uk](m,b),(m′,b′)=1X\nj=−1eikjD\nSsj+m(b),USm′(b′)E\n.\n(A5)\nThis is precisely Eq. (10) we set out to derive. An\nanalogous form was written down in particular cases in\nRefs [21, 22, 25]. The sum has an interpretation of “re-\naligning” propagated densities back into the basis, where\nthe mapping between densities and extensive observables\nis one-to-one. For details about this interpretation see\nAppendix A in Ref. [24].\nEq. (A5) applies in many circuits studied in the liter-\nature, such as the brickwall circuits, where δr=s= 2,\nsee Fig. 2. It also applies in the whole range of canonical\ngeometries introduced in Ref. [40]. In the circuits consid-\nered in the bulk of this paper (see Fig. 3), the spreading\nis faster, though; there δr= 6 and s= 3 and therefore\n\n16\nmore than 3 terms in Eq. (A4) can be non-zero. One can,\nhowever, use a trick described in the following section.\n3. Space-time symmetries\nWhile matrix elements of the truncated propagator for\narbitrary spreading δrcan be evaluated by Eq. (A4), the\nexpressions can sometimes be simplified by exploiting a\nspace-time symmetry. Namely, in both the brickwall cir-\ncuit defined in Eq. (12) and the 3-site generalization of\nthe brickwall defined in Eq. (14), one can notice that\nsuccessive layers (i.e., Uodd/evenorU1/2/3) are transla-\ntions of the previous layers. This can be interpreted as\na space-time symmetry; translations in space are equiva-\nlent to translations by a fraction of a period in time. In\nequation, valid for both of the mentioned cases,\nS†U(0,1)S=U(1/s,1 + 1 /s), (A6)\nwhere Sis the 1-site translation operator to the left,\nS†(a⊗ 1)S= 1⊗a,sis the number of sites of translation\ninvariance (and, crucially, also the number of layers), and\nU(t1, t2) denotes the propagator from time t1tot2. The\ntime is defined to be propagated by 1 /safter the appli-\ncation of each layer, in particular the Floquet propagator\nisU≡U(0,1).\nSpace-time symmetries and their implications are dis-\ncussed in greater detail Ref. [40]. An important result\nfor a large class of circuits, including both considered in\nthis paper, is that their propagator can be written in the\nfollowing way\nU=S−s(S˜U)s, (A7)\nwhere ˜Uis a Floquet propagator of a circuit of some other\n(preferably simpler) geometry. In both of the considered\nexamples, it is the propagator for 1 layer, ˜U=Uoddfor\nthe brickwall (12) and ˜U=U1for its 3-site generaliza-\ntion (14).\nSince S−sis just a phase in the quasi-momentum eigen-\nbasis, one can redefine S˜Uto be the new equivalent Flo-\nquet propagator (i.e., the propagator for one period). In\nparticular, for the 3-site generalization of the brickwall,\nthis reduces the spreading in one period from δr= 6 to\nδr= 2< s= 3, allowing us to use the simpler expres-\nsion for the matrix elements of the truncated propagator,\nEq. (10), and allowing for bigger numerically reachable\nsupport (it requires working with operators supported\non at most r+ 2δr=r+ 4 sites, instead of r+ 12),\nsee Appendix B 1 for details about the numerics. Since\nlong-time behavior does not depend on the redefinition\nof the Floquet propagator, the extracted RP resonances\nmust be the same. One must merely account that the new\nFloquet propagator S˜Upropagates only for time 1 /sand\nthus take the s-th power of the extracted resonances. Ad-\nditionally, if one also studies the phase of RP resonances,\nthey must be rotated in each ksector by the appropriate\nphase stemming from S−sin Eq. (A7).Appendix B: Numerical methods\n1. Spectrum of the truncated propagator\nRP resonances are extracted numerically by diagonal-\nizingU(r)\nk, which can be represented as a finite matrix\nwith matrix elements explicitly given by Eq. (10). Sup-\nportrbasis has3s\n44relements [see Eq. (A1)], thus result-\ning in a3s\n44r×3s\n44rmatrix. Importantly, the evaluation\nof matrix elements is performed on propagated observ-\nables, which have support r+ 2δrand are, therefore,\nnumerically represented as3s\n44(r+2δr)vectors.\nExact diagonalization can be used for smaller supports,\nup to around r= 7 for our case of s= 3. At larger sup-\nports, we are constrained by memory and are forced to\nuse iterative methods, which target only a finite number\nof eigenvalues. Since we are typically interested only in\nthe first few leading eigenvalues, this can be done effi-\nciently. We must only evaluate Eq. (10) on the fly, i.e.,\nby directly acting on a given observable. The calculations\nare again done on propagated observables with support\nr+ 2δr, which means that the maximum reachable sup-\nport is constrained also by the amount of spreading. In\nthe considered cases, this approach gives us access to\nsupport around r= 13. We use Arnoldi iteration imple-\nmented in ARPACK [58].\nThe truncated propagator is generically a non-normal\nmatrix (i.e., it does not commute with its Hermitian ad-\njoint) and its eigenvectors are, therefore, not orthogo-\nnal. This can lead to numerical difficulties and subtleties\nwhen interpreting its spectrum [59]. It was already ob-\nserved in Ref. [19] that RP resonance eigenvectors are\nsingular objects, however, to what extent non-normality\ncauses other problems when extracting RP resonances\nremains to be seen.\n2. Correlation functions\nThroughout the paper we numerically calculate con-\nnected infinite temperature autocorrelation functions,\nthat is\nCAA(t) =⟨A(t)A(0)⟩=1\n2NtrA(t)A(0) (B1)\n=1\n2NX\nψ⟨ψ|A(t)A(0)|ψ⟩,\nwhere the sum is taken over some normalized basis of the\nHilbert space, and Nis the number of sites in the circuit.\nFor clarity, we assumed traceless A, although the follow-\ning discussion simply generalizes to any A. Eq. (B1) can\nbe numerically evaluated by definition up to N= 18.\nIn bigger systems, we are constrained by memory and\nmust use the typicality approach. Namely, fluctuations\nofCAA(t) in a single random state are of the order of\n∼1/2N/2. Therefore, the trace in Eq. (B1) can be eval-\nuated only in 1 random state, resulting in error on the\n\n17\norder of ∼3·10−5atN= 30. Crucially, both AandU\nmust be sufficiently local for this approach and their ac-\ntion on a state must be implemented efficiently. That is,\nthe whole 2N×2Nmatrices must not be saved in mem-\nory, their action on a state must be evaluated on the fly\n[e.g., by Eqs (5), (12) and (14)].\n3. Domain wall quench\nTo determine the diffusion constant independently of\nRP resonances, we simulate unitary evolution starting\nwith a weakly polarized domain wall state (see Sec. III A).\nIn order to probe the behavior in the thermodynamic\nlimit, it is crucial that the numerical simulation only goes\nup to times, where the boundary has not yet meaning-\nfully affected the dynamics. In diffusive systems, this\ntime can be approximated by the expected spreading of\nthe diffusive front t∼x2\nmax/D. To reach big enough\ntimes, we use TEBD [43, 44]. The diffusion constant\nis expressed from the transferred magnetization in state\ndefined in Eq. (18). According to Eq. (19), this involves\nevaluating\n\nσz\nj(t)\u000b\nρ= trρ(t)σz\nj. (B2)\nThere are multiple ways to numerically calculate this, for\nexample one can write either ρorσj\nzas a matrix product\noperator (MPO) and evolve them, or appropriately sam-\nple and evolve pure states. For small µ, evolving ρturns\nout to have the slowest growth of Schmidt coefficients and\nis therefore the most efficient to simulate. This is compat-\nible with previous findings in similar systems, for details\nsee Ref. [42]. We used this method in the main part of the\npaper and additionally set the maximum bond dimension\ntoχ= 256, discarding other components. Additionally,\nwe checked the results indeed converged by considering\ndifferent χand estimated the error in the diffusion con-\nstant Dby fitting the prediction in Eq. (19) to different\ntime windows.\nAppendix C: Ruelle-Pollicott resonances through\nweak dephasing\nIn this section we demonstrate the weak Lindbladian\ndissipation-based method, proposed in Ref. [15], on cir-\ncuits without any conservation, and briefly show it gives\nthe same results as the truncated propagator method.\nAdditionally, we show that in circuits we studied, the\nconvergence of the Lindblad method can sometimes be\nrather tricky.\nThe idea is [15] to introduce some kind of Lindbladian\ndissipation of strength γ, obtaining the open system ver-\nsion of the Heisenberg propagator Uγ(which, in this case,\nis the exponent of the Lindbladian). The leading RP res-\nonances λthen correspond to the leading eigenvalues of\nthe Heisenberg propagator λUγin the zero noise γ→0+limit and in the thermodynamic limit, i.e.,\nλ= lim\nγ→0+lim\nN→∞λU(γ), (C1)\nwhere Nis the system size. The order of limits is impor-\ntant, the opposite order always gives 0.\nWhat kind of dissipation to take is not entirely clear,\nlikely some locality is required. What we want to demon-\nstrate is that the speed of convergence of the above lim-\nits might be strongly dependent on the chosen form, or,\nequivalently, on the chosen 2-qubit gate Vfor a given\ndissipation. In this paper we fix dissipation to dephasing\nand test circuits with different gates V. More specifically,\nthe considered system’s Heisenberg propagator is\nD(A):=γNX\nj=1\u0000\nσz\njAσz\nj−A\u0001\n, (C2)\nU(A):= eD(U†AU),\nwhere Uis the brickwall circuit Floquet propagator with-\nout magnetization conservation defined in Eq. (12). In\nother words, every time step consists of first evolving\nwith the brickwall circuit propagator and then applying\ndephasing with strength γon every site.\nOne can calculate the leading eigenvalue of Uγwith\nan iterative solver (similar to what we did with the trun-\ncated propagator, described in Appendix B 1) for differ-\nentNandγ. Extrapolation to zero noise and the ther-\nmodynamic limit must be done carefully [15]. Namely, if\none observes the curve |λU(γ)|for fixed N, we see that it\n“breaks” (i.e., abruptly changes its slope) at some small\nγ. This change has a physical origin: for small γthe scat-\ntering length becomes larger than the system size and one\nstarts to be influenced by finite-size (boundary) effects.\nTherefore, in order to obtain the correct decay rate of the\nbulk physics from a finite- Ndata one has to extrapolate\nthe curve |λU(γ)|from finite (large) γdown to γ= 0.\nAn example of such behavior is seen in numerical data\nfor one realization of a brickwork circuit (with a rather\ntricky instance of V) shown in Fig. 10a. Instead of λwe\nplotνU(γ) =−log|λU(γ)|, like in Ref. [15]. We observe\nνU(γ) is independent of the system size Nfor big enough\nγ≳0.15, which matches the predictions of Ref. [15].\nFrom data at γ >0.15, we can extrapolate (black dashed\ncurve) to obtain the prediction for the leading RP res-\nonance |λ| ≈e−0.403≈0.668. Comparing this to the\nvalue obtained by the truncated propagator (Fig. 10a.i)\nand to decay of correlation functions (Fig. 10b), we see\nit is incorrect. The long time decay is actually governed\nby|λ| ≈e−0.181≈0.835, which the truncated propaga-\ntor predicts correctly (in this case, this is the maximum\nλ1(k) over all k, which occurs at k= 0).\nIf we study νU(γ) more carefully though, we can get\nthe correct RP resonance from the Lindbladian approach\ntoo. Calculating νU(γ) for smaller γ, we in fact observe\nanother breaking of the curve. This time, the curves for\ndifferent Ndo not overlap, but we can still extrapolate\nfor each Nseparately and then try to do N→ ∞ . Such\n\n18\n0.00 0.05 0.10 0.15 0.20 0.25 0.30γ\n0.00.20.40.60.8νU=−log|λU|\nN= 10\nN= 12\n6 8 10 12\nN0.70.80.91.0|λ|\nFirst breaking\nSecond breaking\nTruncated prop.\nRP resonance\n(a)\n(a.i)\n0 10 20 30 40\nt10−410−310−210−1100const.×|CAA(t)|/N\n|λ1|≈0.835\n|λ1|≈0.668σz\nGUE(b)\nFIG. 10. Extraction of the leading RP resonance by a weak-dephasing Lindbladian [see Eq. (C2)], an analogous figure to\nFig. 2. (a) The leading eigenvalue for the open brickwall quantum circuit with a Haar-random gate V. Dashed and dotted\nlines denote extrapolation to zero noise; the dashed line is a quadratic fit, while the dotted are a linear fit. The inset\n(a.i) shows the convergence of the second breaking extrapolation (dotted lines) compared to the first breaking extrapolation\n(dashed lines) and the RP resonance estimated from r= 12 truncated propagator (the biggest gap is at k= 0). (b) Infinite\ntemperature autocorrelation function of extensive k= 0 observables, defined in Eq. (4). The local densities are either σz(i.e.,\nzmagnetization) or chosen randomly according to the Gaussian unitary ensemble [37]. The dashed and dotted lines denote\nthe decays ∝ |λ|tgoverned by the first and second breaking predictions, respectively. The numerics are done in a finite circuit\nwith N= 30 sites, the σzcorrelation function is multiplied by 9 for better presentation.\na procedure is also shown in Fig. 10a (colored dotted\ncurves), with the convergence with Nshown in the in-\nset (a.i). While the procedure does not yet converge for\naccessible N, it is in the vicinity of what appears to be\nthe correct RP resonance as obtained from the truncated\npropagator. Additionally, in Fig. 10 we see that the early\ntime behavior of the magnetization correlation function\nseems to be governed by |λ| ≈0.668. The initial breaking\nof the νU(γ) therefore might not be spurious, but rather\nan indication of a subleading resonance.\nWe must emphasize that the showcased behavior is not\ngeneric. Double breaking is not always observed; some-\ntimes the correct RP resonance is given by the first break-\ning, sometimes not even by the second. Additionally,\nwe do not necessarily see a 2-step relaxation in generic\ncorrelation functions. All in all, this example showcases\nthat, in our experience, the Lindbladian method is less\nrobust. Taking the double limit involves making extrap-\nolations that are sometimes hard to make. Furthermore,\nif we would want to treat systems with a conservation\nlaw, like magnetization, the method is much more cum-\nbersome, and it is not clear how to do the momentum\nresolution.\nAppendix D: Plateaus in correlation functions due\nto the powers of the conserved quantity\nPowers of a conserved quantity are also conserved\nquantities. When studying physical properties of many-\nbody systems they can be often ignored because a powerof a local operator is a non-local operator. In finite sys-\ntems with size N, however, their impact can be non-\nnegligible. Namely, local observables can have a finite\noverlap with some power of a conserved quantity and\nthus cause its correlation function to saturate to a higher\nvalue than the expected fluctuations (see Appendix B 2).\nAlthough this mechanism is simple, we explicitly demon-\nstrate it in the case of magnetization conservation, since\none needs to be aware of it when interpreting finite-size\nnumerics.\nLetQbe a conserved quantity, i.e., U(Q) =Q, which\nmeans its correlation functions is constant,\nCQQ(t) =⟨Q(t)Q(0)⟩=⟨Q, Q⟩, (D1)\nwhere the angled brackets denote the Hilbert-Schmidt\ninner product, ⟨A, B⟩:=1\n2NtrA†B. We additionally as-\nsumed Qto be Hermitian and traceless for simplicity,\nalthough a similar conclusion can be made also in the\ngeneral case.\nIf an observable overlaps with Q,cQ:=⟨Q,A⟩\n⟨Q,Q⟩̸= 0, we\nalso expect its correlation function to be constant after\na long enough time. Namely, we can decompose Ain an\northogonal basis including Q, obtaining\nCAA(t) =⟨A(t)A(0)⟩=⟨cQQ(t) +···, cQQ(0) +···⟩\n− − − →\nt→∞|cQ|2⟨Q, Q⟩=⟨Q, A⟩2\n⟨Q, Q⟩. (D2)\nHere dots denote the decaying part, i.e, we assumed the\nsystem to be mixing and that Qis the only conserved\n\n19\n0 5 10 15 20 25 30\nt10−1100|CAA(t)|/NN= 12\nN= 15\nN= 18\nN= 21N= 24\nN= 27\nN= 30\n(a)\n9 12 15 18 21242730\nN10−12×10−13×10−1|CAA(t→∞ )|/N\n2\nN−1 (b)\nFIG. 11. Finite-size plateaus in the autocorrelation function of A=P\njσz\njσz\nj+1. Shown are numerical results for a circuit\nwith a random 3-site magnetization-conserving gate (Sec. III) and different N. Subfigure (a) shows time dependence, while (b)\nshows the values of plateaus in (a). The red dashed line are the leading asymptotics (D6).\nquantity Aoverlaps with. We also assumed Ato be trace-\nless for simplicity.\nLet’s now consider the correlation function of a 2-site\nextensive observable\nA=X\njσz\njσz\nj+1 (D3)\nevolved by a magnetization-conserving circuit. Ahas\nzero overlap with the conserved M=P\njσz\nj, so its corre-\nlation function should decay to 0 in the thermodynamic\nlimit N→ ∞ . While this is true, it also has a non-zero\noverlap with powers of Mand its correlation function\nwill therefore exhibit a finite-size plateau.\nAlthough Aoverlaps with all even powers of magneti-\nzation, the leading order of the plateau in 1 /Nwill be\ncaused by M2. For Qone can now take M2made trace-\nless (i.e., with the identity projected out), Q=M2−N 1.\nA short calculation gives\n\nM2−N 1, A\u000b\n= 2N, (D4)\n\nM2−N 1, M2−N 1\u000b\n= 2N(N−1). (D5)\nWhich results in the finite-size plateau\n1\nNCAA(t→ ∞ ) =1\nN⟨Q, A⟩2\n⟨Q, Q⟩=2\nN−1. (D6)\nWe show the result for the correlation function normal-\nized by 1 /N, since for extensive observables CAA(0)∝N.\nThe correlation functions of Ain circuits with a ran-\ndom 3-site gate (i.e., the one described in Sec. III) and\ndifferent Nare shown in Fig. 11a. The plateaus are\nshown in Fig. 11b and match the predicted ∼1/Nscaling\nat large N.Appendix E: Stretched-exponential decay of local\ncorrelation functions\nRecently [54], it was argued that all local correlation\nfunctions unrelated to transport decay as stretched ex-\nponentials in diffusive one-dimensional systems, that is\nas∼e−Btαfor some νandα. For translationally-\ninvariant Floquet systems, Ref. [54] predicts α= 2/3 and\nnumerically confirms the prediction for magnetization-\nconserving brickwall circuits.\nWe check this in the 3 −site generalized brickwall cir-\ncuit (see Sec. III) by analyzing the local correlation func-\ntion of xmagnetization, which is not expected to be de-\nscribed by an effective (hydrodynamic) theory of trans-\nport [49, 54]. The numerics are shown in Fig. 12, and we\naverage over space to obtain better statistics. The aver-\nage is done on the absolute value squared, since averaging\njust⟨σx(i, t)σx(0,0)⟩would yield the extensive xmagne-\ntization correlation function. While the numerics shown\nsuggest the decay is better described by a stretched ex-\nponential with α= 2/3 than an exponential, fitting the\npower αhas a big uncertainty (we obtain α∼0.4−0.8\nfor different models with fitting uncertainties at the order\nof∼0.1).\nAs argued in Sec. IV B, stretched-exponential decays\nare governed by RP continuums with closing gap. An\ninteresting open question is what happens for correlation\nfunctions of extensive observables, which are essentially\nthe Fourier transform of local observables. Our numer-\nics seem to indicate exponential decay, at least for big k\n(see Sec. III B). Asymptotics for correlation functions for\nk= 0 are less clear numerically, the extensive xmagneti-\nzation does not seem to be in the asymptotic regime for\naccessible time scales and additionally heavily oscillates.\n\n20\n0 2 4 6\nt2/310−610−410−2100/summationtext\ni|/angbracketleftσx(i,t)σx(0,0)/angbracketright|2Case 1\nCase 2\nCase 3\nFIG. 12. Local correlation function of xmagnetization. The\ncorrelation functions are calculated in a circuit with a random\n3-site magnetization-conserving gate (Sec. III) with N= 30\nsites and averaged over space. Dashed lines show a stretched\nexponential e−Bt2/3fit.\nAppendix F: Local spin current\nIn this section, we provide additional details about the\ndiffusive behavior of the local spin current discussed in\nSec. IV B 1. The spin current jis defined by the dis-\ncrete continuity equation, Eq. (21). The diffusive predic-\ntions come from the diffusive local correlation function\nof magnetization, Eq. (15), and the continuity equation.\nFor clarity, we use the continuous space-time continuity\nequation\n∂tσz(x, t) =−∂xj(x, t), (F1)where j(x, t) is the continuous version of the spin current\nfrom Eq. (21). We derive\n⟨j(x, t)j(0,0)⟩=−Zx\n−∞dx′∂t⟨σz(x′, t)j(0,0)⟩(F2)\n=−Zx\n−∞dx′∂t⟨σz(0,0)j(−x′,−t)⟩\n=Zx\n−∞dx′Zx′\n−∞dx′′∂2\nt⟨σz(0,0)σz(−x′′,−t)⟩\n=Zx\n−∞dx′Zx′\n−∞dx′′∂2\nt⟨σz(x′′, t)σz(0,0)⟩,\nwhere we used translational invariance and the cyclic\nproperty of trace. Plugging in the diffusive magnetiza-\ntion correlation function (15), we obtain\n⟨j(x, t)j(0,0)⟩=x2−2Dt\n8√\nπDt5/2e−x2\n4Dt. (F3)\nSimilarly, as in Sec. III A, the correlation function\nof extensive current with good quasi-momentum Jk:=P\nle−ilk/3jlcan now be studied. We obtain\n1\nN\nJk(t)Jk(0)†\u000b\n→Z\n⟨j(x, t)j(0,0)⟩e−ikxdx(F4)\n∝k2e−Dk2t.\nThe decay is, therefore, governed by an RP resonance\nwith the same kdependence as the one governing the\ndecay of magnetization [cf. Eq. (16)], with the prefactor\nk2interpreted as an expansion coefficient.\n\n",
    "source": "http://arxiv.org/abs/2506.24097v1",
    "authors": [
      "Urban Duh",
      "Marko Žnidarič"
    ],
    "categories": [
      "cond-mat.stat-mech",
      "nlin.CD",
      "quant-ph"
    ],
    "type": "content"
  },
  {
    "id": "2506.24090v1_abstract",
    "title": "State Change via One-Dimensional Scattering in Quantum Mechanics",
    "content": "Title: State Change via One-Dimensional Scattering in Quantum Mechanics\n\nAbstract: We consider a pair of particles that interact in a one-dimensional setting\nvia a delta-function potential. One of the particles is confined to a\none-dimensional box, and the other particle is free. The free particle is\nincident from the left with specified energy, and it may cause changes in state\nof the confined particle before flying away to the left or to the right. We\npresent a non-perturbative formulation and computational scheme that determines\nthe probability of any such outcome, as a function of the initial state of the\nconfined particle and the energy of the incident particle.",
    "source": "http://arxiv.org/abs/2506.24090v1",
    "authors": [
      "Olivia Pomerenk",
      "Charles S. Peskin"
    ],
    "categories": [
      "quant-ph"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24090v1_content",
    "title": "State Change via One-Dimensional Scattering in Quantum Mechanics",
    "content": "arXiv:2506.24090v1  [quant-ph]  30 Jun 2025State Change via One-Dimensional Scattering in\nQuantum Mechanics\nOlivia Pomerenk and Charles S. Peskin\nJune 2025\nAbstract\nWe consider a pair of particles that interact in a one-dimensional setting via\na delta-function potential. One of the particles is confined to a one-dimensional\nbox, and the other particle is free. The free particle is incident from the left with\nspecified energy, and it may cause changes in state of the confined particle before\nflying away to the left or to the right. We present a non-perturbative formulation\nand computational scheme that determines the probability of any such outcome, as\na function of the initial state of the confined particle and the energy of the incident\nparticle.\n1 Introduction\nQuantum mechanics gets its name from the observed discreteness of the states of quan-\ntum systems, and such observations always involve transitions between states. In atomic\nphysics, for example, the energy levels of atoms are observed by means of spectroscopy,\nin which photons are emitted or absorbed and the changes in energy level are thereby\ninferred. The description of this particular process requires quantum electrodynamics,\nhowever, which is beyond the scope of the present paper.\nIn ordinary non-relativistic quantum mechanics, changes in the state of a system can be\nbrought about via interaction with another system, and the mathematical framework for\nstudying such changes is time-dependent perturbation theory. There is something peculiar\nabout this procedure, however, which can be brought out by considering the composite\nsystem as a system in its own right. Like any quantum-mechanical system, the composite\nsystem has a spectrum of stationary states, and its most general state is a superposition\nof its stationary states. Such a superposition is characterized by amplitudes ck, and the\nphysically important quantities are |ck|2, which do not change with time. Thus, from the\npoint of view of the composite system, nothing is happening! How can it be, then, that\nthe interaction with another system is causing changes in the state of the original system?\nThe answer to this question requires the theory of measurement. After the interaction\nbetween the two systems has taken place for some time, we can measure some property\n1\n\nof the original system, and may find that it has changed. It is the probability of such a\nchange that is calculated by time-dependent perturbation theory.\nIn the present paper we consider a particularly simple example of state change via\nscattering, in which the measurement step is essentially built into the interaction process,\nand in which we are able to compute the probabilities of a finite number of different\npossible outcomes without resort to any kind of perturbation theory. This is done by\nformulating the problem as an infinite system of integral equations, which are solved\nnumerically with second-order accuracy. This approach thus differs from that of previous\nworks, e.g., [4], in which time-dependent perturbation theory is used to characterize the\neffect of an external non-autonomous perturbation on a bound state. This work instead\ntreats the composite system (external system interacting with a bound state) altogether.\nIn one spatial dimension, we consider a two-particle system. One of the particles is\nconfined to a box (i.e., an interval) of length L, but the walls of the box are transparent to\nthe other particle, which moves freely on the whole real line. The two particles interact via\na delta-function potential, which may be attractive or repulsive. In the scattering problem\nthat we consider, there is an incoming state, in which the the free particle is incident from\nthe left with a definite energy, and the confined particle is in a particular stationary state\nof a particle in a one-dimensional box. This incoming state is a product state of the\nwavefunctions of the two particles, which means that the particles are independent in the\ndistant past, before any interaction has occurred. The outgoing state is nota product\nstate, however. The interaction has correlated the states of the two particles, so that\nobservation of the outgoing state of the free particle determines the corresponding state\nof the confined particle (assuming that the incoming states were known).\n2 Problem statement\nWe consider scattering in one spatial dimension by a local interaction between two par-\nticles, with one particle free and the other confined to a one-dimensional (1D) box. The\nparticles respectively have mass m1andm2. The particle with mass m2is confined to the\ninterval (0 , L), while the particle with mass m1is free.\nThe configuration space of the system is\nR×(0, L), (1)\nand the time-independent Schr¨ odinger equation is\n−ℏ2\n2m1∂2Ψ\n∂x2\n1−ℏ2\n2m2∂2Ψ\n∂x2\n2+µ0δ(x1−x2)Ψ = EΨ (2)\nwith boundary conditions\nΨ(x1,0) = Ψ( x1, L) = 0 . (3)\nThe constant µ0∈Rhas units of energy ·length. Positive µ0>0 is associated with\na repulsive interaction potential, while negative µ0<0 is associated with an attractive\n2\n\ninteraction potential. The interaction takes place along the line x1=x2in configuration\nspace.\nLet\nϕn(x1) =ZL\n0sin\u0010nπx 2\nL\u0011\nΨ(x1, x2)dx (4)\nΨ(x1, x2) =2\nL∞X\nn=1sin\u0010nπx 2\nL\u0011\nϕn(x1) (5)\nbe a Fourier decomposition of the wavefunction Ψ such that the boundary conditions\n(3) are satisfied by (5). To get an equation for ϕn(x1), we multiply both sides of (2) by\nsin(nπx 2/L) and, making use of (5), integrate with respect to x2over (0 , L). This yields\n−ℏ2\n2m1d2ϕn\ndx2(x)+ℏ2\n2m2\u0010nπ\nL\u00112\nϕn(x)+2µ0\nLC(x) sin\u0010nπx\nL\u0011∞X\nn′=1sin\u0012n′πx\nL\u0013\nϕn′(x) =Eϕn(x)\n(6)\nwhere we have simplified the notation by writing x=x1, as the integration with respect\ntox2has removed all dependencies on x2. Here,\nC(x) =(\n1, x∈(0, L)\n0, x /∈(0, L).(7)\nWe seek a solution to (6) in the form\nϕn(x) =δnn0eik0x+ Φ n(x) (8)\nin which Φ n(x) involves only outgoing waves or decays exponentially as |x| → ∞ .\nHere, n0corresponds to the index of the stationary state of the confined particle in the\n1D box, and k0is the wavenumber of the incident free particle. This particular form (8)\nis chosen to represent the composite state of the incoming wave and the confined particle\nas a product state, as the two sub-systems must be independent. This product state\ntakes the form eik0x1sin(n0πx2/L), which has the interpretation that the free particle\n(with coordinate x1) is in a state of definite momentum and energy and is completely\nde-localized, while the confined particle (with coordinate x2) is in the stationary state\nwith index n0of a particle in a 1D box. The Fourier transform with respect to x2of this\nproduct state is δnn0eik0x, which is the form given above. Thus, there is no correlation\nbetween the particles in the incoming state, but correlation may be generated by the\ninteraction in the outgoing state.\nWe require that the term δnn0eik0xbe a solution to (6) for x /∈(0, L). This implies\nthat the total energy is\nE=ℏ2\n2m1k2\n0+ℏ2\n2m2\u0010n0π\nL\u00112\n. (9)\n3\n\nThus, (6) becomes\n−ℏ2\n2m1d2Φn\ndx2(x) +ℏ2\n2m2\u0010nπ\nL\u00112\nΦn(x). . .\n+2µ0\nLC(x) sin\u0010nπx\nL\u0011∞X\nn′=1sin\u0012n′πx\nL\u0013\n(δn′n0eik0x+ Φ n′(x)) =Eϕn(x).\n(10)\nMultiplying both sides of (10) by 2 m1/ℏ2and making use of (9) yields\n−d2Φn\ndx2(x) +\u0012m1\nm2\u0010π\nL\u00112\n(n2−n2\n0)−k2\n0\u0013\nΦn(x)\n=−4m1µ0\nℏ2LC(x) sin\u0010nπx\nL\u0011∞X\nn′=1sin\u0012n′πx\nL\u0013\n(δn′n0eik0x+ Φ n′(x)).(11)\nLet\nan=m1\nm2\u0010π\nL\u00112\n(n2−n2\n0)−k2\n0, n = 1,2, . . . (12)\nThen a1<0 and an>0 for sufficiently large n. We assume that ∄nsuch that an= 0.\nNext, define\nkn=√−an, a n<0 (13)\nλn=√an, a n>0. (14)\nFornsuch that an<0, introduce the Green’s function\nGn(x) =−1\n2ikn(\ne−iknx, x < 0\neiknx, x > 0(15)\nand for nsuch that an>0, define\nGn(x) =1\n2λn(\neλnx, x < 0\ne−λnx, x > 0.(16)\nThen, for all n= 1,2, . . .,Gnsatisfies\n−d2Gn\ndx2(x) +anGn(x) =δ(x). (17)\nIt follows by linearity that (11) may be written as\nΦn(x) =−4m1µ0\nℏ2LZL\n0Gn(x−x′) sin\u0012nπx′\nL\u0013∞X\nn′=1sin\u0012n′πx′\nL\u0013\n(δn′n0eik0x′+ Φ n′(x′))dx′.\n(18)\n4\n\nDefining\nAnn′(x) =4m1µ0\nℏ2Lsin\u0010nπx\nL\u0011\nsin\u0012n′πx\nL\u0013\n, (19)\nthis becomes\nΦn(x) +∞X\nn′=1ZL\n0Gn(x−x′)Ann′(x′)Φn′(x′)dx′=−ZL\n0Gn(x−x′)Ann0(x′)eik0x′dx′.(20)\nThis is a system of Fredholm integral equations of the second kind.\n3 Interpretation of results\nAssuming that we can solve (20), we discuss in this section a physical interpretation of\nthe solution Φ n(x). First, consider the formulation of probability flux, which is derived\nfor the time-dependent Schr¨ odinger equation,\niℏ∂Ψ\n∂t=−ℏ2\n2m1∂2Ψ\n∂x2\n1−ℏ2\n2m2∂2Ψ\n∂x2\n2+µ0δ(x1−x2)Ψ. (21)\nManipulation of (21) yields\n∂\n∂t|Ψ|2+∂\n∂x1ℏ\n2im1\u0012\nΨ∂Ψ\n∂x1−Ψ∂Ψ\n∂x1\u0013\n+∂\n∂x2ℏ\n2im2\u0012\nΨ∂Ψ\n∂x2−Ψ∂Ψ\n∂x2\u0013\n= 0, (22)\nwhich has the form of a continuity equation, i.e.,\n∂ρ\n∂t+∂f1\n∂x1+∂f2\n∂x2= 0 (23)\nwhere\nρ=|Ψ|2(24)\nf1=ℏ\n2im1\u0012\nΨ∂Ψ\n∂x1−Ψ∂Ψ\n∂x1\u0013\n(25)\nf2=ℏ\n2im2\u0012\nΨ∂Ψ\n∂x2−Ψ∂Ψ\n∂x2\u0013\n. (26)\nIn particular, if Ψ has time dependence e−iωt, then this time dependence cancels out of\nthe above.\nDefine the integral of the flux associated with particle 1,\nF(x1) =ZL\n0f1(x1, x2)dx2. (27)\n5\n\nTo evaluate F(x1), we make use of (5) and (25) to obtain\nΨ∂Ψ\n∂x1=\u00122\nL\u00132X\nn,n′=1sin\u0010nπx 2\nL\u0011\nsin\u0012n′πx2\nL\u0013\nϕn′(x1)∂ϕn\n∂x1(x1). (28)\nIntegrating both sides of (28) over (0 , L) with respect to x2removes the dependence on\nx2, and so we again drop the subscript 1 on xfor ease of notation. We obtain\nF(x) =ℏ\n2im12\nL∞X\nn=1\u0012\nϕn(x)dϕn\ndx(x)−ϕn(x)dϕn\ndx(x)\u0013\n. (29)\nNote the absence of interference terms between different n.\nForx > L , we write\nϕn(x) =(\nϕn(L)eikn(x−L)an<0\nϕn(L)e−λn(x−L)an>0.(30)\nThen, for nsuch that an>0,\nϕn(x)dϕn\ndx(x)−ϕn(x)dϕn\ndx(x) = 0 . (31)\nSubstitution of (30) into (29) therefore implies that, for x > L ,\nF(x) =2\nLX\nn:an<0ℏkn\nm1|ϕn(L)|2=2\nLX\nn:an<0ℏkn\nm1\f\fΦn(L) +δnn0eik0L\f\f2. (32)\nForx < 0 and n̸=n0, a similar argument holds with kn→ − kn,λn→ − λn, and\nϕn(L)→ϕn(0). For the particular case of n=n0andx <0, we have\nϕn0(x) =eik0x+ Φ n0(0)e−ik0x(33)\nand so\nF(x) =2\nLℏk0\nm1−2\nLX\nn:an<0ℏkn\nm1\f\fΦn(0)\f\f2. (34)\nNote that F(x) is independent of x, which implies conservation of probability.\nWe see that 2 ℏk0/Lm 1is the incoming flux of probability, so we divide F(x) by this\nand make the definitions\np+\nn=kn\nk0\f\fΦn(L) +δnn0eik0L\f\f2(35)\np−\nn=kn\nk0\f\fΦn(0)\f\f2(36)\nfornsuch that an<0. Here, p+\nnis the probability that particle 1 exits to the right with\nwavenumber kn, leaving particle 2 in the nthstationary state of a particle in a box, and p−\nn\nis the probability that particle 1 exits to the left, leaving particle 2 in the nthstationary\nstate. The total probability that particle 2 is in stationary state nas a consequence of\nthe interaction is thus\npn=p+\nn+p−\nn. (37)\n6\n\n3.1 Restriction to finitely many outcomes\nThere are a finite number of possible outcomes of (35) and (36) due to the restriction\nan<0. Each of the different outcomes conserve energy. The energy of the incoming\nstate, by (9), is\nE0=ℏ2\n2m1k2\n0+ℏ2\n2m2\u0010n0π\nL\u00112\n. (38)\nThe energy for either of the states n(i.e., with particle 1 exiting to the right or left while\nleaving particle 2 in the nthstationary state) is\nEn=ℏ2\n2m1k2\nn+ℏ2\n2m2\u0010nπ\nL\u00112\n. (39)\nSubstituting (12) and (13) into the above yields En=E0.\nFurther, it must be the case that\nX\nn:an<0(p+\nn+p−\nn) =X\nn:an<0pn= 1. (40)\nThis is an immediate consequence of the fact that F(x) is independent of x: equating\nthe right hands sides of (32) and (34) gives an equation which may be manipulated to\nproduce (40).\nIn the next section, we present a method with which to solve (20) numerically for Φ n(x)\non the interval [0 , L], and to thus compute the transmission and reflection probabilities\ngiven by (35) and (36).\n4 Numerical method\nWe seek to solve (20) for Φ n(x) with n= 1, ..., T on the interval x∈[0, L]. First, denote\nthe right hand side of (20) by\nfn(x) =−ZL\n0Gn(x−x′)Ann0(x′)eik0x′dx′, (41)\nwhich is known for x∈[0, L]. Equation (20) may be written as a system of Tequations:\nΦ1(x) +∞X\nn′=1Z\nG1(x−x′)A1n′(x′)Φn′(x′)dx′=f1(x)\nΦ2(x) +∞X\nn′=1Z\nG2(x−x′)A2n′(x′)Φn′(x′)dx′=f2(x)\n...\nΦT(x) +∞X\nn′=1Z\nGT(x−x′)ATn′(x′)Φn′(x′)dx′=fT(x).(42)\n7\n\nIn order to make this a system of Tequations for Tunknowns, and thus uniquely deter-\nmine Φ n(x), we must truncate the infinite series within each equation at n′=T:\nΦ1(x) +TX\nn′=1Z\nG1(x−x′)A1n′(x′)Φn′(x′)dx′=f1(x)\nΦ2(x) +TX\nn′=1Z\nG2(x−x′)A2n′(x′)Φn′(x′)dx′=f2(x)\n...\nΦT(x) +TX\nn′=1Z\nGT(x−x′)ATn′(x′)Φn′(x′)dx′=fT(x).(43)\nSo long as Tis sufficiently large, this truncation should be appropriate for x∈[0,1]. This\nis becayse, by (15) and (16), the quantity Gn(x) decays exponentially as n→ ∞ for any\nx∈R. Thus, |Φn(x)| →0 for all x.\nNow we discretize equations (43) using Nequally spaced quadrature nodes {x1, ..., x N}\non [0 , L] such that x1= 0 and xN=L. Introduce the notation for the kernel, which is\nknown for any ( n1, n2, xi, xj):\nKij\nn1n2=Gn1(xi−xj)An1n2(xj). (44)\nThen, a discretization of (43) is\nΦ1(xi) +X\njKij\n11Φ1(xj)wj+X\njKij\n12Φ2(xj)wj+···+X\njKij\n1TΦT(xj)wj=f1(xi)\nΦ2(xi) +X\njKij\n21Φ1(xj)wj+X\njKij\n22Φ2(xj)wj+···+X\njKij\n2TΦT(xj)wj=f2(xi)\n...\nΦT(xi) +X\njKij\nT1Φ1(xj)wj+X\njKij\nT2Φ2(xj)wj+···+X\njKij\nTTΦT(xj)wj=fT(xi)(45)\nwhere the wjare precomputed quadrature weights for the nodes xj. Here, we use trape-\nzoidal quadrature. Each line of the above discretization corresponds to Nequations, one\nfor each of the Nquadrature nodes. As there are Tlines of (45), we have a system of NT\nequations for NTunknowns.\nRewriting (45) slightly yields\nX\nj\u0002\n(δij+Kij\n11wj)Φ1(xj) +Kij\n12Φ2(xj)wj+···+Kij\n1TΦT(xj)wj\u0003\n=f1(xi)\nX\nj\u0002\nKij\n21Φ1(xj)wj+ (δij+Kij\n22wj)Φ2(xj) +···+Kij\n2TΦT(xj)wj\u0003\n=f2(xi)\n...X\nj\u0002\nKij\nT1Φ1(xj)wj+Kij\nT2Φ2(xj)wj+···+ (δij+Kij\nTTwj)ΦT(xj)\u0003\n=fT(xi).(46)\n8\n\nWe construct a square block matrix Bwhich has size NT×NT. There are T2blocks,\neach of which contains an N×Nmatrix. At the ( n1, n2) block, the ( i, j) entry is\nBij=δijδn1n2+Kij\nn1n2wj. (47)\nNote that n1, n2= 1, ..., T andi, j= 1, ..., N .\nThe general system may then be expressed in the block matrix form\nBΦ =\nI+\nK11. . .K1T\n.........\nKT1. . .KTT\n\nw. . . 0\n.........\n0. . .w\n\nΦ =f (48)\nwhere\nw=\nw1. . . 0\n.........\n0. . . w N\n (49)\nand\nKn1n2=\nK11\nn1n2. . . K1N\nn1n2.........\nKN1\nn1n2. . . KNN\nn1n2\n. (50)\nThe solution Φ n(xi) is computed by numerically solving (48), e.g., by using the backslash\noperator in MATLAB. With this solution in hand, we compute probabilities of change of\nstate of the confined particle via (35) and (36).\n5 Calculation of state change probabilities\nHere we present the structure of state change probabilities in a variety of cases. Through-\nout the section, we take the special case m1=m2:=m, i.e., both particles have equal\nmass.\n5.1 Dimensionless quantities\nHere we introduce a set of dimensionless quantities with which to characterize this prob-\nlem. These will guide the numerical experiments that follow. The characteristic unit of\nmass is m, the unit of length is L, and the unit of time is T=mL2/ℏ, so that ℏ= 1 in\nthese units. Then, a unit of energy is EU=ℏ/T=ℏ2/(mL2).\nThe dimensionless energy of the incident particle is then\n˜E=E\nEU=mL2E\nℏ2=L2k2\n0 (51)\nand the dimensionless interaction strength is\ng=mLµ 0\nℏ2. (52)\n9\n\nThe third dimensionless parameter is n0, the stationary state of the confined particle in\nthe incoming state.\nWe may then define the dimensionless quantity\nϵ=gp\n˜E=g\nLk0=mµ0\nk0ℏ2, (53)\nwhich encodes the strength of the interaction relative to the dimensionless wavenum-\nber of the incident particle. This quantity will be useful to characterize the numerical\nexperiments that follow.\nWe will explore the effects of varying the dimensionless interaction strength g, the\ndimensionless wavenumber Lk0, and the initial stationary state n0on the probabilities of\nchange of state of the confined particle. Unless stated otherwise, we use T= 50 truncation\nterms and N= 2Tgrid points in a given numerical discretization.\nIn the tableaux that follow, we plot all possible outcome probabilities pn(total prob-\nability that the interaction leaves particle 2 in eigenstate n),p−\nn(probability that the\ninteraction leaves particle 2 in eigenstate nand particle 1 exits to the left), and p+\nn(prob-\nability that the interaction leaves particle 2 in eigenstate nand particle 1 exits to the\nright) against incident dimensionless wavenumbers Lk0∈(0,30) of particle 1. We use 500\nevenly spaced values of Lk0within this interval, with each value of Lk0corresponding to\na distinct numerical solve of (48). Each figure comprises six panels, all of which involve\nthe confined particle lying in a particular energy state n0prior to the interaction. Within\neach figure, panels (a-c) are associated with ϵ >0, i.e., a repulsive potential, while panels\n(d-f) are associated with ϵ <0, i.e., the equal and opposite attractive potential. In each\npanel, all possible outcomes are shown except for those associated with n=n0, i.e., we\nonly display outcomes which involve a change in the confined particle’s energy state. As\ndiscussed earlier, there are only finitely many outcomes available for a given value of Lk0.\n5.2 Probability structure in the case n0= 1\nWe assign the confined particle to be in the ground state prior to the interaction – that is,\nwe fix n0= 1. We show the structure of available outcomes for high (Fig. 1), moderate\n(Fig. 2), and low (Fig. 3) interaction strengths.\nFig. 1 displays results for a high interaction strength with g∼ O(104). Correspond-\ningly, the parameter ϵ, which encodes the relative interaction strength, is quite high, and\nranges from ϵ∼ O(103) toϵ∼ O(105).\nWe report several comments and observations regarding Fig. 1. First, further increas-\ning the strength of the interaction (by increasing g) even by several orders of magnitude\ndoes not appreciably affect the results of Fig. 1: these represent the limiting high-strength\ninteraction case. Also, perhaps unsurprisingly due to the high interaction strength, the\noverall probability of reflection (panels b,e) is several orders of magnitude larger than that\nof transmission (panels c,f). Next, we note that the number of available outcomes, indexed\nbyn, increases for higher Lk0. These outcomes grow rapidly yet continuously. Finally,\nand less intuitively, the structure of outcome probabilities is identical in the repulsive case\n(panels a-c) as in the attractive case (panels d-f).\n10\n\nFigure 1: Probability structure with n0= 1 and a high interaction strength. The\nparameter ϵranges from O(103)−O(105). (a-c) are associated with repulsive interaction,\nand (d-f) are associated with attractive interaction. (a,d) represent total probabilities\nindexed by the outcome n; (b,e) represent reflection probabilities indexed by n; and (c,f)\nrepresent transmission probabilities indexed by n.\nFigure 2: Probability structure with n0= 1 and a moderate interaction strength. The\nparameter ϵranges from O(0.1)− O(10). (a-c) are associated with repulsive interaction,\nand (d-f) are associated with attractive interaction. (a,d) represent total probabilities\nindexed by the outcome n; (b,e) represent reflection probabilities indexed by n; and (c,f)\nrepresent transmission probabilities indexed by n.\n11\n\nFigure 3: Probability structure with n0= 1 and a weak relative interaction strength.\nThe parameter ϵranges from O(10−9)− O(10−7). (a-c) are associated with repulsive\ninteraction, and (d-f) are associated with attractive interaction. (a,d) represent total\nprobabilities indexed by the outcome n; (b,e) represent reflection probabilities indexed by\nn; and (c,f) represent transmission probabilities indexed by n.\nFig. 2 displays results in the same format as Fig. 1, with all parameters identical\nexcept a decreased g∼ O(1), which corresponds to a moderate interaction strength. This\nis associated with ϵ∼ O(0.1) to ϵ∼ O(10).\nAgain, we report some observations. In stark contrast to Fig. 1, Fig. 2 displays very\ndifferent outcomes for a repulsive (panels a-c) versus an attractive (panels d-f) interaction\npotential. In particular, the attractive potential gives rise to a highly disordered array of\noutcomes, especially for Lk0∈(7,20). It may be that such choppy behavior is associated\nwith quasi-bound states, in which the incident particle is likely to be trapped but still\nultimately exits the box with probability 1. Also, the probability of reflection exceeds\nthat of transmission in the repulsive case, but the opposite is true in the attractive case.\nFinally, compared to the peaks of Fig. 1, the peaks of Fig. 2 are markedly sharper and\nnarrower (although still continuous).\nFig. 3 displays results in the same format as Figs. 1 and 2, with all parameters\nidentical except a further decreased g∼ O(10−8), i.e., a low interaction strength. This\nis associated with ϵ∼ O(10−9) toϵ∼ O(10−7). Just as in the high interaction strength\ncase, we report that the very low interaction results are essentially unchanged (up to the\nscaling factor ϵ2) as the interaction strength is decreased further. Also similarly to the\nhigh interaction strength case (Fig. 1), the attractive and repulsive results are identical;\nthese only differ in the moderate interaction strength case (Fig. 2). Finally, we note that\nthe peaks of probability display a highly regular pattern, with a peak for a given outcome\n12\n\nFigure 4: Probability structure with n0= 5 and a high interaction strength. The\nparameter ϵranges from O(103)−O(105). (a-c) are associated with repulsive interaction,\nand (d-f) are associated with attractive interaction. (a,d) represent total probabilities\nindexed by the outcome n; (b,e) represent reflection probabilities indexed by n; and (c,f)\nrepresent transmission probabilities indexed by n.\nnarising sharply and then decaying rapidly in a narrow range of Lk0.\n5.3 Probability structure in the case n0= 5\nWe now assign the confined particle to be in a higher-energy stationary state prior to the\ninteraction – that is, we fix n0= 5. We repeat the experiments of the previous section and\nshow results for high (Fig. 4), moderate (Fig. 5), and low (Fig. 6) interaction potentials.\nThe results for n0= 5 largely mirror those of n0= 1. That is, for very high interaction\nstrengths, i.e., large ϵ(Fig. 4), as well as for very weak interaction strengths, i.e., small\nϵ(Fig. 6), the probability structure is identical in the repulsive and attractive cases.\nOnly the moderate- ϵcase (Fig. 5) exhibits different outcomes for attractive and repulsive\ninteractions. Another similarity between the cases n0= 1 and n0= 5 is that very strong\npotentials render the probability of transmission to be several orders of magnitude lower\nthan that of reflection – no matter the state of the confined particle, the incident particle\nis generally unlikely to exit to the right when the interaction is very strong.\nThe effect of possible quasi-bound states is again visible in the attractive, moderately\nstrong interaction potential case of Fig. 5(c-f). Sharp peaks arise and the structure of pn\nis highly irregular.\nThe overall structure for n0= 5 is significantly more complex than that for n0= 1; the\nconfined particle’s energy may decrease or increase depending on Lk0. Outcomes in which\n13\n\nFigure 5: Probability structure with n0= 5 and a moderate interaction strength. The\nparameter ϵranges from O(0.1)− O(10). (a-c) are associated with repulsive interaction,\nand (d-f) are associated with attractive interaction. (a,d) represent total probabilities\nindexed by the outcome n; (b,e) represent reflection probabilities indexed by n; and (c,f)\nrepresent transmission probabilities indexed by n.\nFigure 6: Probability structure with n0= 5 and a low interaction strength. The param-\neterϵranges from O(0.1)− O(10). (a-c) are associated with repulsive interaction, and\n(d-f) are associated with attractive interaction. (a,d) represent total probabilities indexed\nby the outcome n; (b,e) represent reflection probabilities indexed by n; and (c,f) represent\ntransmission probabilities indexed by n.\n14\n\nFigure 7: Second-order convergence of numerical method. Arbitrarily chosen parameters\nLk0= 20 and g= 80 are fixed, and the number of mesh points Nvaries with the trunca-\ntion order TbyN= 2T. Successive refinement in Tyields second-order convergence.\nthe confined particle loses energy arise for even very small Lk0, whereas if the particle\nbegins in the ground energy state with n0= 1, there is a range of small Lk0for which the\nparticle remains in the ground state with probability 1.\n6 Convergence of numerical method\nHere we demonstrate that the numerical method presented to solve (20) exhibits second-\norder convergence with respect to simultaneous refinement in the truncation order Tand\nthe number of mesh points N. We arbitrarily fix Lk0= 20 and g= 80, so that ϵ=O(10),\nand compute the set of probabilities of all possible outcomes pnforT= 10,20, . . .100.\nWe relate NtoTviaN= 2T. We then plot the quantity\n|pi+1\nn−pi\nn|, i = 1, . . .9 (54)\nagainst Ton a log-log plot in Fig. 7. The slope of the line is −2.08, which confirms\nsecond-order convergence of the numerical method.\n7 Summary and conclusions\nIn this paper, we have considered the one-dimensional scattering of a free particle against\na confined particle, in which the two particles interact via a delta-function potential\nwhich may be repulsive or attractive. The confined particle is restricted to move in the\ninterval (0 , L), while the space available to the free particle is the whole real line, and the\nboundaries x= 0 and x=Lare transparent to the free particle.\n15\n\nStarting from the time-independent Schrodinger equation, we have derived an infi-\nnite system of integral equations, which we have solved numerically by truncation and\ndiscretization. The computational results show that the method is second-order accurate.\nWe have considered an incoming product state in which the free particle is incident\nfrom the left with prescribed energy, and the confined particle is in one of the possible sta-\ntionary states of a particle in a one-dimensional box. We have calculated the probabilities\nof the finitely many possible outcomes, each of which corresponds to the confined particle\nbeing in some (possibly different) stationary state of a particle in a one-dimensional box,\nand the free particle flying out to the right or left with some definite energy. It is a direct\nconsequence of conservation of energy that there are only finitely many possible outcomes.\nIn some of these outcomes, the free particle loses energy and the particle in the box moves\nup by one or more energy levels, but this is only possible if the free particle has sufficient\nincoming energy. In other outcomes, the free particle stimulates a loss of energy of the\nconfined particle, provided that the confined particle was not in its ground state initially.\nWe have presented computational results which show how the probabilities of these\ndiscrete outcomes depend on the dimensionless wavenumber of the free particle for low,\nmoderate, and high delta-potential interaction strengths, in both the attractive and repul-\nsive cases. These representative examples cover two types of cases: one with the confined\nparticle initially in its ground state, n0= 1, and another with the confined particle initially\nin an excited state, specifically n0= 5.\nAn important motivation of this work has been to understand the nature of state\nchange in quantum mechanics. Since understanding is subjective, only the reader can\ndecide whether or not we have been successful in this goal. In the process, however, we\nhave developed a general non-perturbative framework that can be used to calculate the\nprobabilities of different outcomes in one-dimensional scattering. Whether this framework\ncan be of use, for example in the study of quantum wires [5, 1] and quantum dots [2, 3],\nremains to be seen.\nAcknowledgments\nWe thank Michael Weinstein for helpful discussions.\nReferences\n[1] LI Goncharov, AM Yafyasov, and DE Tsurikov. A semispectral approach for the\nefficient calculation of scattering matrices in quasi-1D quantum systems and trans-\nmission coefficients for the Landauer formula. Journal of Computational Electronics ,\n13:885–893, 2014.\n[2] Lucjan Jacak, Pawel Hawrylak, and Arkadiusz Wojs. Quantum dots . Springer Science\n& Business Media, 2013.\n16\n\n[3] Leo Kouwenhoven and Charles Marcus. Quantum dots. Physics World , 11(6):35,\n1998.\n[4] A Soffer and Michael I Weinstein. Nonautonomous Hamiltonians. Journal of statistical\nphysics , 93:359–391, 1998.\n[5] JY Vaishnav, A Itsara, and EJ Heller. Hall of mirrors scattering from an impurity\nin a quantum wire. Physical Review B—Condensed Matter and Materials Physics ,\n73(11):115331, 2006.\n17\n\n",
    "source": "http://arxiv.org/abs/2506.24090v1",
    "authors": [
      "Olivia Pomerenk",
      "Charles S. Peskin"
    ],
    "categories": [
      "quant-ph"
    ],
    "type": "content"
  },
  {
    "id": "2506.24086v1_abstract",
    "title": "MotionGPT3: Human Motion as a Second Modality",
    "content": "Title: MotionGPT3: Human Motion as a Second Modality\n\nAbstract: Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.",
    "source": "http://arxiv.org/abs/2506.24086v1",
    "authors": [
      "Bingfan Zhu",
      "Biao Jiang",
      "Sunyi Wang",
      "Shixiang Tang",
      "Tao Chen",
      "Linjie Luo",
      "Youyi Zheng",
      "Xin Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24086v1_content",
    "title": "MotionGPT3: Human Motion as a Second Modality",
    "content": "arXiv:2506.24086v1  [cs.CV]  30 Jun 2025MotionGPT3: Human Motion as a Second Modality\nBingfan Zhu1Biao Jiang2Sunyi Wang1Shixiang Tang4Tao Chen2\nLinjie Luo3Youyi Zheng1†Xin Chen3†\n1Zhejiang University2Fudan University3ByteDance\n4The Chinese University of HongKong\nhttps://github.com/OpenMotionLab/MotionGPT3\nAbstract\nThough recent advances in multimodal models have demonstrated strong capabili-\nties and opportunities in unified understanding and generation, the development of\nunified motion-language models remains underexplored. To enable such models\nwith high-fidelity human motion, two core challenges must be addressed. The first\nis the reconstruction gap between the continuous motion modality and discrete\nrepresentation in an autoregressive manner, and the second is the degradation of\nlanguage intelligence during unified training. Inspired by the mixture of experts,\nwe propose MotionGPT3, a bimodal motion-language model that treats human\nmotion as a second modality, decoupling motion modeling via separate model\nparameters and enabling both effective cross-modal interaction and efficient multi-\nmodal scaling training. To preserve language intelligence, the text branch retains\nthe original structure and parameters of the pretrained language model, while\na new motion branch is integrated via a shared attention mechanism, enabling\nbidirectional information flow between two modalities. We first employ a motion\nVariational Autoencoder (V AE) to encode raw human motion into latent represen-\ntations. Based on this continuous latent space, the motion branch predicts motion\nlatents directly from intermediate hidden states using a diffusion head, bypassing\ndiscrete tokenization. Extensive experiments show that our approach achieves\ncompetitive performance on both motion understanding and generation tasks while\npreserving strong language capabilities, establishing a unified bimodal motion\ndiffusion framework within an autoregressive manner.\n1 Introduction\nRecent large-scale pre-trained models [ 43,21,45,55] have advanced multi-modal understanding and\ngeneration, highlighting strong potential for aligning diverse modalities such as language and human\nmotion. While prior work focuses mainly on text-driven motion synthesis [ 36,31,46,9,35,49,37],\nunified motion-language models for both understanding and generation remain underexplored.\nPrevious works leverages large language models (LLMs) for multi-modal tasks by unifying different\nmodalities through a language-centric framework. [ 40,33,12] treat image and motion as a foreign\nlanguage, handling them in the same manner as text, and non-text modalities like motion are first\nencoded into discrete tokens—typically via VQ-based models—so that they can be processed within\nthe same token space as text [ 9,53,48], where these discrete tokens enable seamless integration with\ntransformer-based language models and support motion generation and captioning using uniform\ninput-output formats. Understanding and generation while retaining the intelligence of language, we\npropose a bimodal motion-language framework. However, this discrete formulation overlooks the\nfundamental domain gap between symbolic representation and continuous modality [ 42]. Tokenizing\nmotion into fixed codebook entries inevitably introduces approximation errors, which impose an\nundesired limit on the generated motion quality.\n†Corresponding authors.\n\nPlease describe this motion and \ngenerate future actions.\nMotion Branch 𝓜Text Branch 𝓣\nA person is bends over and picks things\nup with left hand, then walks forward .\nFigure 1: MotionGPT3 is a bimodal motion-language model capable of bidirectional understanding\nand generation across motion and language. The model adopts a hybrid framework to encode and\nalign semantic from both modalities. Through cross-modal attention, MotionGPT3 enables effective\ninformation exchange and supports generation in either modality.\nRecent multimodal models have increasingly favored unified architectures that process multiple\nmodalities within a single model [ 34,55,45,12,41,25,51]. Most of these models incorporate\nmodality-specific heads or supervision objectives to enhance performance across diverse tasks, while\nfully fine-tuning all backbone parameters. However, joint training on two modalities, such as in\nMotionGPT [ 12], can lead to optimization conflicts, leading to trade-offs, as shown in Fig. 2, between\nnewly introduced motion tasks and existing language performance.\nTo achieve a unified framework for human motion understanding and generation, two challenges\nneed to be addressed: 1) the inherent gap between the continuous motion modality and the discrete\ntokenization required by the auto-regressive generative framework, and 2) the difficulty of involving\nnew motion tasks while preserving the original language intelligence of the pre-trained large model. To\naddress these challenges, we aim to build a framework that not only supports multi-modal integration\nbut also preserves the distinct distribution of each modality. Insightfully, the mixture-of-experts\n(MoE) approach [ 11] can achieve an adaptive training strategy based on sparse models, offering\nmodularity and flexibility for learning new modalities. Following [ 2] and Mixture-of-Transformers\n(MoT) [ 17], we thus treat human motion as a second modality via a motion expert branch, enabling\nthe model to incorporate new motion tasks while preserving language intelligence within a unified\nframework for both motion understanding and generation.\nIn this work, we propose a bimodal motion-language framework, termed MotionGPT3, that builds\nupon pre-trained language models and introduces a dedicated motion module to support both motion\nunderstanding and generation. Specifically, instead of tokenizing motion sequences into discrete\ncodes, we leverage a pre-trained motion variational autoencoder (V AE) to map motion sequences into\na continuous latent space. This design preserves the temporal continuity and fine-grained dynamics\nof human motion, avoiding the information loss and structural artifacts often introduced by discrete\ntokenization. To enable effective joint modeling, we adopt GPT-2 [ 28] as the language backbone and\nintroduce motion-specific modules with independent parameters. These modules interact with the\nlanguage backbone through shared attention layers, allowing information exchange while maintaining\nmodality-specific processing capacity. During training, the language model weights are frozen, while\nonly the motion-related components are optimized. In contrast to decoding motion branch hiddens\ninto full-resolution motion data, we operate latent diffusion to enhance efficiency and robustness.\nTo effectively model motion branch under guidance of a pre-trained language model, we design a\nthree-stage training scheme. First, with the text branch frozen, the motion branch is pre-trained on\ntext-to-motion generation.Next, cross-modal alignment is learned through motion-to-text and motion\nprediction tasks. Finally, all parameters are jointly fine-tuned. This bimodal framework allows each\nmodule guided by its own objective , improving both efficiency and scalability without excessive\nresource costs. Experiments show our improvements in both motion generation and understanding\ntasks: compared to other models, our approach reduces the training time by 2-3X while achieving\nhigher metrics. MotionGPT3 achieves SOTA performance on text-to-motion, motion-to-text, and\nmotion prediction tasks, without compromising language proficiency.\nWe summarize our contributions as follows: (1) We propose MotionGPT3, a bimodal motion-language\nframework that treats human motion as a second language, learns the motion modality, and preserves\nthe strong intelligence of pretrained language models. (2) We introduce motion latent diffusion for the\nautoregressive backbone, using a diffusion head to bridge the continuous motion modality with this\nunified next-token prediction framework, enabling more faithful and diverse motion generation. (3)\nWe introduce a three-stage training scheme to better align motion with the pretrained language space\nwithin this bimodal architecture, achieving competitive results compared to other state-of-the-art\nmethods. All training and inference codes will be publicly available.\n2\n\nCan you describe the weather today?\nstretching left to right and both arms going \nover thedler\nIt has been cold, cool and rainy.person's squats down using his right leg, and then stands back up.\nOursMotionGPT\nOursMotionGPT\nFigure 2: The unified model MotionGPT [ 12] enables bidirectional however fails to properly answer\nnon-motion-relevant questions. In contrast, equiped with independent weights for motion processing,\nMotionGPT3 retains general language intelligence and effectively handling motion-unrelated tasks.\n2 Related Work\nUnified Human Motion Modeling. Human motion modeling has evolved from task-specific de-\nsigns to unified frameworks that aim to jointly handle motion understanding and generation. Early\napproaches focus on text-conditioned motion synthesis using either diffusion models [ 36] or V AE-\nbased latent modeling [ 46]. To align motion modeling with the paradigm of language modeling,\nrecent methods leverage quantization techniques such as VQ-V AE [ 38,5] to discretize motion into\ntokens, enabling transformer-based generation [ 48,53]. However, such discretization introduces\nquantization noise and fails to capture motion’s inherent continuity and dynamics. Unified models\nlike MotionGPT [ 12], MotionGPT-2 [ 41] and LMM [ 51] adopts a language model backbone and\ndiscretized motion tokens to enable bi-directional translation. Park et al. [ 25] extend this framework to\nhuman interaction reasoning with RQ-V AE-based tokenization and a shared transformer architecture.\nExtensions to these frameworks incorporate reasoning over interaction [ 25] and scale across modali-\nties including music [ 47,20]. While these unified architectures demonstrate versatility, they often\nsuffer from cross-modal interference due to shared backbones, limiting scalability and robustness.\nThis has motivated hybrid strategies [ 32] that combine discrete and continuous representations and\ndecouple modality-specific encoders, thereby enhancing alignment and expressiveness.\nMultimodal Understanding and Generation In parallel, unified multimodal models in the vision-\nlanguage domain have demonstrated strong capabilities by treating inputs from diverse modalities\n(text, image, audio, etc.) as token sequences within a shared generative framework [ 1,34,55,47,20,\n25]. To enabling joint training over multimodal streams, Show-o [ 1] directly uses image patches as\ninputs, while Chameleon [ 34] discretizes visual inputs and concatenate them with text embeddings,\nand Transfusion [ 55] encoded as latent patches using a V AE. These models employ masked or causal\nattention mechanisms to align cross-modal features and support joint reasoning. While these unified\nmodels show strong cross-modal abilities, they typically require full fine-tuning of the backbone\nnetwork and often lead to performance degradation on the original modalities due to optimization\nconflicts. Even with carefully designed training objectives to balance modality-specific supervision,\nnew modalities can \"contaminate\" the representations learned for existing ones, highlighting the\nchallenge of preserving language understanding while scaling to new domains.\nMixture-of-Experts. To overcome the limitations of monolithic architectures, Mixture-of-Experts\n(MoE) models adopt a modular design where different experts handle different modalities or tasks.\nThese models route different modalities through specialized expert modules while maintaining a\nshared interface for cross-modal communication. This separation mitigates the interference between\nmodalities and offers greater flexibility in adapting to new input types. For instance, MoT [ 17]\nintroduces modality-specific experts with shared attention layers, facilitating modular training and\nreducing interference. LMFusion [ 32] retains the backbone of large language models and integrates\nvisual inputs with minimal modification, preserving pre-trained capabilities. In the motion domain,\nmodels as Large Motion Model (LMM) [ 51] adopts a Transformer-based diffusion model and in-\ntegrates MoE modules to separately process features from text, speech, music, and video. Motion\nAnything [ 54] introduces an attention-based mask modeling mechanism for fine-grained spatiotem-\nporal control, using modality-specific encoding heads and decoupled understanding pathways. These\nframeworks offer scalability, resource efficiency, and improved generalization when integrating new\ndata types. Motivated by these advantages, we adopt a MoE-style architecture that isolates motion\nrepresentation learning while leveraging the robust language understanding of pre-trained models.\n3\n\nℰ 𝜎 𝜇Tokenizer\n𝓜\n𝓣\nLM HeadDiffusion\n 𝐡𝐞𝐚𝐝  𝓗\nResBlock\n FinalLayer \n𝓏଴\nOutput \nTokens𝒟\nAdaLN LinearLatentText Motion\nHolderMotion Decoder Motion Encoder\ntextmotionMotion \nBranch\nText\nBranch\nmotion𝓏ଵ\n𝓏௞\n𝓏௧ିଵ\n𝜀௧𝜀ଵ\n𝜀௧ିଵ𝜀௞\n𝓏௧condition\nquerymotion \nlatenttime embeddingAttention\nmotion query\nHybrid Motion-language Model Motion Diffusion in Autoregressive Backbone Motion RepresentationFigure 3: Method overview: MotionGPT3 introduces hybrid motion-language model (Sec. 3.2) that\ntakes motion as a second modality and processes the data through a new branch. We leverage a V AE\nnetwork for continuous motion representation (Sec. 3.1), and design separate training objective for\neach modality (Sec. 3.3). Please refer to Fig. 8 for details of motion-language backbone.\n3 Method\nTo enable effective bimodal resoning of motion-text while retaining the intelligence of language,\nwe propose a bimodal motion-language framework named MotionGPT3. As illustrated in Fig. 3,\nMotionGPT3 comprises three key components: a motion V AE responsible for converting raw motion\ndata into continuous latents (Sec. 3.1), a hybrid motion-text backbone that processes motion and\nlanguage streams separately while allow cross-modal interaction through shared self-attention layers\n(Sec. 3.2), as well as a diffusion-based motion latent generation head that reconstructs motion latents\nfrom the output hidden states of the transformer. (Sec. 3.3). We also introduce a three-stage training\nstrategy of text-motion pre-training, cross-modal alignment, and joint fine-tuning (Sec. 3.4).\nWe first propose the motion V AE composed of a encoder Eand a decoder Dto map an Mframe\nmotion m1:Minto a compact continuous latent vector z∈Rdand reconstruct the motion via m1:M=\nD(z) =D(E(m1:M). Then, given length sentence of Ntokens w1:N={wi}N\ni=1, MotionGPT3 aims\nto generate a response either in natural language or as a motion sequence. Specifically in motion\ngeneration, we obtain a Klength sequence of intermedia hidden states, from which the diffusion\nheadDpredicts a single motion latent zand leads to the synthesized motion ˆm1:M.\n3.1 Motion Representation in discrete/continuous Tokens\nTo align motion representation with the autoregressive generation paradigm of large language models\n(LLMs), previous approaches typically adopt quantization-based methods such as Vector Quantized\nVariational Autoencoders (VQ-V AE) to convert motion sequences into discrete token sequences.\nWhile discrete tokens allow for seamless integration with LLMs using standard next-token prediction\nobjectives such as cross-entropy loss, the quantization process inevitably introduces information loss,\nlimiting the expressiveness of motion representation.\nIn contrast, continuous latent representation offers a more informative and expressive encoding\nof motion dynamics, preserving subtle temporal variations. However, this comes at the cost of\ncompatibility, as continuous representations are inherently misaligned with the discrete nature of\ntoken-based generation in LLMs, and requires more sophisticated modeling to support generation.\nTo bridge this gap, we adopt a motion variational autoencoder (V AE) as the continuous tokenizer to\nencode motion into a compact continuous latent space. And Further introduce a simple yet effective\nstrategy to predict motion latents from the hidden states of the backbone.\nOur transformer-based motion V AE [ 46] consists of an encoder Eand a decoder D, enhanced with\nlong skip connections , encodes motion sequences into a compact yet expressive latent space that\nenables accurate semantic understanding and supporting high-fidelity, diverse motion generation.\nSpecifically, given an input motion sequence x1:Lof an arbitrary length L, the motion encoder E\nprocesses both the motion grans and a set of learnable distribution tokens to infer the parameters of a\n4\n\nGaussian distribution, namely the mean µmand standard deviation σm. A latent vector z∈R256is\nthen sampled from this distribution using the reparameterization trick. The decoder Dmldperforms\ncross-attention over the latent vector zto query Lmotion tokens, which are then projected back into\nthe raw motion space to produce the reconstructed sequence ˆm1:L. To train this V AE, we follow\n[26, 46] and employ the Kullback-Leibler (KL) divergences loss to regularize the latent space.\n3.2 Bimodal Motion-Language Framework\nTo accommodate the distinct characteristics of language and motion modalities while enabling\nefficient cross-modal interaction, we extend a decoder-only transformer backbone [ 28] with a parallel\nmotion-specific processing branch. While structurally identical to the original language model, this\nmotion branch is initialized from scratch and trained independently to better capture the unique\ninductive biases inherent to motion data. Unlike prior approaches [ 12] that merge all modalities into\na single stream, our bimodal motion-language architecture preserves modality-specific pathways: a\ntext branch Tand a motion branch M. Cross-modal interactions are enabled selectively via shared\nattention layers, while feedforward and normalization layers remain separate. This modular design\nmaintains the original language modeling capability, while allowing the motion branch to benefit\nfrom the well-pretrained text representations through guided learning.\nHybrid Sequence Route. As illustrated in Fig. 3, given an input sequence S=s1:kthat may contain\nboth language and motion elements, we first embed each element into either a text embeddings τior\na motion latent zi, preserving their original sequence order. A routing indicator ϑiis associated with\neach token to specify its modality: tokens with ϑi= 0are then processed by the text branch T, while\nthose with ϑi= 1are then routed to the motion branch M. This routing strategy enables flexible\nmultimodal modeling without collapsing the modalities into a single embedding space. Instead,\ninteraction occurs through attention layers, where separately processed hidden states are re-aligned\nand fused according to their original ordering.\nNetwork Architecture. Each branch maintains its own feedforward layers and normalization stacks.\nIn self-attention layers, the input sequences are first projected independently into hidden states ht\nfor text and hmfor motion, which are then reassembled in their original input order to perform\nunified cross-modal attention. This architectural design enables both modality-specific processing and\nflexible inter-modal information exchange, laying the foundation for high-quality, condition-aware\nmotion-language generation.\nAdaption for motion. To support unified modeling of text and motion within a single framework, we\nextend the original text vocabulary Vtwith two motion-specific special tokens such as <som> (start\nof motion), <eom> (end of motion) which allow precise control over motion sequence boundaries.\nand<mholder_in> (motion input placeholder), <mholder_out> (motion output placeholder) which\nact as placeholders to latter replace input motion or receive motion conditioning from autoregressive\ngeneration. The input embedding layer and output projection head of the text branch are resized\naccordingly to accommodate the extended vocabulary. In contrast, as the motion branch does not\nrely on a tokenized vocabulary or codebook, we instead introduce a Motion Understanding Head\n(MotionUndHead), which maps continuous motion latents into the transformer input space. For\ngeneration tasks, a lightweight MLP replaces the language head to project the hidden states back into\nthe motion latent space defined by the V AE.\n3.3 Motion Diffusion in Autoregressive Backbone\nTo enhance the fidelity and diversity of motion generation, we introduce a lightweight diffusion\nmodule that responsible for mapping distribution into the latent space of our motion V AE. Inspired\nby recent advances in diffusion-based generative models [ 15], our approach integrates a conditional\ndenoising diffusion probabilistic model (DDPM) [ 10] with the autoregressive transformer backbone.\nThe diffusion head is designed to be computationally efficient, operating exclusively in a low-\ndimensional space and introducing minimal overhead during training and inference.\nDiffusion process. We leverage the principles of diffusion models to representing arbitrary probability\ndistributions, within the motion latents space specifically. In the forward process, Gaussian noise\nis incrementally added to the clean motion latent z0overTtimesteps, forming a noisy sequence\nztt= 1T. Each noisy sample is generated according to zt=√\n¯αtz 0+√1−¯αtϵ, ϵ ∼ N(0, I),\nwhere ¯αtis the cumulative product of noise scheduling coefficients. The denoising model ϵθ(zt|t, c),\n5\n\nMotion generation\n𝒟ℋ\nMotion understandingText to Motion Pre‐trainingStage 1\nTokenizer\nDiffusion\nHead\nText Branch Motion  BranchJoint OptimizationStage 3\nℰ\n Tokenizer\nDiffusion\nHead\nCross EntropyText Branch Motion BranchCross‐modal AlignmentStage 2\nTokenizer ℰ\nDiffusion\nHead\nCross Entropy\nText Branch Motion Branch\n(a) Model learn to generate motion properly. (b) Bidirectional task s enhances alignment. (c) Fine-tune all parameters with instr uction.\n(d) We use 𝐾<motion_holder> tokens to query motion hidden states, then \nguide ℋto generate a single motion latent.(e) Text tokens are generated in an auto-regressive \nmanner until the <eos> is predicted.Figure 4: We propose a three-stage alignment strategy for our hybrid motion-language model: (a)\nThe text branch is frozen, and only motion output is supervised. (b) Motion reasoning is introduced\nto further align the motion branch with language, with supervision on both modalities. (c) All\nmodules are jointly fine-tuned with text branch unfrozen. (d)(e) shows inference process, where each\nrectangle block represents for a whole text/motion branch, while shadowed ones are inactive modules.\nModalities are color-coded: blue for text and orange for motion. Shadowed orange squares represent\n<motion_out>, and orange-outlined squares indicate boundary tokens <som> or <eom>.\nconditioned on auxiliary input c, is trained to predict the added noise ϵ, effectively learning to reverse\nthe diffusion process. In our case, ccorresponds to the output hidden states from the transformer\nbackbone (excluding the final layer). The training objective is the standard diffusion loss [10]:\nLdiff=Ez0, t, ϵh\n|ϵ−ϵθ(zt|t, c)|2\n2i\n, (1)\nwhere z0is sampled from the motion encoder Emld using the ground-truth motion.\nInference Procedure. During inference, the text branch auto-regressively samples tokens from the\ndistribution of previous input pθ(sk|s<k, z)until a special end-of-sequence token is produced.\nTo synthesize motion, a continuous-valued vector, we manually insert Mplaceholder tokens (i.e.,\n<mholder_out>) to prompt the model after <som>. These tokens yield a set of hidden states hi:Min\none pass, which are passed to the diffusion head Hand then run in reverse process to generate the\nnoise-free motion latent z0, which is finally decoded into a raw motion sequence via Dmld.\n3.4 Training Procedure\nSince Our decoder-only backbones is pre-trained solely on textual data using a language-specific vo-\ncabulary Vt, it lacks the ability to guide understanding or generation of motion-related representations.\nWe thus initial the motion branch with random weights, and design a three-stage training procedure\nthat enables it to gradually align with the pre-trained text branch. Specifically, with a pretrained\nmotion vae and text branch, our training include three stage as shown in Fig. 4: (1) Text-to-motion\npretraining, the motion branch learns to generate motion under the guidance of the well-established\nlanguage representations. (2) Cross-modal alignment, which include multiple tasks of text-to-motion\n(T2M), motion-to-text (M2T) and motion prediction. (3) Joint Fine-Tuning, we unfreeze the text\nbranch and fine-tune the entire model with prompt-based instructions.\nText-to-Motion Pretraining. To effectively adapt Mto the motion modality, we start from training\non text-to-motion generation task while keeping the text branch Tfixed. Since the GPT-2 backbone\nis pre-trained purely on natural language data, this stage serves to initialize the motion branch with\nthe ability to interpret semantic information from text and generate plausible motion representations.\nBy using diverse text-motion pairs during this stage, we encourage the motion branch to learn a rich\nand flexible mapping from language to motion latent space.\n6\n\nMethodsRPrecision ↑FID↓ MMDist ↓ Diversity → MModality ↑\nTop1 Top2 Top3\nReal 0.511±0.0030.703±0.0030.797±0.0020.002±02.974±0.0089.503±0.065\nTM2T [9] 0.424±0.0030.618±0.0030.729±0.0021.501±0.0463.467±0.0088.589±0.0582.424±0.093\nT2M [7] 0.457±0.0020.639±0.0030.74±0.0031.067±0.0023.34±0.0089.188±0.0022.09±0.083\nMLD [46] 0.481±0.0030.673±0.0030.772±0.0020.473±0.0133.169±0.019.724±0.0822.413±0.079\nT2M-GPT [48] 0.491±0.0030.68±0.0030.775±0.0020.116±0.0043.118±0.0119.761±0.0811.856±0.011\nReMoDiffuse [50] 0.51±0.0050.698±0.0060.795±0.0040.103±0.0042.974±0.0169.018±0.0751.795±0.043\nDiverseMotion [19] 0.515±0.0030.706±0.0020.802±0.0020.072±0.0042.941±0.0079.683±0.1021.869±0.089\nMoMask [6] 0.521±0.0020.713±0.0020.807±0.0020.045±0.0022.958±0.0089.62±0.0641.241±0.04\nMotionAnything [54] 0.546±0.0030.735±0.0020.829±0.0020.028±0.0052.859±0.019.521±0.0832.705±0.06\nMotionGPT3-VQ399 0.300±0.0050.441±0.0130.532±0.020.454±0.0784.937±0.0779.626±0.0156.075±0.695\nUnified+V AE 0.501±0.0030.7±0.0030.792±0.0020.489±0.0172.841±0.0119.674±0.0813.68±\nMotionGPT3 0.5427±0.00280.7354±0.0020.8281±0.00170.2172±0.00972.7932±0.00729.6618±0.07191.3657±0.0461\nTable 1: Comparison of text-to-motion on HumanML3D [8]. These methods are sorted by FID. our\nmodel is trained with text-to-motion task for 100 epochs. The arrows ( →) indicate that closer to Real\nis desirable. Bold and underline indicate the best and the second best result, respectively.\nCross-Modal Alignment. In the second stage, we continue to freeze the text branch and introduce\nadditional objectives to enhance the motion branch’s cross-modal understanding. Specifically, we\nintroduce a set of cross-modal objectives including text-to-motion (T2M), motion-to-text (M2T), and\nmotion prediction. While preserving the generation ability acquired in the first stage, these tasks help\nto equip the motion branch with motion understanding capacity. By training on both generation and\nunderstanding tasks, Mlearns to produce motion representations that are not only informative but\nalso semantically aligned with the language representations encoded by the fixed text branch.\nJoint Fine-Tuning. In the final stage, we conduct instruction-based joint fine-tuning to equip the\nmodel with the ability to flexibly handle diverse motion-language tasks.We follow [ 12] to reformulate\nmotion-language tasks into multi-task instructions including generation, captioning, and prediction,\namong others. During this stage, all parameters of both the text and motion branches are unfrozen and\njointly optimized using the prompt-based instruction tuning framework. Building on the alignment\nestablished in the earlier stages, this full-model training avoids destructive interference and instead\npromotes deeper integration across modalities. This instruction tuning strategy allows the model to\nadapt to downstream applications while maintaining generalization across diverse task formats.\n4 Experiments\nExtensive comparisons demonstrate the performance of our Bimodal motion-language model Mo-\ntionGPT3 across multiple motion-relevant tasks. Details of the dataset settings, evaluation metrics,\nand implementation specifics (Sec. 4.1) are provided. We first present single task of text-to-motion\ngeneration (Sec. 4.2) motion-to-text understanding (Sec. 4.2). Then a uniform comparison with other\nuniformed SOTAs across both tasks is illustrated (Sec. 4.2) The supplements include more qualitative\nresults, and further implementation details.\n4.1 Experimental Setup\nDatasets. We conduct our experiments on the HumanML3D [ 8] dataset, a large-scale benchmark for\ntext-to-motion generation and understanding. It contains 14,616 motion sequences collected from\nAMASS [ 22] and HumanAct12, annotated with 44,970 sequence-level natural language descriptions.\nFor comparison with prior work [ 36,46,12], We use the 263-dim pose representation used in T2M [ 7],\nwhich is a combination of joint velocities, positions and rotations.\nEvaluation Metrics. (1) Motion Quality. Frechet Inception Distance (FID) assess how closely\ngenerated motions match ground truth ones in feature space, indicating overall quality. (2) Generation\nDiversity. Diversity (DIV) measures feature variation across samples, and MultiModality (MM),\nwhich quantifies variation among motion generations from the same textual description. (3) Text-\nMotion Alignment. To evaluate semantic consistency between generated motions and input texts,\nwe adopt motion-text retrieval precision (R-Precision) at Top-1/2/3, and the Multimodal Distance\n(MM Dist), which measures the embedding-space distance between paired modalities. (4) Motion\nCaptioning. For the motion-to-text task, we follow prior work [ 9] and adopt standard NLP metrics\nincluding BLEU [ 24], ROUGE-L [ 18], CIDEr [ 39], and BERTScore [ 52] to evaluate the fluency,\nrelevance, and diversity of generated captions.\n7\n\nMethodsR Precision ↑MM Dist ↓Bleu@1 ↑Bleu@4 ↑Rouge ↑Cider↑BertScore ↑\nTop 1 Top 2 Top 3\nReal 0.523 0.725 0.828 - - - - - -\nTM2T [9] 0.516 - 0.823 2.935 48.9 7.00 38.1 16.8 32.2\nMotionGPT [12] 0.543 - 0.827 2.821 48.2 12.5 37.4 29.2 32.4\nLaMPM2T [16] 0.547 - 0.831 2.808 47.8 13.04 37.1 28.9 32.7\nMoTe [44] 0.577 - 0.871 2.649 46.7 11.15 37.4 31.5 30.3\nUnified+V AE 0.234 0.342 0.426 5.976 31.958 3.262 21.329 7.644 16.197\nMotionGPT3-VQ 0.379 0 .582 0.702 3 .545 26 .771 2 .771 21 .579 0 .552 18 .085\nMotionGPT3 0.573 0.772 0.858 2.482 51.063 8.433 38.694 10.377 31 .992\nTable 2: Comparison of motion captioning on HumanML3D [ 8], the evaluation metrics follow [ 9].\nOur model is trained with motion-to-text task for 100 epochs.\nImplementation details. Our framework comprises three main components: a motion V AE, a\nlightweight diffusion module, and a GPT-style language backbone. We adopt the motion V AE\nintroduced in [ 46], which consist of 9 layers and 4 heads with skip connection, encoding each motion\nsequence into a 1×1×256latent. Our Diffusion Head His implemented as a three-layer MLP with\nhidden dimensionality of 1024, following the structure in [ 15] The diffusion process is trained using\na scaled linear noise schedule over 1000 denoising steps, while inference is accelerated using only\n100 steps by default. We employ GPT-2 [ 28] as the language backbone and follows a decoder-only\narchitecture with 12 transformer layers. Each layer consists of input layer normalization, a 12-head\nself-attention module, a second normalization layer, and a feed-forward network with a hidden size\nof 3072. The motion branch is initialized from scratch, while the language model uses pre-trained\nweights. Specifically, we use a 124M pre-trained GPT-2 model as the language backbone, resulting\nin a total model size of 238M parameters with an extended motion branch. We use the AdamW\noptimizer for all components, with a learning rate of 2×10−4for the motion backbone and 1×10−4\nfor the diffusion head. Training is conducted with a mini-batch of 32. The motion branch is trained\nfor 160k iterations and 300k iterations during text-to-motion pre-training stage and cross-modal\nalignment respectively. Due to the significant reduction in computational by using the motion expert,\nmodel training typically requires only two NVIDIA RTX 3090 GPUs. All studies throughout the\npaper are conducted using a lightweight GPT-2 model with 120M parameters undergoes 80k iterations\ntraining unless otherwise specified.\n4.2 Comparisons\nBy modeling human motion as a second modality alongside language, our bimodal motion-language\nmodel enables interleaved input-output processing and supports a wide range of motion-relevant tasks.\nWe first evaluate the hybrid architecture on individual tasks, including motion generation and motion\nunderstanding. Then, under the full three-stage training paradigm described in Sec. 3.4, we assess our\nmodel’s unified performance across both generation and understanding tasks. The results consistently\nshow that our approach performs competitively under both single-task and unified multi-task settings,\ndemonstrating the efficiency of the proposed framework.\nComparisons on Text-to-Motion Generation. The text-to-motion task involves generating realistic\nand diverse motion sequences conditioned on natural language descriptions. We train a single-\ntask model on this task from scratch and compare its performance with several state-of-the-art\nmethods [ 9,7,46,48,50,19,6,54] on the HumanML3D dataset. Following [ 7], we repeat each\nevaluation 20 times and report results with 95% confidence interval. Tab. 1 demonstrates that our\nmodel achieves competitive or superior performance across core metrics—highlighting the robustness\nof our motion generation approach.\nComparisons on Motion-to-Text Understanding. The motion-to-text task involves understanding\nmotion sequences and generating semantically appropriate textual descriptions. We train a single-task\ncaptioning model from scratch for 100 epochs and compare it against recent SOTA models [ 9,12,\n16,44]. Following [ 12], we use the raw ground truth textual descriptions for evaluation and apply\nthe same metrics introduced in TM2T [ 9]. Results are shown in Tab. 2. Despite significantly fewer\ntraining epochs, our model surpasses existing methods on R-Precision, MMDist, and ROUGE metrics,\ndemonstrating strong capability in motion-language alignment and description generation.\nUnified Motion Understanding and Generation. Finally, we evaluate the unified model trained via\nthe three-stage training scheme introduced in Sec. 3.4, covering both generation and understanding\n8\n\nMethodsText-to-Motion Motion-to-Text\nR TOP1 ↑ FID↓ MM Dist ↓ DIV→ R TOP3 ↑Bleu@1 ↑Bleu@4 ↑Rouge↑\nReal 0.511±0.0030.002±02.974±0.0089.503±0.0650.828 - - -\nTM2T [9] 0.424±0.0031.501±0.0463.467±0.0088.589±0.0580.823 48.9 7.00 38.1\nMotionGPT [12] 0.492±0.0030.232±0.0083.096±0.0089.528±0.0710.827 48.2 12.47 37.4\nMoTe [44] 0.548±0.0020.075±0.0042.867±0.012- 0.871 46.7 11.15 37.4\nLaMP [16] 0.557±0.0030.032±0.0022.759±0.0079.571±0.0690.831 47.8 13.04 37.1\nMotionGPT3 0.546±0.0070.155±0.0212.661±0.0049.911±0.2790.854 52.97 9.793 39.73\nTable 3: Comparison with unified multi-task models on HumanML3D [ 8] dataset. The evaluation\nmetrics are computed using the encoder introduced in [8].\ntasks. Results are reported in Tab. 3. Under our hybrid framework—which unifies motion and\nlanguage processing while maintaining modular independence, our model consistently achieves\ncompetitive performance across all evaluated tasks. Notably, we observe improvements in Retrieval\nPrecision and Multimodal Distance further validating the strength of our architecture. These results\nhighlight that, under the guidance of continuous motion representations, our bimodal model effectively\nsupports cross-modal reasoning within a hybrid framework.\n4.3 Ablation Studies\nOur work proposes a novel strategy to integrate motion modality into large language model (LLM)\ntraining, which includes two key components: 1) separate pipeline for motion and language within a\nhybrid model, 2) the utilization of continuous latents to reprensent motion in autoregressive backbone.\nWe also explore the setting of processing hidden states in Diffusion Head Hand classifier-free\nguidance (CFG) in the diffusion process. More detailed experiments are provided in Supp. Sec. C.\nContinuous Motion Representation. To evaluate the benefit of continuous motion representa-\ntions, we compare our model with a variant that employs discretized motion tokens using vector\nquantization, denoted as MotionGPT3-VQ. The results, shown in last rows of Tab. 1 and Tab. 2,\nindicate that continuous representations significantly outperform discrete ones on both tasks under\nidentical training iterations of 100. In particular, for text-to-motion, discretization decouples temporal\ncoherence across motion clips and introduces quantization noise, which hinders generation quality.\nIn contrast, continuous latent vectors retain compact and expressive semantics, enabling smoother\ngeneration and improved alignment with textual input.\nHybrid Framework. We further examine the modeling strategy by comparing unified modeling like\n[12] against our proposed hybrid approach that processes modalities independently. As shown row\nUnified+VAE in Tab. 2, the unified model benefits from the use of continuous latent motion vectors in\nterms of generation performance, while facing significant performance gap in understanding tasks.\nWe attribute this to interference within the shared latent space, where motion signals may conflict\nwith pre-trained language representations under limited capacity of language backbone.\n5 discussion\nAs an initial attempt to explore human motion generation through unified multimodal understanding\nand generation, the proposed MotionGPT3 still has several limitations. Similar to previous motion\ngeneration works [ 36,12], MotionGPT3 focuses solely on articulated human body motion. In contrast,\nother approaches explore broader aspects such as non-human motions [ 30,29,56] and human-object\ninteractions [ 13,14]. Modeling interactive scenarios within a motion-language framework remains\nan open and intriguing direction, including the generation of controllable motions in 3D scenes [ 23].\nWe summarize MotionGPT3, a bimodal motion-language framework designed to address the chal-\nlenges of unified motion understanding and generation. By introducing motion expert branch and\nleveraging continuous latent representations via a pretrained V AE, our model preserves the fine-\ngrained structure of motion while maintaining the intelligence of the pretrained language backbone.\nThrough a three-stage training scheme and latent diffusion integration, MotionGPT3 effectively\nmitigates the modality gap and the optimization conflict often observed in unified multimodal frame-\nworks. Experimental results validate the effectiveness and efficiency of our approach, demonstrating\ncompetitive performance across multiple tasks.\n9\n\nReferences\n[1]Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena, Arushi Somani, and\nSa˘gnak Ta¸ sırlar. Introducing our multimodal models, 2023.\n[2]Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai,\nLachy Groom, Karol Hausman, Brian Ichter, et al. π0: A vision-language-action flow model for general\nrobot control, 2024. URL https://arxiv. org/abs/2410.24164 , 2024.\n[3]Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, and\nYoungjae Yu. Discord: Discrete tokens to continuous motion via rectified flow decoding. arXiv preprint\narXiv:2411.19527 , 2024.\n[4]Jungbin Cho, Junwan Kim, Jisoo Kim, Minseo Kim, Mingu Kang, Sungeun Hong, Tae-Hyun Oh, and\nYoungjae Yu. Discord: Discrete tokens to continuous motion via rectified flow decoding, 2025.\n[5]Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image\nsynthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages\n12873–12883, 2021.\n[6]Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative\nmasked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 1900–1910, 2024.\n[7]Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and\nnatural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 5152–5161, 2022.\n[8]Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and\nnatural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 5152–5161, June 2022.\n[9]Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the\nreciprocal generation of 3d human motions and texts. In ECCV , 2022.\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems , 33:6840–6851, 2020.\n[11] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local\nexperts. Neural computation , 3(1):79–87, 1991.\n[12] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as a\nforeign language. Advances in Neural Information Processing Systems , 36:20067–20079, 2023.\n[13] Jiaman Li, Alexander Clegg, Roozbeh Mottaghi, Jiajun Wu, Xavier Puig, and C Karen Liu. Controllable\nhuman-object interaction synthesis. In European Conference on Computer Vision , pages 54–72. Springer,\n2024.\n[14] Jiaman Li, Jiajun Wu, and C Karen Liu. Object motion guided human motion synthesis. ACM Transactions\non Graphics (TOG) , 42(6):1–11, 2023.\n[15] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation\nwithout vector quantization. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and\nC. Zhang, editors, Advances in Neural Information Processing Systems , volume 37, pages 56424–56445.\nCurran Associates, Inc., 2024.\n[16] Zhe Li, Weihao Yuan, Yisheng He, Lingteng Qiu, Shenhao Zhu, Xiaodong Gu, Weichao Shen, Yuan Dong,\nZilong Dong, and Laurence T Yang. Lamp: Language-motion pretraining for motion generation, retrieval,\nand captioning. arXiv preprint arXiv:2410.07093 , 2024.\n[17] Weixin Liang, Lili Yu, Liang Luo, Srinivasan Iyer, Ning Dong, Chunting Zhou, Gargi Ghosh, Mike Lewis,\nWen-tau Yih, Luke Zettlemoyer, et al. Mixture-of-transformers: A sparse and scalable architecture for\nmulti-modal foundation models. arXiv preprint arXiv:2411.04996 , 2024.\n[18] Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization branches\nout, pages 74–81, 2004.\n[19] Yunhong Lou, Linchao Zhu, Yaxiong Wang, Xiaohan Wang, and Yi Yang. Diversemotion: Towards diverse\nhuman motion generation via discrete diffusion. arXiv preprint arXiv:2309.01372 , 2023.\n10\n\n[20] Mingshuang Luo, Ruibing Hou, Zhuo Li, Hong Chang, Zimo Liu, Yaowei Wang, and Shiguang Shan.\nM3gpt: An advanced multimodal, multitask framework for motion comprehension and generation. arXiv\npreprint arXiv:2405.16273 , 2024.\n[21] Yiyang Ma, Xingchao Liu, Xiaokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie,\nHaowei Zhang, Liang Zhao, et al. Janusflow: Harmonizing autoregression and rectified flow for unified\nmultimodal understanding and generation. arXiv preprint arXiv:2411.07975 , 2024.\n[22] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Gerard Pons-Moll, and Michael J. Black. Amass:\nArchive of motion capture as surface shapes. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , October 2019.\n[23] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo\nWang. Tokenhsi: Unified synthesis of physical human-scene interactions through task tokenization. arXiv\npreprint arXiv:2503.19901 , 2025.\n[24] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In Proceedings of the 40th annual meeting of the Association for Computational\nLinguistics , pages 311–318, 2002.\n[25] Jeongeun Park, Sungjoon Choi, and Sangdoo Yun. A unified framework for motion reasoning and\ngeneration in human interaction, 2025.\n[26] Mathis Petrovich, Michael J. Black, and Gül Varol. TEMOS: Generating diverse human motions from\ntextual descriptions. In European Conference on Computer Vision (ECCV) , 2022.\n[27] Mathis Petrovich, Michael J Black, and Gül Varol. Tmr: Text-to-motion retrieval using contrastive 3d\nhuman motion synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npages 9488–9497, 2023.\n[28] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog , 1(8):9, 2019.\n[29] Nadine Rueegg, Silvia Zuffi, Konrad Schindler, and Michael J. Black. BARC: Learning to regress 3D dog\nshape from images by exploiting breed information. In IEEE/CVF Conf. on Computer Vision and Pattern\nRecognition (CVPR) , pages 3876–3884, June 2022.\n[30] Nadine Rüegg, Shashank Tripathi, Konrad Schindler, Michael J Black, and Silvia Zuffi. Bite: Beyond\npriors for improved three-d dog pose estimation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 8867–8876, 2023.\n[31] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative\nprior. arXiv preprint arXiv:2303.01418 , 2023.\n[32] Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, and Lili\nYu. Lmfusion: Adapting pretrained language models for multimodal generation, 2025.\n[33] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei\nLiu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 11050–11059, 2022.\n[34] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint\narXiv:2405.09818 , 2024.\n[35] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing\nhuman motion generation to clip space. In Computer Vision–ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23–27, 2022, Proceedings, Part XXII , pages 358–374. Springer, 2022.\n[36] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Amit H Bermano, and Daniel Cohen-Or. Human\nmotion diffusion model. arXiv preprint arXiv:2209.14916 , 2022.\n[37] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human\nmotion diffusion model. arXiv preprint arXiv:2209.14916 , 2022.\n[38] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural\ninformation processing systems , 30, 2017.\n[39] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description\nevaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages\n4566–4575, 2015.\n11\n\n[40] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit\npretraining for vision and vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pages 19175–19186, 2023.\n[41] Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan,\nShixiang Tang, and Dan Xu. Motiongpt-2: A general-purpose motion-language model for motion\ngeneration and understanding. arXiv preprint arXiv:2410.21747 , 2024.\n[42] Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging\ncontinuous and discrete tokens for autoregressive visual generation, 2025.\n[43] Chengyue Wu, Xiaokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie,\nXingkai Yu, Chong Ruan, et al. Janus: Decoupling visual encoding for unified multimodal understanding\nand generation. arXiv preprint arXiv:2410.13848 , 2024.\n[44] Yiming Wu, Wei Ji, Kecheng Zheng, Zicheng Wang, and Dong Xu. Mote: Learning motion-text diffusion\nmodel for multiple generation tasks. arXiv preprint arXiv:2411.19786 , 2024.\n[45] Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao\nGu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify\nmultimodal understanding and generation. arXiv preprint arXiv:2408.12528 , 2024.\n[46] Chen Xin, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Executing your\ncommands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) , June 2023.\n[47] Fuming You, Minghui Fang, Li Tang, Rongjie Huang, Yongqi Wang, and Zhou Zhao. Momu-diffusion:\nOn learning long-term motion-music synchronization and correspondence. In The Thirty-eighth Annual\nConference on Neural Information Processing Systems , 2024.\n[48] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu,\nand Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations.\nInProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2023.\n[49] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu.\nMotiondiffuse: Text-driven human motion generation with diffusion model. IEEE transactions on pattern\nanalysis and machine intelligence , 46(6):4115–4128, 2024.\n[50] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and\nZiwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 364–373, 2023.\n[51] Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi\nZhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion\ngeneration. In European Conference on Computer Vision , pages 397–421. Springer, 2024.\n[52] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text\ngeneration with bert. arXiv preprint arXiv:1904.09675 , 2019.\n[53] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, and\nWanli Ouyang. Motiongpt: Finetuned llms are general-purpose motion generators. In Proceedings of the\nAAAI Conference on Artificial Intelligence , 2024.\n[54] Zeyu Zhang, Yiran Wang, Wei Mao, Danning Li, Rui Zhao, Biao Wu, Zirui Song, Bohan Zhuang, Ian Reid,\nand Richard Hartley. Motion anything: Any to motion generation. arXiv preprint arXiv:2503.06955 , 2025.\n[55] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn,\nXuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images\nwith one multi-modal model. arXiv preprint arXiv:2408.11039 , 2024.\n[56] Silvia Zuffi, Angjoo Kanazawa, and Michael J. Black. Lions and tigers and bears: Capturing non-rigid, 3D,\narticulated shape from images. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 3955–3963. IEEE Computer Society, 2018.\n12\n\nAppendix\nThis appendix provides qualitative comparison results (Sec. A), disccusion of continuous/ discrete\nmotion representation (Sec. B), additional experiments (Sec. C) on motion branch size and connection\ntype, motion supervision, training stages. We also analyze the training speed of our model (Sec. C.1),\nprovide more implementation details (Sec. D) . Please note our examination of training process\n(Sec. C.1), metrics report on TMR evaluator (Sec. C.2), evaluations on our training scheme (Sec. C.5),\nand analysis on bimodal architecture (Sec. C.3).\nVideo. We have provided the supplemental video to illustrate our results. In this video, we show 1)\ncomparisons of text-to-motion, 2) comparisons of motion captioning, and 3) more results on motion\ngeneration andm motion captioning. We suggest watching this video for dynamic motion results.\nCode is available in supplements. We provide example code files, which include the process of the\ntraining and evaluation of our MotionGPT3 models, as well as several example results. All codes,\ndata, and pre-trained models will be fully released.\nA Qualitative Results\nWe visualize some qualitative results on the comparison of text-to-motion ( cf.Fig. 4), motion-to-text\n(cf.Fig. 5), and our result gallery on multiple tasks ( cf.Fig. 6).\nB Discrete Token vs Continuous Token\nMethod MPJPE PAMPJPE ACCL APE A VE\nV AE 43.906 31.356 5.93 0.0581 0.0504 0.0277 0.0619 0.0179 0.0177 0.0012 0.0185\nVQ 46.828 33.668 7.629 0.0829 0.0804 0.0316 0.0930 0.0240 0.0239 0.0015 0.0253\nTable 7: Reconstruction Metrics of MLD V AE [ 46] and VQ-V AE [ 12]. Quantization process brings\ninformation loss in both encoding and decoding, resulting in frame-wise noise.\nReconstruction Task. To leverages large language models (LLMs) for motion-related multi-modal\ntasks, previous works usually use VQ-V AE to quantize motion latents into discrete tokens. This\nreformulates motion modeling as a classification task, simplifying the high-dimensional space by\nmapping continuous motions to a finite token set [ 3]. Although discrete representation suits language-\nbased architectures, quantization introduces approximation errors, information loss, and issues like\ncodebook collapse—ultimately harming motion fidelity and diversity. First, the discrete process\ninevitably introduces approximation errors, which leads to undesired limitation on the generated\nmotion quality. Second, a single discrete token often corresponds to multiple distinct motion frames\n(one-to-many mapping), causing ambiguous reconstructions as well as frame-wise noise (see Tab. 7),\ndisrupting disrupts motion smoothness.\nMore Discussions on VQ. Limited codebook sizes further restrict generation quality. MoMask [ 6]\naddresses this by using hierarchical quantization with Residual Vector Quantization (RVQ), and two\ntransformers to predict base and residual tokens. TokenBridge [ 42] make a further step by combining\nquantized and continuous latents via post-training quantization and nested autoregressive structures to\nreduce loss. Despite these advances, discrete methods inherently suffer from limited expressiveness\nand frame-wise noise artifacts. DisCoRD [ 4] mitigates these issues using a motion decoder that\nprogressively mapping discrete token predictions back to continuous raw motions, achieving higher\nfidelity and smoothness. They also propose the symmetric Jerk Percentage Error (sJPE) to evaluate\nunder-reconstruction and frame noise, showing that despite of carefully designed, discrete models still\nexhibit significant artifacts, whereas continuous models generate smoother, more natural motions.\n13\n\nText-to-Motion Task\n“A person walks forward until he loses his \nbalance and tilts far to the left , then stumbles to \nhis left .”“A person takes a few slighly hurried \nsteps without raising his arms .”“A person squats down , her left \nleg crosses her right leg , and then \nshe stands back up.”“A woman standing up throws\nsomething forward, which makes \none step forward with her right foot .”“Person walks to the right and makes a \nu-turn clockwise and returns to the left \nof her initial position facing away.”\nMotionGPT MoMask Ours mld MotionGPT MoMask Ours mldFigure 4: Comparison on text-driven motion generation. The provided state-of-the-art methods\nare under the same training and inference setting on HumanML3D [ 8]. Misaligned motions are\nindicated by red text and bounding boxes. The results indicate that our bimodal motion-language\nframework exhibits strong text comprehension capabilities, leading to more accurate and coherent\nmotion generation.\n14\n\n“The man is marching like a soldier.”“The person places his left hand \non a seat and sits down.”\n“ a person walks forward quickly .”Motion-to-Text Task\n“a person walking forward \nswinging both left and right arms.”“a person sits down and begins \nto tremble.”“Person walks quickly down a \nshort incline.”\nMotionGPT\nOursReal\n“someone moving from side to side .”“person is sitting down with arms \nhanging.”“A person walks down something \nfast.”\nInput\nMotionsFigure 5: Comparison on motion caption. The provided state-of-the-art methods are under the same\ntraining and inference setting on HumanML3D [8].\nCan you give me a random motion description?“Person runs to right and then turns \nright, while rasing their right hand.\"“A person raises their right arm \nand puts it back down. ”“A person swings their left arm back to \nthe side and then throws something with \ntheir right hand.\"Motion-to-Text Task\nTextual Question-to-Answer TaskText-to-Motion Task\n“Person runs forward, pauses, then \ncontinues running.”“someone execures a roundhouse \nkick with his left foot.”\n“A person runs to his right and then curves to the \nleft and continues to run then stops. ”“a person walks in a curved line. ”“a figure steps backward cockily, \nswinging their arms. ”\n“Practice regularly, engage with media and speak with others.”“Turquoise.It is a captivating blend of blue and green, reminiscent of tropical waters.”“A person hops forward then turns counterclockwise and starts walking back.”\n“A person hops four times, walking from right to left.”\n“A person lifts both his hands and rubs them.”\nGive me some advice on learning English.Can you give me a random color and describe this color?\nFigure 6: Gallery for the results of MotionGPT3. Samples are from our best model trained\nwith instruction-based generation and understanding tasks for text-to-motion generation, motion\nunderstanding, and text-based question answering tasks. Dynamic visualization is provided in our\nsupplemental video.\n15\n\nGeneration Task. As shown in Tab. 8, we further experiment on these two representations with our\nproposed hybrid architecture, the results demonstrate the effectiveness of continuous representation\nfor both understanding and generation tasks. In summary , while discrete representations enable\nscalable and language-model-compatible motion modeling, continuous representations better capture\nthe fine-grained dynamics and natural continuity essential for high-quality human motion synthesis.\nMethodsText-to-Motion Motion-to-Text\nepoch R TOP1 ↑FID↓MM Dist ↓DIV→ epoch R TOP3 ↑Bleu@1 ↑Bleu@4 ↑Rouge↑\nVQ 199 0.258 0 .542 5 .364 9 .274 99 0.765 47.043 7.234 39.244\n399 0.3 0 .454 4 .937 9 .626 199 0.752 41.579 6.304 35.746\nV AE 199 0.501 0 .489 2 .841 9 .674 99 0.859 50.707 8.383 38.225\nTable 8: Comparison on discrete and continuous representation, for both text-to-motion generation\nand motion-to-text understanding tasks. The reported results are evaluated on models trained on\nsingle task. The results indicate that continuous representation performs better in both understanding\nand generation, and requires fewer training iterations while achieving higher quality.\nC Additional Experiments\nWe conduct several experiments to further evaluate the performance of the proposed model. First,\nSec. C.1 presents an analysis on training process, validating the architectural design of MotionGPT3.\nNext, we assess text-to-motion generation using the TMR metrics in Sec. C.2. Following this, we\ninvestigate the design choices of our motion branch and cross-modal attention mechanisms (Sec. C.3),\nas well as the diffusion process used for motion supervision (Sec. C.4). Finally, we examine the\neffectiveness of our three-stage training strategy in Sec. C.5.\nC.1 Training Time\nTo validate our two main design choices: 1) the bimodal architecture and 2) continuous motion\nrepresentation, we conduct comparisons on the text-to-motion task, evaluating both training speed\nand validation-time metrics. As shown in Fig. 7, our method outperforms the baseline in terms of\nboth faster training and improved final performance. All experiments are conducted using the GPT-2\nbase model (124M parameters), with the same training amd inference setting on HumanML3D [7].\nR TOP3 MultiModal Dist\nVAE+Bimodal (Ours)\nVAE+Unified\nVQ+Bimodal\n~2x speed~3x speedDiffusion Loss\nFigure 7: Comparison on different model of continuous/ discrete representation, and unified/ bimodal\narchitecture (Ours) on motion generation of validation set. Our hybrid architecture with continuous\nmotion representation helps accelerate training for about 2x, as well as achieves better quality.\nFirst , our hybrid model processes motion and text separately in different branches, employing distinct\nsupervision strategies to minimize optimization conflicts and associated trade-offs during training.\nThis design results in approximately a twofold acceleration in training speed, as illustrated by the\nleftmost curve in Fig. 7. Notice that we do not compare training speed with discrete representation\nmethods directly, as differences in objectives make such comparisons less meaningful.\nSecond , regarding generation quality measured by R TOP3 and MultiModal Distance, our approach\nalso demonstrates clear advantages. The discrete representation baseline (VQ + Bimodal, green\ncurve) reaches 0.5 R TOP3 early in training, but further epochs bring limited improvement. By\ncontrast, comparing the cyan and pink curves reveals that incorporating the hybrid architecture raises\nthe upper bound of generation quality. Notably, our model achieves the performance of the unified\nbaseline trained for roughly 110 epochs within just 20 epochs.\n16\n\nOur efficiency stems from both modal-aware architecture and compact, high-dimensional continuous\nlatent vectors that serve as rich representations. This design enables the backbone to learn meaningful\ncorrelations with fewer iterations and contributes both to improved generation quality and reduced\ntraining time, demonstrating our advantages especially when computational resources are limited.\nC.2 More Metrics from TMR\nWe provide our quantitative results mainly on T2M [ 7] metrics, where the motion and text feature\nextractors are trained by contrastive learning and optimized to produce closely aligned embeddings\nfor matching text-motion pairs.\nWhen evaluated on the HumanML3D dataset using these evaluators, recent state-of-the-art methods [ 6,\n50] consistently achieve very high scores, and several recent approaches [ 44,16] have reported\nquantitative results that even exceed those of the ground-truth (GT) data. These results surpassing\nGT may suggest that the generated motions can approximate real motions reasonably well within\nthe constraints of the current evaluation framework. However, considering that the GT embeddings\neffectively represent an upper bound for matching scores, the practical significance of differences\namong methods achieving near or above GT performance might be limited.\nTo complement these observations, we explore an additional evaluation metric [ 27] intended to\nprovide a more nuanced assessment of motion generation quality (results in Tab. 9). TMR also\nperforms contrastive training, with the usage of both positive and negative samples to better structure\nthe latent space. They reports better results than [7].\nProtocol Methods Text-motionretrieval Motion-textretrieval\nR@1↑R@2↑R@3↑R@5↑R@10↑MedR↓R@1↑R@2↑R@3↑R@5↑R@10↑MedR↓\n(a) All TEMOS 2.12 4.09 5.87 8.26 13.52 173.00 3.86 4.54 6.94 9.38 14.00 183.25\nT2M 1.80 3.42 4.79 7.12 12.47 81.00 2.92 3.74 6.00 8.36 12.95 81.50\nTMR 5.68 10.59 14.04 20.34 30.94 28.00 9.95 12.44 17.95 23.56 32.69 28.50\nMotionGPT 7.16 12.50 15.85 21.53 30.20 38.00 11.31 13.91 19.39 24.13 31.80 36.25\nOurs 10.01 18.41 24.20 31.82 43.04 16.00 14.19 17.91 24.50 31.36 41.58 17.50\n(b) All with threshold TEMOS 5.21 8.22 11.14 15.09 22.12 79.00 5.48 6.19 9.00 12.01 17.10 129.00\nT2M 5.30 7.83 10.75 14.59 22.51 54.00 4.95 5.68 8.93 11.64 16.94 69.50\nTMR 11.60 15.39 20.50 27.72 38.52 19.00 13.20 15.73 22.03 27.65 37.63 21.50\nMotionGPT 14.32 21.01 25.94 33.39 43.84 15.00 14.42 16.83 22.70 27.69 35.06 30.50\nOurs 21.99 29.84 36.45 44.89 55.52 7.00 19.00 22.10 30.06 37.00 47.38 12.50\n(c) Dissimilar subset TEMOS 33.00 42.00 49.00 57.00 66.00 4.00 35.00 44.00 50.00 56.00 70.00 3.50\nT2M 34.00 48.00 57.00 72.00 84.00 3.00 34.00 47.00 59.00 72.00 83.00 3.00\nTMR 47.00 61.00 71.00 80.00 86.00 2.00 48.00 63.00 69.00 80.00 84.00 2.00\nMotionGPT 51.00 64.00 71.00 74.00 80.00 1.00 53.00 62.00 68.00 76.00 81.00 1.00\nOurs 63.00 75.00 83.00 89.00 94.00 1.00 61.00 77.00 79.00 85.00 94.00 1.00\n(d) Small batches TEMOS 40.49 53.52 61.14 70.96 84.15 2.33 39.96 53.49 61.79 72.40 85.89 2.33\nT2M 52.48 71.05 80.65 89.66 96.58 1.39 52.00 71.21 81.11 89.87 96.78 1.38\nTMR 67.16 81.32 86.81 91.43 95.36 1.04 67.97 81.20 86.35 91.70 95.27 1.03\nLaMP 67.18 81.90 87.04 92.00 95.73 - 68.02 82.10 87.50 92.20 96.90 -\nMotionGPT 58.07 69.91 74.34 79.17 86.36 1.18 58.71 69.64 74.36 79.45 86.02 1.16\nOurs 75.16 86.86 87.18 91.20 94.46 1.00 74.13 91.10 94.43 97.26 97.38 1.00\nTable 9: Comparisons on TMR evaluator. MotionGPT3outperforms the baseline [ 12]. Metrics for\nTEMOS [ 26], T2M [ 7], and TMR [ 27] are taken from the TMR paper, while LaMP [ 16] results from\ntheir published paper and are limited to the protocol of \"small batches\". We evaluate pretrained\nmodels of MotionGPT and MotionGPT3TMR’s official retrieval protocol.\nC.3 Motion Branch with Bimodal Connection\nAs alternative configurations, our hybrid model supports different parameter settings for the text and\nmotion branches, as well as various cross-modal information exchange mechanisms. We evaluate the\nparameter design choices of the motion branch and investigate the impact of different cross-modal\nattention mechanisms on the text-to-motion generation task.\nParameters amount. First, we explore the influence of parameters design and show the results in\nTab. 10. With a 124M pretrained GPT-2 model as text branch, experiments A-E show that increasing\nsize of motion branch generally improves text-motion alignment, with slight trade-offs in FID. And\nexperiment C, which uses a half-sized motion branch, achieves competitive overall performance and\nperforms particularly well in terms of FID. Additionally, experiments F (vs. C) and G (vs. E) indicate\nthat a larger text branch can further enhances motion generation quality.\n17\n\nText Motion R TOP1 ↑R TOP2 ↑R TOP3 ↑FID↓MMDist ↓ DIV MModality ↑\nA 124M 23M 0.496 0.705 0.804 0.274 2.897 10.25 2.958\nB 124M 32M 0.523 0.725 0.827 0.138 2.760 10.267 2.528\nC 124M 51M 0.538 0.747 0.841 0.136 2.685 10.379 2.374\nD 124M 79M 0.541 0.744 0.843 0.205 2.686 10.312 2.253\nE 124M 114M 0.555 0.755 0.845 0.197 2.617 10.464 1.950\nF 355M 59M 0.549 0.755 0.847 0.208 2.605 10.249 2.626\nG 774M 116M 0.560 0.764 0.848 0.155 2.606 10.297 2.375\nTable 10: Evaluation on different parameters quantity of motion branch. All models are trained under\n200k iterations on text-to-motion generation task only. Our model can achieve decent comparable\nwith a about one fourth parameters of text backbone.\n0 1 2 3 4 5 6 7 8 9 10 11\nA1⇋\nA2 ⇋\nB1⇋ ⇋ ⇋ ⇋\nB2 ⇋ ⇋ ⇋ ⇋\nB3 ⇋ ⇋ ⇋ ⇋\nC1⇋ ⇋ ⇋ ⇋ ⇋ ⇋\nC2 ⇋ ⇋ ⇋ ⇋ ⇋ ⇋\nD1⇋ ⇋ ⇋ ⇋ ⇋ ⇋\nD2 ⇋ ⇋ ⇋ ⇋ ⇋ ⇋\nE⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋ ⇋\nTable 11: Cross-modal connection activated in each experiment, where ⇋indicates shared self-\nattention operation and blank denotes that attention is limited in each single branch.\nL R TOP 1 ↑R TOP 2 ↑R TOP 3 ↑FID↓MMDist ↓DIV→ MModality ↑\nA1 0.473 0.668 0.773 0.230 3.134 10.047 3.687\nA2 0.473 0.671 0.775 0.117 3.135 9.961 3.293\nB2 0.496 0.700 0.799 0.243 2.996 10.387 2.527\nB3 0.507 0.708 0.807 0.150 2.920 10.236 2.378\nC1 0.510 0.715 0.813 0.176 2.901 10.345 2.515\nC2 0.509 0.709 0.809 0.133 2.954 10.255 2.411\nD1 0.503 0.704 0.805 0.173 2.946 10.259 2.307\nD2 0.517 0.724 0.829 0.169 2.844 10.386 2.099\nE 0.553 0.760 0.851 0.211 2.624 10.412 2.177\n(a) Effect of Cross-Modal Attention in text-to-motion generation task. More layers and\nlater-stage interactions yield better results.\nlastL R TOP 1 ↑R TOP 2 ↑R TOP 3 ↑FID↓MMDist ↓DIV→ MModality ↑\n1 0.472 0.669 0.779 0.104 3.114 10.121 2.764\n2 0.501 0.698 0.803 0.160 2.955 10.277 2.529\n3 0.510 0.719 0.819 0.148 2.884 10.336 2.412\n4 0.504 0.705 0.811 0.139 2.894 10.276 2.329\n5 0.515 0.727 0.825 0.138 2.844 10.489 2.135\n6 0.509 0.709 0.809 0.133 2.954 10.255 2.411\n(b) Comparison of layer numbers for cross-modal attention. As the number of cross-modal\ninteraction layers increases, performance first improves and then decrease slightly.\nTable 12: Evaluation on cross-modal connection. We visulize the attention settings in Tab. 11. (b) and\n(c) report the quantitative results for text-to-motion task. All models are trained under 200k iterations,\nwith a pretrained GPT-2 model (124M) as text backbone. See Sec. C.3 for more analysis.\nCross-modal Connection. We explore various configurations of cross-modal attention to examine\nhow the newly introduced motion branch learns aligned representations with the pre-trained text\nmodules (Tab. 12). Experiments A1-A2,C1–C2, and B1–B3in Tab. 12a demonstrate that given\nthe same number of layers, placing cross-modal connections in later layers generally leads to\nbetter performance in both generation quality and distribution similarity to ground-truth motion.\nExperiments D1andD2further support this finding. Although both models apply cross-modal\nattention with identical spacing and layer numbers, D1activates the connections from the first to\nthe second-last layer, while D 2applies them from the second to the last layer. Despite the minor\narchitectural difference, D1demonstrates a clear advantage in evaluation results.\n18\n\nWe also investigate the effect of activating cross-modal attention in the last Llayers of the network,\nas shown in Tab. 12b. Within the range of 2 to 6 layers, increasing the number of cross-modal\nlayers generally enhances motion quality, as reflected by smaller FID scores which indicating closer\nalignment with ground-truth motion in the latent space. However, the improvement is not strictly\nmonotonic: notably, the performance at 6 layers slightly declines compared to 5 layers in terms of R\nPricision scores. Further investigation into this non-linear trend is left for future work.\nC.4 V AE and Diffusion Head\nAblation on Diffusion Head in Motion Generation. We conduct ablation studies on the diffusion\nheadHof our motion branch to analyze how different processing strategies of hidden states impact\nmotion generation performance. Specifically, we investigate the number of motion holders (h_num)\nused as queries to extract hidden states from the auto-regressive backbone, testing values of 1, 4 and\n8 (CDE). Additionally, we compare different mapping methods from these query results to the latent\ncondition input for the diffusion head, including linear layers and multi-head attention(MHA) (FG).\nFurthermore, we examine the effect of employing classifier-free guidance (CFG) during training by\ninitializing the fake condition input either with random Gaussian noise (A) or zeros (B). Finally, as a\nbaseline, we evaluate a setup without the diffusion head, where the backbone’s hidden state is directly\nsupervised using an MSE loss (G).\ncfgRPrecision ↑FID↓ MMDist ↓ DIV→ MModality ↑\nTop1 Top2 Top3\nreal 0.519 0 .724 0 .82 2 .753 9 .941\n1 0 .534 0 .727 0 .828 0 .143 2 .714 10 .086 1.717\n2 0.555 0.754 0 .843 0 .123 2 .601 10 .006 1 .321\n3 0 .554 0.756 0.850 0.103 2.585 9.926 1.272\n4 0 .552 0 .753 0 .848 0 .098 2 .589 9 .911 1 .258\n5 0 .552 0 .752 0 .848 0.094 2.593 9 .906 1 .248\n6 0 .546 0 .751 0 .849 0.094 2.598 9 .900 1 .243\n7 0 .547 0 .748 0 .847 0 .096 2 .606 9 .895 1 .241\n8 0 .546 0 .750 0 .847 0 .099 2 .610 9 .898 1 .248\n9 0 .545 0 .750 0 .844 0 .105 2 .615 9 .882 1 .253\n10 0 .546 0 .748 0 .844 0 .109 2 .62 9 .87 1 .281\n15 0 .541 0 .739 0 .839 0 .12 2 .653 9 .873 1 .312\n20 0 .533 0 .728 0 .826 0 .134 2 .739 9 .827 1 .385\nTable 13: Ablation on classifier-free guidance for diffusion head setting.\nClassifier Free Guidance. we conduct experiments on classifier-free guidance (CFG) for motion\ngeneration tasks. We find that ω= 5.0achieves the best FID on the text-to-motion task, while\nω= 3.0achieves the best R-Precision and MultiModal Distance.\nMHA h_num CFGRPrecision ↑FID↓MMDist ↓DIV→ MModality ↑\nTop1 Top2 Top3\nA! 4 randinit 0.547 0.751 0.850 0.149 2.578 10.041 2.265\nB! 4 zeroinit 0.542 0.749 0.843 0.091 2.598 10.036 2.160\nC! 8 % 0.531 0.733 0.836 0.185 2.655 10.154 2.198\nD! 4 % 0.529 0.730 0.839 0.166 2.645 10.012 2.350\nE! 1 % 0.525 0.729 0.831 0.164 2.678 10.090 2.514\nF% 4 % 0.521 0.731 0.829 0.178 2.713 9.985 2.603\nG% 1 % 0.525 0.729 0.829 0.283 2.689 10.069 2.719\nH - 4 - 0.518 0.725 0.823 0.276 2.705 9.758 2.175\nTable 14: Ablation study on motion head design and loss functions. All variants are trained on the\nT2M task for 200k iterations.\n19\n\nC.5 Effectiveness of Training Scheme\nWe have illustrated our three-stage training scheme in Fig. 3. In this section, we conduct an ablation\nstudy to evaluate the effectiveness of each stage, including text-to-motion pre-training (first stage,\n100k iterations), cross-modal alignment (second stage, 300k iterations), and joint fine-tuning (third\nstage, 140k iterations). Furthermore, we perform a two-stage experiment without freezing the\nparameters of the text branch, since under this setting the third stage becomes redundant. We evaluate\nthe trained models on two representative tasks: motion generation and motion captioning. The\ncorresponding results are summarized in Tab. 15.\ntext branch Stage1 Stage2 Stage3Text-to-Motion Motion-to-Text\nR TOP3 ↑FID↓MMDist ↓R TOP1 ↑Bleu@4 ↑ Bert↑\nA fixed ! % % 0.820 0.202 2.787 - - -\nB fixed ! ! % 0.850 0.094 2.592 0.570 8.801 28.363\nC training ! ! ! 0.850 0.099 2.590 0.579 8.412 28.627\nD fixed % ! % 0.734 0.623 3.337 0.510 6.25 31.777\nE training % ! ! 0.789 0.258 2.933 0.573 8.119 32.131\nF training ! % - 0.834 0.34 2.698 - - -\nG training ! ! - 0.804 0.176 2.837 0.365 3.577 0.713\nTable 15: Evaluation of the training scheme on the GPT-2 small models. Stage1: t2m Pre-training,\nStage2: Cross Modal Alignment, Stage3: Joint Fintuning.\nText-to-motion Pre-training vsCross-Modal Alignment . In the first stage, training is conducted\nsolely on the text-to-motion generation task. The second stage jointly optimizes for both motion\nunderstanding and generation tasks. The comparison between experiments A and B demonstrates\nthat pre-training on the generation task does not negatively affect subsequent motion understanding\ntraining. However, when multi-task training is performed from scratch without the first stage\n(experiment D), the quality of motion generation significantly degrades.\nJoint Fine-tuning. Since the second stage already aligns the motion and text branches through\nmulti-task training, bringing their representations closer, the third stage does not yield substantial\nimprovements in language capabilities.\nFixed text branch. To mitigate the adverse effects of new modality training on the language branch,\nwe propose fixing the text branch parameters during the first two stages. Experiments F and G, where\nall network parameters are updated simultaneously, show that the model can outperform the baseline\n(experiment A) on the motion generation task in the first stage. However, when entering multi-task\ntraining in the second stage, these models exhibit decreased performance on the text-to-motion task\nand fail to achieve satisfactory language generation capabilities. This decline is attributed to the\nnegative impact of updating the text branch parameters during the first stage training, which results in\na significant loss of language intelligence.\nC.6 Ablation on Model Size\nTab. 16 shows comparison on model size for our bimodal backbone. With 3x the number of\nparameters, GPT-2 medium backbone slightly outperforms the GPT-2 samll, despite a decrease in\nFID. We attribute this performance gap to the larger model’s greater capacity to learn high-level\ndistributions, while may requires more carefully tuned training data and optimization to fully realize\nits potential.\nbackbone text Parameters all parameters cfg R1 R2 R3 FID MMDist DIV\nGPT-2 small 124M 238M 1.0 0.534 0 .739 0 .842 0 .222 2 .61 10 .256\nGPT-2 medium 355M 692M 1.0 0.558 0 .756 0 .852 0 .235 2 .553 10 .238\nGPT-2 small 124M 238M 3.0 0.552 0 .759 0 .852 0 .173 2 .554 10 .239\nGPT-2 medium 355M 692M 3.0 0.568 0 .766 0 .860 0 .192 2 .489 10 .084\nTable 16: Comparison on different model size of the GPT-2 backbone. We train all the models on\ntext-to-motion task on HumanML3D dataset for 200 iterations.\n20\n\nHybrid Motion -language Model (SEC. 3.2) Diffusion for Autoregressive Model (SEC. 3.3)Text Embedding\n Linear\nLinear\nℰ\nText Tokenshow\nare\nyouholder\nholder\nholder\nholder\nheyGlobal queryLM Head\nProbability distribution\nNext Token PredictionSoftmaxQKV Projector QKV Projector\nFFN FFN\nrearrange\nSelf -Attention\n×𝑁𝑁\nMHA𝒟𝒟𝑐𝑐\nDiffusion\nheadℋ\nAdaLNMotion Latent \nReplacement\nMotion Representation (SEC. 3.1)Figure 8: Details of our bimodal motion-language model. Given motion and text inputs, tokens\nfrom each modality are processed through their respective branches, with motion components in\norange and text components in blue. These tokens are then rearranged into their original input order\nbefore cross-modal self-attention. After passing through N hybrid layers, text generation proceeds via\nautoregressive next-token prediction, while motion generation is performed through a diffusion head.\nD Details on MotionGPT3 Models\nDiffusion loss for motion generation. We detail the process by which the diffusion head generates\nnoise-free motion latent representations from the hidden states of the motion branch. As illustrated in\nFig. 3 and Fig. 4, a motion holder token, denoted as < motion out > , is introduced as a query to\nextract the corresponding output hidden states. After a feed-forward operation, we obtain khidden\nstates corresponding to the holder inputs, represented as h∈RK×256. These multiple hidden states\nare then aggregated into a single global condition vector c∈R1×256through an attention mechanism,\nwhich is subsequently combined with the time step embedding t.\nFollowing this, the noisy latent zt∈R1×256at the current time step is transformed under the guidance\nof adaptive layer normalization (AdaLN) to predict the noise component ϵt. Utilizing this predicted\nnoise, we compute the posterior distribution to obtain the input latent for the next time step, zt−1.\nDuring training, the time step tis randomly sampled, and corresponding noise is added to the ground\ntruth motion latent. The diffusion head is then trained to detect and remove this noise. At inference\ntime, a Gaussian noise latent ztis iteratively denoised until t= 0, yielding a noise-free latent\nrepresentation which is then decoded by the motion decoder to generate the raw motion sequence.\nCross-Entropy Loss for boundary tokens. In addition to the <motion_out> token, we introduce\ntwo boundary tokens, <som> (start of motion) and <eom> (end of motion), to distinguish motion\ngeneration phases from text generation. During generation, the <som> token is predicted from the\nlanguage model’s next-token prediction mechanism, while the <eom> token is appended directly to\nthe token sequence after a single forward pass of motion generation. Accordingly, during training,\nwe incorporate a prediction loss for the <som> token but do not require the network to learn the\ngeneration of the <eom> token.\n21\n\n",
    "source": "http://arxiv.org/abs/2506.24086v1",
    "authors": [
      "Bingfan Zhu",
      "Biao Jiang",
      "Sunyi Wang",
      "Shixiang Tang",
      "Tao Chen",
      "Linjie Luo",
      "Youyi Zheng",
      "Xin Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.CL"
    ],
    "type": "content"
  },
  {
    "id": "2506.24085v1_abstract",
    "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention",
    "content": "Title: Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention\n\nAbstract: Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.",
    "source": "http://arxiv.org/abs/2506.24085v1",
    "authors": [
      "Wonwoong Cho",
      "Yanxia Zhang",
      "Yan-Ying Chen",
      "David I. Inouye"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.24085v1_content",
    "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention",
    "content": "arXiv:2506.24085v1  [cs.CV]  30 Jun 2025Imagine for Me: Creative Conceptual Blending of Real\nImages and Text via Blended Attention\nWonwoong Cho∗Yanxia Zhang†Yan-Ying Chen†David I. Inouye∗\nProduct Design\nCharacter Design\nGraphic & Interior(top) sneakers    (bottom) handbag(1) bicycle   (2) car   (3) headphone   (4) truck\n(top) monster cartoon character(bottom) owl cartoon character\nliving room\n(left) bear (bottom) eye\nFigure 1: Visual and textual conceptual blending results of IT-Blender based on FLUX.1-dev.\n∗Elmore Family School of Electrical and Computer Engineering, Purdue University\n†Toyota Research Institute\nPreprint. Under review.\n\nAbstract\nBlending visual and textual concepts into a new visual concept is a unique and\npowerful trait of human beings that can fuel creativity. However, in practice, cross-\nmodal conceptual blending for humans is prone to cognitive biases, like design\nfixation, which leads to local minima in the design space. In this paper, we propose\na T2I diffusion adapter “IT-Blender” that can automate the blending process to\nenhance human creativity. Prior works related to cross-modal conceptual blending\nare limited in encoding a real image without loss of details or in disentangling\nthe image and text inputs. To address these gaps, IT-Blender leverages pretrained\ndiffusion models (SD and FLUX) to blend the latent representations of a clean\nreference image with those of the noisy generated image. Combined with our\nnovel blended attention, IT-Blender encodes the real reference image without loss\nof details and blends the visual concept with the object specified by the text in a\ndisentangled way. Our experiment results show that IT-Blender outperforms the\nbaselines by a large margin in blending visual and textual concepts, shedding light\non the new application of image generative models to augment human creativity.\nOur project website is: https://imagineforme.github.io/ .\n1 Introduction\n“Conceptual integration is at the heart of imagination” — Fauconnier and Turner [2008]\nConceptual integration/blending [Fauconnier and Turner, 1998, 2008, Coulson, 2001] is a theory in\nCognitive Science, which can describe the human’s cognitive process combining a visual and textual\nconcepts into a new idea. It is one of the most essential virtues in the creative industries (e.g., product\ndesign, character design, fashion design, interior design, graphic design, art, and advertisement)\nbecause conceptual blending can provide inspirational and creative design ideas by creating new\ncombinations or reinventing existing ones [Gabora, 2002].\nPrior works Yang [2009], Hyun and Lee [2018], Cai et al. [2023] have shown that exploring the\ndesign concepts and space as much as possible can produce better design results especially during\nthe early phase of the design process (e.g., Conceptual Design [Otto, 2003] and the SCAMPER\nmethod [Eberle, 1996] in Concept Generation [Ulrich and Eppinger, 2016]).\nHowever, there can be two challenges to perform cross-modal visual and textual conceptual blending\nin practice. First, human’s creativity easily got stuck in the suboptimal as shown in design fixation\n(i.e., a tendency of a designer to overly adhere to a limited set of solutions) [Jansson and Smith, 1991]\nand Einstellung effect (i.e., a cognitive bias from past experiences or familiar solutions to a problem,\npreventing them from exploring better alternatives) [Luchins, 1942].\nSecond, cross-modal conceptual blending itself is not a trivial task. It can be achieved by selective\nprojection process determining what and where to integrate the given multiple concepts [Fauconnier\nand Turner, 1998]. It involves the laborious process for identifying features in each condition and\ncomparing the semantic correspondence to find a way to meaningfully blend them together.\nRecent significant advances of text-to-image (T2I) diffusion models [Rombach et al., 2022, Saharia\net al., 2022] and their applications for adding an image condition led us to the question, “Can\npretrained diffusion models be used for cross-modal conceptual blending to augment creativity?”\nIf so, it can be very useful by 1) providing numerous conceptual blending results to explore broader\ndesign possibilities and 2) automating the conceptual blending process to minimize the time required\nto manually illustrate all design ideas. For example, suppose that we want to come up with a creative\nproduct design for sneakers. Instead of struggling with imagining what to combine with and how to\napply the selective projection, we can simply give a prompt like “a photo of sneakers, creative design.”\nand give a set of reference images with a target concept and appearance, e.g., a sport car image for\n“sleek” or any knitted items for “warm” and “cozy” (e.g., Fig. 1). We may also apply the same style\nto the multiple objects (e.g., bicycle and car) or add multiple visual concepts to the generated results.\nEven random reference images can be used to provide a serendipitous inspiration.\nThe question is how to perform selective projection in diffusion models, which must be done to\nachieve cross-modal conceptual blending. We think the key is the attention module [Vaswani et al.,\n2\n\n2017] (which is one of the most crucial components of modern diffusion models [Rombach et al.,\n2022, Black Forest Labs, 2024]) because its mechanism, comparing similarity and selectively applying\nthe value, is conceptually close to the selective projection.\nEarlier work, such as IP-Adapter [Ye et al., 2023] and BLIP-Diffusion [Li et al., 2023] proposed\nencoder-based methods to incorporate a reference image into text-guided generation with additional\ntraining. Although they show decent performance in blending visual and textual concepts with a\nfast inference time, their methods are limited in 1) disentangling textual and visual conditions and\n2) preserving the detailed visual concept of the reference image due to the dependencies on the text\ncross-attention module and an external image encoder.\nMeanwhile, RIV AL [Zhang et al., 2023a] and StyleAligned [Hertz et al., 2024] have shown the\npotential of the pretrained self-attention module of the T2I diffusion models in blending cross-modal\nconcepts. Although they showed impressive performance in disentangling cross-modal concepts\nand applying detailed visual concepts from their own denoising chain to another, their performance\nis limited when a real reference image is conditioned due to the distribution shift of the inversion\nchain [Zhang et al., 2023a]. They also have a slower inference time than the encoder-based methods.\nFilling the gap between both baseline approaches, we propose a novel image adapter “Image-and-Text\nConcept Blender” (IT-Blender) that can imagine for us by blending cross-modal concepts with fast\ninference time. IT-Blender learns to blend visual concepts from a real image without loss of details,\nin a disentangled manner from the textual concept (i.e., text determines semantics while a reference\nimage determines visual concepts such as texture, material, color, and local shape).\nBriefly, instead of using an external image encoder, we leverage the denoising network as an image\nencoder to maintain the details of visual concepts. As opposed to recent related literature without an\nexternal image encoder [Wu et al., 2025, Tan et al., 2024], our proposed method does not have any\narchitectural dependency (i.e., applicable to both UNet-based [Rombach et al., 2022] and DiT-based\ndiffusion models [Black Forest Labs, 2024]). We design a novel Blended Attention on top of the\nself-attention module, where detailed visual concepts can be preserved, and textual concepts are\nphysically separated, encouraging disentanglement of textual and visual concepts. Blended Attention\nis trained to be specialized in finding a semantic correspondence between two latents; one from the\nreal reference image and the other from the generated image.\nOur baseline experiment results on disentanglement, concept preservation, and blending score (in\nAppendices) demonstrate that IT-Blender outperforms the baselines in cross-modal conceptual\nblending in both UNet-based (SD 1.5) and DiT-based (FLUX) architectures.\n2 Related Works\nIn this section, we introduce previous studies related to visual-and-textual conceptual blending, based\non diffusion models [Ho et al., 2020, Dhariwal and Nichol, 2021, Song et al.].\nApplications for spatially aligned control. Prior works [Zhang et al., 2023b, Mou et al., 2024,\nHertz et al., 2022, Tumanyan et al., 2023, Liu et al., 2024] have achieved impressive performance\nin spatially aligned control. However, their methods are mainly designed for local photo editing\ninstructed by text, which is not suitable for our conceptual blending task to augment creativity.\nApplications based on text cross-attention module related to conceptual blending. IP-Adapter [Ye\net al., 2023], BLIP-Diffusion [Li et al., 2023], and ELITE [Wei et al., 2023] are closely related to\nconceptual blending task. They proposed an adapter based on text cross-attention module to encode\nand incorporate reference image information into the text-guided image generation process. Even\nthough decently working for cross-modal conceptual blending, their methods are limited in two\naspects. First, the encoder-based methods often fail in disentangling visual and textual concepts. This\nis because a reference image relies on the text cross-attention module, potentially entangling the\ncross-modal information. Second, encoder-based methods are limited in blending the detailed visual\nconcepts because of a dependency on an external image encoder, where visual details can be lost.\nApplications of self-attention module related to conceptual blending. Self-attention module\nis shown to be effective in combining two spatial features. RIV AL [Zhang et al., 2023a] and\nStyleAligned [Hertz et al., 2024] proposed to modify the self-attention module to be a sort of cross-\nattention form. Starting from the noise corresponding to the real reference image through inversion\nmethods [Song et al., 2020, Mokady et al., 2023], they combine a denoising chain with an inversion\n3\n\nchain to blend the spatial features. Although they can blend cross-modal concepts without training,\ntheir methods are inherently limited when a real reference image is given, due to the distributional gap\nbetween the latents from the inversion chain and the denoising chain [Zhang et al., 2023a]. Similar\nideas are used in [Cao et al., 2023, Alaluf et al., 2024], but their methods are specifically designed for\nnon-rigid image editing or a blending of two visual concepts in a disentangled manner.\nTransformer-based applications related to conceptual blending. StyleDrop [Sohn et al., 2023]\nproposes to finetune LoRA [Hu et al., 2022] to blend cross-modal concepts. However, their method\nis limited in scalability, as a separate set of LoRA modules needs to be optimized for each visual\nconcept. Recently, UNO [Wu et al., 2025], OminiControl [Tan et al., 2024], and IC-LoRA [Huang\net al., 2024] have shown impressive performance in subject-driven image generation by leveraging\ndiffusion transformers as image encoder. However, their sequentially concatenating methods are\nonly applicable to Diffusion Transformers (e.g., FLUX). Moreover, their methods are not suitable for\nconceptual blending because of the strong subject preservation.\nGenerative models augmenting human creativity. Previous studies [Franceschelli and Musolesi,\n2024, Hwang, 2022] have shown the potential of generative models for augmenting creativity. Cai\net al. [2023] proposed a diffusion framework to diversify image generations to provide inspiration for\ndesigners. CreativeConnect [Choi et al., 2024] proposed generative AI pipelines that can help graphic\ndesigners to have more design ideas by reference recombination process. Creative Blends [Sun et al.,\n2025] proposed a system that takes multiple textual concepts as input from users and outputs an\nimage with the blended concepts. The conducted user study shows that visualizing these blended\nconcepts can reduce cognitive load for participants and also foster creativity.\n3 Method\nIn this Section, we describe our proposed method (IT-Blender) that adapts the pretrained projection\nlayers of self-attention module to the visual and textual conceptual blending task. In Section 3.1, we\nfirst describe the preliminaries of the T2I diffusion models. In Section 3.2, we introduce IT-Blender\nwith a novel blended attention module that can blend the visual concept of a reference image into the\ntext-guided generation process with enhanced semantic correspondence retrieval for the real image.\n3.1 Preliminaries\nStableDiffusion and FLUX. StableDiffuison (SD) [Rombach et al., 2022] is widely used open source\ndiffusion models for T2I synthesis. SD is trained with a denoising objective [Ho et al., 2020], and\nUNet [Ronneberger et al., 2015] is used as denoising networks. FLUX [Black Forest Labs, 2024] is\nadvanced diffusion models based on diffusion transformers (DiT) [Peebles and Xie, 2023], which is\ntrained with a score matching objective. SD 1.5 and FLUX.1-dev are used in our experiment.\nSelf-Attention module and its application. Self-attention (SA) module [Zhang et al., 2019, Rombach\net al., 2022, Black Forest Labs, 2024] is one of the most important components of modern diffusion\nmodels. It not only learns to capture long-range dependencies, but also learns to encode spatial\nrepresentations optimized for similarity comparison; what to aggregate and what to ignore based on\nsemantic correspondence of the input itself. In our paper, the projection layers WQ,WK, andWVare\npretrained weights of SA module. For brevity, we omit the layer notation for the projection layers.\nAs mentioned earlier, Zhang et al. [2023a], Hertz et al. [2024] have shown that visual concept\nof a reference image can be blended in the generation process of pretrained T2I models without\nadditional training. The detailed methodologies differ, but conceptually they suggested image Cross\nAttention (imCA) between two latents; Znoisyfrom a denoising chain and Zinvfrom an inversion\nchain, i.e., imCA (Znoisy, Zinv) = imCA (Znoisy, Zinv;WQ, WK, WV).This indicates the key and\nvalue of the SA module from Zinvare combined with the query of the SA module from Znoisy, i.e.,\nσ\u0000\n(ZnoisyWQ)(ZinvWK)T/√dk\u0001\nZinvWV. Note that the operation of imCA is essentially a cross-\nattention mechanism, but used in a distinct way, i.e., cross-attention in SA layers with WQ,WK,WV.\n3.2 Image and Text Blender (IT-Blender)\nSetup and overview. We aim to generate an image where cross-modal concepts from a given real\nimage and a text prompt are naturally blended without loss of details, in a disentangled manner. As\nmentioned in Section 1, attention module can be a key to implement the conceptual blending process.\n4\n\nOne key observation for the inversion-based imCA approaches is that: they have the advantage\nin applying the details of the visual concepts in a disentangled manner, while the performance is\ndegraded when real images are given as input due to the distribution shift of the inversion chain.\nHence, our goal is to have a real image adapter that is trained to incorporate a given reference image\ninto the pretrained projection space of the SA module. Since textual concepts are constantly provided\nthrough the text CA modules (which are physically separated from the SA modules), IT-Blender aims\nto blend visual concepts from the reference image with the text-guided generation process.\nref stream 𝑡=0\tnoisy stream 𝑡=𝑡\t𝑍!\"#(ℓ)𝑍'()*+(ℓ)𝑍'()*+(ℓ,-)𝑍!\"#(ℓ,-)𝐵𝐴\n𝑆𝐴(b) IT-Blender overview 𝑍!\"#(ℓ)𝑍'()*+(ℓ)𝑍'()*+(ℓ,-)𝑍!\"#(ℓ,-)𝑖𝑚𝐶𝐴𝑆𝐴(a) Naïve imCA-based approachref stream 𝑡=0\tnoisy stream 𝑡=𝑡\t\nFigure 2: IT-Blender overviewOur method only trains the newly introduced\nadapter parameters while freezing all the pre-\ntrained weights, similar to prior works [Mou\net al., 2024, Zhang et al., 2023b, Ye et al., 2023,\nTan et al., 2024, Wu et al., 2025]. The denois-\ning objective is used for SD1.5 [Rombach et al.,\n2022] and the denoising score matching objec-\ntive is used for FLUX [Black Forest Labs, 2024].\nThe challenges are 1) how to encode a real image\nwithout loss of details, and 2) how to blend the\nencoded real image feature into the projection\nspace of the pretrained SA module.\nNative image encoding. Interestingly, diffusion models already know how to encode a real image\nXrefinto the denoising networks. It can be simply achieved by forwarding a clean version of Xref\nwitht= 0. This provides a sequence of latent representations across the Llayers of the denoising\nnetworks: (Z(1)\nref, Z(2)\nref, ..., Z(L)\nref). This representation has some similarities to the inversion methods\nin which there is a latent representation at every layer of the network for each denoising step. However,\nit is fundamentally different because it’s timestep is set to 0 for all denoising steps, i.e., the clean\nlatent representations can be used at every timestep. We hypothesize that these clean representations\nare more helpful for conceptual blending because they encode the details of the clean image rather\nthan noisy images as in inversion-based methods. Furthermore, our approach does not require image\ninversion, which is computationally expensive.\nDespite the benefits of this clean representation, it is unclear how to incorporate a set of clean latent\nfeatures per layer from the denoising networks into the regular denoising process. One simple naïve\napproach inspired by prior works is to simply use an imCA module to blend the clean reference\nlatent Zrefinto the noisy latent Znoisy, i.e., replace SA(Znoisy)modules with imCA (Znoisy, Zref), as\nshown in Fig. 2 (a). While in theory this could be done without retraining by using the pretrained\nself-attention module weights as in Hertz et al. [2024], Zhang et al. [2023a], the performance would\nbe poor because of a significant distribution shift; the reference latents are from a clean image with\nt= 0 while the noisy latents are from noisy images with a t≥0. Fig. 8 (a) shows the empirical\nverification of the hypothesis. Thus, a new blending module and finetuning method is needed that can\nuse the clean latents but seamlessly blend the visual concept information into the noisy latents.\n𝑊!𝑊\"𝑊#𝑊!𝑊\"$𝑊#$𝛼\n𝑍!\"#$%(ℓ)𝑍)*+(ℓ)+𝑀!\"𝑊!𝑊\"𝑊#Self-Attention𝑀!\"Self-AttentionImage Cross-Attention𝑍!\"#$%(ℓ,-)\nFigure 3: Blended attention at ℓ-th layer.IT-Blender. To bridge the gap, we design IT-Blender\nto have our novel blended attention (BA) module with\ntrainable parameters that can learn how to map the clean\nZrefto the Znoisyin the projection space.\nAs shown in Fig. 2 (b), IT-Blender has two streams; noisy\nstream and reference stream. The noisy stream refers to\nthe regular denoising chain from t=Ttot= 0 during\nsampling or randomly sampled tduring training. The\nreference stream is for encoding a reference image without\nany noise. Along this stream, t= 0is constantly given for\nboth training and sampling. The same text prompt is used\nfor both streams. The training objective is applied only to\nthe noisy stream.\nBlended Attention (BA). As shown in Fig. 3, we design\nblended attention to have a residual structure with two\nterms; the first term on the left is the original pretrained self-attention module, which can keep the\n5\n\nestimation on the original trajectory. The second imCA term on the right is the key to blended\nattention, which enables a blending of visual and textual concepts by bridging the clean reference\nstream with the noisy stream in the output space of the SA module. The ℓ-th self-attention layers of\nthe denoising networks are changed to our blended attention as shown in the equation below:\nBA=SA(Z(ℓ)\nnoisy) +αimCA (Z(ℓ)\nnoisy,SA(Z(ℓ)\nref);WQ, WK′, WV′), (1)\nwhere WK′andWV′are trainable parameters. The layer notation for the projection layers is omitted\nfor the brevity purpose. αis set to be 1 during the training while set to be a constant <1during\nsampling. In our experiments, we empirically used α= 0.25for SD and α= 0.6for FLUX (the\nvisualization of varying αs is shown in Fig. 17). For training, both WK′andWV′are randomly\ninitialized. The imCA term in Eq. 1 plays a role in dynamically aligning SA(Z(ℓ)\nref)toSA(Z(ℓ)\nnoisy)in\nthe output space of SA by optimizing WK′andWV′to fetch the useful visual information to denoise\nfrom the reference stream, driven by the query from the noisy stream.\n4 Experiments\nDetailed experiment settings and implementation details are provided in Section A and B.\nData. For training and testing SD 1.5 and FLUX, we used a squared subset of LAION2B-en-aesthetic\ndataset [OpenDiffusionAI, 2025, Schuhmann et al., 2022], which contains around 300k squared\nimages (with at least a resolution of 1,024 ×1,024) and their paired text prompt.\nMetrics for baseline comparison. We mainly evaluate how well the textual and visual concepts\nare disentangled. In our cross-modal blending task, semantics (i.e., object) must be determined by a\ntext prompt, and visual concepts (e.g., texture, ingredient, material, color, and local shapes) need to\nbe determined by a reference image. If visual and textual concepts are disentangled well, each of\nthem should maintain high consistency after being blended with different combinations. Therefore,\nthe key to the evaluation is to measure set consistencies for visual concept and the textual concept,\nrespectively. To measure the textual set consistency, we compare a set of generated samples with a\nfixed text prompt but with different reference images. The generated object must be consistent, and\nthus we used CLIP [Radford et al., 2021] to measure the semantic similarity between all pairs of\nthe generated images with a fixed prompt. To measure the visual set consistency, we compare the\ngenerated samples with a fixed visual prompt but with different text prompts. DINO is used to focus\nmore on pure visual similarity, not semantics, following previous studies [Ruiz et al., 2023, Hertz\net al., 2024]. Next, we measure the correct class predictions to measure whether the generated results\npreserve the textual concept. ChatGPT4.1 [OpenAI, 2023] is used. For SD evaluation, 200 unseen\nsamples with 30 text prompts are used (6,000 samples per baseline in total). For FLUX evaluation,\n200 unseen samples with 20 text prompts are used (4,000 samples per baseline in total). We also\nreport the blending score and analysis in Section D.1.\n4.1 Baseline Comparison (SD)\nBaselines. To compare the performance of cross-modal conceptual blending in SD, we use two\nencoder-based methods (BLIP-Diffusion [Li et al., 2023] and IP-Adapter [Ye et al., 2023]) and two\ninversion-based methods (RIV AL [Zhang et al., 2023a] and StyleAligned [Hertz et al., 2024]). We\nused SD 1.5 for all the baselines while SDXL [Podell et al., 2023] is used in StyleAligned [Hertz\net al., 2024] as their performance in SD 1.5 is worse by a large margin.\nResults. Both encoder-based baselines (IP-Adapter and BLIP-Diffusion) show similar patterns.\nFirst, the visual concept frequently dominates the generation process, and thus the generated images\nsometimes do not look like the object given as a text prompt (e.g., the flower train of IP-Adapter and\nthe robot of BLIP-Diffusion in Fig. 5). The same pattern is observed in quantitative evaluations. The\nencoder-based baselines show the lowest visual and textual set consistencies (Fig. 4 top). This is\nbecause they frequently miss the textual concept, yielding inconsistency of the textual and visual sets.\nSimilarly, the classification results in Fig. 4 bottom show that IP-Adapter and BLIP-Diffusion often\nmiss the target object; out of 200 samples, on average over the prompts, only around 100 samples are\nclassified as a target object. We believe this is because their methods rely on the text CA module,\nwhich can inherently limit the disentanglement of visual and textual concepts.\n6\n\ntrain\nheelsmotor-cyclerobot\nalienhouseIT-BlenderIP-AdapterStyleAlignedRIVALBLIPDiffusionIT-BlenderRIVAL\nStyleAlignedTextual set examplesVisual set examplesFigure 5: Qualitative comparisons with the baselines in StableDiffusion. For each column of the\ntextual set examples, every two row with the same text prompt need to be semantically consistent.\nEach column of the visual set examples need to be visually consistent.\nSecond, when the textual concept is properly applied, the generated results from IP-Adapter and\nBLIP-Diffusion often lose the details of the visual concept (e.g., the strawberry heels of IP-Adapter\nand the motorcycle of BLIP-Diffusion in Fig. 5). Additional DINO similarity experiments between a\ngenerated image and a reference image (IT-Blender ( 0.837 ), IP-Adapter (0.812), and BLIP-Diffusion\n(0.821)) support the observations. This is because IT-Blender does not rely on an external image\nencoder, while natively encodes images with the denoising networks, retaining visual details better.\nAs for inversion-based baselines, StyleAligned frequently misses the textual concept, as shown in\nthe motorcycle and house examples in Fig. 5. The lowest classification score in Fig. 4 bottom also\nquantiatively supports the observation. RIV AL shows worse performance than IT-Blender in both\ntextual and visual set consistencies. This is because their inversion-based method is not specialized in\nretrieving semantic correspondence between the reference and the generated images, and thus the\nvisual concepts are inconsistently applied to the generated images given varying inputs.\nFigure 4: Visualizations of the\nquantitative comparison with the\nSD 1.5 baselines.IT-Blender shows good performance in blending visual and\ntextual concepts in a disentangled manner, as shown in the\nsecond-best visual set consistency and the best textual set con-\nsistency. The highest class prediction also supports the strong\nperformance of IT-Blender in rigidly applying textual concepts.\nThe superior disentanglement performance of IT-Blender is\nattributed to 1) self-attention-based design, which separates the\nvisual and textual concepts, and 2) strong semantic correspon-\ndence retrieval by blended attention, with which the given visual\nconcepts can be consistently applied given varying inputs.\nAdditional baseline comparisons are provided in Section D,\ne.g., “blending score” by ChatGPT and occasional unrealistic\ngenerations of inversion-based baselines in SD.\n4.2 Baseline Comparison (FLUX)\nBaselines. To compare the cross-modal conceptual blending\nperformance in FLUX, we used three open-source baselines;\nIP-Adapter [Ye et al., 2023], OminiControl [Tan et al., 2024],\nand UNO [Wu et al., 2025]. For IP-Adapter, among two popular\nopen source implementations, we used InstantX implementa-\ntion [Team, 2024] as it is much better in blending visual and\ntextual concepts. Both OminiControl and UNO are designed for subject-driven image generation by\ntraining additional lora modules on top of the pretrained FLUX. The experiment results of IP-Adapter\nand UNO are based on FLUX.1-dev while OminiControl is based on FLUX.1-schnell.\n7\n\nDRAGON\nbackpackt-shirt\nkitchenIT-BlenderIP-AdapterOminiControlUNOIT-Blender\nIP-AdapterUNO\ncastle\nVisual set examplesTextual set examplesFigure 7: Qualitative comparisons with the baselines in FLUX.\nResults. As UNO and OminiControl are specifically trained for subject-driven image generation with\npaired data, their models are not suitable for blending visual and textual concepts, especially when\ngiven visual and textual conditions are not highly correlated. As can be seen in Fig. 7, UNO and\nOminiControl show strong reference preservation, as shown in the basket-printed t-shirts. However,\nOminiControl often fails in incorporating the visual concept from the reference image (e.g., the\nbackpack and kitchen examples), while UNO often fails in incorporating the textual concept (e.g., the\ncastle and kitchen examples). IP-Adapter decently blends the visual and textual concepts, but they\nmiss the details of the visual concepts (e.g., the dragons in the second and the fourth rows).\nFigure 6: Visualizations of the\nquantitative comparison with the\nFLUX baselines.We also observe the similar patterns in the quantitative ex-\nperiment results. OminiControl shows strong text guidance\neffect (e.g., the highest textual set consistency and classifica-\ntion in Fig. 6) while inconsistent reference image effect (e.g.,\nthe lowest visual set consistency). UNO shows relatively ro-\nbust performance in preserving the given object in our task,\nas shown by the high visual set consistency score. However,\nthe given text prompt is often ignored, which is shown by the\nlowest textual set consistency. IP-Adapter demonstrates lower\nvisual and textual set consistencies compared to ours, simialr\nto the SD experiment results. Compared to the baselines, IT-\nBlender shows the second-best textual set consistency and the\nbest visual set consistency, showing superior performance in\ncross-modal conceptual blending.\n4.3 Ablation Study and New Applications\nIn this section, we present interesting applications and visualize\nthe attention mask to better understand what IT-Blender learns.\nMore results are provided in the Appendices (e.g., applying\nmultiple visual concepts in Section C and more interesting\nresults in Section E).\nEffects of the blended attention module. To intuitively under-\nstand what blended attention learns, we visualize the self-attention mask of BA modules in FLUX.\nFig. 8 (a) shows the results. The attention masks of IT-Blender captures the visually corresponding\ntexture area from the reference image. For example, the yellow star from the whale example mostly\ncaptures the fur area of the bird in the reference image, while the pink star mostly captures the feather\narea. However, the attention mask of the naive imCA-based approach (Fig. 2 (a)), does not capture\na meaningful area, and thus the generated results are also significantly degraded. This verifies our\nhypothesis that the distribution shift between clean ZrefandZnoisyis significant, and therefore training\nWK′andWV′of blended attention is needed to bridge ZrefandZnoisy.\n8\n\n, lizard\n.. in the street.. in the beach wearing sunglasses and a hat.. in the garden… on a floating board in the lake\nUNOIT-Blender\nwhaletotoroIT-BlenderNaïveimCA(a)(b)Figure 8: (a) attention mask visualization of IT-Blender and naïve imCA (Fig. 2 (a)). (b) our blended\nresults can be applied to subject-driven generative models to create interesting novel scenes.\nliving roomkitchenbedroombathroom\ncabinetrocking chairdesklamp\nblousedresspantsheelsblazerpantsblousedress\nFigure 9: Feasible design examples when the given visual and textual concepts are semantically close.\nFesible design. As shown in the owls with diverse desserts in Fig. 1, IT-Blender can create experimen-\ntal design in a realistic way, which can inspire humans. Interestingly, we also observe that IT-Blender\ncan generate feasible design outcomes as well, especially when a reference image is semantically\nclose to the object given in the text prompt. For example, as shown in Fig. 9, given an indoor or\noutdoor reference image, IT-Blender can generate the target room with surprisingly coherent visual\nconcepts with the given reference image. Furniture or apparel could be another example.\nAdditional results. Given a fixed visual and textual concepts, IT-Blender can generate diverse images\nwith varying random noise, as shown in Fig. 10. The creative object generated by IT-Blender can be\nsynthesized in novel scenes with subject-driven models, as shown in Fig. 8 (b).\nkitchenmonstersneakers\nFigure 10: The results are generated with varying noise.\n5 Conclusion\nIn this paper, we propose IT-Blender that can augment human creativity by automating the cross-\nmodal conceptual blending process of a real image and text. First, IT-Blender uses native denoising\nnetworks to encode a real reference image to minimize the loss of visual details, with fast inference\ntime. Second, the encoded visual feature is fed into our novel blended attention modules, which are\ntrained to bridge the distribution shift between the clean reference image and the noised generated\nimage. Third, our blended attention modules are built upon the self-attention module, which can\ndisentangle the textual concept and the visual concept by design. In both SD and FLUX, the\nexperiment results demonstrate that IT-Blender outperforms the baselines in blending cross-modal\nconcepts in terms of disentangling cross-modal concepts and preserving textual and visual concepts.\nThe blending score further verifies the superior performance of IT-Blender in cross-modal conceptual\nblending. Further discussion of future directions, limitations, and societal impact is provided in\nSection F. We hope that our research will be able to draw attention to the potential of image-generative\nmodels to augment human creativity.\n9\n\nReferences\nGilles Fauconnier and Mark Turner. The way we think: Conceptual blending and the mind’s hidden complexities .\nBasic books, 2008.\nGilles Fauconnier and Mark Turner. Conceptual integration networks. Cognitive science , 22(2):133–187, 1998.\nSeana Coulson. Semantic leaps: Frame-shifting and conceptual blending in meaning construction . Cambridge\nUniversity Press, 2001.\nLiane Gabora. Cognitive mechanisms underlying the creative process. In Proceedings of the 4th conference on\nCreativity & cognition , pages 126–133, 2002.\nMaria C Yang. Observations on concept generation and sketching in engineering design. Research in Engineering\nDesign , 20:1–11, 2009.\nKyung Hoon Hyun and Ji-Hyun Lee. Balancing homogeneity and heterogeneity in design exploration by\nsynthesizing novel design alternatives based on genetic algorithm and strategic styling decision. Advanced\nEngineering Informatics , 38:113–128, 2018.\nAlice Cai, Steven R Rick, Jennifer L Heyman, Yanxia Zhang, Alexandre Filipowicz, Matthew Hong, Matt\nKlenk, and Thomas Malone. Designaid: Using generative ai and semantic diversity for design inspiration. In\nProceedings of The ACM Collective Intelligence Conference , pages 1–11, 2023.\nKevin N Otto. Product design: techniques in reverse engineering and new product development . 2003.\nBob Eberle. Scamper on: Games for imagination development . Prufrock Press Inc., 1996.\nKarl T Ulrich and Steven D Eppinger. Product design and development . McGraw-hill, 2016.\nDavid G Jansson and Steven M Smith. Design fixation. Design studies , 12(1):3–11, 1991.\nAbraham S Luchins. Mechanization in problem solving: The effect of einstellung. Psychological monographs ,\n54(6):i, 1942.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 10684–10695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. Advances in neural information processing systems , 35:36479–\n36494, 2022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. Advances in neural information processing systems , 30, 2017.\nBlack Forest Labs. Flux.1 [dev]. https://huggingface.co/black-forest-labs/FLUX.1-dev , 2024.\nAccessed: 2025-04-27.\nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for\ntext-to-image diffusion models. arXiv preprint arXiv:2308.06721 , 2023.\nDongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable\ntext-to-image generation and editing. Advances in Neural Information Processing Systems , 36:30146–30166,\n2023.\nYuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation by aligning diffusion inversion\nchain. Advances in Neural Information Processing Systems , 36:30641–30661, 2023a.\nAmir Hertz, Andrey V oynov, Shlomi Fruchter, and Daniel Cohen-Or. Style aligned image generation via shared\nattention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages\n4775–4785, 2024.\nShaojin Wu, Mengqi Huang, Wenxu Wu, Yufeng Cheng, Fei Ding, and Qian He. Less-to-more generalization:\nUnlocking more controllability by in-context generation. arXiv preprint arXiv:2504.02160 , 2025.\nZhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and\nuniversal control for diffusion transformer. arXiv preprint arXiv:2411.15098 , 2024.\n10\n\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural\ninformation processing systems , 33:6840–6851, 2020.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural\ninformation processing systems , 34:8780–8794, 2021.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In International Conference on\nLearning Representations .\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\nInProceedings of the IEEE/CVF international conference on computer vision , pages 3836–3847, 2023b.\nChong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter:\nLearning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of\nthe AAAI conference on artificial intelligence , volume 38, pages 4296–4304, 2024.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt\nimage editing with cross attention control. arXiv preprint arXiv:2208.01626 , 2022.\nNarek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition , pages 1921–1930, 2023.\nBingyan Liu, Chengyu Wang, Tingfeng Cao, Kui Jia, and Jun Huang. Towards understanding cross and self-\nattention in stable diffusion for text-guided image editing. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition , pages 7817–7826, 2024.\nYuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encoding visual\nconcepts into textual embeddings for customized text-to-image generation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , pages 15943–15953, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 , 2020.\nRon Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 6038–6047, 2023.\nMingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free\nmutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 22560–22570, 2023.\nYuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Cross-image attention\nfor zero-shot appearance transfer. In ACM SIGGRAPH 2024 Conference Papers , pages 1–12, 2024.\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,\nLu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint\narXiv:2306.00983 , 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu\nChen, et al. Lora: Low-rank adaptation of large language models. ICLR , 1(2):3, 2022.\nLianghua Huang, Wei Wang, Zhi-Fan Wu, Yupeng Shi, Huanzhang Dou, Chen Liang, Yutong Feng, Yu Liu, and\nJingren Zhou. In-context lora for diffusion transformers. arXiv preprint arXiv:2410.23775 , 2024.\nGiorgio Franceschelli and Mirco Musolesi. Creativity and machine learning: A survey. ACM Computing Surveys ,\n56(11):1–41, 2024.\nAngel Hsing-Chi Hwang. Too late to be creative? ai-empowered tools in creative processes. In CHI conference\non human factors in computing systems extended abstracts , pages 1–9, 2022.\nDaEun Choi, Sumin Hong, Jeongeon Park, John Joon Young Chung, and Juho Kim. Creativeconnect: Supporting\nreference recombination for graphic design ideation with generative ai. In Proceedings of the 2024 CHI\nConference on Human Factors in Computing Systems , pages 1–25, 2024.\nZhida Sun, Zhenyao Zhang, Yue Zhang, Min Lu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Creative\nblends of visual concepts. In CHI, 2025.\n11\n\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In Medical image computing and computer-assisted intervention–MICCAI 2015: 18th interna-\ntional conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18 , pages 234–241. Springer,\n2015.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF\ninternational conference on computer vision , pages 4195–4205, 2023.\nHan Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial\nnetworks. In International conference on machine learning , pages 7354–7363. PMLR, 2019.\nOpenDiffusionAI. laion2b-en-aesthetic-square. https://huggingface.co/datasets/opendiffusionai/\nlaion2b-en-aesthetic-square , 2025.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset\nfor training next generation image-text models. Advances in neural information processing systems , 35:\n25278–25294, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning , pages 8748–8763. PmLR, 2021.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth:\nFine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , pages 22500–22510, 2023.\nOpenAI. Gpt-4 technical report, 2023. URL https://arxiv.org/abs/2303.08774 . arXiv:2303.08774.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint\narXiv:2307.01952 , 2023.\nInstantX Team. Instantx flux.1-dev ip-adapter page. https://huggingface.co/InstantX/FLUX.\n1-dev-IP-Adapter , 2024.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101 ,\n2017.\nDongmin Park, Sebin Kim, Taehong Moon, Minkyu Kim, Kangwook Lee, and Jaewoong Cho. Rare-to-frequent:\nUnlocking compositional generation power of diffusion models on rare concepts with llm guidance. arXiv\npreprint arXiv:2410.22376 , 2024.\n12\n\nAppendices\nContents of Appendices\nFigure 11: Stylized brand logos by IT-Blender with FLUX.\n• Appendix A: Experiment Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n• Appendix B: Implementation Details\n• Appendix C: Multiple Visual Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n• Appendix D: Additional Baseline Comparisons\n–D.1: Comparison of Blending Score by ChatGPT (SD and FLUX) . . . . . . . . . . . . . . 16\n–D.2: Qualitative Observation Report (SD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n(A limitation of training-free inversion-based method)\n• Appendix E: Additional Results and Analysis\n–E.1: Effect of αof Blended Attention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n–E.2: Softmax Temperature Control (heuristic for multiple reference images)\n–E.3: Additional Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n• Appendix F: Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n–F.1: Interesting Future Directions\n–F.2: Limitations\n–F.3: Societal Impact\n13\n\nA Experiment Settings\nSD setting. To evaluate the performance with the baselines in SD, we sample 200 samples per prompt.\nThe 30 prompts that we used are as follows:\ncar, bus, bicycle, chair, truck, tank, lamp, handbag, backpack, heels,\ntrain, rabbit cartoon character, owl cartoon character, mouse cartoon\ncharacter, castle, headphone, motorcycle, kettle, vacuum, toy airplane,\nrobot, sneakers, dragon cartoon character, reindeer cartoon character,\nalien cartoon character, living room, bathroom, bedroom, kitchen, house\nFLUX setting. We sample 200 samples per prompt. The 20 prompts that we used are as follows:\ncar, bicycle, chair, lamp, headphone, truck, sneakers, handbag, backpack,\nt-shir\", lizard, fish, owl cartoon character, monster cartoon character,\ndragon, living room, kitchen, castle, 3D apple logo, 3D toyota logo\nB Implementation Details\nTo train IT-Blender with SD 1.5, we use 1 NVIDIA RTX 6000 with a batch size of 16. To train\nIT-Blender with FLUX, we use 4 NVIDIA L40S GPUs with a total batch size of 16. IT-Blender\ntraining and sampling require two streams, as shown in Fig. 2. We simply concatenate them in the\nbatch dimension so that the key-value injections from the reference stream can be easily achieved in\neach Blended Attention processor.\nWe train IT-Blender for 5 epochs with a learning rate of 1e-5 in SD 1.5. We train IT-Blender for 1-2\nepochs with a learning rate of 2e-5 in FLUX. AdamW [Loshchilov and Hutter, 2017] is used in both\nsettings with betas = [0.9, 0.99] andweight_decay = 0.01 .\n14\n\nC Multiple Visual Concepts\nIT-Blender can apply multiple visual concepts from multiple reference images.\nThe naive way is to add additional imCA terms in Eq. 1, e.g.,\nBA=SA(Z(ℓ)\nnoisy) +αimCA (Z(ℓ)\nnoisy,SA(Z(ℓ)\nref1);WQ, WK′, WV′) (2)\n+αimCA (Z(ℓ)\nnoisy,SA(Z(ℓ)\nref2);WQ, WK′, WV′),\nwhere Zref1andZref2mean two reference images. However, we empirically observe that the results\nnaively mingles the visual features for each query coordinate, which makes the generated image less\nconspicuous where the visual feature comes from.\nTo tackle this problem, we came up with a simple idea; concatenating the multiple reference images\nin sequence dimension before applying softmax of Attention, e.g., Q∈RHW×Dand{K, V} ∈\nR2HW×D, when two reference images are used. In this way, BA module can exclusively (not strictly\nthough as it is softmax, not hardmax) fetch the visual features from the multiple reference images. We\nused this approach to blend multiple visual concepts. More examples are provided below in Fig. 12.\nAnother possible way would be to concatenate multiple reference images in height or weight dimen-\nsion of the reference image, similar to Huang et al. [2024].\nRef1Ref2IT-BlenderRef1Ref2IT-BlenderRef1Ref2IT-Blender“A photo of sneakers, imaginative, creative, design”\nFigure 12: Examples by IT-Blender with FLUX, generated with multiple reference images.\n15\n\nD Additional Baseline Comparisons\nD.1 Comparison of Blending Score by ChatGPT (SD and FLUX)\nTo further measure the blending performance, we use ChatGPT 4.1 [OpenAI, 2023] with a detailed\nrubric, inspired by the high correlation between human and state-of-the-art LLMs in measuring text\nand image alignment [Park et al., 2024].\nTo evaluate, the same samples with the main experiments are used, i.e., the 6000 samples in SD and\n4000 samples in FLUX. The results are as shown below:\nFigure 13: Visualizations of the blending score comparisons with the baselines in SD (left) and FLUX\n(right).\nFig. 13 shows the blending score measure by ChatGPT, given a specific rubric. As shown in the\nSD-based and Flux-based results, IT-Blender shows the rigid and best performance with the highest\nmean and lowest variance.\nAccording to the rubric, the highest mean around 8 indicates that our blending results have most\nelements from both inputs, and they are well integrated.\nThe low variance of IT-Blender indicates that both concepts are consistently blended in a plausible\nway, without failed or unbalanced integration.\nWe further visualize the top 10%, 50% (median), and 90% samples in terms of the blending score\nin Fig. 14 and Fig. 15. The high blending scores around 8-9 show decent performance in blending\nvisual and textual concepts while the low blending scores around 1-2 show poor performance, e.g.,\nonly applying one concept or blending cross-modal concepts weakly.\n16\n\nSD15 Top 10 % samples in terms of blending score\n“A photo of a rabbit cartoon character”SD15 Top 90 % samples in terms of blending scoreSD15 Top 50 % samples in terms of blending score (median)RefRefRefRefRefIT-BlenderIP-AdapterBLIPDiffRIVALStyleAligned(9)(8)(8)(8)(8)\nRefRefRefRefRefIT-BlenderIP-AdapterBLIPDiffRIVALStyleAligned(8)(6)(6)(6)(6)\nRefRefRefRefRefIT-BlenderIP-AdapterBLIPDiffRIVALStyleAligned(6)(1)(1-2)(2)(2)Figure 14: Visualization of top 10%, 50%, and 90% samples in terms of blending score (SD). The\nnumbers below each baseline name indicate the blending scores the displayed samples got.\n17\n\nRefRefRefRefIT-BlenderIP-AdapterUNOOminiCtrlFLUX Top 10 % samples in terms of blending score\n“A photo of sneakers, imaginative, creative, design”(9)(8-9)(8)(8)\nRefRefRefRefIT-BlenderIP-AdapterUNOOminiCtrlFLUX Top 90 % samples in terms of blending score(7)(1)(1)(1)\nRefRefRefRefIT-BlenderIP-AdapterUNOOminiCtrlFLUX Top 50 % samples in terms of blending score (median)(8)(6-7)(4)(4-5)\nFigure 15: Visualization of top 10%, 50%, and 90% samples in terms of blending score (FLUX). The\nnumbers below each baseline name indicate the blending scores the displayed samples got.\n18\n\nQuery for measuring blending score. The prompt we used to measure the blending score is as\nfollows:\nYou are a helpful assistant who evaluates how well textual and visual\nconcepts are blended in the image generation process. The object in the\ngiven first image is conceptually blended result given the text prompt and\nthe second reference image. Text determines semantics while the reference\nimage determines visual concepts such as texture, material, color, and\nlocal shape. Evaluate how closely the visual concept in the provided image\naligns with the textual concept in the text prompt and the visual concept\nfrom the second image. Identify significant overlaps or discrepancies in\nterms of global object shape, local shape, appearance, texture, material,\ncolor, and all the detailed visual components. Analyze the conceptual\nsimilarity between the first provided generated image and the text prompt:\n[PROMPT]. You also need to consider the conceptual similarity between\nthe first provided generated image and the second provided reference\nimage. Provide a concise explanation for your evaluation. Note that we\nare evaluating cross modal conceptual blending, and thus if one of the\ncrossmodal concepts does not present in the generated image, it has to be\nconsidered as failed, even though the first image perfectly matches the\nsecond image.\nFirst image: [GEN_IMAGE]\nSecond image: [REF_IMAGE]\nThe object in the given first image is conceptually blended result given\nthe text prompt and the second image. Evaluate how closely the visual\nconcept in the provided image aligns with the textual concept in the text\nprompt and the visual concept from the second image. Identify significant\noverlaps or discrepancies in terms of shape, appearance, composition, and\noverall impression. Provide a concise explanation for your evaluation.\nGive a score from 1 to 10, according to the following criteria:\n10 Perfect conceptual integration: The generated image seamlessly\nincorporates all core semantic and stylistic elements from both the text\nand the visual concept. There’s no ambiguity in the fusion; it reflects a\ndeep, coherent synthesis of the two modalities.\n9 Near-perfect integration: Strong conceptual blending with only extremely\nminor details or subtleties missing from either modality. The result is\nstill fully coherent and creatively unified.\n8 Excellent with minor trade-offs: Most elements from both inputs are\npresent and well-integrated, but one or two key aspects may be simplified.\nThe conceptual overlap is still meaningful.\n7 Very good blend, slightly unbalanced: Clear depiction of both concepts\nwith small discrepancies—e.g., one modality slightly dominates the fusion.\nStill communicates a unified concept.\n6 Mostly present, but noticeable gaps: Both modalities are represented,\nbut some important attributes (e.g. color, pose, key terms, or symbolic\nfeatures) are missing or only vaguely suggested.\n5 Moderate representation: Some elements from both text and image are\ndepicted, but several key parts are ignored or distorted. The blend may\nfeel partial or underdeveloped.\n4 Unbalanced or sparse blend: One modality is clearly underrepresented or\nthe blend feels superficial. Visuals may include token features from one\nsource without meaningful synthesis.\n19\n\n3 Weak conceptual integration: Few recognizable aspects from both text\nand image appear; blending feels incomplete or accidental rather than\nintentional.\n2 Minimal blending: Image mostly reflects one modality, with only token\nor confused reference to the other. Viewers may struggle to infer any\ndeliberate fusion.\n1 Failed integration: Generated image does not meaningfully reflect either\nthe textual concept or the visual input. No clear blending is achieved.\nProvide your score and explanation (within 20 words) in the following\nformat: ### SCORE: score ### EXPLANATION: explanation\nExplanations provided by ChatGPT for blending score. The explanations for the lowest row for\neach of top 10%, 50%, and 90% are as follows:\n• SD15\n1. IT-Blender\n(a)(Top 10%) “The rabbit cartoon’s form is clear and well-blended with the cookie’s\ntexture and color, though small stylization remains.”\n(b)(Top 50%) “The image blends a rabbit cartoon character with fabric and color from\nthe bows, but lacks full cartoon stylization.”\n(c)(Top 90%) “The rabbit shows garden background and chef attire (from the refer-\nence), but lacks strong cartoon character cues from text.”\n2. IP-Adapter\n(a)(Top 10%) “Rabbit cartoon is fully integrated; clothing shows reference outfit’s\ncolors and stripes, but lacks emblem and exact shape.”\n(b)(Top 50%) “The rabbit matches the text but is realistically rendered, not cartoon-\nlike; the garden environment strongly reflects the reference image.”\n(c)(Top 90%) “The generated image contains only the visual reference style, with no\ntrace of the \"rabbit cartoon character\" concept.”\n3. BLIP-Diffusion\n(a) (Top 10%) “”\n(b)(Top 50%) “The generated image uses the reference’s color and texture but misses\nkey elements of \"rabbit cartoon character\" in pose and style.”\n(c)(Top 90%) “The generated image reflects only the bread visual, lacking any \"rabbit\ncartoon character\" elements from the text prompt.”\n4. RIV AL\n(a)(Top 10%) “Strong rabbit cartoon character blending, large expressive eyes and\npose, but clothing details lack reference’s color and pattern accuracy.”\n(b)(Top 50%) “The generated image shows a real rabbit and kids with cartoonish face\npaint, but lacks full cartoon character integration.”\n(c)(Top 90%) “Image matches the rabbit cartoon text prompt, but shows almost no\nvisual or conceptual blending with the milk bottles.”\n5. StyleAligned\n(a)(Top 10%) “Strong blend; rabbit cartoon matches text while outfit, pose, and\nbackground clearly reflect the reference image. Minor stylization only.”\n(b)(Top 50%) “Rabbit forms and some cartoon stylization are present, but features are\nindistinct and visuals are muddled.”\n(c) (Top 90%) “”\n• FLUX\n1. IT-Blender\n(a)(Top 10%) “The sneaker perfectly incorporates the reference flowers’ color, mate-\nrial, and shape, with only minor textural differences from the reference.”\n(b)(Top 50%) “Sneakers integrate clock’s pink color, shiny texture, and green leaf\nelements, but clock face and apple shape are abstracted.”\n20\n\n(c)(Top 90%) “Strong sneaker form integrates basketball court elements and colors,\nbut local sneaker details and textures are somewhat abstracted.”\n2. IP-Adapter\n(a)(Top 10%) “The sneaker integrates floral elements—shape and details—from the\nbouquet while retaining clear sneaker form, with only minor detail loss.”\n(b)(Top 50%) “Sneakers (text) are clearly integrated into the villa pool scene (refer-\nence), but sneakers’ material/style don’t borrow villa textures.”\n(c)(Top 90%) “No sneakers are present; the image depicts buildings and cityscape,\nfailing both text and visual blending criteria.”\n3. UNO\n(a)(Top 10%) “Sneaker shape is clear and main structure matches \"sneakers\", but\ndonut texture dominates, slightly stylizing the footwear concept.”\n(b)(Top 50%) “Only the clothing from the reference is blended; no real sneaker shape\nfrom the text is present, making fusion superficial.”\n(c)(Top 90%) “The generated image only depicts a man with a yellow headscarf, not\nsneakers; it fails cross-modal blending.”\n4. OminiControl\n(a)(Top 10%) “The sneaker adopts the Buddha statue’s ivory color, material, and some\nsmooth texture, but lacks significant Buddha-specific shapes.”\n(b)(Top 50%) “The sneaker incorporates a doll’s head, referencing the baby, but lacks\ndeeper integration of baby features, mainly merging objects.”\n(c)(Top 90%) “The generated image is a sneaker, matching only the text prompt, with\nno visual or conceptual blending of the cake reference.”\n21\n\nD.2 Qualitative Observation Report (SD)\nwe observe that the training-free inversion-based baselines sometimes lie off the manifold, so the\nresults are not realistic when cross-modal concepts are blended. We think this is an inherent limitation\nof training-free methods (in exchange for the benefit of “training free”), which intervene in the\nsampling trajectory. As shown in Fig. 16, The training-free methods RIV AL and StyleAligned\nsometimes unrealistically blend the results. The encoder-based baselines IP-Adapter and BLIP-\nDiffusion often miss the text prompt while the generated results are realistic. IT-Blender combines\nthe benefits, consistently and realistically blending both concepts.\nreindeerIT-BlenderIP-AdapterBLIPDiffusionRIVALStyleAlignedIT-BlenderIP-AdapterBLIPDiffusionRIVALStyleAlignedtoyairplaneIT-BlenderIP-AdapterBLIPDiffusionRIVALStyleAlignedIT-BlenderIP-AdapterBLIPDiffusionRIVALStyleAligned\nFigure 16: Additional qualitative comparisons (SD).\n22\n\nE Additional Results and Analysis\nE.1 Effect of αof Blended Attention\nWe visualize the effect of αof Eq. 1 in Fig. 17. When α= 0, no effect is applied as the imCA term\nin Eq. 1 becomes zero out. From left to right, as αincreases, we can see that the visual concepts are\nmore blended into the generated image. We empirically found that α= 0.6is the best way to get the\nmost natural blend results. However, depending on the user’s intention, α∈[0.5,0.8]is also good to\ngo with. Especially when reference images and the text prompt are semantically close, α >0.6can\nbe effective, as shown in some of the results in section E.3.\n𝛼=0𝛼=0.2(Text only)𝛼=0.4𝛼=0.5𝛼=0.6𝛼=0.7𝛼=0.8𝛼=1.0\n“A photo of an angel, imaginative, creative, design”(default)Recommended range\nFigure 17: Visualization of the effect of alpha in blended attention with FLUX.\nE.2 Softmax Temperature Control (heuristic for multiple reference images)\nWe empirically observe that applying low temperature to the logits before applying softmax can\nsharpen the softmax distribution, possibly helping to prevent ambiguous mixtures of visual concepts\nin exchange for image fidelity. The attention formulation with the temperature can be represented as:\nAttention (Q, K, V ;temp ) =softmax\u0012QKT\n√dk·temp\u0013\nV. (3)\n1/temp = 1.0indicates the default attention mask while (1/temp )>1.0means the attention mask\nwith a sharpened distribution. As shown in the white boxes in Fig. 18, applying lower temperature\ncan make the generated results have more conspicuous visual concept. For example, it is vague to\ndetermine whether the owl’s eyes when 1/temp = 1 come from the first reference image or the\nsecond reference image. On the other hand, when 1/temp = 1.5, we can see that the cream texture\nof the first reference image is drawn more clearly in the generated images. We empirically observe\nthat setting 1<1/temp < 1.5can help mitigate ambiguous mixtures of visual concepts when using\nmultiple reference images. However, note that values of 1/temp > 1.0may degrade image fidelity.\nReference images“owl cartoon character”\nseed42seed521/𝑡𝑒𝑚𝑝=1.01/𝑡𝑒𝑚𝑝=1.51/𝑡𝑒𝑚𝑝=3.0(default)\nFigure 18: Visualization of the effect of temperature on the attention mask. Lower temperatures\nresult in less ambiguous and more conspicuous application of visual concepts in exchange for the\nimage fidelity. We empirically observe that 1<1/temp < 1.5can mitigate the ambiguity when\nmultiple reference images yield ambiguous mixtures of visual concepts.\n23\n\nE.3 Additional Results\nIn this section, we show additional feasible use cases of IT-Blender in diverse design fields. Reference\nimages and a text prompt are semantically close. More additional results with the original resolution\ncan be found on our project page: https://imagineforme.github.io/ .\nwomancartoon character Character Design (𝛼=0.6) \nReferenceReferencemonsterReferencemonster\nFigure 19: Feasible character design examples by IT-Blender with FLUX.\n24\n\nGraphic Design (α=0.7)womanReferenceeyelionbutterflyflowerhouse\n“A photo of long hair, imaginative, creative, design”Figure 20: Feasible graphic design examples by IT-Blender with FLUX.\n25\n\nFashion Design (α=0.6,0.8)\n“A photo of a handbag, imaginative, creative, design” (𝛼=0.8)Reference\n(𝛼=0.6)\nheels\nblousedresshandbagheelsribbon bowFigure 21: Feasible fashion design examples by IT-Blender with FLUX.\n26\n\nProduct Design (α=0.7,0.8)bicycleReference\nmotorcyclesuvsports cartruckvan(𝛼=0.7)(𝛼=0.8)(𝛼=0.8)(𝛼=0.8)(𝛼=0.8)(𝛼=0.8)\nFigure 22: Feasible product design examples by IT-Blender with FLUX.\n27\n\nInterior Design (α=0.7)Referencebarrestaurantkitchenbedroomliving room\nArchitectural Design (α=0.6)castlehouse\ntowerhouse\nFigure 23: Feasible interior and architectural design examples by IT-Blender with FLUX.\n28\n\nArt (α=0.8)boat floatingon the riverReferencebeachEiffel T owerfishlighthousetree\nFigure 24: Feasible art examples by IT-Blender with FLUX.\n29\n\nF Discussion\nF.1 Interesting Future Directions\nOur proposed blended attention module learns to be specialized in retrieving semantic correspondence\nbetween the real image and the generated image, and it combines the visual concept with the text-\nguided generated image in a plausible way. We believe this technique can be useful in other creativity\nfields as well, such as music, text and video. For example, suppose we have music generative models.\nGiven an arbitrary table tapping sound, the generated music would have the table tapping sound as\na central theme in a plausible way. In another case, suppose that we have text generative models.\nGiven a dialogue from a specific target person as input to the BA module, the generated text will be\npersonalized for that individual.\nF.2 Limitations\nEven though IT-Blender shows impressive performance in cross modal conceputal blending, there can\nbe several limitations. First, visual concept subtraction is not working well. It would be interesting if\nvisual concept subtraction could be achieved.\nSecond, the global shape variation of the generated objects is limited. In IT-Blender, the semantics of\nthe generated image are determined by a textual condition, and the visual concepts, such as color,\ntexture, local shape, and material, are determined by the reference image. As can be seen in our\nexperiments, the visual concepts can be applied with a large variation. However, the variation of the\nglobal shape (i.e., the object) is relatively limited, e.g., given “heels”, the results literally look like\n“heels”. We believe human designers can imagine global shape as well, which we think can be the\ngap with IT-Blender.\nThird, there is room for fully supporting human designers. The aesthetic (i.e., how it looks) is one of\nthe most important features of design, for which IT-Blender can significantly help human designers.\nHowever, a good human designer can consider many other features, such as functionality, usability,\ndurability, affordability, and cultural relevance, for which IT-Blender may not be helpful. Further\nexploration and research are needed for AI that can consider all the important features in design.\nF.3 Societal Impact\nPositive societal impact. IT-Blender can augment human creativity, especially for people in creative\nindustries, e.g., design and marketing. With IT-Blender, designers might be able to have better final\ndesign outcome by exploring wide design space in the ideation stage.\nNegative societal impact. As shown in Fig. 9 and Fig. 19-24, IT-Blender can be used to apply the\ndesign of an existing product to the new products. The user must be aware of the fact that they can\ninfringe on the company’s intellectual property if a specific texture pattern or material combination is\nregistered. We encourage users to use IT-Blender to augment creativity in the ideation stage, rather\nthan directly having a final design outcome.\n30\n\n",
    "source": "http://arxiv.org/abs/2506.24085v1",
    "authors": [
      "Wonwoong Cho",
      "Yanxia Zhang",
      "Yan-Ying Chen",
      "David I. Inouye"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "type": "content"
  },
  {
    "id": "2506.23225v1_abstract",
    "title": "Masked Gated Linear Unit",
    "content": "Title: Masked Gated Linear Unit\n\nAbstract: Gated Linear Units (GLUs) have become essential components in the feed-forward networks of state-of-the-art Large Language Models (LLMs). However, they require twice as many memory reads compared to feed-forward layers without gating, due to the use of separate weight matrices for the gate and value streams. To address this bottleneck, we introduce Masked Gated Linear Units (MGLUs), a novel family of GLUs with an efficient kernel implementation. The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating (MoEG) architecture that learns multiple binary masks, each determining gate or value assignments at the element level on a single shared weight matrix resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly kernel that yields up to a 19.7× inference-time speed-up over a naïve PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs despite added architectural complexity on an RTX5090 GPU. In LLM experiments, the Swish-activated variant SwiMGLU preserves its memory advantages while matching-or even surpassing-the downstream accuracy of the SwiGLU baseline.",
    "source": "https://arxiv.org/pdf/2506.23225",
    "authors": [
      "Yukito Tajima",
      "Nakamasa Inoue",
      "Yusuke Sekikawa",
      "Ikuro Sato",
      "Rio Yokota"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "type": "abstract"
  },
  {
    "id": "2506.23225v1_content",
    "title": "Masked Gated Linear Unit",
    "content": "# Masked Gated Linear Unit\n\nYukito Tajima, Nakamasa Inoue, Yusuke Sekikawa, Ikuro Sato, Rio Yokota\nInstitute of Science Tokyo, Japan; Denso IT Laboratory, Japan\n\n## Abstract\n\nGated Linear Units (GLUs) have become essential components in the feed-forward networks of state-of-the-art Large Language Models (LLMs). However, they require twice as many memory reads compared to feed-forward layers without gating, due to the use of separate weight matrices for the gate and value streams. To address this bottleneck, we introduce Masked Gated Linear Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.\n\n## Key Contributions\n\n### 1. Mixture of Element-wise Gating (MoEG) Architecture\n- Learns multiple binary masks that determine gate or value assignments at the element level\n- Uses a single shared weight matrix instead of separate matrices for gate and value streams\n- Results in reduced memory transfer during inference\n\n### 2. FlashMGLU Kernel Implementation\n- Hardware-friendly CUDA kernel implementation\n- Achieves up to 19.7× inference-time speed-up over naïve PyTorch MGLU implementation\n- 47% more memory-efficient than standard GLUs\n- 34% faster than standard GLUs despite added architectural complexity\n- Tested on RTX5090 GPU with significant performance improvements\n\n### 3. SwiMGLU Variant\n- Swish-activated variant of MGLU\n- Preserves memory advantages while maintaining or exceeding accuracy\n- Matches or surpasses downstream accuracy of SwiGLU baseline in LLM experiments\n\n## Technical Details\n\n### Architecture Comparison\n- **Standard GLU**: Uses separate weight matrices W_g and W_v for gate and value streams\n- **MGLU (Single-mask variant)**: Uses single weight matrix W with learnable binary mask M to decompose into complementary projections\n- **Core mechanism**: Leverages complementary masks M̄_i = 1 - M_i for simultaneous gate and value computation\n\n### Performance Metrics\n- **Memory reduction**: 29.1% reduction in computational cost of projection layers\n- **Memory efficiency**: 37.5% reduction in memory usage during inference\n- **Speed improvement**: 12.51× faster than naïve PyTorch implementation\n- **Hardware optimization**: Custom CUDA and Triton kernels for optimal performance\n\n### Implementation Features\n- **FlashMGLU kernel**: Efficient CUDA implementation with vectorized operations\n- **Multiple mask support**: Architecture supports 1, 2, 4, or 8 masks for different complexity-performance trade-offs\n- **Complementary masking**: Simultaneous computation of gate x(M_i ⊙ W) and value x(M̄_i ⊙ W)\n- **Hardware-friendly design**: Optimized for modern GPU architectures\n\n## Applications and Impact\n\n### Large Language Models\n- Direct replacement for standard GLU layers in transformer architectures\n- Maintains model accuracy while significantly reducing memory footprint\n- Enables deployment on resource-constrained hardware\n- Particularly beneficial for inference-heavy applications\n\n### Efficiency Gains\n- **Inference optimization**: Substantial speed-ups during model inference\n- **Memory bandwidth**: Reduced pressure on memory subsystem\n- **Energy efficiency**: Lower computational requirements translate to reduced energy consumption\n- **Scalability**: Benefits increase with model size and deployment scale\n\n## Experimental Results\n\n### Benchmark Performance\n- Comprehensive evaluation on various LLM architectures\n- Comparison against standard SwiGLU implementations\n- Performance validation across different model sizes\n- Hardware benchmarking on RTX5090 and H100 GPUs\n\n### Key Findings\n- MGLUs maintain competitive accuracy while providing substantial efficiency gains\n- FlashMGLU kernel significantly outperforms both naive implementations and standard GLUs\n- Memory efficiency improvements are consistent across different model configurations\n- Performance benefits scale with the number of masks used in the architecture\n\n## Technical Innovation\n\n### Masked Gating Mechanism\nThe core innovation lies in replacing separate weight matrices with a single shared matrix and learnable binary masks. This approach:\n- Reduces memory bandwidth requirements\n- Enables efficient vectorized operations\n- Maintains the expressiveness of traditional GLU architectures\n- Allows for hardware-optimized implementations\n\n### Kernel Optimization\nFlashMGLU represents a significant advancement in GPU kernel optimization for neural network operations:\n- Custom CUDA implementation with attention to memory access patterns\n- Vectorized operations using half-precision arithmetic\n- Efficient handling of binary mask operations\n- Optimized for both training and inference scenarios\n\nThis work demonstrates how architectural innovations combined with careful hardware optimization can achieve substantial improvements in both efficiency and performance for large language models, making them more accessible and practical for deployment in resource-constrained environments.",
    "source": "https://arxiv.org/pdf/2506.23225",
    "authors": [
      "Yukito Tajima",
      "Nakamasa Inoue",
      "Yusuke Sekikawa",
      "Ikuro Sato",
      "Rio Yokota"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "type": "content"
  }
]